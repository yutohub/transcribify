[
  {
    "start": 170,
    "end": 8842,
    "text": "こんにちは、トランスフォーマーについての私のビデオへようこそ。"
  },
  {
    "start": 8986,
    "end": 18846,
    "text": "以前、トランスフォーマーについて語ったビデオがあったのですが、音質が良くなかったので、視聴者の方のご指摘通り、本当に大成功を収めました。"
  },
  {
    "start": 19028,
    "end": 22030,
    "text": "視聴者からは、音質を改善するよう提案された。"
  },
  {
    "start": 22100,
    "end": 24366,
    "text": "だからこのビデオを撮っているんだ。"
  },
  {
    "start": 24548,
    "end": 30034,
    "text": "基本的に同じことをやるが、いくつか改良を加えるので、前のシリーズを見る必要はない。"
  },
  {
    "start": 30082,
    "end": 37254,
    "text": "このビデオを見て、自分のミスや改善点を補うことができた。"
  },
  {
    "start": 37292,
    "end": 43558,
    "text": "トランスフォーマーモデルをゼロからコーディングする方法については、私の別のビデオをご覧になることをお勧めする。"
  },
  {
    "start": 43654,
    "end": 49274,
    "text": "モデル自体のコーディング方法、データ上でのトレーニング方法、推論方法。"
  },
  {
    "start": 49392,
    "end": 60410,
    "text": "少し長い旅になるが、変圧器の話をする前に、まずリカレント・ニューラル・ネットワークについて話したい。"
  },
  {
    "start": 60490,
    "end": 68494,
    "text": "以前使われていたネットワークでは、シーケンスとシーケンス・ジョブのタスクのほとんどにトランスフォーマーが導入されていた。"
  },
  {
    "start": 68622,
    "end": 70100,
    "text": "復習しておこう。"
  },
  {
    "start": 71190,
    "end": 81474,
    "text": "リカレント・ニューラル・ネットワークは、変圧器よりもずっと以前から存在し、ある入力シーケンスを別の出力シーケンスに対応付けることができた。"
  },
  {
    "start": 81522,
    "end": 86360,
    "text": "この場合、入力はxで、入力シーケンスyが欲しい。"
  },
  {
    "start": 86730,
    "end": 91238,
    "text": "以前やったのは、シーケンスを1つのアイテムに分割することだった。"
  },
  {
    "start": 91334,
    "end": 106080,
    "text": "リカレント・ニューラル・ネットワークに最初の項目を入力として与え、通常はゼロのみで構成される初期状態とともにx 1を与え、リカレント・ニューラル・ネットワークは出力、仮にy 1と呼ぶことにする。"
  },
  {
    "start": 106530,
    "end": 109534,
    "text": "これは最初のタイムステップで起こった。"
  },
  {
    "start": 109732,
    "end": 112638,
    "text": "そして、隠された状態を手に入れた。"
  },
  {
    "start": 112724,
    "end": 119406,
    "text": "これは、次の入力トークンとともに、前の時間ステップのネットワークの隠れた状態と呼ばれる。"
  },
  {
    "start": 119518,
    "end": 126146,
    "text": "ネットワークは2番目の出力トークンy 2を生成しなければならない。"
  },
  {
    "start": 126328,
    "end": 143418,
    "text": "3番目の時間ステップでは、前の時間ステップの隠れ状態と入力状態、つまり3番目の時間ステップの入力トークンを取り込み、ネットワークは次の出力トークンを生成しなければならない。"
  },
  {
    "start": 143584,
    "end": 152670,
    "text": "トークンがn個ある場合、n個のシーケンス入力をn個のシーケンス出力にマッピングするにはn回の時間ステップが必要である。"
  },
  {
    "start": 153330,
    "end": 157566,
    "text": "これは多くの仕事ではうまくいったが、いくつかの問題があった。"
  },
  {
    "start": 157668,
    "end": 158960,
    "text": "おさらいしておこう。"
  },
  {
    "start": 160690,
    "end": 169778,
    "text": "繰り返しのニューラルネットワークの問題点は、まず、長いシーケンスを処理するのに時間がかかることだ。"
  },
  {
    "start": 169944,
    "end": 187074,
    "text": "そのため、シーケンスが長ければ長いほど、この計算も長くなり、長いシーケンスに対してネットワークを訓練するのは容易ではありませんでした。"
  },
  {
    "start": 187202,
    "end": 191110,
    "text": "2つ目の問題は、バニシングと爆発するグラデーションだった。"
  },
  {
    "start": 191270,
    "end": 202878,
    "text": "さて、これらの用語や表現は、インターネットや他のビデオで耳にしたことがあるかもしれないが、現実的なレベルではどのような意味なのか、簡単に説明しよう。"
  },
  {
    "start": 203044,
    "end": 211002,
    "text": "ご存知のように、Pytorchのようなフレームワークは、ネットワークを計算グラフに変換する。"
  },
  {
    "start": 211146,
    "end": 214618,
    "text": "基本的には、計算グラフがあるとする。"
  },
  {
    "start": 214794,
    "end": 216450,
    "text": "これはニューラルネットワークではない。"
  },
  {
    "start": 216950,
    "end": 224862,
    "text": "非常にシンプルで、ニューラルネットワークとは何の関係もないが、我々が抱えている問題を示す計算グラフを作るつもりだ。"
  },
  {
    "start": 225016,
    "end": 230040,
    "text": "つの入力、xともう1つの入力、仮にyとする。"
  },
  {
    "start": 231770,
    "end": 233282,
    "text": "我々の計算グラフ。"
  },
  {
    "start": 233346,
    "end": 235878,
    "text": "まず、この2つの数字を掛け合わせるとしよう。"
  },
  {
    "start": 235964,
    "end": 245260,
    "text": "xにyを掛けたものをfと呼ぶことにする。"
  },
  {
    "start": 245630,
    "end": 254430,
    "text": "掛け算をして、その結果をzと呼ぶことにしよう。"
  },
  {
    "start": 254500,
    "end": 261550,
    "text": "このzの関数gを、仮にzの2乗と等しいとしよう。"
  },
  {
    "start": 262370,
    "end": 270034,
    "text": "例えば、Pytorchがやっていることは、Pytorchは通常、損失関数を計算することだ。"
  },
  {
    "start": 270152,
    "end": 275694,
    "text": "PyTorchは各重みに関して損失関数の導関数を計算します。"
  },
  {
    "start": 275822,
    "end": 279222,
    "text": "この場合、g関数の微分を計算するだけである。"
  },
  {
    "start": 279276,
    "end": 282434,
    "text": "すべての入力に対する出力関数。"
  },
  {
    "start": 282562,
    "end": 301440,
    "text": "つまり、xに関するgの導関数は、fに関するgの導関数にxに関するfの導関数を掛けたものに等しい。"
  },
  {
    "start": 302610,
    "end": 304894,
    "text": "この2つは相殺されるはずだ。"
  },
  {
    "start": 304932,
    "end": 306890,
    "text": "これはチェーンルールと呼ばれる。"
  },
  {
    "start": 307050,
    "end": 311994,
    "text": "さて、おわかりのように、計算の連鎖が長ければ長いほど、だ。"
  },
  {
    "start": 312042,
    "end": 316734,
    "text": "ノードの数が多ければ多いほど、乗算の連鎖は長くなる。"
  },
  {
    "start": 316782,
    "end": 321794,
    "text": "なぜなら、このノードとこのノードの距離は2だからだ。"
  },
  {
    "start": 321832,
    "end": 324740,
    "text": "100人、あるいは1000人いるとしよう。"
  },
  {
    "start": 325770,
    "end": 332102,
    "text": "この数字が0.5で、この数字が0.5だと想像してみよう。"
  },
  {
    "start": 332156,
    "end": 338870,
    "text": "また、掛け算の結果は、最初の2つの数より小さい数になる。"
  },
  {
    "start": 338940,
    "end": 342970,
    "text": "2分の1だからゼロポイント25になる。"
  },
  {
    "start": 343040,
    "end": 345500,
    "text": "2分の1を掛けると4分の1。"
  },
  {
    "start": 346350,
    "end": 353146,
    "text": "1より小さい数が2つあり、それを掛け合わせると、さらに小さい数になる。"
  },
  {
    "start": 353248,
    "end": 360350,
    "text": "1より大きい数字が2つあり、それを掛け合わせると、2つともより大きい数字ができる。"
  },
  {
    "start": 360500,
    "end": 368274,
    "text": "非常に長い計算の連鎖があれば、最終的には非常に大きな数になるか、あるいは非常に小さな数になる。"
  },
  {
    "start": 368472,
    "end": 380070,
    "text": "というのも、GPUのCPUはある精度、たとえば32ビットか64ビットまでの数値しか表現できないからだ。"
  },
  {
    "start": 380220,
    "end": 386758,
    "text": "この数値が小さくなりすぎると、この数値の出力への寄与は非常に小さくなる。"
  },
  {
    "start": 386844,
    "end": 403840,
    "text": "パイトーチや自動計算機（フレームワークとでも言おうか）がウエイトの調整方法を計算するとき、ウエイトは非常にゆっくりと動く。"
  },
  {
    "start": 404450,
    "end": 412960,
    "text": "これは、勾配が消滅していることを意味し、他のケースでは爆発的に大きくなる可能性がある。"
  },
  {
    "start": 413810,
    "end": 414958,
    "text": "これは問題だ。"
  },
  {
    "start": 415044,
    "end": 419506,
    "text": "次の問題は、昔の情報にアクセスすることの難しさだ。"
  },
  {
    "start": 419688,
    "end": 420642,
    "text": "どういう意味ですか？"
  },
  {
    "start": 420696,
    "end": 430214,
    "text": "つまり、前のスライドを思い出していただきたいが、最初の入力トークンは最初の状態とともにリカレント・ニューラル・ネットワークに与えられる。"
  },
  {
    "start": 430412,
    "end": 434386,
    "text": "ここで、リカレント・ニューラル・ネットワークは、計算の長いグラフであると考える必要がある。"
  },
  {
    "start": 434498,
    "end": 436662,
    "text": "それは、新しい隠された状態を作り出す。"
  },
  {
    "start": 436796,
    "end": 442774,
    "text": "次に、新しい隠れ状態を次のトークンと共に使用して、次の出力を生成する。"
  },
  {
    "start": 442902,
    "end": 457098,
    "text": "入力シーケンスが非常に長い場合、最後のトークンは、この長い乗算の連鎖のために、最初のトークンからの寄与がほとんどなくなった隠された状態を持つことになる。"
  },
  {
    "start": 457274,
    "end": 462970,
    "text": "実際、最後のトークンは最初のトークンにあまり依存しない。"
  },
  {
    "start": 463050,
    "end": 477286,
    "text": "というのも、例えば、私たちは人間として、かなり長い文章でも、200語前に見た文脈が現在の文脈にまだ関連していることを知っているからだ。"
  },
  {
    "start": 477468,
    "end": 481830,
    "text": "これはRnNではマッピングできなかったものだ。"
  },
  {
    "start": 482410,
    "end": 485378,
    "text": "だから変圧器があるんだ。"
  },
  {
    "start": 485554,
    "end": 497494,
    "text": "トランスフォーマーは、リカレント・ニューラル・ネットワークを使ってこれらの問題を解決する。"
  },
  {
    "start": 497542,
    "end": 502140,
    "text": "最初のマクロブロックはエンコーダーと呼ばれるもので、この部分だ。"
  },
  {
    "start": 502510,
    "end": 514498,
    "text": "番目のマクロ・ブロックはデコーダーと呼ばれるもので、この2番目の部分、3番目の部分、一番上にあるのがわかると思うが、単なるリニア・レイヤーだ。"
  },
  {
    "start": 514584,
    "end": 518334,
    "text": "機能と2つのレイヤー。"
  },
  {
    "start": 518382,
    "end": 521854,
    "text": "エンコーダーとデコーダーはこの接続でつながっている。"
  },
  {
    "start": 521902,
    "end": 528242,
    "text": "エンコーダーの出力がデコーダーの入力として送られているのがわかるだろう。"
  },
  {
    "start": 528306,
    "end": 530294,
    "text": "また、その方法も見てみよう。"
  },
  {
    "start": 530412,
    "end": 537490,
    "text": "まず最初に、説明の際に使ういくつかの表記から始めよう。"
  },
  {
    "start": 537650,
    "end": 542198,
    "text": "数学の復習も兼ねて、この表記に慣れておくといいだろう。"
  },
  {
    "start": 542294,
    "end": 546550,
    "text": "行列の乗算についてまず知っておく必要がある。"
  },
  {
    "start": 546630,
    "end": 552758,
    "text": "入力行列があるとしよう。"
  },
  {
    "start": 552944,
    "end": 557502,
    "text": "シーケンス・バイ・Dモデル、なぜシーケンス・バイ・Dモデルと呼ばれるのか、その理由を見てみよう。"
  },
  {
    "start": 557556,
    "end": 565620,
    "text": "各行が単語である6×512の行列があるとする。"
  },
  {
    "start": 567030,
    "end": 571618,
    "text": "この言葉は文字でできているのではなく、512個の数字でできている。"
  },
  {
    "start": 571704,
    "end": 576898,
    "text": "各単語は512の数字で表される。"
  },
  {
    "start": 576984,
    "end": 583474,
    "text": "この列に512本、この列に512本、といった具合だ。"
  },
  {
    "start": 583602,
    "end": 585014,
    "text": "12345."
  },
  {
    "start": 585052,
    "end": 586838,
    "text": "ここにもう一人必要だ。"
  },
  {
    "start": 586924,
    "end": 594380,
    "text": "よし、最初の単語をa、2番目の単語をb、C-D-Eとfと呼ぼう。"
  },
  {
    "start": 594830,
    "end": 601338,
    "text": "この行列に別の行列、例えばこの行列の転置行列を掛けるとしよう。"
  },
  {
    "start": 601434,
    "end": 605310,
    "text": "これは行が列になる行列である。"
  },
  {
    "start": 605890,
    "end": 621666,
    "text": "3、4、5、6、この単語はここ、BCDE、fになる。"
  },
  {
    "start": 621848,
    "end": 635106,
    "text": "となると、各列に512の数字が並ぶことになる。以前は行に数字が並んでいたが、これからは列に並ぶことになるからだ。"
  },
  {
    "start": 635138,
    "end": 639450,
    "text": "ここには512番がある。"
  },
  {
    "start": 639950,
    "end": 644874,
    "text": "これは512×6のマトリックスである。"
  },
  {
    "start": 644992,
    "end": 647082,
    "text": "ここに括弧を付け加えさせてもらおう。"
  },
  {
    "start": 647216,
    "end": 650150,
    "text": "掛け算をすれば、新しい行列ができる。"
  },
  {
    "start": 650230,
    "end": 655594,
    "text": "つまり、内側の次元をキャンセルして、外側の次元に到達するのだ。"
  },
  {
    "start": 655642,
    "end": 657774,
    "text": "が6×6になる。"
  },
  {
    "start": 657972,
    "end": 660570,
    "text": "列×6列になる。"
  },
  {
    "start": 660650,
    "end": 662000,
    "text": "描こう。"
  },
  {
    "start": 662450,
    "end": 665550,
    "text": "この出力マトリックスの値はどのように計算するのか？"
  },
  {
    "start": 665630,
    "end": 667300,
    "text": "これは6対6だ。"
  },
  {
    "start": 668470,
    "end": 673614,
    "text": "これは最初の行と最初の列の内積である。"
  },
  {
    "start": 673662,
    "end": 676440,
    "text": "これはaにaを掛けたものである。"
  },
  {
    "start": 677130,
    "end": 680946,
    "text": "2番目の値は、2列目の最初の行である。"
  },
  {
    "start": 681058,
    "end": 688194,
    "text": "3つ目の値は、3列目から最後の列までの最初の行である。"
  },
  {
    "start": 688242,
    "end": 690886,
    "text": "aにfを掛けたもの、など。"
  },
  {
    "start": 690998,
    "end": 697222,
    "text": "ドット積とは何かというと、基本的には最初の行の最初の数字を取る。"
  },
  {
    "start": 697286,
    "end": 699434,
    "text": "ここには512の数字がある。"
  },
  {
    "start": 699552,
    "end": 701306,
    "text": "ここには512の数字がある。"
  },
  {
    "start": 701408,
    "end": 706426,
    "text": "の場合、最初の行の最初の数字と最初の列の最初の数字を取る。"
  },
  {
    "start": 706458,
    "end": 714354,
    "text": "最初の行の2番目の値、最初の列の2番目の値、これらを掛け合わせるのだ。"
  },
  {
    "start": 714552,
    "end": 717618,
    "text": "そして、これらの数字をすべて足す。"
  },
  {
    "start": 717704,
    "end": 731782,
    "text": "例えば、この数字にこれを掛けたものと、この数字にこれを掛けたものと、この数字にこれを掛けたものと、この数字にこれを掛けたものと、この数字にこれを掛けたものと、この数字を全部足したものになる。"
  },
  {
    "start": 731916,
    "end": 735110,
    "text": "これはドット積aである。"
  },
  {
    "start": 735260,
    "end": 740678,
    "text": "次のスライドではこの表記を多用するので、私たちはこの表記に慣れておく必要がある。"
  },
  {
    "start": 740774,
    "end": 746250,
    "text": "エンコーダーを見ることからトランスフォーマーの旅を始めよう。"
  },
  {
    "start": 746590,
    "end": 750950,
    "text": "エンコーダは入力埋め込みから始める。"
  },
  {
    "start": 751030,
    "end": 752910,
    "text": "入力エンベッディングとは何か？"
  },
  {
    "start": 753490,
    "end": 756122,
    "text": "まずは文章から。"
  },
  {
    "start": 756266,
    "end": 760142,
    "text": "この場合、6単語の文章がある。"
  },
  {
    "start": 760276,
    "end": 762174,
    "text": "それをトークン化するんだ。"
  },
  {
    "start": 762212,
    "end": 764618,
    "text": "文をトークンに変換する。"
  },
  {
    "start": 764714,
    "end": 765982,
    "text": "トークン化とはどういう意味ですか？"
  },
  {
    "start": 766046,
    "end": 768420,
    "text": "私たちはそれらを一つの単語に分割した。"
  },
  {
    "start": 769030,
    "end": 774274,
    "text": "常に1つの単語で文章を分割する必要はない。"
  },
  {
    "start": 774392,
    "end": 780534,
    "text": "文章を1単語よりもさらに小さな部分に分割することもできる。"
  },
  {
    "start": 780572,
    "end": 790026,
    "text": "各単語を複数の単語に分割することで、この文章をたとえば20個のトークンに分割することもできる。"
  },
  {
    "start": 790208,
    "end": 798214,
    "text": "これは、最近のトランス・モデルの多くでは通常行われていることだが、ここではそれ以外は行わない。"
  },
  {
    "start": 798262,
    "end": 800070,
    "text": "イメージするのは本当に難しい。"
  },
  {
    "start": 800230,
    "end": 807534,
    "text": "この入力文があり、トークンに分割され、各トークンが1つの単語であるとしよう。"
  },
  {
    "start": 807732,
    "end": 818110,
    "text": "次のステップは、これらの単語を数字にマッピングし、その数字が語彙における単語の位置を表す。"
  },
  {
    "start": 818190,
    "end": 823940,
    "text": "学習セットに現れる可能性のある単語をすべて集めた語彙があるとする。"
  },
  {
    "start": 824310,
    "end": 827598,
    "text": "それぞれの単語は、この語彙の中で位置を占める。"
  },
  {
    "start": 827694,
    "end": 831190,
    "text": "つまり、たとえばvoyeurという単語は105の位置を占めることになる。"
  },
  {
    "start": 831260,
    "end": 836150,
    "text": "catという単語は6500の位置を占めることになる。"
  },
  {
    "start": 836490,
    "end": 843530,
    "text": "見ての通り、この猫はこの猫と同じ数字を持っている。"
  },
  {
    "start": 843950,
    "end": 852000,
    "text": "これらの数字を入力IDと呼び、512サイズのベクトルにマッピングする。"
  },
  {
    "start": 852530,
    "end": 861790,
    "text": "このベクトルは512個の数字からなるベクトルで、常に同じ単語を常に同じ埋め込みに対応させる。"
  },
  {
    "start": 862290,
    "end": 866014,
    "text": "しかし、この数字は固定されていない。"
  },
  {
    "start": 866142,
    "end": 868210,
    "text": "これは我々のモデルのパラメーターだ。"
  },
  {
    "start": 868280,
    "end": 875186,
    "text": "私たちのモデルは、言葉の意味を表すように数字を変化させることを学ぶ。"
  },
  {
    "start": 875288,
    "end": 883014,
    "text": "語彙は固定されているため、入力IDが変化することはないが、埋め込みはモデルの学習過程とともに変化する。"
  },
  {
    "start": 883132,
    "end": 888486,
    "text": "埋め込み数は、損失関数の必要性に応じて変化する。"
  },
  {
    "start": 888588,
    "end": 894106,
    "text": "入力埋め込みは、基本的に512サイズの埋め込みに単語をマッピングする。"
  },
  {
    "start": 894128,
    "end": 901534,
    "text": "この量を512 Dモデルと呼ぶのは、論文でも使われているのと同じ名前だからだ。"
  },
  {
    "start": 901652,
    "end": 903280,
    "text": "注意を払うだけでいい。"
  },
  {
    "start": 905010,
    "end": 909310,
    "text": "エンコーダーの次のレイヤー、位置エンコードを見てみよう。"
  },
  {
    "start": 910450,
    "end": 912590,
    "text": "ポジションエンコーディングとは何か？"
  },
  {
    "start": 913430,
    "end": 932802,
    "text": "私たちが望むのは、各単語が文中での位置について何らかの情報を持っていることです。今、埋め込みである単語の行列を作りましたが、それらは特定の単語が文中のどこにあるかという情報を伝えていません。"
  },
  {
    "start": 932866,
    "end": 935394,
    "text": "これはポジションエンコーディングの仕事である。"
  },
  {
    "start": 935522,
    "end": 944678,
    "text": "つまり、近い単語は近い単語として扱い、遠い単語は遠い単語として扱うのだ。"
  },
  {
    "start": 944774,
    "end": 950590,
    "text": "私たちが目で見ている特別な情報を、モデルにも見てもらいたい。"
  },
  {
    "start": 950660,
    "end": 954282,
    "text": "では、例えばこの文章を見たとき、位置エンコーディングとは何か？"
  },
  {
    "start": 954346,
    "end": 967346,
    "text": "私たちは、目から与えられる部分的な情報を持っているため、その単語がエンコードと比較され、より遠いものであることを知っている。"
  },
  {
    "start": 967368,
    "end": 974850,
    "text": "文の中で単語が部分的にどのように分布しているかという情報をモデルに与える必要がある。"
  },
  {
    "start": 975370,
    "end": 981382,
    "text": "位置エンコーディングは、モデルが学習できるパターンを表したい。"
  },
  {
    "start": 981516,
    "end": 983080,
    "text": "どうなるか見てみよう。"
  },
  {
    "start": 984810,
    "end": 987042,
    "text": "元の文章があるとしよう。"
  },
  {
    "start": 987106,
    "end": 988694,
    "text": "あなたの猫はかわいい猫だ。"
  },
  {
    "start": 988812,
    "end": 993734,
    "text": "まず、前のレイヤーを使って埋め込みに変換する。"
  },
  {
    "start": 993782,
    "end": 998266,
    "text": "入力埋め込みは512サイズの埋め込みである。"
  },
  {
    "start": 998448,
    "end": 1005178,
    "text": "次に、位置エンコーディングベクトルと呼ばれる特殊なベクトルを作成し、これらの埋め込みに追加する。"
  },
  {
    "start": 1005274,
    "end": 1013134,
    "text": "この赤で示したベクトルはサイズ512のベクトルであり、これは学習されない。"
  },
  {
    "start": 1013182,
    "end": 1017106,
    "text": "それは一度計算されたもので、トレーニングの過程で身につくものではない。"
  },
  {
    "start": 1017208,
    "end": 1018382,
    "text": "直った。"
  },
  {
    "start": 1018526,
    "end": 1023970,
    "text": "このベクトルは文中の単語の位置を表す。"
  },
  {
    "start": 1024390,
    "end": 1029910,
    "text": "これでサイズのベクトルが出力されるはずだ。"
  },
  {
    "start": 1029980,
    "end": 1037542,
    "text": "この数字とこの数字、この数字とこの数字を合計しているのだから。"
  },
  {
    "start": 1037596,
    "end": 1040922,
    "text": "第一の次元と第一の次元、第二の次元と第二の次元。"
  },
  {
    "start": 1040976,
    "end": 1046166,
    "text": "入力ベクトルと同じ大きさの新しいベクトルが得られる。"
  },
  {
    "start": 1046278,
    "end": 1048982,
    "text": "この埋め込み位置はどのように計算されるのですか？"
  },
  {
    "start": 1049046,
    "end": 1049900,
    "text": "見てみよう。"
  },
  {
    "start": 1050770,
    "end": 1052602,
    "text": "刑期が短くなったとしよう。"
  },
  {
    "start": 1052666,
    "end": 1058880,
    "text": "あなたの愛猫がそうだとしよう。新聞から次のような表現を見たことがあるかもしれない。"
  },
  {
    "start": 1059250,
    "end": 1075954,
    "text": "そして、このベクトルの各位置について、これらの引数を使った2つの式を使って値を計算する。"
  },
  {
    "start": 1076002,
    "end": 1080994,
    "text": "第1引数は文中の単語の位置を示す。"
  },
  {
    "start": 1081042,
    "end": 1093606,
    "text": "偶数次元の場合は、0、2、4、510、など。"
  },
  {
    "start": 1093638,
    "end": 1105690,
    "text": "最初の式、つまり符号を使い、このベクトルのodの位置には2番目の式を使う。"
  },
  {
    "start": 1105770,
    "end": 1113650,
    "text": "このエンベッディングは、最初のワードエンベッディングゼロであるため、10のpeで計算される。"
  },
  {
    "start": 1113800,
    "end": 1128902,
    "text": "この1はポーズを表し、このゼロは2を表す。"
  },
  {
    "start": 1128956,
    "end": 1142886,
    "text": "の位置でコサインを使い、2つのIは2と等しくなり、I＋1は1と等しくなる。"
  },
  {
    "start": 1143078,
    "end": 1148250,
    "text": "別の文があったとしても、位置エンコーディングが異なることはない。"
  },
  {
    "start": 1148590,
    "end": 1162606,
    "text": "位置エンコーディングは一度だけ計算され、推論中や学習中にモデルが目にするすべての文に再利用されるため、異なる文であっても同じベクトルを持つことになる。"
  },
  {
    "start": 1162788,
    "end": 1165970,
    "text": "位置エンコーディングは一度しか計算しない。"
  },
  {
    "start": 1166120,
    "end": 1169186,
    "text": "モデルを作成したら、それを保存して再利用する。"
  },
  {
    "start": 1169208,
    "end": 1174420,
    "text": "文章をモデルに入力するたびに計算する必要はない。"
  },
  {
    "start": 1176250,
    "end": 1182338,
    "text": "なぜ位置エンコードを表すのに、コサイン関数とサイン関数を選んだのか？"
  },
  {
    "start": 1182434,
    "end": 1185510,
    "text": "この2つの関数のプロットを見てみよう。"
  },
  {
    "start": 1186570,
    "end": 1194710,
    "text": "プロットは位置によって、つまり文中の単語の位置によって、この深さがベクトルに沿った次元であることがわかる。"
  },
  {
    "start": 1194790,
    "end": 1198570,
    "text": "前の表現で見た2つのI。"
  },
  {
    "start": 1199150,
    "end": 1207034,
    "text": "それをプロットすれば、人間としてパターンを見ることができる。"
  },
  {
    "start": 1207082,
    "end": 1210910,
    "text": "さて、エンコーダーの次のレイヤーはマルチヘッドアテンションだ。"
  },
  {
    "start": 1212290,
    "end": 1216830,
    "text": "私たちはマルチヘッドの注意の中には入らない。"
  },
  {
    "start": 1216910,
    "end": 1217250,
    "text": "まず最初に。"
  },
  {
    "start": 1217320,
    "end": 1219902,
    "text": "まず、シングルヘッドを視覚化する。"
  },
  {
    "start": 1219966,
    "end": 1224354,
    "text": "頭1つで自己に注意を払い、それを実行しよう。"
  },
  {
    "start": 1224552,
    "end": 1226242,
    "text": "自己注意とは何か？"
  },
  {
    "start": 1226386,
    "end": 1231762,
    "text": "セルフ・アテンションは、トランスフォーマーを導入する以前からあったメカニズムだ。"
  },
  {
    "start": 1231906,
    "end": 1237278,
    "text": "変圧器の改造でマルチヘッドになっただけだ。"
  },
  {
    "start": 1237474,
    "end": 1240140,
    "text": "自己注意はどのように機能したのか？"
  },
  {
    "start": 1240910,
    "end": 1245994,
    "text": "自己注意を払うことで、モデルは単語を互いに関連付けることができる。"
  },
  {
    "start": 1246192,
    "end": 1251626,
    "text": "さて、単語の意味を捉える入力埋め込みができた。"
  },
  {
    "start": 1251808,
    "end": 1259114,
    "text": "次に、文中の単語の位置に関する情報を与える位置エンコーディングがある。"
  },
  {
    "start": 1259242,
    "end": 1263620,
    "text": "今、私たちはこの自己注意を、単語と単語を関連づけるために求めている。"
  },
  {
    "start": 1263990,
    "end": 1278262,
    "text": "ここで、サイズ512のDモデルを持つ6単語の入力シーケンスがあるとする。"
  },
  {
    "start": 1278316,
    "end": 1284946,
    "text": "のqkとvは同じ行列であり、入力を表す同じ行列である。"
  },
  {
    "start": 1285058,
    "end": 1289882,
    "text": "512次元の6単語の入力。"
  },
  {
    "start": 1289936,
    "end": 1294426,
    "text": "各単語はサイズ512のベクトルで表される。"
  },
  {
    "start": 1294608,
    "end": 1301614,
    "text": "私たちは基本的に、論文で見たこの公式を適用して、注意力（この場合は自己注意力）を計算する。"
  },
  {
    "start": 1301652,
    "end": 1302602,
    "text": "なぜセルフ・アテンションなのか？"
  },
  {
    "start": 1302666,
    "end": 1308634,
    "text": "なぜなら、文中のそれぞれの単語が、同じ文中の他の単語と関連しているからだ。"
  },
  {
    "start": 1308682,
    "end": 1310670,
    "text": "それは自己注目である。"
  },
  {
    "start": 1311730,
    "end": 1316254,
    "text": "入力文であるq行列から始める。"
  },
  {
    "start": 1316302,
    "end": 1317266,
    "text": "イメージしてみよう。"
  },
  {
    "start": 1317288,
    "end": 1323326,
    "text": "例えば、6つの行があり、列には512列がある。"
  },
  {
    "start": 1323358,
    "end": 1330200,
    "text": "さて、これらを描くのは実に難しいが、例えば512の列があり、ここに6つあるとしよう。"
  },
  {
    "start": 1330570,
    "end": 1337618,
    "text": "では、この公式に従って、同じ文章を移し替えて掛ける。"
  },
  {
    "start": 1337714,
    "end": 1355162,
    "text": "を転置したもので、これもまた同じ入力シーケンスである。これを512の平方根で割り、最初の行列表記で見たように、その出力であるソフトマックスを適用する。"
  },
  {
    "start": 1355306,
    "end": 1365890,
    "text": "6×512の行列と512×6の別の行列を掛け合わせると、6×6の新しい行列が得られることを見た。"
  },
  {
    "start": 1366040,
    "end": 1372654,
    "text": "この行列の各値は、最初の行と最初の列の内積を表す。"
  },
  {
    "start": 1372782,
    "end": 1377750,
    "text": "これは、最初の行と2番目の列の内積を表している。"
  },
  {
    "start": 1378490,
    "end": 1382898,
    "text": "ここでの数値は実際にはランダムに生成されるので、数値に集中してはいけない。"
  },
  {
    "start": 1382994,
    "end": 1390010,
    "text": "注目すべきは、ソフトマックスはこれらの値をすべて合計して1になるようにしていることだ。"
  },
  {
    "start": 1390080,
    "end": 1394282,
    "text": "例えば、この行の合計は1になる。"
  },
  {
    "start": 1394416,
    "end": 1397990,
    "text": "この他の列も合計すると1、エトセトラ、エトセトラ。"
  },
  {
    "start": 1398070,
    "end": 1406400,
    "text": "この値は、最初の単語と単語自体の埋め込みとの内積である。"
  },
  {
    "start": 1406770,
    "end": 1414914,
    "text": "この値は、yourという単語の埋め込みとcatという単語の埋め込みとの内積である。"
  },
  {
    "start": 1415112,
    "end": 1422740,
    "text": "この値は、yourという単語の埋め込みとisという単語の埋め込みとの内積である。"
  },
  {
    "start": 1424230,
    "end": 1432662,
    "text": "この値は、ある単語と別の単語との関係がどれだけ強いかをスコアで表す。"
  },
  {
    "start": 1432796,
    "end": 1435106,
    "text": "では、計算式を見てみよう。"
  },
  {
    "start": 1435138,
    "end": 1441894,
    "text": "今のところ、qをkで割ってdkの平方根を掛けたものをソフトマックスに適用しているだけである。"
  },
  {
    "start": 1441942,
    "end": 1443980,
    "text": "Vを掛けていない。"
  },
  {
    "start": 1444510,
    "end": 1446058,
    "text": "前へ進もう。"
  },
  {
    "start": 1446224,
    "end": 1452602,
    "text": "この行列にvを掛けると、6×512の新しい行列が得られる。"
  },
  {
    "start": 1452656,
    "end": 1460638,
    "text": "6×6の行列と6×512の行列を掛け合わせると、6×512の新しい行列ができる。"
  },
  {
    "start": 1460644,
    "end": 1469300,
    "text": "ここで注目していただきたいのは、この行列の次元が、まさにわれわれの出発点となった初期行列の次元であるということである。"
  },
  {
    "start": 1470390,
    "end": 1475134,
    "text": "6行の新しい行列が得られるとはどういうことか。"
  },
  {
    "start": 1475182,
    "end": 1483622,
    "text": "仮に6行512列としよう。"
  },
  {
    "start": 1483676,
    "end": 1487846,
    "text": "には6つの単語があり、各単語は512次元の埋め込みを持つ。"
  },
  {
    "start": 1487948,
    "end": 1504098,
    "text": "さて、このエンベッディングは、入力エンベッディングによって与えられた単語の意味、位置エンコーディングによって加えられた単語の位置だけでなく、何らかの形でこの特別なエンベッディングを表している。"
  },
  {
    "start": 1504134,
    "end": 1513714,
    "text": "これらの値は、この特定の単語と他のすべての単語との関係も捉える特別な埋め込みを表す。"
  },
  {
    "start": 1513912,
    "end": 1526900,
    "text": "この単語をここに埋め込むことで、その単語の意味や文中の位置だけでなく、この単語と他のすべての単語との関係も捉えることができる。"
  },
  {
    "start": 1527270,
    "end": 1530242,
    "text": "これはマルチヘッドへの注目ではないことを忘れないでほしい。"
  },
  {
    "start": 1530306,
    "end": 1532402,
    "text": "私たちはただ、自己注目を見ているだけだ。"
  },
  {
    "start": 1532466,
    "end": 1533800,
    "text": "頭1つ。"
  },
  {
    "start": 1534810,
    "end": 1538410,
    "text": "これがどのようにマルチヘッドアテンションになるかは後述する。"
  },
  {
    "start": 1540990,
    "end": 1544646,
    "text": "セルフ・アテンションには、非常に望ましい特性がある。"
  },
  {
    "start": 1544838,
    "end": 1547654,
    "text": "まず、順列不変である。"
  },
  {
    "start": 1547702,
    "end": 1549926,
    "text": "順列不変とはどういう意味か？"
  },
  {
    "start": 1549958,
    "end": 1557182,
    "text": "つまり、ある行列があったとして、最初に6つの単語からなる行列があったとしよう。"
  },
  {
    "start": 1557236,
    "end": 1562160,
    "text": "この場合、A-B-Cとdの4単語だけとする。"
  },
  {
    "start": 1562530,
    "end": 1576274,
    "text": "先ほどの式を適用することで、aという単語の新しい特殊な埋め込み、bという単語の新しい特殊な埋め込み、cとdという単語の新しい特殊な埋め込みが存在する、この特殊な行列が生成されるとする。"
  },
  {
    "start": 1576312,
    "end": 1578978,
    "text": "aプライム、bプライム、cプライム、dプライムと呼ぼう。"
  },
  {
    "start": 1579074,
    "end": 1584422,
    "text": "この2つの行の位置を変えても、値は変わらない。"
  },
  {
    "start": 1584476,
    "end": 1586802,
    "text": "それに応じて出力の位置も変わる。"
  },
  {
    "start": 1586866,
    "end": 1598278,
    "text": "b素数の値は変化せず、位置が変わるだけで、c素数も位置が変わるだけで、各ベクトルの値は変化しない。"
  },
  {
    "start": 1598454,
    "end": 1601898,
    "text": "今のところ、セルフ・アテンションにパラメーターは必要ない。"
  },
  {
    "start": 1601994,
    "end": 1605678,
    "text": "つまり、モデルによって学習されるパラメーターは導入していない。"
  },
  {
    "start": 1605764,
    "end": 1609198,
    "text": "私はただ、冒頭の一文を取っただけだ。"
  },
  {
    "start": 1609284,
    "end": 1611134,
    "text": "この場合は6つの単語だ。"
  },
  {
    "start": 1611332,
    "end": 1613150,
    "text": "私たちはそれを掛け合わせた。"
  },
  {
    "start": 1613300,
    "end": 1617266,
    "text": "それを512の平方根である一定量で割る。"
  },
  {
    "start": 1617288,
    "end": 1622414,
    "text": "その場合、パラメータを導入しないソフトマックスを適用する。"
  },
  {
    "start": 1622462,
    "end": 1629220,
    "text": "今のところ、自己アテンションは言葉の埋め込み以外のパラメーターを必要としない。"
  },
  {
    "start": 1629770,
    "end": 1633110,
    "text": "これは後にマルチヘッドアテンションを導入する際に変わる。"
  },
  {
    "start": 1634490,
    "end": 1646074,
    "text": "また、ソフトマックス行列の自己注目の各値は、それ自身と他の単語との単語埋め込みの内積であるため、我々は期待している。"
  },
  {
    "start": 1646192,
    "end": 1654160,
    "text": "対角線に沿った値が最大になると予想されるが、これは各単語とそれ自身との内積だからである。"
  },
  {
    "start": 1655090,
    "end": 1659194,
    "text": "このマトリックスには、もうひとつの性質がある。"
  },
  {
    "start": 1659242,
    "end": 1675614,
    "text": "つまり、ソフトマックスを適用する前に、この行列の値を入れ替えるとすると、yourという単語とcatは相互作用させたくないし、let's say isという単語とlovelyは相互作用させたくない。"
  },
  {
    "start": 1675752,
    "end": 1685190,
    "text": "ソフトマックスを適用する前に、この値をマイナス無限大に、またこの値をマイナス無限大に置き換えることができる。"
  },
  {
    "start": 1686570,
    "end": 1691640,
    "text": "を適用すると、ソフトマックスはマイナス無限大をゼロに置き換える。"
  },
  {
    "start": 1692350,
    "end": 1696010,
    "text": "覚えているように、ソフトマックスはeのx乗だからだ。"
  },
  {
    "start": 1696080,
    "end": 1702270,
    "text": "xがマイナス無限大になる場合、eのマイナス無限大乗は非常にゼロに近くなる。"
  },
  {
    "start": 1702340,
    "end": 1704240,
    "text": "基本的にはゼロだ。"
  },
  {
    "start": 1705330,
    "end": 1710570,
    "text": "これは、変圧器のデコーダーで使用する望ましい特性である。"
  },
  {
    "start": 1710650,
    "end": 1713962,
    "text": "では、マルチヘッドアテンションとは何かを見てみよう。"
  },
  {
    "start": 1714026,
    "end": 1720078,
    "text": "今見たのはセルフ・アテンションズであり、それをマルチヘッド・アテンションズに変換したいのだ。"
  },
  {
    "start": 1720174,
    "end": 1725410,
    "text": "これらの表現は新聞で見たことがあるかもしれないが、一つずつ説明するので心配しないでほしい。"
  },
  {
    "start": 1725480,
    "end": 1726660,
    "text": "行こう"
  },
  {
    "start": 1727030,
    "end": 1728866,
    "text": "エンコーダーがあるとしよう。"
  },
  {
    "start": 1728898,
    "end": 1737590,
    "text": "エンコーダー側のトランスフォーマーには、入力文がある。"
  },
  {
    "start": 1737660,
    "end": 1742922,
    "text": "6ワード×512は、各ワードの埋め込みサイズである。"
  },
  {
    "start": 1743056,
    "end": 1745898,
    "text": "この場合、私はDモデルによるシークエンスと呼んでいる。"
  },
  {
    "start": 1745984,
    "end": 1752154,
    "text": "シーケンスとは、スライド左下の凡例にあるように、シーケンスの長さである。"
  },
  {
    "start": 1752282,
    "end": 1757358,
    "text": "Dモデルは埋め込みベクトルのサイズであり、512である。"
  },
  {
    "start": 1757524,
    "end": 1764850,
    "text": "私たちがすることは、写真にあるように、この入力を4つのコピーにすることです。"
  },
  {
    "start": 1764920,
    "end": 1777506,
    "text": "そして、3つの名前がマルチヘッドに送られる。"
  },
  {
    "start": 1777538,
    "end": 1782850,
    "text": "同じ入力が、入力に等しい3つの行列になるのだ。"
  },
  {
    "start": 1782930,
    "end": 1786694,
    "text": "ひとつはクエリ、ひとつはキー、ひとつは値と呼ばれる。"
  },
  {
    "start": 1786892,
    "end": 1790258,
    "text": "基本的には、このインプットを3部コピーする。"
  },
  {
    "start": 1790284,
    "end": 1792362,
    "text": "q、k、vと呼ぶ。"
  },
  {
    "start": 1792496,
    "end": 1794570,
    "text": "もちろん、同じ次元のものだ。"
  },
  {
    "start": 1794990,
    "end": 1796922,
    "text": "マルチヘッドアテンションは何をするのか？"
  },
  {
    "start": 1796976,
    "end": 1805790,
    "text": "まず、これらの3つの行列に、wqwkとwvと呼ばれる3つのパラメータ行列を掛け合わせる。"
  },
  {
    "start": 1806850,
    "end": 1810270,
    "text": "これらの行列はDモデル×Dモデルの次元を持つ。"
  },
  {
    "start": 1810420,
    "end": 1821250,
    "text": "Dモデルで配列された行列とDモデルで配列された行列を掛け合わせると、Dモデルで配列された行列が出力される。"
  },
  {
    "start": 1821320,
    "end": 1830066,
    "text": "基本的に開始行列と同じ次元であり、これらをq prime、k prime、v primeと呼ぶことにする。"
  },
  {
    "start": 1830258,
    "end": 1835282,
    "text": "次のステップは、これらの行列をより小さな行列に分割することだ。"
  },
  {
    "start": 1835346,
    "end": 1843850,
    "text": "この行列qの素数をシーケンス次元またはDモデル次元で分割する方法を見てみよう。"
  },
  {
    "start": 1844430,
    "end": 1848742,
    "text": "マルチヘッドアテンションでは、常にDモデルの次元で分割する。"
  },
  {
    "start": 1848806,
    "end": 1856640,
    "text": "各ヘッドは全文を見るが、各単語の埋め込みのごく一部を見ることになる。"
  },
  {
    "start": 1857010,
    "end": 1860558,
    "text": "仮に512のエンベッディングがあるとする。"
  },
  {
    "start": 1860724,
    "end": 1868798,
    "text": "これは512を4で割ったより小さな埋め込みになり、この量をdkと呼ぶ。"
  },
  {
    "start": 1868894,
    "end": 1873422,
    "text": "dkはDモデルをhで割ったもので、hはヘッド数。"
  },
  {
    "start": 1873486,
    "end": 1876200,
    "text": "この場合、hは4に等しい。"
  },
  {
    "start": 1877290,
    "end": 1880834,
    "text": "これらの小さな行列間の注目度を計算することができる。"
  },
  {
    "start": 1880882,
    "end": 1887240,
    "text": "論文から引用した式を用いて、q 1、k 1、v 1。"
  },
  {
    "start": 1887870,
    "end": 1895900,
    "text": "この結果、ヘッド1、ヘッド2、ヘッド3、ヘッド4と呼ばれる小さな行列ができる。"
  },
  {
    "start": 1896270,
    "end": 1902190,
    "text": "ヘッド1からヘッド4までの寸法はdvで表される。"
  },
  {
    "start": 1902930,
    "end": 1906570,
    "text": "Dvとは何かというと、基本的にはdkに等しい。"
  },
  {
    "start": 1906650,
    "end": 1912766,
    "text": "最後の掛け算がvで行われるからDvと呼ばれているだけで、論文ではDvと呼ばれている。"
  },
  {
    "start": 1912798,
    "end": 1915170,
    "text": "私も同じ名前にこだわっている。"
  },
  {
    "start": 1915830,
    "end": 1929570,
    "text": "次のステップは、論文にあるように、これらの行列をDv次元に沿って連結することだ。"
  },
  {
    "start": 1929640,
    "end": 1940854,
    "text": "この頭をすべて連結すると、hにdvを掛けた新しい行列ができる。"
  },
  {
    "start": 1940982,
    "end": 1943302,
    "text": "周知のように、dvはdkに等しい。"
  },
  {
    "start": 1943366,
    "end": 1946730,
    "text": "hにdvを掛けたものがDモデルとなる。"
  },
  {
    "start": 1946800,
    "end": 1953360,
    "text": "最初の形状が戻ってくるので、ここではDモデルで配列されている。"
  },
  {
    "start": 1954770,
    "end": 1959200,
    "text": "次のステップは、この連結の結果にWoを掛けることである。"
  },
  {
    "start": 1960130,
    "end": 1964394,
    "text": "woはhにdvを掛けた行列である。"
  },
  {
    "start": 1964442,
    "end": 1968562,
    "text": "Dモデルにもう1つの次元であるDモデルを掛け合わせたもの。"
  },
  {
    "start": 1968696,
    "end": 1975720,
    "text": "この結果は、Dモデルによって配列されたマルチヘッド注目の結果である新しいマトリックスである。"
  },
  {
    "start": 1976730,
    "end": 1995478,
    "text": "マルチヘッド注目度では、ここでqプライム、kプライム、vプライムといった行列間の注目度を計算する代わりに、Dモデルの次元に沿ってこれらの行列をより小さな行列に分割し、これらのモル行列間の注目度を計算する。"
  },
  {
    "start": 1995574,
    "end": 2003278,
    "text": "各ヘッドは全文を見ているが、各単語の埋め込みの様相は異なっている。"
  },
  {
    "start": 2003444,
    "end": 2010254,
    "text": "なぜこのようなことをするかというと、それぞれの頭で同じ言葉の異なる側面を観てもらいたいからだ。"
  },
  {
    "start": 2010372,
    "end": 2022530,
    "text": "例えば、中国語でも他の言語でも、一つの単語が名詞になる場合もあれば、動詞になる場合もあれば、副詞になる場合もある。"
  },
  {
    "start": 2023110,
    "end": 2038598,
    "text": "私たちが望むのは、ある頭がその単語を名詞として関連付けることを学び、別の頭がその単語を動詞として関連付けることを学び、さらに別の頭がその動詞を目的語や副詞として関連付けることを学ぶことだ。"
  },
  {
    "start": 2038774,
    "end": 2042410,
    "text": "だからこそ、マルチヘッドに注目してほしいのだ。"
  },
  {
    "start": 2042910,
    "end": 2051518,
    "text": "さて、アテンションは視覚化できるということをネットで見たことがあるかもしれないが、その方法を紹介しよう。"
  },
  {
    "start": 2051684,
    "end": 2056570,
    "text": "q行列とk行列の注目度を計算する。"
  },
  {
    "start": 2056650,
    "end": 2076440,
    "text": "この演算を行うと、qのsoft maxにkを掛け合わせたものをdkの平方根で割ったものが、前に見たようにシーケンスごとの新しい行列となり、これが2つの単語の関係の強さを表すスコアを表す。"
  },
  {
    "start": 2076970,
    "end": 2088294,
    "text": "これは論文から引用したもので、すべてのヘッドがどのように機能するかを示している。"
  },
  {
    "start": 2088332,
    "end": 2096346,
    "text": "例えば、この仕事に集中し、この言葉をここで作ると、作るということが難しいという言葉と関係していることがわかる。"
  },
  {
    "start": 2096448,
    "end": 2099254,
    "text": "この単語は、ここでは異なる頭によって使われている。"
  },
  {
    "start": 2099302,
    "end": 2102720,
    "text": "青い頭、赤い頭、緑の頭。"
  },
  {
    "start": 2103250,
    "end": 2109390,
    "text": "バイオレット・ヘッドがこの2つの単語を関連付けていないとしよう。"
  },
  {
    "start": 2109460,
    "end": 2116850,
    "text": "バイオレットヘッドやピンクヘッド、バイオレットヘッドやピンクヘッドによって、作ることと難しいことは関係ない。"
  },
  {
    "start": 2116920,
    "end": 2123300,
    "text": "彼らはmakingという単語を他の単語、例えばこの2009という単語と関連づけているのだ。"
  },
  {
    "start": 2124870,
    "end": 2126230,
    "text": "なぜそうなるのか？"
  },
  {
    "start": 2126300,
    "end": 2137160,
    "text": "というのも、このピンクの頭には、他の頭には見えない埋め込み部分が見えていて、それがこの2つの言葉の相互作用を可能にしているのかもしれないからだ。"
  },
  {
    "start": 2140910,
    "end": 2147126,
    "text": "また、なぜこの3つの行列がクエリーのキーとバリューと呼ばれるのか不思議に思うかもしれない。"
  },
  {
    "start": 2147318,
    "end": 2153174,
    "text": "なるほど、用語はデータベース用語やPythonのような辞書から来ている。"
  },
  {
    "start": 2153302,
    "end": 2156862,
    "text": "私なりの解釈も述べたい。"
  },
  {
    "start": 2156916,
    "end": 2157918,
    "text": "とても簡単な例だ。"
  },
  {
    "start": 2158004,
    "end": 2162080,
    "text": "とても分かりやすいと思う。"
  },
  {
    "start": 2163250,
    "end": 2169570,
    "text": "パイソンの辞書のようなもの、あるいはキーと値を持つデータベースがあるとしよう。"
  },
  {
    "start": 2169990,
    "end": 2176242,
    "text": "キーは映画のカテゴリーで、値はそのカテゴリーに属する映画である。"
  },
  {
    "start": 2176306,
    "end": 2178920,
    "text": "私の場合は、1つの値を入れただけだ。"
  },
  {
    "start": 2179530,
    "end": 2183634,
    "text": "タイタニック』を含むロマンチック部門がある。"
  },
  {
    "start": 2183682,
    "end": 2186754,
    "text": "ダークナイト』などのアクション映画もある。"
  },
  {
    "start": 2186882,
    "end": 2194618,
    "text": "また、クエリーを行うユーザーがいて、そのクエリーが愛であったとしよう。"
  },
  {
    "start": 2194704,
    "end": 2199820,
    "text": "これらの単語はすべて、実際にはサイズ512の埋め込みで表現される。"
  },
  {
    "start": 2200190,
    "end": 2206478,
    "text": "私たちのトランスフォーマーがすることは、この愛という言葉を512の埋め込みに変換することだ。"
  },
  {
    "start": 2206564,
    "end": 2217886,
    "text": "これらのクエリーと値はすべて、すでに512のエンベッディングであり、数式と同じように、クエリーとすべてのキーの間のドット積を計算する。"
  },
  {
    "start": 2217918,
    "end": 2226210,
    "text": "つまり、ご記憶の通り、計算式はクエリのソフトマックスにキーの移調を掛け、モデルの平方根で割ったものだ。"
  },
  {
    "start": 2226280,
    "end": 2234934,
    "text": "これは、すべてのクエリーとすべてのキー、この場合はloveという単語とすべてのキーの、1つずつの内積である。"
  },
  {
    "start": 2235132,
    "end": 2244170,
    "text": "これは、ある値を増幅させるスコア、あるいは他の値を増幅させないスコアとなる。"
  },
  {
    "start": 2245630,
    "end": 2253870,
    "text": "この場合、loveとromanticという単語が互いに関連するように埋め込むことができる。"
  },
  {
    "start": 2253940,
    "end": 2261242,
    "text": "ラブとコメディーという言葉も互いに関連しているが、ラブとロマンティックという言葉のようにそれほど強烈な関係ではない。"
  },
  {
    "start": 2261306,
    "end": 2265506,
    "text": "というより、なんというか、あまり強い関係ではない。"
  },
  {
    "start": 2265688,
    "end": 2269282,
    "text": "ホラーと愛はまったく関係ないのかもしれない。"
  },
  {
    "start": 2269336,
    "end": 2272660,
    "text": "もしかしたら、ソフト・マックスのスコアはゼロに近いかもしれない。"
  },
  {
    "start": 2276310,
    "end": 2282006,
    "text": "エンコーダーの次のレイヤーは、加算とノルムだ。"
  },
  {
    "start": 2282188,
    "end": 2285618,
    "text": "加算とノルムを導入するには、レイヤーの正規化が必要だ。"
  },
  {
    "start": 2285714,
    "end": 2287990,
    "text": "レイヤーの正規化とは何か？"
  },
  {
    "start": 2289150,
    "end": 2292554,
    "text": "レイヤーの正規化とは"
  },
  {
    "start": 2292672,
    "end": 2295098,
    "text": "では、実際に例を挙げてみよう。"
  },
  {
    "start": 2295184,
    "end": 2298374,
    "text": "n個の商品があるとする。"
  },
  {
    "start": 2298502,
    "end": 2304654,
    "text": "この場合、nは項目1、項目2、項目3の3つに等しい。"
  },
  {
    "start": 2304772,
    "end": 2307866,
    "text": "これらのアイテムにはそれぞれ特徴がある。"
  },
  {
    "start": 2307978,
    "end": 2309706,
    "text": "埋め込みかもしれない。"
  },
  {
    "start": 2309738,
    "end": 2316910,
    "text": "例えば、それは512サイズのベクトルの特徴かもしれないが、何千もの特徴からなる非常に大きな行列かもしれない。"
  },
  {
    "start": 2316990,
    "end": 2317890,
    "text": "そんなことはどうでもいい。"
  },
  {
    "start": 2318040,
    "end": 2329618,
    "text": "これらの項目の平均と分散をそれぞれ独立に計算し、それぞれの値をこの式で与えられる別の値に置き換えるのである。"
  },
  {
    "start": 2329714,
    "end": 2335560,
    "text": "基本的には、新しい値がすべてゼロから1の範囲になるように正規化している。"
  },
  {
    "start": 2336810,
    "end": 2346214,
    "text": "実際には、この新しい値にガンマと呼ばれるパラメータを掛け合わせ、さらにベータと呼ばれる別のパラメータを加える。"
  },
  {
    "start": 2346262,
    "end": 2349530,
    "text": "このガンマとベータは学習可能なパラメータである。"
  },
  {
    "start": 2349950,
    "end": 2363410,
    "text": "モデルは、増幅させたい値を増幅させ、増幅させたくない値を増幅させないように、これらのパラメーターの乗算や加算を学習する必要がある。"
  },
  {
    "start": 2365030,
    "end": 2369410,
    "text": "ただ正規化するだけでなく、いくつかのパラメータを導入している。"
  },
  {
    "start": 2369990,
    "end": 2378742,
    "text": "paperswithcode.comから、バッチノルムとレイヤーノルムの違いを見ることができる、とても素晴らしい視覚化を見つけた。"
  },
  {
    "start": 2378876,
    "end": 2385362,
    "text": "見てわかるように、レイヤーの正規化では、nがバッチ次元であるかどうかを計算している。"
  },
  {
    "start": 2385506,
    "end": 2390090,
    "text": "バッチ内の1つの項目に属するすべての値を計算している。"
  },
  {
    "start": 2390910,
    "end": 2397126,
    "text": "バッチ・ノルムでは、すべてのバッチについて同じ特徴を計算している。"
  },
  {
    "start": 2397238,
    "end": 2399670,
    "text": "バッチ内の全アイテムの"
  },
  {
    "start": 2399750,
    "end": 2404538,
    "text": "例えば、バッチの異なる項目の値をミックスしている。"
  },
  {
    "start": 2404634,
    "end": 2412350,
    "text": "レイヤーの正規化では、バッチ内の各アイテムを独立して扱い、それぞれの平均と分散を持つ。"
  },
  {
    "start": 2414630,
    "end": 2416546,
    "text": "デコーダーを見てみよう。"
  },
  {
    "start": 2416728,
    "end": 2421890,
    "text": "さて、エンコーダーでは入力エンベッディングを見た。"
  },
  {
    "start": 2422310,
    "end": 2427160,
    "text": "この場合、それらは出力埋め込みと呼ばれるが、基本的な作業は同じである。"
  },
  {
    "start": 2427610,
    "end": 2435270,
    "text": "ここにも位置エンコーディングがあり、これもエンコーダーと同じである。"
  },
  {
    "start": 2435610,
    "end": 2463118,
    "text": "次のレイヤーはマスクされたマルチヘッド・アテンションで、ここにもマルチヘッド・アテンションがある。"
  },
  {
    "start": 2463214,
    "end": 2467490,
    "text": "この接続は、デコーダーからの問い合わせである。"
  },
  {
    "start": 2467910,
    "end": 2474110,
    "text": "このマルチ・ヘッド・アテンションでは、もはやセルフ・アテンションではなく、クロス・アテンションなのだ。"
  },
  {
    "start": 2474190,
    "end": 2478390,
    "text": "2つのセンテンスを取るので、1つはエンコーダー側から送られる。"
  },
  {
    "start": 2478460,
    "end": 2493130,
    "text": "エンコーダーの出力を提供し、それをキーと値として使用するエンコーダーを書きましょう。一方、マスクされたマルチヘッド注目の出力は、このマルチヘッド注目のクエリーとして使用されます。"
  },
  {
    "start": 2494430,
    "end": 2500726,
    "text": "マスクされた多頭注意は、デコーダーの入力文の自己注意である。"
  },
  {
    "start": 2500758,
    "end": 2513594,
    "text": "デコーダーの入力文をエンベッディングに変換し、位置エンコーディングを加え、クエリーキーと値が同じ入力シーケンスであるこのマルチヘッドアテンションに与える。"
  },
  {
    "start": 2513722,
    "end": 2523534,
    "text": "キーと値はエンコーダーから送られてくる。"
  },
  {
    "start": 2523582,
    "end": 2525060,
    "text": "その後、加算とノルムを行う。"
  },
  {
    "start": 2525430,
    "end": 2530230,
    "text": "フィードフォワードは、完全に接続されたレイヤーである。"
  },
  {
    "start": 2530570,
    "end": 2538342,
    "text": "次に、フィードフォワードの出力を加算層とノルム層に送り、最後に線形層に送る。"
  },
  {
    "start": 2538476,
    "end": 2544730,
    "text": "マスクされた多頭注意と通常の多頭注意との違いを見てみよう。"
  },
  {
    "start": 2546110,
    "end": 2550950,
    "text": "私たちが望んでいるのは、モデルに因果関係を持たせることだ。"
  },
  {
    "start": 2551030,
    "end": 2557502,
    "text": "つまり、ある位置での出力は、その前の位置の単語によってのみ左右されるということだ。"
  },
  {
    "start": 2557636,
    "end": 2561134,
    "text": "モデルには未来の言葉が見えてはならない。"
  },
  {
    "start": 2561252,
    "end": 2563060,
    "text": "どうすればそれを達成できるのか？"
  },
  {
    "start": 2563910,
    "end": 2572334,
    "text": "ご覧いただいたように、注目度計算式のソフトマックスの出力は、この行列の並び順である。"
  },
  {
    "start": 2572462,
    "end": 2587880,
    "text": "ある単語と他の単語の相互作用を隠したい場合は、ソフトマックスを適用する前にこの値を削除し、マイナス無限大に置き換えて、ソフトマックスがこの値をゼロに置き換えるようにする。"
  },
  {
    "start": 2588250,
    "end": 2591930,
    "text": "私たちは、私たちが望まないすべての相互作用のためにこれを行う。"
  },
  {
    "start": 2592000,
    "end": 2594954,
    "text": "今後の言葉は見てほしくない。"
  },
  {
    "start": 2594992,
    "end": 2605582,
    "text": "私たちは、cat is a lovely cat（猫は愛くるしい）を見てほしいとは思っていないし、catという単語が未来の単語を見てほしいとも思っていない。"
  },
  {
    "start": 2605716,
    "end": 2611790,
    "text": "他の言葉も同じです。"
  },
  {
    "start": 2612610,
    "end": 2621682,
    "text": "この対角線上にある値をすべて置き換えていることがわかる。"
  },
  {
    "start": 2621736,
    "end": 2634422,
    "text": "これは行列の主対角線であり、この対角線より上にあるすべての値をマイナス無限大に置き換え、ソフトマックスがそれらをゼロに置き換えるようにしたい。"
  },
  {
    "start": 2634556,
    "end": 2640642,
    "text": "このメカニズムがマルチヘッド注目のどの段階で導入されるかを見てみよう。"
  },
  {
    "start": 2640786,
    "end": 2652874,
    "text": "ソフトマックスを適用する前に、これらのモル行列間の注目度を計算する場合、qは1、kは1、vは1となる。"
  },
  {
    "start": 2652922,
    "end": 2666030,
    "text": "これと、これと、これと、これと、マイナス無限大で、ソフトマックスを適用し、ソフトマックスがこれらの値をゼロに変換する。"
  },
  {
    "start": 2666110,
    "end": 2678962,
    "text": "もし相互作用を望まないのであれば、モデルはこの相互作用から情報を得られないので、相互作用をさせないように学習する。"
  },
  {
    "start": 2679026,
    "end": 2681522,
    "text": "この言葉は相互作用できないようなものだ。"
  },
  {
    "start": 2681586,
    "end": 2686120,
    "text": "では、トランスフォーマーモデルの推論とトレーニングの仕組みを見てみよう。"
  },
  {
    "start": 2686490,
    "end": 2692934,
    "text": "前にも言ったように、私たちは翻訳作業に取り組むことになる。"
  },
  {
    "start": 2692982,
    "end": 2700750,
    "text": "視覚化しやすく、すべてのステップを理解しやすいので、モデルのトレーニングから始めよう。"
  },
  {
    "start": 2700900,
    "end": 2705642,
    "text": "英語のI love you very muchからイタリア語のI love you very muchへ。"
  },
  {
    "start": 2705706,
    "end": 2706586,
    "text": "ダイヤモンド・マルト"
  },
  {
    "start": 2706618,
    "end": 2710430,
    "text": "とてもシンプルな文章で、説明するのは簡単だ。"
  },
  {
    "start": 2711090,
    "end": 2712160,
    "text": "行こう。"
  },
  {
    "start": 2712630,
    "end": 2722414,
    "text": "まず、トランスフォーマーモデルの説明から始め、エンコーダーに送られる英文から始める。"
  },
  {
    "start": 2722462,
    "end": 2729010,
    "text": "この英文に2つの特別なトークンを付加する。"
  },
  {
    "start": 2729090,
    "end": 2733186,
    "text": "ひとつは文頭、もうひとつは文末と呼ばれる。"
  },
  {
    "start": 2733298,
    "end": 2746214,
    "text": "この2つのトークンは語彙から取られているので、文の開始位置と終了位置をモデルに伝える語彙の特別なトークンである。"
  },
  {
    "start": 2746342,
    "end": 2748540,
    "text": "なぜそれが必要なのかは、後ほど説明する。"
  },
  {
    "start": 2748990,
    "end": 2755150,
    "text": "とりあえず、文に特別なトークンを前置し、特別なトークンを後置すると考えてほしい。"
  },
  {
    "start": 2755810,
    "end": 2765650,
    "text": "そして私たちがすることは、写真を見てわかるように、入力を入力エンベッディングに変換し、位置エンコーディングを加えて、エンコーダーに送ることだ。"
  },
  {
    "start": 2766310,
    "end": 2769042,
    "text": "これがDモデルによるエンコーダーの入力シーケンスである。"
  },
  {
    "start": 2769096,
    "end": 2777314,
    "text": "それをエンコーダーに送ると、エンコーダーはDモデルによってシーケンスをエンコードした出力を生成し、それをエンコーダー出力と呼ぶ。"
  },
  {
    "start": 2777442,
    "end": 2789154,
    "text": "先に見たように、エンコーダーの出力は入力行列と同じ次元を持つ別の行列であり、その中に埋め込みが行われる。"
  },
  {
    "start": 2789202,
    "end": 2799722,
    "text": "この埋め込みが特別なのは、ここで見た入力埋め込みによって与えられた単語の意味だけでなく、その単語の意味をも捉えているからである。"
  },
  {
    "start": 2799776,
    "end": 2810474,
    "text": "これによって、位置エンコーディングによって与えられた位置だけでなく、同じ文中のすべての単語と他の単語との相互作用もわかる。"
  },
  {
    "start": 2810522,
    "end": 2814958,
    "text": "これはエンコーダーだから、自己の注意力について話しているんだ。"
  },
  {
    "start": 2815054,
    "end": 2821330,
    "text": "それは、文中の各単語と、同じ文中の他のすべての単語との相互作用である。"
  },
  {
    "start": 2822790,
    "end": 2825294,
    "text": "この文章をイタリア語に変換したい。"
  },
  {
    "start": 2825342,
    "end": 2831746,
    "text": "デコーダーの入力は、文の先頭t amomaltoである。"
  },
  {
    "start": 2831938,
    "end": 2838586,
    "text": "トランスの写真からわかるように、ここの出力は右にずれているのがわかる。"
  },
  {
    "start": 2838768,
    "end": 2840122,
    "text": "右シフトとはどういう意味か？"
  },
  {
    "start": 2840176,
    "end": 2845370,
    "text": "基本的には、SOS（文頭）と呼ばれる特別なトークンを前置することを意味する。"
  },
  {
    "start": 2846670,
    "end": 2854922,
    "text": "また、この2つのシーケンスは、トランスフォーマーをコード化する際に実際に使用されるものであることにもお気づきだろう。"
  },
  {
    "start": 2854986,
    "end": 2872850,
    "text": "もし、ティアモルトのような文章や非常に長い文章があったとしても、それらをトランスフォーマーに送ると、実はすべて同じ長さになるのだ。"
  },
  {
    "start": 2873190,
    "end": 2874034,
    "text": "どうすればいいのか？"
  },
  {
    "start": 2874072,
    "end": 2878274,
    "text": "パディング・ワードを追加して、希望の長さにするのだ。"
  },
  {
    "start": 2878322,
    "end": 2885618,
    "text": "もし我々のモデルが1000のシーケンス長をサポートできるなら、この場合は4つのトークンがある。"
  },
  {
    "start": 2885714,
    "end": 2893446,
    "text": "996トークンのパディングを追加して、この文が配列長に達するのに十分な長さにする。"
  },
  {
    "start": 2893558,
    "end": 2897450,
    "text": "もちろん、そうしないとイメージしにくいから、ここではやらない。"
  },
  {
    "start": 2898270,
    "end": 2901330,
    "text": "よし、この入力をデコーダーのために準備しよう。"
  },
  {
    "start": 2901510,
    "end": 2904922,
    "text": "埋め込みに変換を加える。"
  },
  {
    "start": 2904986,
    "end": 2906922,
    "text": "位置エンコーディングを追加する。"
  },
  {
    "start": 2907066,
    "end": 2911434,
    "text": "そして、マスクされたマルチヘッドアテンションにまずそれを送る。"
  },
  {
    "start": 2911482,
    "end": 2922110,
    "text": "エンコーダーの出力をキーと値としてデコーダーに送る。"
  },
  {
    "start": 2922190,
    "end": 2925966,
    "text": "問い合わせは覆面から来ている。"
  },
  {
    "start": 2925998,
    "end": 2935558,
    "text": "キーと値はエンコーダーの出力であり、このブロックの出力である。"
  },
  {
    "start": 2935644,
    "end": 2944410,
    "text": "この大きなブロックは、エンコーダーと同じように、Dモデルでシーケンスされたマトリックスになる。"
  },
  {
    "start": 2944990,
    "end": 2952798,
    "text": "しかし、これはDモデルであり、512サイズのベクトルであるため、まだ埋め込みであることがわかる。"
  },
  {
    "start": 2952884,
    "end": 2957802,
    "text": "このエンベッディングを辞書にどのように関連づけることができるだろうか？"
  },
  {
    "start": 2957866,
    "end": 2963082,
    "text": "私たちの語彙の中で、この言葉が何であるかを理解するにはどうしたらいいのだろう。"
  },
  {
    "start": 2963226,
    "end": 2971506,
    "text": "そのため、Dモデルによるシーケンスを、語彙サイズによるシーケンスにマッピングする線形レイヤーが必要なのだ。"
  },
  {
    "start": 2971608,
    "end": 2983880,
    "text": "これは、モデルによって出力される実際のトークンが何であるかを理解することができるように、見た埋め込みごとに、語彙の中でその単語がどの位置にあるかを教えてくれる。"
  },
  {
    "start": 2985770,
    "end": 2991862,
    "text": "その後、ソフトマックスを適用し、ラベルが完成する。"
  },
  {
    "start": 2991926,
    "end": 2994886,
    "text": "モデルが出力することを期待するもの。"
  },
  {
    "start": 2994998,
    "end": 3008298,
    "text": "この英文が与えられると、モデルはこのティアモ・マルトの文末を出力すると予想され、これをラベルまたはターゲットと呼ぶ。"
  },
  {
    "start": 3008474,
    "end": 3014446,
    "text": "モデルの出力とそれに対応するラベルが得られたら、何をするかというと、損失を計算する。"
  },
  {
    "start": 3014558,
    "end": 3020530,
    "text": "この場合、クロスエントロピーの損失であり、その損失をすべての重みに逆伝播する。"
  },
  {
    "start": 3021190,
    "end": 3026210,
    "text": "では、なぜsosとEosという特別なトークンがあるのかを理解しよう。"
  },
  {
    "start": 3026710,
    "end": 3030262,
    "text": "基本的に、ここではシークエンスの長さが4であることがわかる。"
  },
  {
    "start": 3030396,
    "end": 3032786,
    "text": "実際には、パディングがあるから1000なんだ。"
  },
  {
    "start": 3032818,
    "end": 3034706,
    "text": "パットがないとしよう。"
  },
  {
    "start": 3034738,
    "end": 3042102,
    "text": "文頭のt amamaltoと文末のt amamaltoの4つのトークンだ。"
  },
  {
    "start": 3042166,
    "end": 3051454,
    "text": "このモデルでは、文頭トークンを検出すると、最初のトークンを出力tとして出力する。"
  },
  {
    "start": 3051652,
    "end": 3055978,
    "text": "tが表示されたら、弾薬を出力する。"
  },
  {
    "start": 3056074,
    "end": 3070530,
    "text": "アンモが表示されるとマルトを出力し、マルトが表示されると文末を出力して、翻訳が完了したことを示す。"
  },
  {
    "start": 3073910,
    "end": 3089378,
    "text": "ビデオの冒頭で約束したように、リカーラル・ニューラル・ネットワークでは、n個の入力シーケンスをn個の出力シーケンスにマッピングするためにn回の時間ステップが必要なのだ。"
  },
  {
    "start": 3089554,
    "end": 3092678,
    "text": "この問題はトランスで解決できるだろう。"
  },
  {
    "start": 3092774,
    "end": 3100026,
    "text": "そう、解決したのだ。ここでわかるように、forループは一切していない。"
  },
  {
    "start": 3100128,
    "end": 3105306,
    "text": "エンコーダーに入力シーケンスを与え、デコーダーに入力シーケンスを与える。"
  },
  {
    "start": 3105418,
    "end": 3107450,
    "text": "私たちはいくつかのアウトプットを出した。"
  },
  {
    "start": 3107530,
    "end": 3111902,
    "text": "私たちはラベルとクロスエントロピーの損失を計算した。"
  },
  {
    "start": 3111956,
    "end": 3114334,
    "text": "すべては1回のタイムステップで起こる。"
  },
  {
    "start": 3114452,
    "end": 3128310,
    "text": "これはトランスフォーマーの力であり、非常に長いシーケンスを、チャットやGPT、GPT、バートなどで見ることができる非常に素晴らしいパフォーマンスで、非常に簡単かつ高速にトレーニングすることができたからだ。"
  },
  {
    "start": 3130090,
    "end": 3132440,
    "text": "推論の仕組みを見てみよう。"
  },
  {
    "start": 3134250,
    "end": 3137238,
    "text": "ここでもまた、私たちの英文I love you very muchがある。"
  },
  {
    "start": 3137324,
    "end": 3141290,
    "text": "それをイタリア語の文章、アモ・マルトにマッピングしたい。"
  },
  {
    "start": 3142510,
    "end": 3148934,
    "text": "エンコーダーの入力は文頭である。"
  },
  {
    "start": 3148982,
    "end": 3149834,
    "text": "とても愛しているよ。"
  },
  {
    "start": 3149872,
    "end": 3150970,
    "text": "文末。"
  },
  {
    "start": 3151710,
    "end": 3153814,
    "text": "入力埋め込みに変換する。"
  },
  {
    "start": 3153862,
    "end": 3155194,
    "text": "次に位置エンコーディングを加える。"
  },
  {
    "start": 3155242,
    "end": 3158682,
    "text": "エンコーダーの入力を準備し、それをエンコーダーに送る。"
  },
  {
    "start": 3158826,
    "end": 3163634,
    "text": "エンコーダーはdmodelによってシーケンスされた出力を生成する。"
  },
  {
    "start": 3163672,
    "end": 3171220,
    "text": "それは、意味や位置だけでなく、すべての単語と他の単語との相互作用を捉える特別な埋め込みの連続である。"
  },
  {
    "start": 3172470,
    "end": 3184434,
    "text": "デコーダーには文頭だけを渡し、シーケンス長に達するだけのパディング・トークンを加える。"
  },
  {
    "start": 3184562,
    "end": 3193702,
    "text": "文頭トークンをモデルに与え、この単一トークンを埋め込みに変換する。"
  },
  {
    "start": 3193766,
    "end": 3198102,
    "text": "位置エンコーディングを追加し、デコーダー入力としてデコーダーに送る。"
  },
  {
    "start": 3198246,
    "end": 3211246,
    "text": "デコーダーは、クエリーとエンコーダーからのキーと値を入力とし、Dモデルによってシーケンスされた出力を生成する。"
  },
  {
    "start": 3211428,
    "end": 3218690,
    "text": "繰り返しになるが、リニアレイヤーに語彙に投影してもらいたい。"
  },
  {
    "start": 3219350,
    "end": 3232338,
    "text": "ソフトマックスを適用することで、ロジットが与えられたときに、ソフトマックスで最大のスコアを持つ出力単語の位置を選択する。"
  },
  {
    "start": 3232514,
    "end": 3236978,
    "text": "こうして、どの単語をボキャブラリーから選べばいいかがわかるのだ。"
  },
  {
    "start": 3237154,
    "end": 3245050,
    "text": "モデルが正しくトレーニングされていれば、最初の出力トークンはtになるはずである。"
  },
  {
    "start": 3245710,
    "end": 3248730,
    "text": "しかし、これはタイムスタンプ1で起こる。"
  },
  {
    "start": 3248800,
    "end": 3253146,
    "text": "モデル変換モデルをトレーニングする場合、それは1パスで行われる。"
  },
  {
    "start": 3253248,
    "end": 3261114,
    "text": "1つの入力シーケンスと1つの出力シーケンスがあり、それをモデルに与え、1つの時間ステップを行い、推論するときにモデルがそれを学習する。"
  },
  {
    "start": 3261162,
    "end": 3267794,
    "text": "しかし、トークンごとに行う必要がある。"
  },
  {
    "start": 3267832,
    "end": 3278198,
    "text": "ステップ2では、英文が変わっていないため、エンコーダーの出力を再計算する必要はない。"
  },
  {
    "start": 3278284,
    "end": 3283974,
    "text": "エンコーダーが同じ出力を出すことを望む。"
  },
  {
    "start": 3284092,
    "end": 3314530,
    "text": "そして、前の文の出力をデコーダーの入力に追加し、前のステップのエンコーダーの出力と一緒に再びデコーダーに送り、デコーダー側から出力シーケンスを生成する。"
  },
  {
    "start": 3315350,
    "end": 3326578,
    "text": "前にも言ったように、英文はまったく変わっていないので、時間ステップごとにエンコーダーの出力を再計算しているわけではない。"
  },
  {
    "start": 3326664,
    "end": 3335058,
    "text": "時間ステップごとに、前のステップの出力をデコーダーの入力に追加しているからだ。"
  },
  {
    "start": 3335154,
    "end": 3351230,
    "text": "タイムステップ3でも同じことをし、タイムステップ4でも同じことをする。うまくいけば、文末トークンを見た時点で推論を止めることができる。"
  },
  {
    "start": 3351730,
    "end": 3353998,
    "text": "これが推論の仕組みだ。"
  },
  {
    "start": 3354164,
    "end": 3362730,
    "text": "翻訳モデルのようなモデルを推論する際に、なぜ4つの時間ステップが必要なのか。"
  },
  {
    "start": 3362900,
    "end": 3365086,
    "text": "推論には多くの戦略がある。"
  },
  {
    "start": 3365198,
    "end": 3367694,
    "text": "私たちが使ったのは貪欲な戦略と呼ばれるものだ。"
  },
  {
    "start": 3367742,
    "end": 3373620,
    "text": "ステップごとに、ソフトマックスの最大値を持つ単語を取得する。"
  },
  {
    "start": 3374150,
    "end": 3384694,
    "text": "しかし、この戦略も通常は悪くないが、より良い戦略もあり、その一つがビーム・サーチと呼ばれるものだ。"
  },
  {
    "start": 3384892,
    "end": 3388002,
    "text": "ビームサーチでは、常に貪欲になるのではなく"
  },
  {
    "start": 3388146,
    "end": 3390518,
    "text": "だから貪欲なんだ。"
  },
  {
    "start": 3390614,
    "end": 3407630,
    "text": "貪欲にソフトの最大値を取るのではなく、上位b個の値を取り、その各選択肢について、上位b個の値のそれぞれについて次に可能なトークンが何かをステップごとに推論する。"
  },
  {
    "start": 3407780,
    "end": 3415122,
    "text": "最も確率の高いb個の配列を持つものだけを残し、他は削除する。"
  },
  {
    "start": 3415256,
    "end": 3419380,
    "text": "これはビーム・サーチと呼ばれ、一般的にはこちらの方が性能が良い。"
  },
  {
    "start": 3420710,
    "end": 3422740,
    "text": "見てくれてありがとう。"
  },
  {
    "start": 3423510,
    "end": 3429762,
    "text": "長いビデオになったが、トランスフォーマーの各側面を見ていく価値は十分にあった。"
  },
  {
    "start": 3429906,
    "end": 3453182,
    "text": "トランスフォーマーモデルをゼロからコーディングする方法についての私の別のビデオもぜひご覧ください。このビデオでは、トランスフォーマーモデルをコーディングしながら、その構造を改めて説明するだけでなく、あなたが選んだデータセットでそれをトレーニングする方法と、それを推論する方法もお見せします。"
  },
  {
    "start": 3453316,
    "end": 3463150,
    "text": "また、GitHub上のコードと、Colab上でモデルを直接トレーニングするためのcolab notebookも提供した。"
  },
  {
    "start": 3464210,
    "end": 3471642,
    "text": "チャンネル登録をして、わからなかったことを教えてください。"
  },
  {
    "start": 3471706,
    "end": 3480022,
    "text": "この種のビデオの問題点、あるいはこの特定のビデオの問題点を教えてください。"
  },
  {
    "start": 3480156,
    "end": 3483380,
    "text": "ありがとうございました。良い一日をお過ごしください。"
  }
]