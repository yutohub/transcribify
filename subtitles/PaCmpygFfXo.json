[
  {
    "start": 330,
    "end": 2030,
    "text": "皆さん、こんにちは。"
  },
  {
    "start": 2180,
    "end": 11130,
    "text": "次にやりたいことは、MicrogradのようにMakemoreを作り上げることだ。"
  },
  {
    "start": 11290,
    "end": 17774,
    "text": "マイクログラッドと同じように、ステップ・バイ・ステップで組み立てていく。"
  },
  {
    "start": 17812,
    "end": 19840,
    "text": "ゆっくりと、そして一緒に作り上げていくつもりだ。"
  },
  {
    "start": 20210,
    "end": 22090,
    "text": "さて、マケモアとは？"
  },
  {
    "start": 22250,
    "end": 27494,
    "text": "メイクモアはその名の通り、与えたものをより多く作る。"
  },
  {
    "start": 27612,
    "end": 29474,
    "text": "これが名前の例だ。"
  },
  {
    "start": 29522,
    "end": 32326,
    "text": "TXTは、より多くを作るためのデータセット例です。"
  },
  {
    "start": 32508,
    "end": 37750,
    "text": "名前、TXTを見ると、非常に大きな名前のデータセットであることがわかる。"
  },
  {
    "start": 38170,
    "end": 41846,
    "text": "ここにはいろいろなタイプの名前がある。"
  },
  {
    "start": 41878,
    "end": 47260,
    "text": "実際、私が政府のウェブサイトで無作為に見つけた32,000人の名前があると思う。"
  },
  {
    "start": 47790,
    "end": 54240,
    "text": "このデータセットでもっと作る訓練をすれば、このようなものをもっと作るように学習するだろう。"
  },
  {
    "start": 55250,
    "end": 62234,
    "text": "特にこの場合、名前に似ているようで実はユニークな名前が増えることになる。"
  },
  {
    "start": 62362,
    "end": 67666,
    "text": "赤ちゃんが生まれ、名前を決めようとしているのなら、クールで新しい響きのユニークな名前を探しているのかもしれない。"
  },
  {
    "start": 67768,
    "end": 69346,
    "text": "マケモアが助けてくれるかもしれない。"
  },
  {
    "start": 69528,
    "end": 75700,
    "text": "以下は、ニューラルネットワークをデータセットで訓練した後の世代例である。"
  },
  {
    "start": 76170,
    "end": 79602,
    "text": "ユニークな名前の例をいくつか挙げてみよう。"
  },
  {
    "start": 79746,
    "end": 85462,
    "text": "ダンテール、イロット、ゼンディなど。"
  },
  {
    "start": 85596,
    "end": 89750,
    "text": "だから、これらはすべて名前のように聞こえるが、もちろん名前ではない。"
  },
  {
    "start": 90570,
    "end": 94650,
    "text": "ボンネットの下にあるのは、文字レベルの言語モデルだ。"
  },
  {
    "start": 94800,
    "end": 99450,
    "text": "つまり、ここにあるすべての行を例として扱っているということだ。"
  },
  {
    "start": 99600,
    "end": 104922,
    "text": "それぞれの例では、それらをすべて個々の文字の並びとして扱っている。"
  },
  {
    "start": 105066,
    "end": 110522,
    "text": "リースがその例で、これが一連の登場人物だ。"
  },
  {
    "start": 110586,
    "end": 113754,
    "text": "私たちがマケモアを作り上げているのは、そういうレベルなんだ。"
  },
  {
    "start": 113882,
    "end": 123090,
    "text": "文字レベルの言語モデルというのはどういうことかというと、文字の並びをモデル化して、その並びの中で次の文字を予測する方法を知っているということなんだ。"
  },
  {
    "start": 123590,
    "end": 132274,
    "text": "では、実際に文字レベルの言語モデルを大量に実装することになるが、これはシーケンスの次の文字を予測するのに関わるニューラルネットワークという意味である。"
  },
  {
    "start": 132402,
    "end": 133430,
    "text": "とてもシンプルだ。"
  },
  {
    "start": 133500,
    "end": 140674,
    "text": "ビグラムやバッグ・オブ・ルート・モデル、多言語パーセプトロン、リカレント・ニューラル・ネットワーク、そして最新の変換器まで。"
  },
  {
    "start": 140802,
    "end": 147690,
    "text": "実際、私たちが作るトランスは、GPTをご存じであれば、基本的にGPT-2と同等のトランスになります。"
  },
  {
    "start": 148030,
    "end": 149290,
    "text": "それはちょっと大きな問題だ。"
  },
  {
    "start": 149360,
    "end": 155978,
    "text": "現代のネットワークであり、シリーズが終わるころには、それが登場人物のレベルでどのように機能するのか、実際に理解できるようになるだろう。"
  },
  {
    "start": 156154,
    "end": 170798,
    "text": "さて、ここでの拡張性を感じてもらうために、文字の次は単語レベルに時間をかけることになるだろう。"
  },
  {
    "start": 170974,
    "end": 177974,
    "text": "その後、ドリーや安定した拡散など、画像や画像テキストネットワークに入ることになるだろう。"
  },
  {
    "start": 178092,
    "end": 180614,
    "text": "とりあえず、ここから始めなければならない。"
  },
  {
    "start": 180732,
    "end": 182242,
    "text": "文字レベルの言語モデリング。"
  },
  {
    "start": 182306,
    "end": 183190,
    "text": "行こう。"
  },
  {
    "start": 183340,
    "end": 186966,
    "text": "前回と同じように、まったく白紙のJupyterノートブック・ページから始める。"
  },
  {
    "start": 187068,
    "end": 191130,
    "text": "まず、基本的にデータセット名txTをロードしたい。"
  },
  {
    "start": 191710,
    "end": 199050,
    "text": "TXTの名前を読み込むために開き、すべてを巨大な文字列にして読み込む。"
  },
  {
    "start": 199710,
    "end": 204350,
    "text": "となると、膨大な文字列なので、個々の単語だけを取り出してリストに入れることになる。"
  },
  {
    "start": 204500,
    "end": 211310,
    "text": "その文字列に対してsplit linesを呼び出し、すべての単語をpythonの文字列リストとして取得してみましょう。"
  },
  {
    "start": 211970,
    "end": 221330,
    "text": "基本的には、例えば最初の10語を見て、emma、olivia、avaなどのリストを見ることができる。"
  },
  {
    "start": 221480,
    "end": 226760,
    "text": "このページのトップを見ると、確かにそうなっている。"
  },
  {
    "start": 228090,
    "end": 229414,
    "text": "それはいいことだ。"
  },
  {
    "start": 229612,
    "end": 234310,
    "text": "このリストを見ると、おそらく頻度順に並んでいるのだろうと感じる。"
  },
  {
    "start": 236170,
    "end": 238326,
    "text": "さて、これがその言葉だ。"
  },
  {
    "start": 238428,
    "end": 241818,
    "text": "では、実際にこのデータセットについてもう少し学んでみたい。"
  },
  {
    "start": 241904,
    "end": 243322,
    "text": "総語数を見てみよう。"
  },
  {
    "start": 243376,
    "end": 245740,
    "text": "およそ32,000人になると予想している。"
  },
  {
    "start": 246350,
    "end": 248938,
    "text": "では、例えば最も短い単語は何だろう？"
  },
  {
    "start": 249104,
    "end": 253438,
    "text": "各単語のLenの最小値。"
  },
  {
    "start": 253604,
    "end": 260862,
    "text": "最短の単語は、単語数wに対して長さw、2、最大長wとなる。"
  },
  {
    "start": 260996,
    "end": 264442,
    "text": "最長の単語は15文字になります。"
  },
  {
    "start": 264586,
    "end": 267218,
    "text": "では、最初の言語モデルを考えてみよう。"
  },
  {
    "start": 267384,
    "end": 276354,
    "text": "前述したように、文字レベルの言語モデルは、その前の具体的な文字列がすでに与えられている場合に、次の文字列を予測するものである。"
  },
  {
    "start": 276552,
    "end": 285366,
    "text": "さて、ここで私たちが認識しなければならないのは、イザベラのような単語ひとつひとつに、実はかなりの数の例が詰め込まれているということだ。"
  },
  {
    "start": 285548,
    "end": 297740,
    "text": "というのも、イザベラのような単語がデータセットの中に存在するということは、つまり、Iという文字が名前の並びの中で最初に来る可能性が非常に高い文字であるということを物語っているからだ。"
  },
  {
    "start": 298590,
    "end": 302880,
    "text": "sはIの後に来る可能性が高い。"
  },
  {
    "start": 304290,
    "end": 314262,
    "text": "aの後に来る可能性の高いキャラクターは、bの後に来る可能性が非常に高いイザベル、そしてイザベルの後に続くaに至る。"
  },
  {
    "start": 314426,
    "end": 323554,
    "text": "それは、イザベラの後に単語が終わる可能性が非常に高いということだ。"
  },
  {
    "start": 323752,
    "end": 329446,
    "text": "これは、私たちが注意しなければならないもうひとつの明確な情報のようなものだ。"
  },
  {
    "start": 329628,
    "end": 337890,
    "text": "というわけで、このような文字列の後に続くであろう統計的な構造という点で、1つ1つの単語には多くのことが詰まっている。"
  },
  {
    "start": 338050,
    "end": 340166,
    "text": "となると、もちろん、個々の言葉だけを持っているわけではない。"
  },
  {
    "start": 340268,
    "end": 342102,
    "text": "実際には3万2,000本ある。"
  },
  {
    "start": 342156,
    "end": 344380,
    "text": "だから、ここにはモデルとしての構造がたくさんある。"
  },
  {
    "start": 344830,
    "end": 350380,
    "text": "さて、まず始めに、ビグラム言語モデルの構築から始めたいと思います。"
  },
  {
    "start": 351150,
    "end": 356526,
    "text": "さて、ビグラム言語モデルでは、常に一度に2文字だけを扱う。"
  },
  {
    "start": 356708,
    "end": 363470,
    "text": "私たちは与えられた1つの文字だけを見て、次の文字を予測しようとしている。"
  },
  {
    "start": 363890,
    "end": 369746,
    "text": "どのキャラクターがrをフォローしそうか、どのキャラクターがaをフォローしそうか、といった具合に。"
  },
  {
    "start": 369768,
    "end": 376642,
    "text": "私たちはそのような小さなローカルな構造をモデル化しているだけで、もっと多くの情報を持っているかもしれないという事実を忘れている。"
  },
  {
    "start": 376776,
    "end": 380146,
    "text": "私たちはいつも、次のキャラクターを予測するために前のキャラクターを見ているだけだ。"
  },
  {
    "start": 380248,
    "end": 383910,
    "text": "非常にシンプルで弱い言語モデルだが、手始めには最適だと思う。"
  },
  {
    "start": 384060,
    "end": 387994,
    "text": "ではまず、データセットに含まれるグラム別に、それらがどのようなものかを見てみよう。"
  },
  {
    "start": 388032,
    "end": 390806,
    "text": "これらのビッグラムは、やはり2文字が並んでいるだけである。"
  },
  {
    "start": 390918,
    "end": 396090,
    "text": "wとwordsに対して、ここにあるそれぞれのwは個々の単語であり、文字列である。"
  },
  {
    "start": 399630,
    "end": 403462,
    "text": "この単語を連続した文字で反復したい。"
  },
  {
    "start": 403606,
    "end": 406702,
    "text": "一度に2文字ずつ、単語の中を滑らせる。"
  },
  {
    "start": 406836,
    "end": 424450,
    "text": "ところで、パイソンでこれを行う面白い、素敵な、かわいい方法は、zip off wとwの1、1列で文字1、文字2に対して次のようなことを行うことだ。"
  },
  {
    "start": 424600,
    "end": 427282,
    "text": "全部の単語はやらないで、最初の3単語だけやろう。"
  },
  {
    "start": 427336,
    "end": 429830,
    "text": "これがどのように機能するか、すぐにお見せしよう。"
  },
  {
    "start": 429980,
    "end": 434550,
    "text": "とりあえず、基本的には例として、一番最初の単語「エマ」だけをやってみよう。"
  },
  {
    "start": 435290,
    "end": 437726,
    "text": "私たちがいかにエマを持っているかわかるだろう。"
  },
  {
    "start": 437858,
    "end": 440666,
    "text": "これは単にemm maをプリントするだけである。"
  },
  {
    "start": 440848,
    "end": 445286,
    "text": "これが機能するのは、wが文字列エマだからだ。"
  },
  {
    "start": 445398,
    "end": 448390,
    "text": "ある列のWは文字列mmaである。"
  },
  {
    "start": 448550,
    "end": 457178,
    "text": "zipは2つのイテレータを受け取り、それらをペアにして、連続するエントリーのタプルに対するイテレータを作成する。"
  },
  {
    "start": 457354,
    "end": 463280,
    "text": "もしこれらのリストのどれかが他のリストより短ければ、そのまま停止してリターンする。"
  },
  {
    "start": 463650,
    "end": 469862,
    "text": "基本的に、それがエマを返す理由だ。"
  },
  {
    "start": 470026,
    "end": 475742,
    "text": "このイテレータの2番目の要素がなくなったので、ZIPはそのまま終了する。"
  },
  {
    "start": 475806,
    "end": 477610,
    "text": "だから、このようなタプルしか得られないのだ。"
  },
  {
    "start": 477710,
    "end": 479030,
    "text": "かなりかわいい。"
  },
  {
    "start": 479690,
    "end": 482982,
    "text": "これらは最初の単語の連続要素である。"
  },
  {
    "start": 483116,
    "end": 487954,
    "text": "ここで注意しなければならないのは、実際にはこの3つの例以外にも情報があるということだ。"
  },
  {
    "start": 488002,
    "end": 495180,
    "text": "前述したように、eが最初に来る可能性が非常に高く、この場合のaは最後に来ることがわかっている。"
  },
  {
    "start": 495550,
    "end": 507520,
    "text": "これを行う1つの方法は、基本的に、ここに特別な配列を作成し、ここに特別なスタート・トークンを幻覚化することだ。"
  },
  {
    "start": 508370,
    "end": 512206,
    "text": "特別なスタートのように呼ぶつもりだ。"
  },
  {
    "start": 512388,
    "end": 520850,
    "text": "これは、1つの要素にwを加え、さらに特殊な終了文字を加えたリストである。"
  },
  {
    "start": 521190,
    "end": 525146,
    "text": "ここでwのリストをラップしているのは、wが文字列だからだ。"
  },
  {
    "start": 525198,
    "end": 530200,
    "text": "エマ、\"w \"のリストには個々のキャラクターが表示されます。"
  },
  {
    "start": 530810,
    "end": 539720,
    "text": "ということは、今もう一度、wの反復処理ではなく、文字の反復処理を行うと、次のようになる。"
  },
  {
    "start": 540090,
    "end": 541730,
    "text": "の可能性が高い。"
  },
  {
    "start": 541810,
    "end": 544650,
    "text": "これは開始文字とeの図である。"
  },
  {
    "start": 544720,
    "end": 548870,
    "text": "これはaと特別な終了文字のバイオグラムである。"
  },
  {
    "start": 549030,
    "end": 553770,
    "text": "これで、例えばオリビアやエヴァがどう見えるかを見ることができる。"
  },
  {
    "start": 554450,
    "end": 558158,
    "text": "実際、データセット全体に対してこのようなことができる可能性がある。"
  },
  {
    "start": 558244,
    "end": 559246,
    "text": "それは載せません。"
  },
  {
    "start": 559268,
    "end": 560510,
    "text": "それはやりすぎだろう。"
  },
  {
    "start": 560660,
    "end": 574302,
    "text": "どの文字が他の文字に続く可能性が高いかという統計を学習するために、バイグラム言語モデルで最も単純な方法は、単純に数を数えることである。"
  },
  {
    "start": 574446,
    "end": 581638,
    "text": "基本的には、これらの組み合わせのどれが、これらの単語のトレーニングセットでどれだけの頻度で出現するかを数えるだけだ。"
  },
  {
    "start": 581804,
    "end": 587186,
    "text": "グラム単位のカウントを管理する辞書のようなものが必要だろう。"
  },
  {
    "start": 587298,
    "end": 592590,
    "text": "辞書bを使い、グラムごとにマッピングする。"
  },
  {
    "start": 592690,
    "end": 604110,
    "text": "Bygramは文字1、文字2のタプルであり、BygramでのBはBygramのB getとなり、基本的にはBygramでのBと同じである。"
  },
  {
    "start": 604610,
    "end": 612846,
    "text": "バイグラムが辞書Bにない場合、デフォルトでゼロに1を足した値を返すようにしたい。"
  },
  {
    "start": 613028,
    "end": 618062,
    "text": "これは基本的に、すべてのバイグラムを合計し、その発生頻度をカウントする。"
  },
  {
    "start": 618206,
    "end": 626420,
    "text": "印刷をなくそう、いや、印刷はそのままにして、この場合のBが何であるかだけを検査しよう。"
  },
  {
    "start": 627030,
    "end": 630246,
    "text": "多くのビラムが一度だけ起こることがわかる。"
  },
  {
    "start": 630348,
    "end": 637698,
    "text": "これは3回起こったと言われているので、aは3回エンディング文字になったことになる。"
  },
  {
    "start": 637884,
    "end": 641100,
    "text": "エマ、オリビア、アバはすべてaで終わる。"
  },
  {
    "start": 641870,
    "end": 644380,
    "text": "だから、このようなことが3度も起きたのだ。"
  },
  {
    "start": 646590,
    "end": 648940,
    "text": "では、すべての単語についてやってみよう。"
  },
  {
    "start": 651250,
    "end": 653790,
    "text": "おっと、プリントするんじゃなかった。"
  },
  {
    "start": 655010,
    "end": 656590,
    "text": "消すつもりだったんだ。"
  },
  {
    "start": 656740,
    "end": 658240,
    "text": "殺そう。"
  },
  {
    "start": 658690,
    "end": 660398,
    "text": "とにかく走ろう。"
  },
  {
    "start": 660564,
    "end": 663854,
    "text": "これでBはデータセット全体の統計量を持つことになる。"
  },
  {
    "start": 664052,
    "end": 668190,
    "text": "これらは、個々のピラミッドの全単語にわたるカウントである。"
  },
  {
    "start": 668350,
    "end": 672340,
    "text": "例えば、最も一般的なものと最も一般的でないものを見てみよう。"
  },
  {
    "start": 673350,
    "end": 674766,
    "text": "Pythonのこの手のキモ。"
  },
  {
    "start": 674798,
    "end": 679410,
    "text": "私が好きな最もシンプルな方法は、bアイテムを使うことだ。"
  },
  {
    "start": 679570,
    "end": 690646,
    "text": "B項目はキーと値のタプルを返し、この場合、キーは文字のバイオグラムで、値はカウントである。"
  },
  {
    "start": 690838,
    "end": 709034,
    "text": "デフォルトでは、ソートはタプルの最初の項目で行われるが、タプルの2番目の要素である値でソートしたい。"
  },
  {
    "start": 709082,
    "end": 710240,
    "text": "それが重要な価値だ。"
  },
  {
    "start": 710610,
    "end": 723874,
    "text": "キー・イコール・ラムダを使いたい。このラムダは、キーの値を受け取り、キーの値をゼロではなく1に返す。"
  },
  {
    "start": 723992,
    "end": 732498,
    "text": "これらの要素の数でソートしたい。"
  },
  {
    "start": 732674,
    "end": 735362,
    "text": "ここにあるのはビグラムである。"
  },
  {
    "start": 735426,
    "end": 740522,
    "text": "Qとrは1回しか発生せず、dzは1回しか発生しない。"
  },
  {
    "start": 740656,
    "end": 746214,
    "text": "これを逆に並べ替えると、グラム別に最も可能性の高いものが表示される。"
  },
  {
    "start": 746342,
    "end": 751646,
    "text": "nは、何度も何度もエンディングの登場人物であったことがわかる。"
  },
  {
    "start": 751748,
    "end": 756960,
    "text": "nがaに続くことがほとんどで、それも可能性の高い組み合わせだ。"
  },
  {
    "start": 758690,
    "end": 764180,
    "text": "これは、データセット全体で達成された個々のカウントのようなものだ。"
  },
  {
    "start": 764950,
    "end": 773170,
    "text": "さて、この情報をpythonの辞書ではなく、2次元の配列に保持する方が、実はかなり便利になりそうです。"
  },
  {
    "start": 773590,
    "end": 785058,
    "text": "行は図の最初の文字、列は2番目の文字になる。"
  },
  {
    "start": 785154,
    "end": 792458,
    "text": "二次元配列の各エントリーは、データセットにおいて、最初の文字が二番目の文字に続く頻度を示す。"
  },
  {
    "start": 792624,
    "end": 798694,
    "text": "特に、これから使う配列表現、つまりライブラリーはPytorchのものだ。"
  },
  {
    "start": 798822,
    "end": 809390,
    "text": "Pytorchはディープラーニング・ニューラルネットワークのフレームワークだが、その一部にはこのトーチ・テンソルがあり、多次元配列を作成して非常に効率的に操作することができる。"
  },
  {
    "start": 809810,
    "end": 813970,
    "text": "Pytorch をインポートしてみよう。"
  },
  {
    "start": 814630,
    "end": 817246,
    "text": "であれば、配列を作ることができる。"
  },
  {
    "start": 817438,
    "end": 823934,
    "text": "ゼロの配列を作り、この配列のサイズを与える。"
  },
  {
    "start": 823982,
    "end": 826500,
    "text": "例として3×5の配列を作ってみよう。"
  },
  {
    "start": 827030,
    "end": 831266,
    "text": "これは3×5のゼロの配列である。"
  },
  {
    "start": 831458,
    "end": 836662,
    "text": "デフォルトでは、データ型の略であるDtypeがfloat 32であることに気づくだろう。"
  },
  {
    "start": 836716,
    "end": 841766,
    "text": "カウントを表現するため、これらの数値は単精度浮動小数点数である。"
  },
  {
    "start": 841878,
    "end": 845500,
    "text": "実際にDtypeを32のトーチ・ドットとして使ってみよう。"
  },
  {
    "start": 845950,
    "end": 849910,
    "text": "これらは32ビットの整数である。"
  },
  {
    "start": 850070,
    "end": 854190,
    "text": "これで、このテンソルの中に整数のデータがあることがわかっただろう。"
  },
  {
    "start": 854530,
    "end": 860570,
    "text": "テンソルを使うことで、個々のエントリーを本当に効率よく操作できるようになった。"
  },
  {
    "start": 860730,
    "end": 865678,
    "text": "例えば、このビットを変更したい場合、テンソルにインデックスを付けなければならない。"
  },
  {
    "start": 865774,
    "end": 872894,
    "text": "特にここでは、ゼロインデックスなので、これが最初の行である。"
  },
  {
    "start": 872942,
    "end": 878566,
    "text": "これは行インデックス1、列インデックス0123である。"
  },
  {
    "start": 878748,
    "end": 882840,
    "text": "コンマ3が1つなら、それを1にすればいい。"
  },
  {
    "start": 883610,
    "end": 886200,
    "text": "それなら、あそこに1本ある。"
  },
  {
    "start": 887050,
    "end": 889190,
    "text": "もちろん、こんなこともできる。"
  },
  {
    "start": 889260,
    "end": 893594,
    "text": "今、あそこにいるのは2人か3人だろう。"
  },
  {
    "start": 893792,
    "end": 899946,
    "text": "また、例えばゼロを5とすると、aはこっちの5となる。"
  },
  {
    "start": 900128,
    "end": 903110,
    "text": "これが、配列にインデックスを付ける方法だ。"
  },
  {
    "start": 903270,
    "end": 906138,
    "text": "もちろん、私たちが興味を持っているアレイは、もっともっと大きなものだ。"
  },
  {
    "start": 906234,
    "end": 913854,
    "text": "アルファベット26文字と、2つの特殊文字、sとeがある。"
  },
  {
    "start": 914052,
    "end": 919070,
    "text": "26プラス2、つまり28×28の配列が欲しい。"
  },
  {
    "start": 919230,
    "end": 923490,
    "text": "大文字の \"n \"と呼ぶことにしよう。"
  },
  {
    "start": 924470,
    "end": 925960,
    "text": "これを消させてくれ。"
  },
  {
    "start": 926650,
    "end": 930198,
    "text": "これはゼロから始まる配列で、28×28。"
  },
  {
    "start": 930364,
    "end": 934390,
    "text": "では、これをここにコピーペーストしてみよう。"
  },
  {
    "start": 934540,
    "end": 940060,
    "text": "これから消去する辞書bの代わりに、nがある。"
  },
  {
    "start": 940990,
    "end": 951270,
    "text": "さて、ここでの問題は、文字列であるこれらの文字があるにもかかわらず、基本的に配列にインデックスを付けなければならず、整数を使用してインデックスを付けなければならないことだ。"
  },
  {
    "start": 951350,
    "end": 955018,
    "text": "文字から整数へのルックアップテーブルのようなものが必要だ。"
  },
  {
    "start": 955194,
    "end": 957738,
    "text": "このような文字配列を作ってみよう。"
  },
  {
    "start": 957914,
    "end": 962538,
    "text": "この方法は、文字列のリストであるすべての単語を取り出します。"
  },
  {
    "start": 962714,
    "end": 965534,
    "text": "そのすべてを連結して巨大な文字列にする。"
  },
  {
    "start": 965582,
    "end": 968610,
    "text": "これは、単にデータセット全体を1つの文字列としたものである。"
  },
  {
    "start": 969190,
    "end": 978690,
    "text": "セット・コンストラクタはこの巨大な文字列を受け取り、重複した文字列を捨てる。"
  },
  {
    "start": 978850,
    "end": 986760,
    "text": "この集合は、すべての小文字の集合となり、全部で26個あるはずだ。"
  },
  {
    "start": 988490,
    "end": 995706,
    "text": "しかし、奇妙な任意の方法でソートされたリストはいらない。"
  },
  {
    "start": 995728,
    "end": 998860,
    "text": "aからzまで並べ替えたい。"
  },
  {
    "start": 999790,
    "end": 1001280,
    "text": "ソートされたリスト。"
  },
  {
    "start": 1001810,
    "end": 1003550,
    "text": "それが私たちのキャラクターだ。"
  },
  {
    "start": 1005490,
    "end": 1007870,
    "text": "さて、私たちが欲しいのは、前述したようにこのルックアップテーブルである。"
  },
  {
    "start": 1007940,
    "end": 1010654,
    "text": "に特別なSを作ろう。"
  },
  {
    "start": 1010692,
    "end": 1015438,
    "text": "ここではsを文字列または文字と呼ぶことにする。"
  },
  {
    "start": 1015534,
    "end": 1023490,
    "text": "これは、これらの文字の列挙におけるisに対するsからIへのマッピングとなる。"
  },
  {
    "start": 1024230,
    "end": 1031894,
    "text": "enumerateは基本的に、整数インデックスとリストの実際の要素に対するイテレータを与える。"
  },
  {
    "start": 1032012,
    "end": 1034790,
    "text": "であれば、文字を整数にマッピングしていることになる。"
  },
  {
    "start": 1035130,
    "end": 1042060,
    "text": "sからIへのマッピングは、aから0、bから1など、zから25までのマッピングである。"
  },
  {
    "start": 1044030,
    "end": 1045434,
    "text": "それはここで役に立つだろう。"
  },
  {
    "start": 1045472,
    "end": 1055040,
    "text": "zが25だったのだから、sが26になり、eのsからIが27になるように設定しなければならない。"
  },
  {
    "start": 1055890,
    "end": 1057498,
    "text": "これがルックアップだ。"
  },
  {
    "start": 1057594,
    "end": 1062634,
    "text": "ここに来て、文字1と文字2をそれぞれの整数にマッピングすることができる。"
  },
  {
    "start": 1062762,
    "end": 1068580,
    "text": "これは文字1のS2 Iとなり、ix 2は文字2のS2 Iとなる。"
  },
  {
    "start": 1069270,
    "end": 1074446,
    "text": "これで、この行を配列を使って実行できるはずだ。"
  },
  {
    "start": 1074558,
    "end": 1077222,
    "text": "Nはix 1, ix 2."
  },
  {
    "start": 1077356,
    "end": 1080390,
    "text": "これは以前紹介した2次元配列のインデックスだ。"
  },
  {
    "start": 1080540,
    "end": 1085560,
    "text": "正直に言って、プラスイコール1だ。"
  },
  {
    "start": 1086170,
    "end": 1094390,
    "text": "この配列は、28×28の大きな配列で、すべてのカウントを得ることができる。"
  },
  {
    "start": 1094470,
    "end": 1097894,
    "text": "nと表示すれば、これが配列となる。"
  },
  {
    "start": 1097942,
    "end": 1099750,
    "text": "もちろん、見た目は醜い。"
  },
  {
    "start": 1099830,
    "end": 1104650,
    "text": "この醜い混乱を消し去り、もう少しきれいにイメージしてみよう。"
  },
  {
    "start": 1104810,
    "end": 1108350,
    "text": "そのためにMatplotlibというライブラリを使う。"
  },
  {
    "start": 1108690,
    "end": 1114930,
    "text": "Matplotlibは、カウント配列のpltimショーのようなことができるように、図を作成することができます。"
  },
  {
    "start": 1116070,
    "end": 1120878,
    "text": "これは28×28の配列で、これは構造体である。"
  },
  {
    "start": 1120974,
    "end": 1123710,
    "text": "これでもまだ、かなり醜いと言える。"
  },
  {
    "start": 1123870,
    "end": 1127074,
    "text": "私たちは、それをもっときれいにビジュアル化しようと思っています。"
  },
  {
    "start": 1127112,
    "end": 1129080,
    "text": "そのためにたくさんのコードを書いた。"
  },
  {
    "start": 1129690,
    "end": 1136498,
    "text": "まず必要なのは、この配列、つまり辞書を反転させることだ。"
  },
  {
    "start": 1136594,
    "end": 1142962,
    "text": "s to IはsからIへのマッピングで、I to Sではこの辞書を逆にする。"
  },
  {
    "start": 1143026,
    "end": 1146422,
    "text": "全アイテムのイテレータを作成し、その配列を反転させる。"
  },
  {
    "start": 1146566,
    "end": 1152030,
    "text": "I two sは、0から1へ、1からbへと反比例する。"
  },
  {
    "start": 1152530,
    "end": 1153966,
    "text": "それが必要だ"
  },
  {
    "start": 1154148,
    "end": 1158590,
    "text": "次に、これをもう少しきれいにしようと思いついたコードがこれだ。"
  },
  {
    "start": 1160450,
    "end": 1167154,
    "text": "図を作り、nをプロットし、後でいろいろなことを視覚化する。"
  },
  {
    "start": 1167272,
    "end": 1169860,
    "text": "これがどういうものかを理解してもらうために、ちょっと走らせてみよう。"
  },
  {
    "start": 1171910,
    "end": 1172706,
    "text": "いいかい？"
  },
  {
    "start": 1172888,
    "end": 1184520,
    "text": "配列が間隔をあけて配置されているのがおわかりだろうか。"
  },
  {
    "start": 1185130,
    "end": 1187718,
    "text": "aはjに175回続く。"
  },
  {
    "start": 1187884,
    "end": 1196220,
    "text": "ここでやっていることは、まず配列全体を表示し、次に個々のセルを繰り返し処理することだ。"
  },
  {
    "start": 1196670,
    "end": 1204526,
    "text": "これは整数Iと整数jの逆写像Iからsへの文字列である。"
  },
  {
    "start": 1204628,
    "end": 1207950,
    "text": "これらは文字表現におけるビグラムである。"
  },
  {
    "start": 1208530,
    "end": 1215838,
    "text": "次に、ビグラム・テキストだけをプロットし、このビラムが出現する回数をプロットする。"
  },
  {
    "start": 1216014,
    "end": 1222782,
    "text": "さて、ここにドット・アイテムがあるのは、これらの配列にインデックスを付けると、これらはトーチ・テンソルになるからだ。"
  },
  {
    "start": 1222926,
    "end": 1225810,
    "text": "テンソルが戻ってくるのがわかるだろう。"
  },
  {
    "start": 1225960,
    "end": 1231590,
    "text": "この型を見ると、ただの整数149だと思うだろうが、実際はトーチテンソルだ。"
  },
  {
    "start": 1231930,
    "end": 1240394,
    "text": "つまり、ドット・アイテムを実行すると、個々の整数がポップ・アウトされ、149となる。"
  },
  {
    "start": 1240592,
    "end": 1242266,
    "text": "そこで起きていることだ。"
  },
  {
    "start": 1242368,
    "end": 1244780,
    "text": "これらは見栄えを良くするためのオプションに過ぎない。"
  },
  {
    "start": 1245150,
    "end": 1247210,
    "text": "この配列の構造は？"
  },
  {
    "start": 1249070,
    "end": 1253738,
    "text": "私たちはこれらのカウントをすべて把握しており、その中には頻繁に発生するものもあれば、そうでないものもある。"
  },
  {
    "start": 1253904,
    "end": 1258378,
    "text": "さて、これを注意深く観察してみると、実はあまり賢くないことに気づくだろう。"
  },
  {
    "start": 1258554,
    "end": 1264506,
    "text": "というのも、ここに来ると、例えば、完全にゼロの行がまるまる1行あることに気づくだろう。"
  },
  {
    "start": 1264618,
    "end": 1273810,
    "text": "というのも、ByGramの末尾には常に末尾トークンを置くので、末尾文字がByGraMの最初の文字になることはありえないからだ。"
  },
  {
    "start": 1274310,
    "end": 1287430,
    "text": "同様に、S文字がビグラムの2番目の要素になることはありえないので、ここではゼロの列全体を持っている。"
  },
  {
    "start": 1287580,
    "end": 1291558,
    "text": "列全体がゼロ、行全体がゼロ。"
  },
  {
    "start": 1291654,
    "end": 1298410,
    "text": "この小さな2×2の行列でも、起こりうるのはSがeに直接続く場合だけである。"
  },
  {
    "start": 1298560,
    "end": 1302854,
    "text": "文字を持たない単語がある場合、それはゼロではない可能性がある。"
  },
  {
    "start": 1302982,
    "end": 1304686,
    "text": "その場合、単語には文字がない。"
  },
  {
    "start": 1304708,
    "end": 1309918,
    "text": "これは空っぽの単語で、eの後にsが続くだけだが、他の単語はありえない。"
  },
  {
    "start": 1310084,
    "end": 1311630,
    "text": "だから、基本的にスペースを無駄にしている。"
  },
  {
    "start": 1311700,
    "end": 1315410,
    "text": "それだけでなく、ここではsとeが非常に混雑している。"
  },
  {
    "start": 1315560,
    "end": 1322990,
    "text": "自然言語処理では、特殊なトークンを示すためにこのような括弧を使う慣例があるからだ。"
  },
  {
    "start": 1323150,
    "end": 1324898,
    "text": "他のものを使おう"
  },
  {
    "start": 1325064,
    "end": 1327810,
    "text": "全部直して、もっときれいにしよう。"
  },
  {
    "start": 1328150,
    "end": 1330418,
    "text": "特別なトークンを2つ用意するつもりはない。"
  },
  {
    "start": 1330434,
    "end": 1332630,
    "text": "特別なトークンは1つだけだ。"
  },
  {
    "start": 1332970,
    "end": 1336454,
    "text": "n×nの27個の配列になる。"
  },
  {
    "start": 1336492,
    "end": 1343450,
    "text": "セット27では、2つある代わりに1つだけにして、それをドットと呼ぶことにする。"
  },
  {
    "start": 1344670,
    "end": 1349180,
    "text": "よし、これをこっちに振ってみよう。"
  },
  {
    "start": 1350350,
    "end": 1355998,
    "text": "さて、もうひとつやりたいことは、この特別な文字をハーフ・ポジション・ゼロにすることだ。"
  },
  {
    "start": 1356164,
    "end": 1358910,
    "text": "他のすべての文字をオフセットしたい。"
  },
  {
    "start": 1358980,
    "end": 1361470,
    "text": "その方が少し嬉しい。"
  },
  {
    "start": 1362610,
    "end": 1369220,
    "text": "ここでプラス1が必要なのは、最初の文字であるaが1から始まるようにするためである。"
  },
  {
    "start": 1369750,
    "end": 1377474,
    "text": "s2は、今からaになり、1から始まり、ドットはゼロで、私はsを二つ持っています。"
  },
  {
    "start": 1377512,
    "end": 1382198,
    "text": "もちろん、逆マッピングを作成するだけなので、これは変更しない。"
  },
  {
    "start": 1382284,
    "end": 1385590,
    "text": "1がa、2がb、0がドット。"
  },
  {
    "start": 1386410,
    "end": 1387946,
    "text": "我々はそれを覆した。"
  },
  {
    "start": 1388048,
    "end": 1391770,
    "text": "ここには点と点がある。"
  },
  {
    "start": 1392910,
    "end": 1394474,
    "text": "これでうまくいくはずだ。"
  },
  {
    "start": 1394672,
    "end": 1398506,
    "text": "ゼロカウントから始めることを確認する。"
  },
  {
    "start": 1398688,
    "end": 1400446,
    "text": "となると、ここでは28歳まで。"
  },
  {
    "start": 1400468,
    "end": 1401840,
    "text": "27歳まで上がる。"
  },
  {
    "start": 1402530,
    "end": 1410606,
    "text": "これでうまくいくはずだ。"
  },
  {
    "start": 1410788,
    "end": 1413442,
    "text": "ドット・ドットはなかったことになった。"
  },
  {
    "start": 1413496,
    "end": 1416226,
    "text": "空虚な言葉がないからゼロなんだ。"
  },
  {
    "start": 1416408,
    "end": 1423422,
    "text": "それから、この行は単純にすべての最初の文字を数えたものである。"
  },
  {
    "start": 1423566,
    "end": 1429394,
    "text": "jで始まる単語、hで始まる単語、Iで始まる単語、等々。"
  },
  {
    "start": 1429522,
    "end": 1432738,
    "text": "ということは、これらはすべてエンディング・キャラクターということになる。"
  },
  {
    "start": 1432914,
    "end": 1436806,
    "text": "その間に、登場人物が互いにフォローし合う構造がある。"
  },
  {
    "start": 1436988,
    "end": 1441594,
    "text": "これはデータセット全体のカウント配列である。"
  },
  {
    "start": 1441712,
    "end": 1449434,
    "text": "この配列は、実際にこのビグラム文字レベルの言語モデルからサンプリングするために必要なすべての情報を持っている。"
  },
  {
    "start": 1449632,
    "end": 1458558,
    "text": "大雑把に言えば、これからやろうとしていることは、これらの確率とカウントに従って、モデルからサンプリングを始めるということだ。"
  },
  {
    "start": 1458724,
    "end": 1464454,
    "text": "冒頭では、もちろんドット、スタートトークンドットから始める。"
  },
  {
    "start": 1464602,
    "end": 1469860,
    "text": "名前の最初の文字をサンプリングするために、この行を見ている。"
  },
  {
    "start": 1470470,
    "end": 1472654,
    "text": "我々はカウントを持っている。"
  },
  {
    "start": 1472702,
    "end": 1479000,
    "text": "これらのカウントは、これらの文字のどれかが単語を始める頻度を示している。"
  },
  {
    "start": 1479450,
    "end": 1493210,
    "text": "このn行で最初の行を取得する場合、ゼロでインデックスを作成し、その行の残りの行にコロンという表記を使えばいい。"
  },
  {
    "start": 1493630,
    "end": 1501550,
    "text": "n個のゼロ列は、ゼロ行にインデックスを付け、すべての列をつかむ。"
  },
  {
    "start": 1501890,
    "end": 1506186,
    "text": "これで1行目の1次元配列が得られる。"
  },
  {
    "start": 1506218,
    "end": 1507454,
    "text": "ゼロ 4410 410"
  },
  {
    "start": 1507492,
    "end": 1512986,
    "text": "1306-1542など。"
  },
  {
    "start": 1513018,
    "end": 1514202,
    "text": "最初の列だけだ。"
  },
  {
    "start": 1514346,
    "end": 1517010,
    "text": "この形は27だ。"
  },
  {
    "start": 1517080,
    "end": 1518820,
    "text": "27人並んでいるだけだ。"
  },
  {
    "start": 1519750,
    "end": 1526130,
    "text": "もう1つの方法は、実際にこれを与える必要はなく、このようにゼロ行をつかむだけです。"
  },
  {
    "start": 1526200,
    "end": 1527490,
    "text": "これは同等である。"
  },
  {
    "start": 1528070,
    "end": 1529762,
    "text": "これがカウントだ。"
  },
  {
    "start": 1529906,
    "end": 1534726,
    "text": "さて、私たちがやりたいことは、基本的にここからサンプルを採取することだ。"
  },
  {
    "start": 1534908,
    "end": 1538490,
    "text": "これは生カウントなので、実際にはこれを確率に変換しなければならない。"
  },
  {
    "start": 1539070,
    "end": 1541850,
    "text": "確率ベクトルを作成する。"
  },
  {
    "start": 1542750,
    "end": 1548940,
    "text": "nをゼロとし、まずこれをfloatに変換する。"
  },
  {
    "start": 1549870,
    "end": 1553934,
    "text": "さて、これらの整数は浮動小数点数に変換される。"
  },
  {
    "start": 1554052,
    "end": 1558586,
    "text": "floatを作成しているのは、これらのカウントを正規化するためだ。"
  },
  {
    "start": 1558778,
    "end": 1567010,
    "text": "ここで確率分布を作るには、基本的にp÷pの和を割りたい。"
  },
  {
    "start": 1569510,
    "end": 1572018,
    "text": "これで、より小さな数のベクトルが得られる。"
  },
  {
    "start": 1572104,
    "end": 1573646,
    "text": "これらはもはや確率である。"
  },
  {
    "start": 1573758,
    "end": 1578610,
    "text": "もちろん、和で割ったのだから、pの和は1になる。"
  },
  {
    "start": 1578760,
    "end": 1581042,
    "text": "これはいい確率分布だ。"
  },
  {
    "start": 1581106,
    "end": 1582166,
    "text": "それは1つに集約される。"
  },
  {
    "start": 1582268,
    "end": 1587400,
    "text": "これは、任意の1文字が単語の最初の文字になる確率を示している。"
  },
  {
    "start": 1587850,
    "end": 1590582,
    "text": "では、この分布からサンプリングしてみよう。"
  },
  {
    "start": 1590726,
    "end": 1595660,
    "text": "これらの分布から標本化するために、トーチ多項式を使います。"
  },
  {
    "start": 1596030,
    "end": 1611338,
    "text": "torch multinomialは、多項確率分布からのサンプルを返す。これは、確率をくれれば、確率分布に従ってサンプリングされた整数を返すという複雑な言い方である。"
  },
  {
    "start": 1611514,
    "end": 1613130,
    "text": "これはメソッドのシグネチャーである。"
  },
  {
    "start": 1613210,
    "end": 1618530,
    "text": "すべてを決定論的にするために、Pytorchのジェネレーター・オブジェクトを使う。"
  },
  {
    "start": 1619190,
    "end": 1621006,
    "text": "これですべてが決定論的になる。"
  },
  {
    "start": 1621038,
    "end": 1626600,
    "text": "これをあなたのコンピューターで実行すると、私のコンピューターで私が得ているのとまったく同じ結果が得られる。"
  },
  {
    "start": 1627210,
    "end": 1629160,
    "text": "この仕組みをお見せしよう。"
  },
  {
    "start": 1632650,
    "end": 1641018,
    "text": "ここでは、決定論的な方法でトーチ・ジェネレーター・オブジェクトを作成し、私たちが合意できる数字を播種する。"
  },
  {
    "start": 1641184,
    "end": 1644682,
    "text": "ジェネレーターの種はオブジェクトgを与える。"
  },
  {
    "start": 1644816,
    "end": 1659150,
    "text": "このgを乱数を生成する関数に渡せば、torque, randは3つの乱数を生成し、このジェネレーター・オブジェクトを乱数のソースとして使用する。"
  },
  {
    "start": 1660370,
    "end": 1671138,
    "text": "正規化することなく、0と1の間のランダムな数字を表示することができる。"
  },
  {
    "start": 1671224,
    "end": 1678200,
    "text": "なぜなら、同じジェネレーター・オブジェクトを使い続けているからだ。"
  },
  {
    "start": 1678650,
    "end": 1686710,
    "text": "ならば、正規化するために割り算をすれば、たった3つの要素からなる素晴らしい確率分布が得られるだろう。"
  },
  {
    "start": 1687450,
    "end": 1691034,
    "text": "であれば、torchat multinomialを使って、そこからサンプルを抽出することができる。"
  },
  {
    "start": 1691152,
    "end": 1692860,
    "text": "これがその姿だ。"
  },
  {
    "start": 1693550,
    "end": 1700090,
    "text": "トーチ多項式は確率分布のトーチテンソルを取る。"
  },
  {
    "start": 1700910,
    "end": 1703600,
    "text": "そして、サンプルの数、例えば20を要求することができる。"
  },
  {
    "start": 1704450,
    "end": 1715586,
    "text": "置換が真に等しいということは、ある要素を描画するときに、その要素を描画し、再び描画するために、その要素を対象となるインデックスのリストに戻すことができるということである。"
  },
  {
    "start": 1715768,
    "end": 1721358,
    "text": "というのも、デフォルトではなぜかfalseになっているからだ。"
  },
  {
    "start": 1721534,
    "end": 1725426,
    "text": "ただ、気をつけるべきことだと思う。"
  },
  {
    "start": 1725608,
    "end": 1727422,
    "text": "ジェネレーターはここに渡される。"
  },
  {
    "start": 1727496,
    "end": 1731170,
    "text": "我々は常に決定論的な結果、同じ結果を得ることになる。"
  },
  {
    "start": 1731250,
    "end": 1736630,
    "text": "この2つを実行すれば、この分布からたくさんのサンプルが得られることになる。"
  },
  {
    "start": 1737050,
    "end": 1743900,
    "text": "ここで、このテンソルの最初の要素の確率が60％であることに気づくだろう。"
  },
  {
    "start": 1744430,
    "end": 1753600,
    "text": "この20のサンプルのうち、60％はゼロ、30％は1になると予想される。"
  },
  {
    "start": 1754130,
    "end": 1761934,
    "text": "というのも、要素インデックス2が10％の確率しかないからである。"
  },
  {
    "start": 1762052,
    "end": 1767700,
    "text": "実際、私たちが持っているツーは少ないし、好きなだけ試せる。"
  },
  {
    "start": 1768870,
    "end": 1775060,
    "text": "サンプリングすればするほど、これらの数値はだいたいこの分布になるはずだ。"
  },
  {
    "start": 1775850,
    "end": 1791438,
    "text": "ゼロが多く、1が半分、1がその3倍、2がその3倍でなければならない。"
  },
  {
    "start": 1791634,
    "end": 1795594,
    "text": "というのも、2がほとんどなく、1がいくつかあり、そのほとんどが0だからだ。"
  },
  {
    "start": 1795712,
    "end": 1800060,
    "text": "トーション・マルチノミルは、それを実現しているのだ。"
  },
  {
    "start": 1801010,
    "end": 1802426,
    "text": "私たちはこの列に興味がある。"
  },
  {
    "start": 1802458,
    "end": 1808640,
    "text": "このPを作成し、そこからサンプリングすることができる。"
  },
  {
    "start": 1809570,
    "end": 1816690,
    "text": "同じ種を使い、この分布からサンプリングした場合、1つのサンプルだけを得よう。"
  },
  {
    "start": 1818150,
    "end": 1820930,
    "text": "すると、サンプルは例えば13であることがわかる。"
  },
  {
    "start": 1822630,
    "end": 1824690,
    "text": "これがインデックスとなる。"
  },
  {
    "start": 1825110,
    "end": 1828562,
    "text": "が13を包むテンソルであることがわかるだろう。"
  },
  {
    "start": 1828706,
    "end": 1832690,
    "text": "この整数を取り出すには、またしてもドット・アイテムを使わなければならない。"
  },
  {
    "start": 1832850,
    "end": 1836150,
    "text": "今のインデックスは13という数字だけだろう。"
  },
  {
    "start": 1837290,
    "end": 1846042,
    "text": "もちろん、ここでどの文字をサンプリングしているかを正確に把握するために、ixのIとSをマッピングすることもできる。"
  },
  {
    "start": 1846096,
    "end": 1847260,
    "text": "Mをサンプリングしている。"
  },
  {
    "start": 1847950,
    "end": 1852826,
    "text": "私たちの世代では、最初のキャラクターはMだと言っているんだ。"
  },
  {
    "start": 1853018,
    "end": 1859600,
    "text": "この行を見ただけで、Mが描かれ、Mが実際に多くの単語を始めていることがわかる。"
  },
  {
    "start": 1860050,
    "end": 1864626,
    "text": "Mは32,000語のうち2500語から始めた。"
  },
  {
    "start": 1864728,
    "end": 1868978,
    "text": "Mで始まる単語は全体の10％弱である。"
  },
  {
    "start": 1869064,
    "end": 1871940,
    "text": "これは実際、かなり描きそうなキャラクターだった。"
  },
  {
    "start": 1875110,
    "end": 1877054,
    "text": "これは単語の最初の文字になる。"
  },
  {
    "start": 1877112,
    "end": 1884514,
    "text": "というのも、Mはすでにサンプリングされているからだ。"
  },
  {
    "start": 1884642,
    "end": 1892490,
    "text": "次の文字を描くために、ここに戻ってMで始まる行を探そう。"
  },
  {
    "start": 1892640,
    "end": 1896426,
    "text": "Mを見て、私たちはここに列を作っている。"
  },
  {
    "start": 1896608,
    "end": 1903814,
    "text": "M、ドットは516、maはこの数、MBはこの数、等々。"
  },
  {
    "start": 1903862,
    "end": 1905510,
    "text": "これが次の行のカウントである。"
  },
  {
    "start": 1905590,
    "end": 1908442,
    "text": "これが次に生成するキャラクターだ。"
  },
  {
    "start": 1908586,
    "end": 1914100,
    "text": "ループを書き出す準備はできていると思う。"
  },
  {
    "start": 1916150,
    "end": 1921010,
    "text": "それがスタート・トークンだからだ。"
  },
  {
    "start": 1922230,
    "end": 1929750,
    "text": "trueの間、現在いるインデックスに対応する行を取得する。"
  },
  {
    "start": 1929820,
    "end": 1932914,
    "text": "これがp、これがnの配列だ。"
  },
  {
    "start": 1932962,
    "end": 1937080,
    "text": "ixをfloatに変換したものがpである。"
  },
  {
    "start": 1939070,
    "end": 1943180,
    "text": "そして、このpの和が1になるように正規化する。"
  },
  {
    "start": 1945230,
    "end": 1947942,
    "text": "誤って無限ループに陥ってしまった。"
  },
  {
    "start": 1948086,
    "end": 1950426,
    "text": "pの和が1になるように正規化する。"
  },
  {
    "start": 1950608,
    "end": 1955566,
    "text": "次に、このジェネレーター・オブジェクトを初期化する。"
  },
  {
    "start": 1955668,
    "end": 1958350,
    "text": "この分布から1つのサンプルを抽出する。"
  },
  {
    "start": 1960770,
    "end": 1965060,
    "text": "となると、次はどのようなインデックスになるのかがわかることになる。"
  },
  {
    "start": 1966470,
    "end": 1971890,
    "text": "もしサンプリングされたインデックスがゼロなら、その時点でエンドトークンとなる。"
  },
  {
    "start": 1972630,
    "end": 1974100,
    "text": "我々は壊れる。"
  },
  {
    "start": 1975290,
    "end": 1979750,
    "text": "そうでなければ、s、ixの2つのⅠを印刷することになる。"
  },
  {
    "start": 1982170,
    "end": 1984230,
    "text": "I、9のSが2つ。"
  },
  {
    "start": 1985290,
    "end": 1987720,
    "text": "それだけだ。"
  },
  {
    "start": 1988430,
    "end": 1989660,
    "text": "これでうまくいくはずだ。"
  },
  {
    "start": 1990270,
    "end": 1991420,
    "text": "よし、もっとだ。"
  },
  {
    "start": 1992030,
    "end": 1994678,
    "text": "というのが、私たちがサンプリングした名前だ。"
  },
  {
    "start": 1994774,
    "end": 1996490,
    "text": "mから始めた。"
  },
  {
    "start": 1996560,
    "end": 1999690,
    "text": "その次はo、そしてr、そしてdot。"
  },
  {
    "start": 2001390,
    "end": 2004666,
    "text": "このドットもここに印刷した。"
  },
  {
    "start": 2004848,
    "end": 2008160,
    "text": "何回もやるのはやめよう。"
  },
  {
    "start": 2009890,
    "end": 2015060,
    "text": "ここで実際にアウトリストを作ってみよう。"
  },
  {
    "start": 2017030,
    "end": 2022290,
    "text": "印字する代わりに、この文字を追加する。"
  },
  {
    "start": 2024310,
    "end": 2026930,
    "text": "では、最後に印刷しましょう。"
  },
  {
    "start": 2027000,
    "end": 2031222,
    "text": "アウトを全部繋ぎ合わせて、増刷しよう。"
  },
  {
    "start": 2031356,
    "end": 2034866,
    "text": "ジェネレーターのせいで、いつも同じ結果になる。"
  },
  {
    "start": 2035058,
    "end": 2040586,
    "text": "これを数回繰り返したければ、10レンジでIを狙えばいい。"
  },
  {
    "start": 2040688,
    "end": 2044300,
    "text": "10人の名前をサンプリングして、それを10回繰り返せばいい。"
  },
  {
    "start": 2045550,
    "end": 2047820,
    "text": "これが、私たちが公表している名前です。"
  },
  {
    "start": 2048430,
    "end": 2049660,
    "text": "20回やろう。"
  },
  {
    "start": 2054190,
    "end": 2056282,
    "text": "正直に言うと、これはおかしい。"
  },
  {
    "start": 2056416,
    "end": 2060254,
    "text": "私は数分間それを見つめ、本当に正しいのだと確信した。"
  },
  {
    "start": 2060452,
    "end": 2067410,
    "text": "これらのサンプルがとてもひどいのは、ビグラム言語モデルが実は本当にひどいからだ。"
  },
  {
    "start": 2067750,
    "end": 2077620,
    "text": "ここでもう少し生成してみると、ヤヌ・オライリーなどのような名前だが、完全にめちゃくちゃなのがわかるだろう。"
  },
  {
    "start": 2078550,
    "end": 2086422,
    "text": "つまり、これが悪いのは、名前としてhを生成しているが、モデルの目線で考えなければならないからだ。"
  },
  {
    "start": 2086556,
    "end": 2089366,
    "text": "このHが最初のHだとは知らない。"
  },
  {
    "start": 2089468,
    "end": 2092006,
    "text": "知っているのは、Hが以前はそうだったということだけだ。"
  },
  {
    "start": 2092118,
    "end": 2095542,
    "text": "さて、Hが最後の登場人物になる可能性は？"
  },
  {
    "start": 2095686,
    "end": 2097670,
    "text": "まあ、その可能性は高い。"
  },
  {
    "start": 2097750,
    "end": 2103774,
    "text": "だから、ただ最後のキャラクターを作るだけで、その前に他のものがあったとか、その前に他のものがなかったとかは知らない。"
  },
  {
    "start": 2103892,
    "end": 2107930,
    "text": "だから、このような無意味な名前が生成されるのだ。"
  },
  {
    "start": 2108090,
    "end": 2115930,
    "text": "もう一つの方法は、とてもひどいことではあるが、実際には合理的なことをしているのだと自分に思い込ませることだ。"
  },
  {
    "start": 2116090,
    "end": 2120754,
    "text": "この小さな欠片は27個だよね？"
  },
  {
    "start": 2120952,
    "end": 2122260,
    "text": "全長27。"
  },
  {
    "start": 2123030,
    "end": 2125140,
    "text": "こういうのはどうだろう？"
  },
  {
    "start": 2126170,
    "end": 2136200,
    "text": "pが何らかの構造を持つ代わりに、pが27のトーチドットのものだけだったらどうだろう？"
  },
  {
    "start": 2137130,
    "end": 2139034,
    "text": "デフォルトでは、これは浮動小数点数32である。"
  },
  {
    "start": 2139072,
    "end": 2139994,
    "text": "これでいい。"
  },
  {
    "start": 2140112,
    "end": 2141420,
    "text": "27を分ける。"
  },
  {
    "start": 2142750,
    "end": 2149130,
    "text": "私がここでやっているのは、これが一様分布であり、すべての可能性を等しくすることである。"
  },
  {
    "start": 2149790,
    "end": 2151754,
    "text": "私たちはそこからサンプリングすることができる。"
  },
  {
    "start": 2151872,
    "end": 2153680,
    "text": "これでうまくいくかどうか見てみよう。"
  },
  {
    "start": 2154210,
    "end": 2160506,
    "text": "つまり、これは完全に訓練されていないモデルから得られたもので、すべての可能性が等しいということだ。"
  },
  {
    "start": 2160618,
    "end": 2162218,
    "text": "明らかにゴミだ。"
  },
  {
    "start": 2162314,
    "end": 2168354,
    "text": "ということは、グラム数だけで学習させた学習済みモデルがあれば、こうなる。"
  },
  {
    "start": 2168472,
    "end": 2170574,
    "text": "より \"namelike \"になっているのがわかるだろう。"
  },
  {
    "start": 2170622,
    "end": 2171762,
    "text": "実際に機能している。"
  },
  {
    "start": 2171896,
    "end": 2176866,
    "text": "ただ、グラムがあまりにひどいから、我々は今、もっとうまくやらなければならない。"
  },
  {
    "start": 2176888,
    "end": 2188054,
    "text": "というのも、ここでやっていることは、常に前方のカウント行列からnの行をフェッチし、そして常に同じことをやっているからだ。"
  },
  {
    "start": 2188092,
    "end": 2195066,
    "text": "floatに変換し、除算し、このループを毎回繰り返し、これらの行を何度も何度も再正規化しているのだ。"
  },
  {
    "start": 2195088,
    "end": 2197174,
    "text": "極めて非効率的で無駄が多い。"
  },
  {
    "start": 2197302,
    "end": 2203818,
    "text": "私がやりたいのは、実際に確率だけを入れた資本Pの行列を用意することだ。"
  },
  {
    "start": 2203904,
    "end": 2220660,
    "text": "言い換えれば、これはカウントの大文字n行列と同じになるが、どの行にも1に正規化された確率の行があり、どの行にいるかによって定義されるように、その前の文字が与えられたときの次の文字の確率分布を示す。"
  },
  {
    "start": 2221510,
    "end": 2227700,
    "text": "厳密に言うと、僕らがやりたいのは、この列を \"FRONT HERe \"で作って、その列を \"Here \"で使うことなんだ。"
  },
  {
    "start": 2228090,
    "end": 2232440,
    "text": "ここでは、PイコールixのPとしたい。"
  },
  {
    "start": 2232890,
    "end": 2233640,
    "text": "オーケー。"
  },
  {
    "start": 2234810,
    "end": 2246778,
    "text": "もう1つの理由は、効率のためだけでなく、このn次元テンソルを練習し、その操作、特にブロードキャストと呼ばれるものを練習してほしいからだ。"
  },
  {
    "start": 2246944,
    "end": 2257226,
    "text": "というのも、トランスフォーマーまで作り上げるとなると、効率のためにかなり複雑な配列操作をすることになるからだ。"
  },
  {
    "start": 2257338,
    "end": 2260720,
    "text": "そのことを本当に理解し、得意になる必要がある。"
  },
  {
    "start": 2262050,
    "end": 2270180,
    "text": "直感的に言うと、まずNの浮動小数点コピーを取得したい。"
  },
  {
    "start": 2270790,
    "end": 2275542,
    "text": "であれば、THEY SUmが1になるようにすべての行を分割したい。"
  },
  {
    "start": 2275676,
    "end": 2277302,
    "text": "こんなことをやってみたいんだ。"
  },
  {
    "start": 2277356,
    "end": 2279350,
    "text": "P÷Pドットサム。"
  },
  {
    "start": 2280570,
    "end": 2286870,
    "text": "ここで注意しなければならないのは、pの和が実際に和を生成してしまうことだ。"
  },
  {
    "start": 2288010,
    "end": 2301050,
    "text": "申し訳ありませんが、Pイコールn float copy p dot sumは、この行列n全体のカウントのすべてを合計し、すべての総和JUStの単一の番号を与える生成します。"
  },
  {
    "start": 2301200,
    "end": 2303482,
    "text": "それは私たちが望んでいる分け方ではない。"
  },
  {
    "start": 2303546,
    "end": 2309710,
    "text": "同時に、並行して、すべての行をそれぞれの合計で割りたい。"
  },
  {
    "start": 2310610,
    "end": 2327002,
    "text": "この定義では、合計したい入力配列を指定するだけでなく、合計する次元も指定します。"
  },
  {
    "start": 2327166,
    "end": 2330994,
    "text": "特に、行に渡って合計したい。"
  },
  {
    "start": 2331122,
    "end": 2331800,
    "text": "そうだね。"
  },
  {
    "start": 2332330,
    "end": 2337350,
    "text": "さて、ここでもうひとつ注目してほしいのは、キープ・ディムは偽だという主張だ。"
  },
  {
    "start": 2337770,
    "end": 2346620,
    "text": "keep dimがtrueの場合、出力テンソルは入力と同じサイズになる。もちろん、和をとった次元を除けば、1だけとなる。"
  },
  {
    "start": 2347310,
    "end": 2354278,
    "text": "を偽のままパスすると、この次元はしぼんでしまう。"
  },
  {
    "start": 2354384,
    "end": 2361034,
    "text": "つまり、トーチ・サムは和を求め、次元を1つに折りたたむだけでなく、スクイーズと呼ばれる処理も行う。"
  },
  {
    "start": 2361082,
    "end": 2363790,
    "text": "絞り出すところは絞り出す。"
  },
  {
    "start": 2364530,
    "end": 2369890,
    "text": "基本的に、ここで我々が望んでいるのは、代わりに合計軸のドット合計を行うことである。"
  },
  {
    "start": 2370710,
    "end": 2374660,
    "text": "特に、ドット形状が27×27であることに注目してほしい。"
  },
  {
    "start": 2375510,
    "end": 2381960,
    "text": "軸ゼロを横切って合計する場合、ゼロ番目の次元を取り、それを横切って合計することになる。"
  },
  {
    "start": 2382570,
    "end": 2395450,
    "text": "keep dimがtrueの場合、これは列全体のカウントを表示するだけでなく、基本的に27分の1の形をしていることに注目してほしい。"
  },
  {
    "start": 2395520,
    "end": 2396970,
    "text": "行ベクトルを得るだけだ。"
  },
  {
    "start": 2397310,
    "end": 2400902,
    "text": "ここで再び行ベクトルが得られるのは、ゼロ次元を渡したからである。"
  },
  {
    "start": 2400966,
    "end": 2405802,
    "text": "このゼロ次元が1次元になり、和をとって行ができる。"
  },
  {
    "start": 2405866,
    "end": 2414290,
    "text": "というわけで、基本的にはこのように縦に合計を行い、27のカウントのベクトルで1つずつにまとめただけである。"
  },
  {
    "start": 2415270,
    "end": 2419426,
    "text": "彼らをキープしておくと何が起こるかというと、27歳になってしまうんだ。"
  },
  {
    "start": 2419528,
    "end": 2426280,
    "text": "を使うと、その次元が絞り込まれ、27の大きさの1次元のベクトルになる。"
  },
  {
    "start": 2428570,
    "end": 2439030,
    "text": "27行ベクトルによるカウントや列間の合計が得られるからだ。"
  },
  {
    "start": 2439370,
    "end": 2442554,
    "text": "実際には、1次元に沿って反対方向に合計したい。"
  },
  {
    "start": 2442672,
    "end": 2445578,
    "text": "この形が27×1であることがわかるだろう。"
  },
  {
    "start": 2445664,
    "end": 2447206,
    "text": "これは列ベクトルである。"
  },
  {
    "start": 2447318,
    "end": 2451390,
    "text": "これは27×1のカウントのベクトルだ。"
  },
  {
    "start": 2453730,
    "end": 2462270,
    "text": "というのも、ここで起こっているのは、水平方向に進むことで、この27×27の行列が27×1の配列になるからだ。"
  },
  {
    "start": 2463350,
    "end": 2470130,
    "text": "ところで、これらのカウントの実際の数字が同じであることにお気づきだろうか。"
  },
  {
    "start": 2470470,
    "end": 2474782,
    "text": "というのも、この特別なカウント配列はバイグラム統計に由来するものだからだ。"
  },
  {
    "start": 2474846,
    "end": 2485430,
    "text": "実は、偶然か、あるいはこの配列の組み立て方のせいで、列または行に沿った水平方向または垂直方向の合計が同じになることがある。"
  },
  {
    "start": 2486090,
    "end": 2492134,
    "text": "実際にこのケースでやりたいことは、行を横方向に合計することだ。"
  },
  {
    "start": 2492262,
    "end": 2499238,
    "text": "ここで必要なのは、1列のベクトル27個を1つのドット和にすることである。"
  },
  {
    "start": 2499414,
    "end": 2502240,
    "text": "それで割りたい。"
  },
  {
    "start": 2504530,
    "end": 2512606,
    "text": "ここでまた注意しなければならないのは、27×27のPドットを取ることは可能かということだ。"
  },
  {
    "start": 2512708,
    "end": 2520050,
    "text": "27×27の配列を27×1の配列で割ることは可能ですか？"
  },
  {
    "start": 2521190,
    "end": 2523554,
    "text": "それはできる手術ですか？"
  },
  {
    "start": 2523752,
    "end": 2528178,
    "text": "この操作ができるかどうかは、ブロードキャスト・ルールと呼ばれるものによって決定される。"
  },
  {
    "start": 2528274,
    "end": 2543290,
    "text": "torchでbroadcasting semanticsを検索すると、broadcastingと呼ばれる特別な定義があることがわかる。"
  },
  {
    "start": 2543790,
    "end": 2548330,
    "text": "最初の条件は、各テンソルが少なくとも1つの次元を持つことである。"
  },
  {
    "start": 2548480,
    "end": 2554570,
    "text": "の場合、最後尾のディメンジョンからディメンジョンサイズを反復するとき、ディメンジョンサイズは等しくなければならない。"
  },
  {
    "start": 2554650,
    "end": 2557230,
    "text": "どちらかが存在するか、どちらかが存在しないかだ。"
  },
  {
    "start": 2557970,
    "end": 2560270,
    "text": "よし、そうしよう。"
  },
  {
    "start": 2560340,
    "end": 2567822,
    "text": "2つの配列とそのシェイプを整列させる必要があるが、これはとても簡単だ。"
  },
  {
    "start": 2567966,
    "end": 2572034,
    "text": "そして、右から左へと反復する。"
  },
  {
    "start": 2572232,
    "end": 2577726,
    "text": "各次元は等しいか、どちらかが1であるか、どちらかが存在しないかのいずれかでなければならない。"
  },
  {
    "start": 2577838,
    "end": 2580438,
    "text": "この場合、両者はイコールではないが、どちらかが1である。"
  },
  {
    "start": 2580524,
    "end": 2581702,
    "text": "これでいい。"
  },
  {
    "start": 2581836,
    "end": 2583954,
    "text": "であれば、この次元では両者は同等だ。"
  },
  {
    "start": 2584082,
    "end": 2585702,
    "text": "これでいい。"
  },
  {
    "start": 2585836,
    "end": 2591598,
    "text": "すべての寸法に問題がないため、この操作はブロードキャスト可能である。"
  },
  {
    "start": 2591794,
    "end": 2594342,
    "text": "これは、この操作が許可されていることを意味する。"
  },
  {
    "start": 2594486,
    "end": 2599610,
    "text": "27を27で割ると、これらの配列はどうなるのか？"
  },
  {
    "start": 2599760,
    "end": 2604062,
    "text": "それは、この次元を1つにして、それを引き伸ばすということだ。"
  },
  {
    "start": 2604116,
    "end": 2608830,
    "text": "この場合、27と一致するようにコピーされる。"
  },
  {
    "start": 2608980,
    "end": 2620398,
    "text": "この場合、27×1の列ベクトルを27回コピーして、内部的に27×27にする。"
  },
  {
    "start": 2620494,
    "end": 2621698,
    "text": "そう考えることもできる。"
  },
  {
    "start": 2621784,
    "end": 2623950,
    "text": "だから、そのカウントをコピーする。"
  },
  {
    "start": 2624110,
    "end": 2634150,
    "text": "なぜなら、これらのカウントは、この行列のすべての列で割りたいからである。"
  },
  {
    "start": 2634650,
    "end": 2639050,
    "text": "これは実際には、すべての行を正規化することを期待している。"
  },
  {
    "start": 2639550,
    "end": 2645206,
    "text": "例えば、最初の行を取り、その合計を取ることによって、これが正しいことを確認することができる。"
  },
  {
    "start": 2645318,
    "end": 2649690,
    "text": "今は正常化されているので、これが1つになると期待している。"
  },
  {
    "start": 2650190,
    "end": 2657662,
    "text": "というのも、すべての行を正しく正規化すれば、ここでもまったく同じ結果が得られるからだ。"
  },
  {
    "start": 2657716,
    "end": 2658958,
    "text": "これを実行しよう。"
  },
  {
    "start": 2659124,
    "end": 2660560,
    "text": "まったく同じ結果だ。"
  },
  {
    "start": 2661330,
    "end": 2662782,
    "text": "これは正しい。"
  },
  {
    "start": 2662916,
    "end": 2665246,
    "text": "ちょっと怖がらせてやろう。"
  },
  {
    "start": 2665428,
    "end": 2672574,
    "text": "私は基本的に、放送の意味論に目を通すことを強く勧める。"
  },
  {
    "start": 2672702,
    "end": 2675234,
    "text": "早とちりしていいものではない。"
  },
  {
    "start": 2675272,
    "end": 2680326,
    "text": "本当にリスペクトし、理解し、放送のチュートリアルを調べて実践することだ。"
  },
  {
    "start": 2680348,
    "end": 2683730,
    "text": "すぐにバグに見舞われる可能性があるからだ。"
  },
  {
    "start": 2683810,
    "end": 2685240,
    "text": "どういうことか、お見せしよう。"
  },
  {
    "start": 2687050,
    "end": 2690278,
    "text": "ここではpのドットサム1がある。"
  },
  {
    "start": 2690444,
    "end": 2692842,
    "text": "形は27×1。"
  },
  {
    "start": 2692976,
    "end": 2703322,
    "text": "この行を削除して、nを表示させ、カウントを表示させれば、これがすべての行にわたるすべてのカウントであることがわかる。"
  },
  {
    "start": 2703466,
    "end": 2705450,
    "text": "これは27×1列のベクトルである。"
  },
  {
    "start": 2705530,
    "end": 2706160,
    "text": "そうだろう？"
  },
  {
    "start": 2707090,
    "end": 2713120,
    "text": "さて、仮に私が次のようなことを試みたとしよう。"
  },
  {
    "start": 2713890,
    "end": 2714910,
    "text": "それでどうなるんだ？"
  },
  {
    "start": 2714980,
    "end": 2717150,
    "text": "キープすることが真実でなければ、それは偽りだ。"
  },
  {
    "start": 2717230,
    "end": 2722722,
    "text": "それから、資料によれば、このディメンション1は取り除かれる。"
  },
  {
    "start": 2722856,
    "end": 2728966,
    "text": "基本的には、同じカウント、同じ結果を得るだけだが、その形は27×1ではない。"
  },
  {
    "start": 2728988,
    "end": 2729942,
    "text": "ちょうど27歳だ。"
  },
  {
    "start": 2729996,
    "end": 2733560,
    "text": "1本は消えたが、カウントはすべて同じ。"
  },
  {
    "start": 2734170,
    "end": 2739754,
    "text": "この分断、うまくいくと思うだろう。"
  },
  {
    "start": 2739952,
    "end": 2742154,
    "text": "まず、こんなことを書いていいのだろうか？"
  },
  {
    "start": 2742192,
    "end": 2745114,
    "text": "走ることを期待しているのか？"
  },
  {
    "start": 2745152,
    "end": 2746198,
    "text": "放送可能か？"
  },
  {
    "start": 2746294,
    "end": 2748886,
    "text": "この結果が放送可能かどうかを判断してみよう。"
  },
  {
    "start": 2749078,
    "end": 2750018,
    "text": "Pドットサミット"
  },
  {
    "start": 2750054,
    "end": 2752718,
    "text": "ひとつは27という形。"
  },
  {
    "start": 2752884,
    "end": 2754382,
    "text": "これは27×27だ。"
  },
  {
    "start": 2754436,
    "end": 2759760,
    "text": "27×27、27に放送。"
  },
  {
    "start": 2760210,
    "end": 2765342,
    "text": "さて、放送のルールその1、すべての寸法を右に揃える。"
  },
  {
    "start": 2765476,
    "end": 2766062,
    "text": "完了した。"
  },
  {
    "start": 2766196,
    "end": 2769906,
    "text": "右から左へ、すべての次元を反復する。"
  },
  {
    "start": 2770088,
    "end": 2772290,
    "text": "すべての寸法は等しくなければならない。"
  },
  {
    "start": 2772870,
    "end": 2775806,
    "text": "どちらか一方が存在しなければならない。"
  },
  {
    "start": 2775918,
    "end": 2777578,
    "text": "ここではみな平等だ。"
  },
  {
    "start": 2777694,
    "end": 2779698,
    "text": "ここでは次元は存在しない。"
  },
  {
    "start": 2779874,
    "end": 2784054,
    "text": "内部的には、放送がここにひとつを作ることになる。"
  },
  {
    "start": 2784252,
    "end": 2787538,
    "text": "となると、そのうちのひとつが1であることがわかる。"
  },
  {
    "start": 2787644,
    "end": 2791530,
    "text": "これがコピーされ、これが実行され、これが放送される。"
  },
  {
    "start": 2792350,
    "end": 2801974,
    "text": "さて、この放送はうまくいくと思うだろう。"
  },
  {
    "start": 2802022,
    "end": 2803454,
    "text": "これを分割することができる。"
  },
  {
    "start": 2803572,
    "end": 2807630,
    "text": "さて、これを実行すればうまくいくかと思いきや、そうはならない。"
  },
  {
    "start": 2808050,
    "end": 2809338,
    "text": "実際にゴミが出る。"
  },
  {
    "start": 2809434,
    "end": 2812282,
    "text": "なぜなら、これはバグだからだ。"
  },
  {
    "start": 2812426,
    "end": 2818420,
    "text": "このキープ・ディム・イコール・トゥルーが機能するのだ。"
  },
  {
    "start": 2820550,
    "end": 2821810,
    "text": "これはバグだ。"
  },
  {
    "start": 2822790,
    "end": 2826334,
    "text": "どちらの場合も、正しいカウントをしている。"
  },
  {
    "start": 2826382,
    "end": 2831446,
    "text": "私たちは列をまたいで合計しているが、彼らをキープすることが私たちを救い、うまく機能させている。"
  },
  {
    "start": 2831548,
    "end": 2840520,
    "text": "この場合、この時点でこのビデオを一時停止して、なぜこれがバグなのか、なぜここでkeep dimが必要だったのかを考えてみてほしい。"
  },
  {
    "start": 2842170,
    "end": 2846650,
    "text": "オーケー、ではこのために何をすべきかというと、ここでヒントを出そうとしているんだ。"
  },
  {
    "start": 2846720,
    "end": 2849306,
    "text": "この仕組みについて少しヒントを差し上げたときだ。"
  },
  {
    "start": 2849488,
    "end": 2856238,
    "text": "この27のベクトルは、放送内部では27分の1になる。"
  },
  {
    "start": 2856404,
    "end": 2859390,
    "text": "one×27は行ベクトルですよね？"
  },
  {
    "start": 2859540,
    "end": 2862954,
    "text": "今、私たちは27を27で割っている。"
  },
  {
    "start": 2863092,
    "end": 2865742,
    "text": "トーチはこの次元を再現する。"
  },
  {
    "start": 2865886,
    "end": 2876194,
    "text": "基本的には、この行ベクトルを縦に27回コピーする。"
  },
  {
    "start": 2876232,
    "end": 2879750,
    "text": "27×27は正確に揃い、要素ごとに分割される。"
  },
  {
    "start": 2880170,
    "end": 2888070,
    "text": "つまり、基本的にここで起こっていることは、行を正規化する代わりに列を正規化しているということだ。"
  },
  {
    "start": 2889370,
    "end": 2898060,
    "text": "ここで起こっていることは、pの最初の行であるゼロのp、つまりドット・サムが1ではなく7であることを確認できる。"
  },
  {
    "start": 2898510,
    "end": 2902160,
    "text": "和が1になるのは、例として最初の列である。"
  },
  {
    "start": 2903570,
    "end": 2906670,
    "text": "つまり、要約すると、問題はどこから来るのか？"
  },
  {
    "start": 2906740,
    "end": 2913710,
    "text": "放送ルールでは、右から左へ整列する。"
  },
  {
    "start": 2913780,
    "end": 2916030,
    "text": "ディメンションが存在しない場合は、それを作成する。"
  },
  {
    "start": 2916180,
    "end": 2917762,
    "text": "そこで問題が起こる。"
  },
  {
    "start": 2917896,
    "end": 2919454,
    "text": "それでもカウントは正確に行った。"
  },
  {
    "start": 2919502,
    "end": 2925454,
    "text": "行をまたぐカウントを行い、右側のカウントを列ベクトルとして得た。"
  },
  {
    "start": 2925582,
    "end": 2929406,
    "text": "キープ缶が真実であったため、この次元は捨てられた。"
  },
  {
    "start": 2929438,
    "end": 2931270,
    "text": "これで27のベクトルができた。"
  },
  {
    "start": 2931420,
    "end": 2936770,
    "text": "というのも、放送の仕組み上、この27のベクトルは突然行ベクトルになるからだ。"
  },
  {
    "start": 2936930,
    "end": 2939702,
    "text": "とすると、この行ベクトルは縦方向に複製される。"
  },
  {
    "start": 2939846,
    "end": 2946970,
    "text": "すべての点で、逆方向のカウントで割っている。"
  },
  {
    "start": 2948750,
    "end": 2951386,
    "text": "これはうまくいかない。"
  },
  {
    "start": 2951488,
    "end": 2953966,
    "text": "この場合、zをtrueに保つ必要がある。"
  },
  {
    "start": 2954148,
    "end": 2958910,
    "text": "とすれば、ゼロのときのpは正規化されていることになる。"
  },
  {
    "start": 2959730,
    "end": 2963650,
    "text": "逆に、最初の列は正規化されていない可能性がある。"
  },
  {
    "start": 2964470,
    "end": 2966580,
    "text": "これが彼の仕事である。"
  },
  {
    "start": 2967590,
    "end": 2969278,
    "text": "かなり微妙だ。"
  },
  {
    "start": 2969374,
    "end": 2974366,
    "text": "放送に対するリスペクトを持つべきだということを怖がらせる一助になれば幸いだ。"
  },
  {
    "start": 2974398,
    "end": 2978846,
    "text": "注意深く、自分の仕事をチェックし、それがボンネットの中でどのように機能しているかを理解すること。"
  },
  {
    "start": 2978878,
    "end": 2981190,
    "text": "自分の好きな方向に放送されていることを確認してください。"
  },
  {
    "start": 2981260,
    "end": 2986610,
    "text": "そうでなければ、非常に微妙なバグや見つけにくいバグを引き起こすことになる。"
  },
  {
    "start": 2986690,
    "end": 2988114,
    "text": "もうひとつ、効率について。"
  },
  {
    "start": 2988242,
    "end": 2994250,
    "text": "なぜなら、これはpに格納するまったく新しいテンソルを作るからだ。"
  },
  {
    "start": 2994400,
    "end": 2997180,
    "text": "可能であれば、インプレース・オペレーションを使いたい。"
  },
  {
    "start": 2997790,
    "end": 3000002,
    "text": "これはその場での作業となる。"
  },
  {
    "start": 3000086,
    "end": 3001738,
    "text": "もっと速くなる可能性がある。"
  },
  {
    "start": 3001834,
    "end": 3004362,
    "text": "ボンネットの下に新しいメモリーを作るわけではない。"
  },
  {
    "start": 3004426,
    "end": 3006030,
    "text": "それなら、これを消してしまおう。"
  },
  {
    "start": 3006180,
    "end": 3007520,
    "text": "それは必要ない。"
  },
  {
    "start": 3007890,
    "end": 3014446,
    "text": "スペースを無駄にしないためにも、もっと少なくしよう。"
  },
  {
    "start": 3014628,
    "end": 3016882,
    "text": "オーケー、それじゃあ僕らは今、かなりいいところにいるんだ。"
  },
  {
    "start": 3017016,
    "end": 3025454,
    "text": "私たちはビグラム言語モデルを訓練し、どのペアがどれくらいの頻度で出現するかをカウントし、正規化するだけで訓練した。"
  },
  {
    "start": 3025502,
    "end": 3027698,
    "text": "という確率分布が得られる。"
  },
  {
    "start": 3027874,
    "end": 3036690,
    "text": "この配列Pの要素は、ビグラム言語モデルのパラメータであり、ビグラムの統計量を要約する。"
  },
  {
    "start": 3036850,
    "end": 3040050,
    "text": "モデルを訓練し、モデルからサンプリングする方法を知る。"
  },
  {
    "start": 3040140,
    "end": 3046646,
    "text": "次の文字を繰り返しサンプリングし、その都度投入して次の文字を得るだけだ。"
  },
  {
    "start": 3046838,
    "end": 3050954,
    "text": "さて、私がやりたいことは、このモデルの品質をどうにかして評価することだ。"
  },
  {
    "start": 3051072,
    "end": 3055230,
    "text": "このモデルのクオリティをなんとか1つの数字にまとめたい。"
  },
  {
    "start": 3055300,
    "end": 3058766,
    "text": "トレーニングセットの予測能力は？"
  },
  {
    "start": 3058948,
    "end": 3064074,
    "text": "トレーニング・セットでは、トレーニングの損失を評価することができる。"
  },
  {
    "start": 3064202,
    "end": 3071250,
    "text": "このトレーニング・ロスは、マイクログラッドで見たように、このモデルの質の高さを一つの数字で教えてくれる。"
  },
  {
    "start": 3071830,
    "end": 3075780,
    "text": "モデルの品質とその評価方法について考えてみよう。"
  },
  {
    "start": 3076950,
    "end": 3082630,
    "text": "基本的にこれからやることは、以前カウントに使ったこのコードをコピーペーストすることだ。"
  },
  {
    "start": 3084090,
    "end": 3085714,
    "text": "グラム単位で印刷させてください。"
  },
  {
    "start": 3085762,
    "end": 3087650,
    "text": "まずはfストリングスを使う。"
  },
  {
    "start": 3087810,
    "end": 3090774,
    "text": "文字目に続いて2文字目を印刷します。"
  },
  {
    "start": 3090812,
    "end": 3091878,
    "text": "これがビグラムである。"
  },
  {
    "start": 3091974,
    "end": 3093594,
    "text": "それなら、すべての言葉のためにそれをするのは嫌だ。"
  },
  {
    "start": 3093632,
    "end": 3095180,
    "text": "最初の3単語だけやってみよう。"
  },
  {
    "start": 3095870,
    "end": 3099450,
    "text": "ここにはエマ、オリビア、エヴァのビッグラムがいる。"
  },
  {
    "start": 3100110,
    "end": 3107930,
    "text": "さて、私たちがやりたいことは、基本的にモデルがこれらのビグラムのひとつひとつに割り当てる確率を調べることだ。"
  },
  {
    "start": 3108090,
    "end": 3119170,
    "text": "つまり、ix1、ix2の行列Bにまとめられている確率を見て、それを確率としてここに表示することができる。"
  },
  {
    "start": 3120550,
    "end": 3128420,
    "text": "これらの確率は大きすぎるので、パーセントかカラム0.4 fで少し切り捨ててみよう。"
  },
  {
    "start": 3129030,
    "end": 3130066,
    "text": "ここに何がある？"
  },
  {
    "start": 3130088,
    "end": 3134998,
    "text": "データセットに含まれるビッググラムのひとつひとつにモデルが割り当てる確率を見ているわけだ。"
  },
  {
    "start": 3135164,
    "end": 3144630,
    "text": "4％、3％、などなど、27の可能性のある文字やトークンがある。"
  },
  {
    "start": 3144710,
    "end": 3152198,
    "text": "もしすべての可能性が等しいとしたら、これらの確率はだいたい4％になるはずだ。"
  },
  {
    "start": 3152374,
    "end": 3157466,
    "text": "4％以上であれば、バイグラム統計から有益なことがわかったということだ。"
  },
  {
    "start": 3157578,
    "end": 3163870,
    "text": "おおよそ4％のものもあれば、40％、35％といった高いものもあることがわかるだろう。"
  },
  {
    "start": 3163940,
    "end": 3168258,
    "text": "このモデルは、トレーニングセットに含まれるものすべてに、かなり高い確率を割り当てていることがわかる。"
  },
  {
    "start": 3168344,
    "end": 3169940,
    "text": "それはいいことだ。"
  },
  {
    "start": 3170470,
    "end": 3181800,
    "text": "基本的に、非常に優れたモデルを持っている場合、これらの確率は1に近いはずだと予想される。"
  },
  {
    "start": 3182810,
    "end": 3190540,
    "text": "では、これらの確率を、このモデルの質を測る一つの数値にまとめるにはどうしたらいいかを考えてみたい。"
  },
  {
    "start": 3191630,
    "end": 3201190,
    "text": "さて、最尤推定や統計モデリングなどの文献を見ると、ここで一般的に使われているのは尤度と呼ばれるものであることがわかる。"
  },
  {
    "start": 3201350,
    "end": 3205070,
    "text": "尤度はこれらの確率の積である。"
  },
  {
    "start": 3205730,
    "end": 3209146,
    "text": "したがって、これらすべての確率の積が尤度となる。"
  },
  {
    "start": 3209258,
    "end": 3217518,
    "text": "これは、学習させたモデルによって割り当てられたデータセット全体の確率について教えてくれているのだ。"
  },
  {
    "start": 3217614,
    "end": 3219218,
    "text": "それが質の指標となる。"
  },
  {
    "start": 3219384,
    "end": 3224434,
    "text": "モデルをトレーニングする際には、これらの積をできるだけ大きくする必要がある。"
  },
  {
    "start": 3224472,
    "end": 3228740,
    "text": "良いモデルがあれば、これらの確率の積は非常に高くなるはずだ。"
  },
  {
    "start": 3230310,
    "end": 3236230,
    "text": "さて、これらの確率の積は扱いにくいものなので、すべての確率が0から1の間であることがわかるだろう。"
  },
  {
    "start": 3236300,
    "end": 3239500,
    "text": "これらの確率の積は非常に小さな数字になる。"
  },
  {
    "start": 3240830,
    "end": 3247130,
    "text": "便宜上、通常は尤度ではなく、対数尤度と呼ばれるものを使います。"
  },
  {
    "start": 3247870,
    "end": 3250694,
    "text": "この積が可能性である。"
  },
  {
    "start": 3250822,
    "end": 3254510,
    "text": "対数尤度を求めるには、確率の対数をとればよい。"
  },
  {
    "start": 3254930,
    "end": 3256586,
    "text": "つまり確率の対数である。"
  },
  {
    "start": 3256698,
    "end": 3259120,
    "text": "ここでは、xのゼロから1までの対数を持っている。"
  },
  {
    "start": 3259650,
    "end": 3260862,
    "text": "ログは"
  },
  {
    "start": 3260916,
    "end": 3268546,
    "text": "ここでは、確率の単調変換が見られる。"
  },
  {
    "start": 3268728,
    "end": 3272050,
    "text": "確率1は対数確率をゼロにする。"
  },
  {
    "start": 3272200,
    "end": 3279640,
    "text": "そして、確率が低くなるにつれて、対数はどんどんマイナスになり、マイナス無限大のゼロになる。"
  },
  {
    "start": 3281770,
    "end": 3286610,
    "text": "ロック・プローブは、実際には確率のトーチログに過ぎない。"
  },
  {
    "start": 3286770,
    "end": 3289802,
    "text": "それがどんなものかを知るためにプリントアウトしてみよう。"
  },
  {
    "start": 3289936,
    "end": 3293260,
    "text": "ロック・プロブレムもゼロ、4、f。"
  },
  {
    "start": 3296590,
    "end": 3303230,
    "text": "おわかりのように、非常に近い数字、つまり高い数字を差し込むと、どんどんゼロに近づいていく。"
  },
  {
    "start": 3303380,
    "end": 3307518,
    "text": "ということは、非常に悪い確率を差し込むと、どんどんマイナスの数が増えていくことになる。"
  },
  {
    "start": 3307604,
    "end": 3308560,
    "text": "それはまずい。"
  },
  {
    "start": 3310770,
    "end": 3315122,
    "text": "これを使う理由は、利便性のためでしょう？"
  },
  {
    "start": 3315256,
    "end": 3320978,
    "text": "というのも、これらの確率の積a×b×cがある場合、数学的にはこうなるよね？"
  },
  {
    "start": 3321144,
    "end": 3339558,
    "text": "尤度はこれらの確率の積であり、その対数はaの対数、bの対数、cの対数となる。"
  },
  {
    "start": 3339724,
    "end": 3343222,
    "text": "基本的に尤度は確率の積である。"
  },
  {
    "start": 3343286,
    "end": 3348010,
    "text": "対数尤度は、個々の確率の対数の和に過ぎない。"
  },
  {
    "start": 3348750,
    "end": 3358750,
    "text": "対数尤度はゼロから始まり、ここでの対数尤度は単純に積み重ねることができる。"
  },
  {
    "start": 3360290,
    "end": 3370610,
    "text": "そして最後に、対数尤度f文字列を表示することができる。"
  },
  {
    "start": 3371670,
    "end": 3373380,
    "text": "もしかしたら、あなたもご存じかもしれない。"
  },
  {
    "start": 3373750,
    "end": 3376900,
    "text": "つまり、対数尤度はマイナス38となる。"
  },
  {
    "start": 3379910,
    "end": 3383720,
    "text": "さて、私たちは今、実際に欲しいと思っている。"
  },
  {
    "start": 3385130,
    "end": 3387654,
    "text": "ログの尤度はどこまで高くなるのか？"
  },
  {
    "start": 3387772,
    "end": 3389622,
    "text": "ゼロになることもある。"
  },
  {
    "start": 3389756,
    "end": 3392746,
    "text": "すべての確率が1のとき、対数尤度は0になる。"
  },
  {
    "start": 3392848,
    "end": 3396890,
    "text": "そして、すべての確率が低くなると、これはますますマイナスになる。"
  },
  {
    "start": 3397470,
    "end": 3401466,
    "text": "というのも、私たちが欲しいのは損失関数だからだ。"
  },
  {
    "start": 3401568,
    "end": 3408106,
    "text": "損失関数は、損失を最小化しようとしているのだから、低い方が良いという意味を持っている。"
  },
  {
    "start": 3408218,
    "end": 3410190,
    "text": "実際には、これを反転させる必要がある。"
  },
  {
    "start": 3410340,
    "end": 3413970,
    "text": "これが負の対数尤度と呼ばれるものだ。"
  },
  {
    "start": 3415910,
    "end": 3420050,
    "text": "負の対数尤度は、対数尤度の負の値である。"
  },
  {
    "start": 3423830,
    "end": 3428230,
    "text": "ちなみに、これはsの文字列で、調べたい人は負の対数尤度に等しい。"
  },
  {
    "start": 3429210,
    "end": 3431926,
    "text": "今の負の対数尤度は、その負の値でしかない。"
  },
  {
    "start": 3432028,
    "end": 3439510,
    "text": "というわけで、負の対数尤度は、得られる最小値がゼロであるため、非常に優れた損失関数である。"
  },
  {
    "start": 3439660,
    "end": 3444220,
    "text": "それが高ければ高いほど、あなたの予測は外れることになる。"
  },
  {
    "start": 3444590,
    "end": 3451166,
    "text": "もうひとつ、便宜上、正規化することがある。"
  },
  {
    "start": 3451268,
    "end": 3454030,
    "text": "彼らは合計ではなく平均にしたがる。"
  },
  {
    "start": 3454370,
    "end": 3459102,
    "text": "だから、ここでもいくつかのカウントを残しておこう。"
  },
  {
    "start": 3459236,
    "end": 3462602,
    "text": "nプラスイコール1はゼロから始まる。"
  },
  {
    "start": 3462756,
    "end": 3467410,
    "text": "とすると、ここでは正規化対数尤度のようなものができる。"
  },
  {
    "start": 3470390,
    "end": 3475678,
    "text": "カウントで正規化すれば、平均対数尤度が得られる。"
  },
  {
    "start": 3475774,
    "end": 3478840,
    "text": "これは通常、我々の損失関数である。"
  },
  {
    "start": 3479770,
    "end": 3481240,
    "text": "これが私たちが使うものだ。"
  },
  {
    "start": 3482170,
    "end": 3486422,
    "text": "つまり、モデルによって割り当てられたトレーニングセットの損失関数は2.4となる。"
  },
  {
    "start": 3486476,
    "end": 3488278,
    "text": "それがこのモデルのクオリティだ。"
  },
  {
    "start": 3488444,
    "end": 3490714,
    "text": "それが低ければ低いほど、私たちは恵まれている。"
  },
  {
    "start": 3490752,
    "end": 3492940,
    "text": "高ければ高いほど、不利になる。"
  },
  {
    "start": 3493390,
    "end": 3501610,
    "text": "を最小化するパラメータを見つけることである。"
  },
  {
    "start": 3502770,
    "end": 3505214,
    "text": "それは高級モデルのようなものだろう。"
  },
  {
    "start": 3505332,
    "end": 3507838,
    "text": "さて、要約すると、実際にここに書いてみた。"
  },
  {
    "start": 3508004,
    "end": 3515338,
    "text": "の目標は尤度を最大化することであり、これはモデルによって割り当てられたすべての確率の積である。"
  },
  {
    "start": 3515524,
    "end": 3523438,
    "text": "この尤度をモデル・パラメーターに関して最大化したいのだが、我々の場合、ここでいうモデル・パラメーターは表で定義されている。"
  },
  {
    "start": 3523534,
    "end": 3529846,
    "text": "これらの数値、つまり確率は、これまでのダイアグラム言語モデルにおける、ある種のモデル・パラメーターである。"
  },
  {
    "start": 3530028,
    "end": 3534658,
    "text": "ここで注意しなければならないのは、私たちはすべての確率を表形式で保存しているということだ。"
  },
  {
    "start": 3534754,
    "end": 3542902,
    "text": "簡単なプレビューになるが、これらの数字は明示的に保存されるのではなく、ニューラルネットワークによって計算される。"
  },
  {
    "start": 3543046,
    "end": 3544330,
    "text": "それが近づいている。"
  },
  {
    "start": 3544480,
    "end": 3548006,
    "text": "これらのニューラルネットワークのパラメーターを変更し、調整したい。"
  },
  {
    "start": 3548118,
    "end": 3552670,
    "text": "我々は、尤度（確率の積）を最大化するために、これらのパラメータを変更したい。"
  },
  {
    "start": 3553250,
    "end": 3559440,
    "text": "logは単調関数であるため、尤度の最大化はlog尤度の最大化と等価である。"
  },
  {
    "start": 3559810,
    "end": 3561838,
    "text": "これがログのグラフだ。"
  },
  {
    "start": 3562004,
    "end": 3569140,
    "text": "基本的にやっていることは、損失関数をスケーリングしているだけだ。"
  },
  {
    "start": 3569910,
    "end": 3576822,
    "text": "この最適化問題とこの最適化問題は実は等価である。"
  },
  {
    "start": 3576956,
    "end": 3579960,
    "text": "従って、これらは2つの同じ最適化問題である。"
  },
  {
    "start": 3581850,
    "end": 3585750,
    "text": "対数尤度の最大化は、負の対数尤度の最小化と等価である。"
  },
  {
    "start": 3586090,
    "end": 3592666,
    "text": "となると、実際には平均的な負の対数尤度を最小化して2.4のような数字を得ることになる。"
  },
  {
    "start": 3592848,
    "end": 3599180,
    "text": "ということは、これはあなたのモデルの品質を要約したもので、私たちはこれを最小限に抑え、できるだけ小さくしたいのです。"
  },
  {
    "start": 3599550,
    "end": 3602106,
    "text": "最低でもゼロだ。"
  },
  {
    "start": 3602288,
    "end": 3609374,
    "text": "この値が低ければ低いほど、あなたのモデルはデータに高い確率を割り当てていることになり、良い結果をもたらす。"
  },
  {
    "start": 3609492,
    "end": 3614722,
    "text": "では、念のため、トレーニングセット全体の確率を推定してみよう。"
  },
  {
    "start": 3614856,
    "end": 3616050,
    "text": "これを全体にかけてみよう。"
  },
  {
    "start": 3616120,
    "end": 3619380,
    "text": "おっと、print文も削除しよう。"
  },
  {
    "start": 3620630,
    "end": 3623380,
    "text": "よし、2.45、つまりトレーニングセット全体だ。"
  },
  {
    "start": 3624310,
    "end": 3628342,
    "text": "さて、ここでお見せしたいのは、実はどんな単語でも確率を評価できるということだ。"
  },
  {
    "start": 3628396,
    "end": 3638870,
    "text": "例えば、andreという1つの単語をテストしてprint文に戻すと、Andreは実際にはありえない単語のようなものであることがわかる。"
  },
  {
    "start": 3638940,
    "end": 3644378,
    "text": "例えば、平均して3つの確率をロックして表現する。"
  },
  {
    "start": 3644464,
    "end": 3647718,
    "text": "大体、EJは非常に珍しいらしいからだ。"
  },
  {
    "start": 3647734,
    "end": 3648620,
    "text": "一例を挙げよう。"
  },
  {
    "start": 3650050,
    "end": 3652400,
    "text": "よく考えてみてほしい。"
  },
  {
    "start": 3653730,
    "end": 3662746,
    "text": "アンドレとqを足して、アンドレ、Qという確率を検証すると、実際には無限大になる。"
  },
  {
    "start": 3662938,
    "end": 3667474,
    "text": "というのも、我々のモデルではJQの確率は0％だからだ。"
  },
  {
    "start": 3667592,
    "end": 3674206,
    "text": "の対数尤度は、ゼロの対数が負の無限大となるため、無限大の損失となる。"
  },
  {
    "start": 3674398,
    "end": 3675874,
    "text": "これはちょっと望ましくないよね？"
  },
  {
    "start": 3675912,
    "end": 3679094,
    "text": "というのも、ある程度妥当な名前のような文字列を差し込んだからだ。"
  },
  {
    "start": 3679212,
    "end": 3689434,
    "text": "基本的に、このモデルがこの名前を予測する可能性は0％であり、この例での損失は無限大である、ということだ。"
  },
  {
    "start": 3689632,
    "end": 3696620,
    "text": "その理由は、jの後にqが0回続くからだ。"
  },
  {
    "start": 3697070,
    "end": 3697850,
    "text": "Qは？"
  },
  {
    "start": 3697920,
    "end": 3701990,
    "text": "JQはゼロなので、JQの可能性は0％である。"
  },
  {
    "start": 3702150,
    "end": 3703606,
    "text": "実際、ちょっと気持ち悪い。"
  },
  {
    "start": 3703638,
    "end": 3705966,
    "text": "人々はこれを修正することをあまり好まない。"
  },
  {
    "start": 3705988,
    "end": 3710318,
    "text": "モデルを少し滑らかにするために、みんながやりたがるとても簡単な修正がある。"
  },
  {
    "start": 3710324,
    "end": 3711850,
    "text": "モデル・スムージングと呼ばれるものだ。"
  },
  {
    "start": 3712010,
    "end": 3716078,
    "text": "おおよそ何が起こっているかというと、偽のアカウントをいくつか追加することだ。"
  },
  {
    "start": 3716254,
    "end": 3720020,
    "text": "何事にも1つの勘定を加えることを想像してほしい。"
  },
  {
    "start": 3720870,
    "end": 3726530,
    "text": "というように、1つのアカウントを追加し、確率を再計算する。"
  },
  {
    "start": 3727670,
    "end": 3728974,
    "text": "それがモデル・スムージングだ。"
  },
  {
    "start": 3729022,
    "end": 3732678,
    "text": "好きなだけ足してもいいし、5つ足してもいい。"
  },
  {
    "start": 3732844,
    "end": 3737574,
    "text": "ここに追加すればするほど、より均一なモデルになる。"
  },
  {
    "start": 3737692,
    "end": 3742122,
    "text": "もちろん、足す量が少なければ少ないほど、よりピーキーなモデルになる。"
  },
  {
    "start": 3742256,
    "end": 3750220,
    "text": "というのは、かなりまともな数を追加するようなもので、これによって確率行列pにゼロがないことが保証される。"
  },
  {
    "start": 3750750,
    "end": 3753454,
    "text": "だから、もちろん世代は少し変わるだろう。"
  },
  {
    "start": 3753572,
    "end": 3756350,
    "text": "今回の場合はそうではなかったが、原理的には可能だ。"
  },
  {
    "start": 3756500,
    "end": 3760970,
    "text": "その結果、何もかもが無限大になりそうもない。"
  },
  {
    "start": 3761130,
    "end": 3764618,
    "text": "今、我々のモデルは他の確率を予測する。"
  },
  {
    "start": 3764714,
    "end": 3767450,
    "text": "JQの確率が非常に小さくなったことがわかる。"
  },
  {
    "start": 3767610,
    "end": 3773214,
    "text": "というモデルは、これが単語やビグラムであることを非常に驚くべきことだと判断するが、負の無限大を得ることはない。"
  },
  {
    "start": 3773342,
    "end": 3777038,
    "text": "モデル・スムージングと呼ばれるものだ。"
  },
  {
    "start": 3777134,
    "end": 3781366,
    "text": "さて、これで立派なビグラム文字レベルの言語モデルを学習できた。"
  },
  {
    "start": 3781468,
    "end": 3791238,
    "text": "私たちは、すべてのビグラムのカウントを見て、確率分布を得るために行を正規化することによって、ある種のモデルを訓練した。"
  },
  {
    "start": 3791414,
    "end": 3798300,
    "text": "このモデルのパラメータを使って、新しい単語をサンプリングすることもできる。"
  },
  {
    "start": 3799390,
    "end": 3802118,
    "text": "これらの分布に従って新しい名前をサンプリングする。"
  },
  {
    "start": 3802214,
    "end": 3805230,
    "text": "また、このモデルの品質を評価できることもわかった。"
  },
  {
    "start": 3805380,
    "end": 3809818,
    "text": "このモデルの品質は、負の対数尤度という一つの数値に要約される。"
  },
  {
    "start": 3809914,
    "end": 3819380,
    "text": "この数値が低ければ低いほど、学習セットに含まれるすべてのビッグラムにおいて、実際の次の文字に高い確率を与えていることになり、優れたモデルであることを意味する。"
  },
  {
    "start": 3820150,
    "end": 3826142,
    "text": "それはそれでいいことだが、我々は理にかなったことをすることで、このモデルにたどり着いた。"
  },
  {
    "start": 3826206,
    "end": 3830718,
    "text": "私たちはただカウントを行い、そのカウントを正規化していた。"
  },
  {
    "start": 3830894,
    "end": 3834018,
    "text": "さて、私がやりたいのは、別のアプローチを取ることだ。"
  },
  {
    "start": 3834114,
    "end": 3843926,
    "text": "しかし、ビッググラム文字レベルの言語モデリングの問題をニューラルネットワークのフレームワークに置き換えることで、アプローチは大きく異なるものになるだろう。"
  },
  {
    "start": 3844118,
    "end": 3850262,
    "text": "ニューラル・ネットワークのフレームワークでは、アプローチは少し異なるが、結局は非常に似たようなところに行き着く。"
  },
  {
    "start": 3850326,
    "end": 3851580,
    "text": "それについては後で説明する。"
  },
  {
    "start": 3851950,
    "end": 3857166,
    "text": "これでニューラルネットワークはBygram文字レベルの言語モデルとなる。"
  },
  {
    "start": 3857268,
    "end": 3860202,
    "text": "は1文字を入力として受け取る。"
  },
  {
    "start": 3860346,
    "end": 3869182,
    "text": "そして、ニューラルネットワークが、いくつかの重みまたはいくつかのパラメータwを持ち、シーケンスの次の文字の確率分布を出力する。"
  },
  {
    "start": 3869246,
    "end": 3874980,
    "text": "モデルにインプットされたこのキャラクターに何が続きそうかを推測するのだ。"
  },
  {
    "start": 3875830,
    "end": 3884994,
    "text": "それに加えて、負の対数尤度という損失関数があるので、ニューラルネットのパラメーターのどのような設定でも評価できるようになる。"
  },
  {
    "start": 3885042,
    "end": 3894534,
    "text": "その確率分布を見てみよう。ラベルは基本的に、その図の次の文字、つまり2番目の文字の身元を示すだけである。"
  },
  {
    "start": 3894662,
    "end": 3903714,
    "text": "この図では、2番目の文字が実際に次に来るのが何であるかを知ることで、モデルがその文字にどの程度の確率を割り当てているかを見ることができる。"
  },
  {
    "start": 3903862,
    "end": 3906654,
    "text": "であれば、もちろん確率は非常に高い方がいい。"
  },
  {
    "start": 3906852,
    "end": 3910160,
    "text": "というのは、別の言い方をすれば、損失が少ないということだ。"
  },
  {
    "start": 3910690,
    "end": 3918370,
    "text": "損失関数があるので、それを最小化するために、勾配ベースの最適化を使ってこのネットワークのパラメーターを調整する。"
  },
  {
    "start": 3918440,
    "end": 3924158,
    "text": "ニューラルネットが次の文字の確率を正しく予測できるように、重みを調整する。"
  },
  {
    "start": 3924334,
    "end": 3925582,
    "text": "さあ、始めよう。"
  },
  {
    "start": 3925656,
    "end": 3929654,
    "text": "最初にやりたいことは、ニューラルネットワークのトレーニングセットをコンパイルすることだ。"
  },
  {
    "start": 3929692,
    "end": 3934630,
    "text": "すべてのビグラムからなる訓練セットを作成する。"
  },
  {
    "start": 3937630,
    "end": 3946250,
    "text": "このコードはすべてのバイグラムを反復処理するので、このコードをコピー・ペーストする。"
  },
  {
    "start": 3947310,
    "end": 3950454,
    "text": "ここでは単語から始めて、すべてのバイグラムを反復する。"
  },
  {
    "start": 3950502,
    "end": 3954554,
    "text": "以前は、思い出してほしいのだが、我々はカウントをしていた。"
  },
  {
    "start": 3954602,
    "end": 3956574,
    "text": "トレーニングセットを作っているだけだ。"
  },
  {
    "start": 3956772,
    "end": 3960190,
    "text": "さて、このトレーニングセットは2つのリストで構成される。"
  },
  {
    "start": 3962050,
    "end": 3967738,
    "text": "インプットもターゲットもある。"
  },
  {
    "start": 3967914,
    "end": 3971342,
    "text": "ラベルは、グラム単位でXYと表記する。"
  },
  {
    "start": 3971406,
    "end": 3972930,
    "text": "あのキャラクターたちだね？"
  },
  {
    "start": 3973080,
    "end": 3977526,
    "text": "だから、バイラムの最初の文字が与えられ、次の文字を予測するんだ。"
  },
  {
    "start": 3977708,
    "end": 3979410,
    "text": "いずれも整数になる。"
  },
  {
    "start": 3979490,
    "end": 3986520,
    "text": "ここでは、xのアペンドはx1、yのアペンドはix2とする。"
  },
  {
    "start": 3987530,
    "end": 3991446,
    "text": "となると、ここでは整数のリストはいらないことになる。"
  },
  {
    "start": 3991558,
    "end": 3993690,
    "text": "我々はこれらからテンソルを作る。"
  },
  {
    "start": 3993760,
    "end": 4000410,
    "text": "X'sはX'sのテンソルであるTorschであり、Y'sはY'sのテンソルであるTorchである。"
  },
  {
    "start": 4001390,
    "end": 4006810,
    "text": "というのも、すべてを管理できるようにしたいからだ。"
  },
  {
    "start": 4006970,
    "end": 4014180,
    "text": "最初の単語はエマで、XとYが何であるかは明らかだ。"
  },
  {
    "start": 4015350,
    "end": 4020980,
    "text": "ここで、文字1と文字2をプリントしてみよう。"
  },
  {
    "start": 4021510,
    "end": 4024740,
    "text": "これらの文字のbigramsは次のとおりである。"
  },
  {
    "start": 4028730,
    "end": 4034310,
    "text": "この単語ひとつをとっても、先にも述べたように、ニューラルネットワークには12345の例文がある。"
  },
  {
    "start": 4034650,
    "end": 4039210,
    "text": "エマには5つの異なる例があり、それらの例をここに示す。"
  },
  {
    "start": 4039280,
    "end": 4047706,
    "text": "ニューラルネットワークへの入力が整数0の場合、望ましいラベルは整数5であり、これはeに対応する。"
  },
  {
    "start": 4047888,
    "end": 4056510,
    "text": "ニューラルネットワークへの入力が5であるとき、13を入れたときに13が非常に高い確率になるように重みを配置したい。"
  },
  {
    "start": 4056580,
    "end": 4060286,
    "text": "13が入ったときに高い確率で13が入るようにしたい。"
  },
  {
    "start": 4060308,
    "end": 4064926,
    "text": "また、入力されたときの確率も高くしたい。"
  },
  {
    "start": 4064958,
    "end": 4067166,
    "text": "ゼロは非常に高い確率であってほしい。"
  },
  {
    "start": 4067358,
    "end": 4072980,
    "text": "このデータセットには、ニューラルネットへの入力例が5つある。"
  },
  {
    "start": 4074950,
    "end": 4081030,
    "text": "余談だが、これらのフレームワークのAPIの多くには注意が必要だ。"
  },
  {
    "start": 4081370,
    "end": 4087670,
    "text": "私が黙ってtorch tensorを小文字のtで使い、出力が正しく見えたのを見ただろう。"
  },
  {
    "start": 4087820,
    "end": 4091638,
    "text": "テンソルを構築するには、実際には2つの方法があることを知っておく必要がある。"
  },
  {
    "start": 4091734,
    "end": 4098250,
    "text": "トーチ小文字テンソルもあるし、トーチ大文字テンソルクラスもある。"
  },
  {
    "start": 4098590,
    "end": 4100090,
    "text": "の両方を呼ぶことができる。"
  },
  {
    "start": 4100160,
    "end": 4105166,
    "text": "また、トーチ資本テンソルも可能で、XとYも得られる。"
  },
  {
    "start": 4105348,
    "end": 4107280,
    "text": "全然混乱しないね。"
  },
  {
    "start": 4108770,
    "end": 4111390,
    "text": "この2つの違いは何かというスレッドがある。"
  },
  {
    "start": 4111540,
    "end": 4116082,
    "text": "残念なことに、ドキュメントにはその違いがはっきりと書かれていない。"
  },
  {
    "start": 4116136,
    "end": 4126610,
    "text": "小文字のテンソルのドキュメントを見ると、データをコピーすることでオートグラッド履歴のないテンソルを構築している。"
  },
  {
    "start": 4126680,
    "end": 4131318,
    "text": "実際の違いは、私が知る限りでは、このランダムなスレッドで説明されている。"
  },
  {
    "start": 4131484,
    "end": 4145446,
    "text": "結局のところ、torch tensorはdtype、つまりデータ型を自動的に推論するのに対して、torch tensorはフローテンソルを返すだけだ。"
  },
  {
    "start": 4145478,
    "end": 4145942,
    "text": "トーチ"
  },
  {
    "start": 4146006,
    "end": 4147210,
    "text": "小文字のテンソル。"
  },
  {
    "start": 4147790,
    "end": 4157280,
    "text": "確かに、これを大文字のtで構成すると、xのデータ型はfloat 32になることがわかる。"
  },
  {
    "start": 4157970,
    "end": 4165410,
    "text": "トーチ、小文字のテンソル、X Dtypeがintegerになったのがわかるだろう。"
  },
  {
    "start": 4166710,
    "end": 4174046,
    "text": "小文字のtを使うことをお勧めします。"
  },
  {
    "start": 4174238,
    "end": 4186902,
    "text": "基本的に、私がこういったことをいくつか指摘しているのは、あなたに注意を促したいからであり、多くの文書を読み、多くのQ＆Aやこのようなスレッドに目を通すことに慣れてほしいからだ。"
  },
  {
    "start": 4187036,
    "end": 4191286,
    "text": "残念ながら、このようなことは簡単ではないし、あまり文書化されていない。"
  },
  {
    "start": 4191318,
    "end": 4192618,
    "text": "気をつけないといけないよ。"
  },
  {
    "start": 4192704,
    "end": 4196780,
    "text": "なぜなら、それが理にかなっているからだ。"
  },
  {
    "start": 4197950,
    "end": 4201210,
    "text": "だから小文字のテンソルを使っている。"
  },
  {
    "start": 4201280,
    "end": 4206026,
    "text": "では、これらの例をどのようにニューラルネットワークに送り込むかを考えよう。"
  },
  {
    "start": 4206218,
    "end": 4212282,
    "text": "今の例題は整数なので、プラグインするほど簡単ではない。"
  },
  {
    "start": 4212346,
    "end": 4214570,
    "text": "ゼロとか5とか13とか。"
  },
  {
    "start": 4214650,
    "end": 4216462,
    "text": "これは、その人物の指標となる。"
  },
  {
    "start": 4216526,
    "end": 4219714,
    "text": "整数のインデックスをニューラルネットに差し込むことはできない。"
  },
  {
    "start": 4219912,
    "end": 4226782,
    "text": "ニューラル・ネットはニューロンで構成され、ニューロンには重みがある。"
  },
  {
    "start": 4226846,
    "end": 4234118,
    "text": "マイクログラッドで見たように、これらの重みはWXとBの入力に対して乗法的に作用する。"
  },
  {
    "start": 4234204,
    "end": 4241446,
    "text": "だから、入力ニューロンに整数値を入力させ、それに重みを乗算させるのは、あまり意味がない。"
  },
  {
    "start": 4241638,
    "end": 4246758,
    "text": "その代わりに、整数をエンコードする一般的な方法は、1ホットエンコーディングと呼ばれるものだ。"
  },
  {
    "start": 4246934,
    "end": 4257262,
    "text": "あるホットエンコーディングでは、13のような整数を取り、13番目の次元を除いてすべてゼロのベクトルを作り、それを1にする。"
  },
  {
    "start": 4257396,
    "end": 4260480,
    "text": "そのベクトルをニューラルネットに入力することができる。"
  },
  {
    "start": 4261090,
    "end": 4269166,
    "text": "さて、便利なことに、実はPytorchにはトーチの中にワンホット機能というものがある。"
  },
  {
    "start": 4269198,
    "end": 4273010,
    "text": "であれば、整数からなるテンソルを取る。"
  },
  {
    "start": 4275050,
    "end": 4286920,
    "text": "Longは整数で、テンソルやベクトルの大きさを表すクラス数を取る。"
  },
  {
    "start": 4287610,
    "end": 4290094,
    "text": "ここで、トーチを輸入しよう。"
  },
  {
    "start": 4290242,
    "end": 4291782,
    "text": "機能的なSF。"
  },
  {
    "start": 4291846,
    "end": 4293850,
    "text": "これは一般的な輸入方法だ。"
  },
  {
    "start": 4294000,
    "end": 4295980,
    "text": "それなら、Fドットにしよう。"
  },
  {
    "start": 4296590,
    "end": 4303370,
    "text": "エンコードしたい整数を入力するので、実際にはxの配列全体を入力することができる。"
  },
  {
    "start": 4303950,
    "end": 4309342,
    "text": "だから、numクラスが27であることを伝えれば、それを推測する必要はない。"
  },
  {
    "start": 4309396,
    "end": 4311466,
    "text": "まだ13歳だと察したのかもしれない。"
  },
  {
    "start": 4311578,
    "end": 4313380,
    "text": "は正しくない結果をもたらすだろう。"
  },
  {
    "start": 4314550,
    "end": 4315778,
    "text": "これこそホットだ。"
  },
  {
    "start": 4315864,
    "end": 4319970,
    "text": "これをxencodedのxインクと呼ぶことにしよう。"
  },
  {
    "start": 4321990,
    "end": 4335186,
    "text": "そして、xencoded shapeが5×27であることがわかる。少し乱雑なので、もう少しわかりやすくするために、plt im show of x inkで視覚化することもできる。"
  },
  {
    "start": 4335378,
    "end": 4340386,
    "text": "5つの例をすべてベクトルにエンコードしたことがわかる。"
  },
  {
    "start": 4340498,
    "end": 4346026,
    "text": "例題が5つあるので、5つの行があり、それぞれの行がニューラルネットの例題となる。"
  },
  {
    "start": 4346208,
    "end": 4351340,
    "text": "を見ると、該当するビットが1になっており、それ以外はすべて0になっている。"
  },
  {
    "start": 4351870,
    "end": 4358222,
    "text": "ここでは、例えば0番目のビットがオンになり、5番目のビットがオンになる。"
  },
  {
    "start": 4358356,
    "end": 4361450,
    "text": "これらの例では、いずれも13番目のビットがオンになっている。"
  },
  {
    "start": 4361530,
    "end": 4364160,
    "text": "すると、最初のビットがオンになる。"
  },
  {
    "start": 4364550,
    "end": 4369214,
    "text": "これが整数をベクトルにエンコードする方法だ。"
  },
  {
    "start": 4369342,
    "end": 4372014,
    "text": "そうすれば、これらのベクトルをニューラルネットに送り込むことができる。"
  },
  {
    "start": 4372062,
    "end": 4377054,
    "text": "ところで、ここで気をつけなければならないもう一つの問題は、ベクトルエンコーディングのデータ型を見てみよう。"
  },
  {
    "start": 4377102,
    "end": 4379250,
    "text": "データ型には常に注意したい。"
  },
  {
    "start": 4379410,
    "end": 4382694,
    "text": "xencodingのデータ型は何だと思いますか？"
  },
  {
    "start": 4382812,
    "end": 4386306,
    "text": "ニューラルネットに数値を入力する場合、整数である必要はない。"
  },
  {
    "start": 4386338,
    "end": 4390318,
    "text": "様々な値を取ることができる浮動小数点数にしたい。"
  },
  {
    "start": 4390434,
    "end": 4394038,
    "text": "ここでのdtypeは実際には64ビット整数である。"
  },
  {
    "start": 4394214,
    "end": 4401690,
    "text": "その理由は、あるホットがここで64ビットの整数を受け取り、同じデータ型を返したからだろう。"
  },
  {
    "start": 4401840,
    "end": 4408314,
    "text": "hotのシグネチャを見ると、出力テンソルのデータ型であるdtypeさえ取っていない。"
  },
  {
    "start": 4408442,
    "end": 4415566,
    "text": "だから、トーチの多くの関数では、dtype equals torch float 32のようなことはできない。"
  },
  {
    "start": 4415668,
    "end": 4417762,
    "text": "onehotはそれをサポートしていない。"
  },
  {
    "start": 4417896,
    "end": 4433250,
    "text": "そうすることで、すべてが同じになり、すべてが同じように見えるが、dtypeはfloat 32であり、floatはニューラルネットに取り込むことができる。"
  },
  {
    "start": 4433330,
    "end": 4435510,
    "text": "では、最初のニューロンを作ってみよう。"
  },
  {
    "start": 4436010,
    "end": 4449020,
    "text": "このニューロンはこれらの入力ベクトルを見る。マイクログラッドで習ったように、これらのニューロンは基本的に非常に単純な関数を実行する。"
  },
  {
    "start": 4449550,
    "end": 4451966,
    "text": "ここでも同じことができる。"
  },
  {
    "start": 4452068,
    "end": 4454714,
    "text": "まず、このニューロンの重みを定義しよう。"
  },
  {
    "start": 4454762,
    "end": 4458570,
    "text": "基本的に、このニューロンの初期化時の重みは何ですか？"
  },
  {
    "start": 4458730,
    "end": 4461150,
    "text": "トーチランダムで初期化してみよう。"
  },
  {
    "start": 4461570,
    "end": 4468990,
    "text": "Torch randomは正規分布から引かれた乱数でテンソルを埋める。"
  },
  {
    "start": 4469150,
    "end": 4474306,
    "text": "正規分布はこのような確率密度関数を持つ。"
  },
  {
    "start": 4474408,
    "end": 4481910,
    "text": "したがって、この分布から引き出される数字のほとんどは0前後になるが、中には3近くまで高くなるものもある。"
  },
  {
    "start": 4481980,
    "end": 4485830,
    "text": "3倍を超える数字はほとんどないだろう。"
  },
  {
    "start": 4486330,
    "end": 4489820,
    "text": "ここではサイズを入力とする必要がある。"
  },
  {
    "start": 4490350,
    "end": 4493580,
    "text": "サイズは1つで27になるように使うつもりだ。"
  },
  {
    "start": 4494590,
    "end": 4496682,
    "text": "だから、27人ずつだ。"
  },
  {
    "start": 4496736,
    "end": 4498362,
    "text": "では、wを視覚化してみよう。"
  },
  {
    "start": 4498496,
    "end": 4507870,
    "text": "wは27個の数値からなる列ベクトルで、これらの重みが入力に掛けられる。"
  },
  {
    "start": 4508610,
    "end": 4514500,
    "text": "この乗算を実行するために、xencodingを取り、wと乗算することができる。"
  },
  {
    "start": 4514950,
    "end": 4523378,
    "text": "これはPytorchの行列乗算演算子で、この演算の出力は5×1である。"
  },
  {
    "start": 4523544,
    "end": 4525710,
    "text": "なぜ5対1なのかというと、次のような理由だ。"
  },
  {
    "start": 4525870,
    "end": 4532120,
    "text": "5×27のxencodingを27×1にした。"
  },
  {
    "start": 4533530,
    "end": 4543740,
    "text": "行列の掛け算では、この27が掛け算と足し算をするので、出力は5×1になることがわかる。"
  },
  {
    "start": 4544750,
    "end": 4558074,
    "text": "基本的に、この操作で私たちが見ているのは、5つの入力に対するこのニューロンの5つの活性化である。"
  },
  {
    "start": 4558202,
    "end": 4560458,
    "text": "我々はそれらすべてを並行して評価してきた。"
  },
  {
    "start": 4560554,
    "end": 4563390,
    "text": "私たちは、このひとつのニューロンにただひとつの入力を与えたわけではない。"
  },
  {
    "start": 4563470,
    "end": 4567950,
    "text": "5つの入力を同時に同じニューロンに入力した。"
  },
  {
    "start": 4568110,
    "end": 4572946,
    "text": "並行して、パイトーチはWXプラスBを評価した。"
  },
  {
    "start": 4573048,
    "end": 4574462,
    "text": "こちらはただのWXだ。"
  },
  {
    "start": 4574526,
    "end": 4575678,
    "text": "バイアスはない。"
  },
  {
    "start": 4575854,
    "end": 4580578,
    "text": "すべての選手について、w倍x倍という独立した評価をしている。"
  },
  {
    "start": 4580674,
    "end": 4587442,
    "text": "なぜ27個のニューロンが必要なのかは、後で説明しよう。"
  },
  {
    "start": 4587586,
    "end": 4593980,
    "text": "1つのニューロンの存在を示す \"1 \"の代わりに、\"27 \"を使うことができる。"
  },
  {
    "start": 4594510,
    "end": 4609310,
    "text": "とすると、wが27×27のとき、これは5つの入力すべてについて27個のニューロンすべてを並列に評価することになり、より優れた、より大きな結果が得られる。"
  },
  {
    "start": 4609460,
    "end": 4617060,
    "text": "今やったことは、5×27、27×27、そしてこの出力は5×27となった。"
  },
  {
    "start": 4617670,
    "end": 4623554,
    "text": "この形が5×27であることがわかる。"
  },
  {
    "start": 4623752,
    "end": 4626310,
    "text": "ここにあるすべての要素は何を物語っているのか？"
  },
  {
    "start": 4626380,
    "end": 4626950,
    "text": "そうだろう？"
  },
  {
    "start": 4627100,
    "end": 4639210,
    "text": "私たちが作成した27個のニューロンについて、そのニューロンの発火率が5つの例のどれに当てはまるかを教えてくれるのだ。"
  },
  {
    "start": 4639630,
    "end": 4651150,
    "text": "要素、例えば3、コンマ13は、3番目の入力を見ている13番目のニューロンの発火率を示している。"
  },
  {
    "start": 4651810,
    "end": 4663860,
    "text": "この方法は、3番目の入力とこのw行列の13列目の間のドット積によって達成された。"
  },
  {
    "start": 4664790,
    "end": 4665490,
    "text": "いいかい？"
  },
  {
    "start": 4665640,
    "end": 4672846,
    "text": "行列の乗算を使えば、たくさんの入力間のドット積を非常に効率的に評価することができる。"
  },
  {
    "start": 4672878,
    "end": 4680882,
    "text": "すべてのニューロンがWの列に重みを持つような、バッチとたくさんのニューロンでの例。"
  },
  {
    "start": 4681026,
    "end": 4683638,
    "text": "行列の乗算では、ドット積を行うだけだ。"
  },
  {
    "start": 4683724,
    "end": 4695530,
    "text": "並行して、これがそうであることを示すために、xアンクを取り、3行目を取り、wを取り、その13列目を取ることができる。"
  },
  {
    "start": 4697310,
    "end": 4707774,
    "text": "であれば、3つの要素でxencを行い、13でwを掛け合わせ、それを合計すればいい。"
  },
  {
    "start": 4707812,
    "end": 4709470,
    "text": "wxにbを足したものだ。"
  },
  {
    "start": 4709620,
    "end": 4715058,
    "text": "まあ、プラスBはなくて、ただのwx積で、それがこの数字だ。"
  },
  {
    "start": 4715224,
    "end": 4725870,
    "text": "これは、この第1層のすべての入力例とすべての出力ニューロンに対して、行列の乗算演算によって効率的に行われているだけであることがわかる。"
  },
  {
    "start": 4726030,
    "end": 4732934,
    "text": "では、27次元の入力を、27個のニューロンを持つニューラルネットの第1層に入力したわけですね？"
  },
  {
    "start": 4732972,
    "end": 4736898,
    "text": "27個の入力があり、27個のニューロンがある。"
  },
  {
    "start": 4736994,
    "end": 4739718,
    "text": "これらのニューロンは、xをw回実行する。"
  },
  {
    "start": 4739804,
    "end": 4743130,
    "text": "バイアスはないし、10Hのような非線形性もない。"
  },
  {
    "start": 4743200,
    "end": 4746390,
    "text": "直線的なレイヤーにするつもりだ。"
  },
  {
    "start": 4746550,
    "end": 4749110,
    "text": "それに加えて、他のレイヤーも持たない。"
  },
  {
    "start": 4749190,
    "end": 4750074,
    "text": "これで決まりだ。"
  },
  {
    "start": 4750112,
    "end": 4755790,
    "text": "それは、最も間抜けで、最も小さく、最も単純なニューラルネットになる。"
  },
  {
    "start": 4756370,
    "end": 4760800,
    "text": "では、その27のアウトプットに何を求めるかを説明したい。"
  },
  {
    "start": 4761170,
    "end": 4771330,
    "text": "直感的に言えば、私たちがここで作り出そうとしているのは、入力された例ひとつひとつについて、あるシーケンスにおける次の文字の確率分布のようなものを作り出そうとしているのだ。"
  },
  {
    "start": 4771480,
    "end": 4779240,
    "text": "ニューロンが受け持つ27の数字をどう解釈するか、正確なセマンティクスを考えなければならない。"
  },
  {
    "start": 4779610,
    "end": 4784930,
    "text": "さて、直感的にこれらの数字がマイナスであること、そしていくつかの数字がプラスであることなどがわかるだろう。"
  },
  {
    "start": 4785090,
    "end": 4793930,
    "text": "というのも、これらは正規分布パラメータで初期化されたニューラルネット層から出てくるからだ。"
  },
  {
    "start": 4794270,
    "end": 4806206,
    "text": "私たちが欲しいのは、ここにあるようなもの、つまり、ここにある各行がカウント数を教えてくれて、そのカウント数を正規化して確率を求めるようなもので、ニューラルネットから同じようなものが出てきて欲しいのです。"
  },
  {
    "start": 4806388,
    "end": 4809600,
    "text": "今あるのは、マイナスとプラスの数字だけだ。"
  },
  {
    "start": 4810530,
    "end": 4815118,
    "text": "次に、これらの数字が次のキャラクターの確率を表すようにしたい。"
  },
  {
    "start": 4815294,
    "end": 4819170,
    "text": "確率には特別な構造があることがわかるだろう。"
  },
  {
    "start": 4819830,
    "end": 4822690,
    "text": "これらは正の数で、合計すると1になる。"
  },
  {
    "start": 4822840,
    "end": 4825538,
    "text": "だから、ニューラルネットから出てくるわけではない。"
  },
  {
    "start": 4825704,
    "end": 4832642,
    "text": "なぜなら、これらのカウントは正であり、カウントは整数だからである。"
  },
  {
    "start": 4832786,
    "end": 4836438,
    "text": "カウントもまた、ニューラルネットから出力するにはあまり良いものではない。"
  },
  {
    "start": 4836604,
    "end": 4849180,
    "text": "ニューラルネットが出力する27個の数字をどう解釈するかだが、基本的には、この27個の数字は対数である。"
  },
  {
    "start": 4850350,
    "end": 4855818,
    "text": "この表のように直接カウントを与えるのではなく、ログカウントを与えているのだ。"
  },
  {
    "start": 4855994,
    "end": 4860960,
    "text": "カウントを得るために、対数カウントを取り、それを指数化する。"
  },
  {
    "start": 4861330,
    "end": 4865940,
    "text": "さて、指数化は次のような形になる。"
  },
  {
    "start": 4867110,
    "end": 4870846,
    "text": "負か正かのどちらかの数字を取る。"
  },
  {
    "start": 4870958,
    "end": 4872690,
    "text": "実際のライン全体を使う。"
  },
  {
    "start": 4872840,
    "end": 4880200,
    "text": "であるならば、負の数を差し込むと、常に1より下にあるxにeが入ることになる。"
  },
  {
    "start": 4880570,
    "end": 4882680,
    "text": "1より低い数字が出ている"
  },
  {
    "start": 4883370,
    "end": 4892620,
    "text": "ゼロより大きい数字を差し込むと、1より大きい数字が無限大に伸びていく。"
  },
  {
    "start": 4893150,
    "end": 4908634,
    "text": "基本的には、この数字をプラスとマイナス、あちこちに振り分けるのではなく、対数カウントとして解釈する。"
  },
  {
    "start": 4908762,
    "end": 4912000,
    "text": "次に、要素ごとにこれらの数字を指数化していく。"
  },
  {
    "start": 4912690,
    "end": 4915920,
    "text": "これを指数化すると次のようになる。"
  },
  {
    "start": 4916370,
    "end": 4929960,
    "text": "これらの数字が指数を通過したため、すべての負の数が0.338のような1より小さい数に変わり、すべての正の数が1よりさらに大きい正の数に変わったことがわかるだろう。"
  },
  {
    "start": 4930730,
    "end": 4939900,
    "text": "例えば、7はゼロより大きい正の数だ。"
  },
  {
    "start": 4940990,
    "end": 4950794,
    "text": "ここで指数化された出力は、基本的に元々カウントと同等のものとして使用・解釈できるものを与えてくれる。"
  },
  {
    "start": 4950922,
    "end": 4955390,
    "text": "112、751、1、などなど。"
  },
  {
    "start": 4956290,
    "end": 4961230,
    "text": "ニューラルネットは今、カウントを予測しているようなものだ。"
  },
  {
    "start": 4961570,
    "end": 4963938,
    "text": "これらのカウントは正の数である。"
  },
  {
    "start": 4964024,
    "end": 4965378,
    "text": "ゼロ以下になることはない。"
  },
  {
    "start": 4965464,
    "end": 4966610,
    "text": "それは理にかなっている。"
  },
  {
    "start": 4966760,
    "end": 4972980,
    "text": "wの設定によって様々な値を取ることができるようになった。"
  },
  {
    "start": 4974150,
    "end": 4975974,
    "text": "これを分解しよう。"
  },
  {
    "start": 4976172,
    "end": 4979590,
    "text": "これをログカウントと解釈する。"
  },
  {
    "start": 4981210,
    "end": 4984790,
    "text": "つまり、このためによく使われるのが、いわゆるロジットである。"
  },
  {
    "start": 4985130,
    "end": 4988150,
    "text": "これはロジット（対数）である。"
  },
  {
    "start": 4988570,
    "end": 4992970,
    "text": "そして、これらは対数を指数化したカウントのようなものになる。"
  },
  {
    "start": 4993310,
    "end": 4999890,
    "text": "これはn行列に相当し、前回使ったn配列のようなものである。"
  },
  {
    "start": 4999990,
    "end": 5003946,
    "text": "これがnで、これがカウントの配列だ。"
  },
  {
    "start": 5004058,
    "end": 5010880,
    "text": "各行には次の文字のカウントがある。"
  },
  {
    "start": 5012610,
    "end": 5014174,
    "text": "これがカウントだ。"
  },
  {
    "start": 5014302,
    "end": 5019010,
    "text": "今、確率はカウントを正規化したものにすぎない。"
  },
  {
    "start": 5019590,
    "end": 5025858,
    "text": "だから、同じものを見つけるつもりはないが、基本的にあちこちスクロールするつもりはない。"
  },
  {
    "start": 5026024,
    "end": 5027446,
    "text": "これはすでにやったことだ。"
  },
  {
    "start": 5027548,
    "end": 5031586,
    "text": "最初の次元に沿った総和をカウントしたい。"
  },
  {
    "start": 5031698,
    "end": 5034280,
    "text": "私たちはディムを真実として保ちたい。"
  },
  {
    "start": 5034730,
    "end": 5044010,
    "text": "これは、確率を求めるために回数行列の行を正規化する方法である。"
  },
  {
    "start": 5044830,
    "end": 5050670,
    "text": "これが確率であり、これが現在我々が持っているカウントである。"
  },
  {
    "start": 5050740,
    "end": 5062826,
    "text": "確率を表示すると、正規化されているため、コースのすべての行の和が1になることがわかる。"
  },
  {
    "start": 5063018,
    "end": 5066900,
    "text": "この形は5×27だ。"
  },
  {
    "start": 5067350,
    "end": 5074866,
    "text": "つまり、私たちが達成したのは、5つの例のすべてについて、ニューラルネットから生まれた行があるということだ。"
  },
  {
    "start": 5075048,
    "end": 5083350,
    "text": "この変換により、ニューラルネットの出力が確率になることを確認した。"
  },
  {
    "start": 5084010,
    "end": 5090614,
    "text": "私たちのWxはロジットを出し、私たちはそれをログカウントと解釈する。"
  },
  {
    "start": 5090742,
    "end": 5097206,
    "text": "指数化してカウントのようなものを得、そのカウントを正規化して確率分布を得る。"
  },
  {
    "start": 5097398,
    "end": 5100086,
    "text": "これらはすべて微分可能な操作である。"
  },
  {
    "start": 5100278,
    "end": 5109470,
    "text": "私たちが今やっていることは、入力を受け取り、微分可能な演算を行い、それを逆伝播して確率分布を得るということだ。"
  },
  {
    "start": 5109890,
    "end": 5127746,
    "text": "例えば、0番目に投入された例では、この0番目の例はゼロの1ホットベクトルであり、基本的にこの例での投入に相当する。"
  },
  {
    "start": 5127848,
    "end": 5130246,
    "text": "我々はドットをニューラルネットに入力している。"
  },
  {
    "start": 5130348,
    "end": 5142810,
    "text": "ドットをニューラルネットに入力する方法は、まずそのインデックスを取得し、次にそれをホットエンコードし、ニューラルネットに入力すると、この確率分布が出てくるというものだ。"
  },
  {
    "start": 5143310,
    "end": 5147322,
    "text": "その形状は27である。"
  },
  {
    "start": 5147376,
    "end": 5148554,
    "text": "27の数字がある。"
  },
  {
    "start": 5148672,
    "end": 5159200,
    "text": "これは、ニューラルネットが、27人の登場人物のうち、どの人物が次に登場する可能性が高いかを割り出したものと解釈する。"
  },
  {
    "start": 5159730,
    "end": 5166926,
    "text": "重みを調整すると、もちろん、入力した文字に対して異なる確率が得られることになる。"
  },
  {
    "start": 5167118,
    "end": 5177074,
    "text": "そこで今問題になっているのは、最適化して、出てくる確率がかなり良くなるような良いwを見つけることができるかということだ。"
  },
  {
    "start": 5177192,
    "end": 5181126,
    "text": "それで、もう少しわかりやすくなるように、すべてを1つの要約に整理してみた。"
  },
  {
    "start": 5181228,
    "end": 5182422,
    "text": "それはここから始まる。"
  },
  {
    "start": 5182556,
    "end": 5184278,
    "text": "入力データセットがある。"
  },
  {
    "start": 5184444,
    "end": 5190310,
    "text": "ニューラルネットへの入力がいくつかあり、シーケンスの正しい次の文字のラベルがいくつかある。"
  },
  {
    "start": 5190390,
    "end": 5193114,
    "text": "これらはここでは整数である。"
  },
  {
    "start": 5193152,
    "end": 5206110,
    "text": "今はトルシュ・ジェネレーターを使っているので、私が見ているのと同じ数字が表示される。27個のニューロン、重みを生成し、それぞれのニューロンが27個の入力を受け取る。"
  },
  {
    "start": 5208530,
    "end": 5212782,
    "text": "次に、すべての入力例xをニューラルネットに差し込む。"
  },
  {
    "start": 5212836,
    "end": 5214740,
    "text": "これはフォワードパスだ。"
  },
  {
    "start": 5215670,
    "end": 5219890,
    "text": "まず、すべての入力を1つのホットな表現にエンコードしなければならない。"
  },
  {
    "start": 5220310,
    "end": 5221806,
    "text": "我々は27クラスある。"
  },
  {
    "start": 5221838,
    "end": 5231620,
    "text": "これらの整数を渡すと、xインクは5×27のゼロの配列になる。"
  },
  {
    "start": 5232170,
    "end": 5243290,
    "text": "これをニューラルネットの第1層で乗算してロジットを求め、そのロジットを指数化して偽のカウントを求め、そのカウントを正規化して確率を求める。"
  },
  {
    "start": 5244350,
    "end": 5251774,
    "text": "ちなみに最後の2行はソフトマックスと呼ばれるもので、ここに引っ張り出してきた。"
  },
  {
    "start": 5251972,
    "end": 5263030,
    "text": "ソフトマックスはニューラルネットでよく使われるレイヤーで、ロジットであるzを指数化し、除算して正規化する。"
  },
  {
    "start": 5263130,
    "end": 5266270,
    "text": "ニューラルネット層の出力を取る方法だ。"
  },
  {
    "start": 5266350,
    "end": 5269410,
    "text": "これらの出力は正または負になる。"
  },
  {
    "start": 5269750,
    "end": 5272494,
    "text": "は確率分布を出力する。"
  },
  {
    "start": 5272542,
    "end": 5278290,
    "text": "確率と同じように、常に和が1で正の数を出力する。"
  },
  {
    "start": 5278790,
    "end": 5282006,
    "text": "これは正規化関数のようなものだ。"
  },
  {
    "start": 5282108,
    "end": 5285590,
    "text": "ニューラルネット内の他のリニアレイヤーの上に置くことができる。"
  },
  {
    "start": 5285660,
    "end": 5292220,
    "text": "これは基本的にニューラルネットの出力確率を作るもので、非常によく使われる。"
  },
  {
    "start": 5293310,
    "end": 5294522,
    "text": "これがフォワードパスだ。"
  },
  {
    "start": 5294576,
    "end": 5297050,
    "text": "こうしてニューラルネットの出力確率ができた。"
  },
  {
    "start": 5297870,
    "end": 5307850,
    "text": "さて、このフォワードパス全体が微分可能なレイヤーで構成されていることにお気づきだろうか。"
  },
  {
    "start": 5307930,
    "end": 5310110,
    "text": "ここにあるものはすべて逆伝播できる。"
  },
  {
    "start": 5310180,
    "end": 5313130,
    "text": "マイクログラッドの逆伝播の一部を見た。"
  },
  {
    "start": 5313290,
    "end": 5316390,
    "text": "これは単なる掛け算と足し算だ。"
  },
  {
    "start": 5316490,
    "end": 5318690,
    "text": "ここで行われているのは、掛け算と足し算だけだ。"
  },
  {
    "start": 5318760,
    "end": 5320642,
    "text": "バックプロパゲートの方法は知っている。"
  },
  {
    "start": 5320776,
    "end": 5323460,
    "text": "指数関数、我々は逆伝播の方法を知っている。"
  },
  {
    "start": 5323830,
    "end": 5326334,
    "text": "では、ここで合計を出す。"
  },
  {
    "start": 5326462,
    "end": 5331846,
    "text": "和はバックプロパゲーションが容易であり、除算も同様である。"
  },
  {
    "start": 5331948,
    "end": 5336440,
    "text": "ここでのすべては微分可能な操作であり、逆伝播することができる。"
  },
  {
    "start": 5337530,
    "end": 5341418,
    "text": "この確率は27分の5である。"
  },
  {
    "start": 5341584,
    "end": 5346026,
    "text": "すべての例について、1対1の確率のベクトルがある。"
  },
  {
    "start": 5346208,
    "end": 5351354,
    "text": "そしてここで、例を分解するようなものをたくさん書いた。"
  },
  {
    "start": 5351482,
    "end": 5355120,
    "text": "エマを構成する5つの例があるよね？"
  },
  {
    "start": 5356290,
    "end": 5359390,
    "text": "エマの中にはグラム単位で5つある。"
  },
  {
    "start": 5360050,
    "end": 5362554,
    "text": "グラム単位で。"
  },
  {
    "start": 5362602,
    "end": 5367490,
    "text": "例1として、eはドットの直後の開始文字である。"
  },
  {
    "start": 5368230,
    "end": 5371170,
    "text": "これらの指数は0と5である。"
  },
  {
    "start": 5371320,
    "end": 5373620,
    "text": "で、ゼロを入れる。"
  },
  {
    "start": 5374070,
    "end": 5375782,
    "text": "これがニューラルネットの入力だ。"
  },
  {
    "start": 5375916,
    "end": 5380200,
    "text": "ニューラルネットから27個の確率を得る。"
  },
  {
    "start": 5381210,
    "end": 5385714,
    "text": "であれば、eは実際にはドットの後に来るので、ラベルは5となる。"
  },
  {
    "start": 5385842,
    "end": 5387270,
    "text": "それがラベルだ。"
  },
  {
    "start": 5387870,
    "end": 5394218,
    "text": "そして、このラベル5を確率分布のインデックスとして使用する。"
  },
  {
    "start": 5394384,
    "end": 5399610,
    "text": "このインデックス5は012345である。"
  },
  {
    "start": 5399680,
    "end": 5402480,
    "text": "ここにあるこの番号だ。"
  },
  {
    "start": 5403970,
    "end": 5408602,
    "text": "これは基本的に、ニューラルネットが実際の正しい文字に割り当てる確率である。"
  },
  {
    "start": 5408746,
    "end": 5417122,
    "text": "ネットワークは現在、この次の文字、つまりドットに続くeは1％の可能性しかないと考えていることがわかる。"
  },
  {
    "start": 5417176,
    "end": 5422430,
    "text": "というのも、これは実際にトレーニングの例であり、ネットワークは今のところ、これはとてもあり得ないと考えているからだ。"
  },
  {
    "start": 5422510,
    "end": 5426914,
    "text": "それは、Wのセッティングに恵まれなかったからだ。"
  },
  {
    "start": 5427032,
    "end": 5431510,
    "text": "今のところ、このネットワークはこの可能性は低いと考えており、0.1は良い結果ではない。"
  },
  {
    "start": 5431850,
    "end": 5439138,
    "text": "の場合、対数尤度は非常に負になり、負の対数尤度は非常に正になる。"
  },
  {
    "start": 5439314,
    "end": 5443110,
    "text": "ということは、4は非常に高い負の対数尤度となる。"
  },
  {
    "start": 5443190,
    "end": 5446694,
    "text": "ということは、損失が大きくなるということだ。"
  },
  {
    "start": 5446822,
    "end": 5450170,
    "text": "この損失は、対数尤度の平均マイナスである。"
  },
  {
    "start": 5451490,
    "end": 5453390,
    "text": "番目の文字はemである。"
  },
  {
    "start": 5453540,
    "end": 5459200,
    "text": "ここで、Mがeに続く可能性は1％と非常に低いとネットワークも考えていることがわかる。"
  },
  {
    "start": 5461410,
    "end": 5467806,
    "text": "Mに続くMでは2％、Mに続くAでは7％の可能性があると考えた。"
  },
  {
    "start": 5467918,
    "end": 5475086,
    "text": "偶然だが、この確率はかなり高く、したがって負の対数尤度はかなり低い。"
  },
  {
    "start": 5475278,
    "end": 5478194,
    "text": "ようやく、この可能性が1％でも高くなった。"
  },
  {
    "start": 5478322,
    "end": 5488678,
    "text": "全体として、私たちの平均負対数尤度は、損失、合計損失であり、基本的にこのネットワークが、少なくともこの1つの単語について、現在どの程度機能しているかを要約しています。"
  },
  {
    "start": 5488764,
    "end": 5489814,
    "text": "完全なデータセットではない。"
  },
  {
    "start": 5489852,
    "end": 5494214,
    "text": "たった一言で3.76というのは、実は非常に、かなり高い損失である。"
  },
  {
    "start": 5494262,
    "end": 5496646,
    "text": "これはWのセッティングとしてはあまり良くない。"
  },
  {
    "start": 5496838,
    "end": 5498378,
    "text": "今、私たちにできることがある。"
  },
  {
    "start": 5498544,
    "end": 5500700,
    "text": "現在3.76だ。"
  },
  {
    "start": 5501230,
    "end": 5504270,
    "text": "私たちは実際にここに来て、自分のWを変えることができる。"
  },
  {
    "start": 5504340,
    "end": 5505598,
    "text": "私たちはそれを再サンプルすることができる。"
  },
  {
    "start": 5505684,
    "end": 5512240,
    "text": "別のシードを追加して、別のWを得たら、これを再実行しよう。"
  },
  {
    "start": 5512770,
    "end": 5518306,
    "text": "このようにwの設定を変えると、3.37となる。"
  },
  {
    "start": 5518488,
    "end": 5520946,
    "text": "こっちの方がずっといいでしょ？"
  },
  {
    "start": 5521128,
    "end": 5528280,
    "text": "その方がいいのは、実際に次に登場するキャラクターの確率がたまたま高く出るからだ。"
  },
  {
    "start": 5528730,
    "end": 5531446,
    "text": "だから、実際にこれをリサンプリングするだけでも想像がつくだろう。"
  },
  {
    "start": 5531628,
    "end": 5533160,
    "text": "2つ試してみよう"
  },
  {
    "start": 5535450,
    "end": 5537062,
    "text": "オーケー、これはあまり良くなかった。"
  },
  {
    "start": 5537196,
    "end": 5538362,
    "text": "もうひとつやってみよう。"
  },
  {
    "start": 5538496,
    "end": 5539820,
    "text": "3つ試してみよう"
  },
  {
    "start": 5540830,
    "end": 5544518,
    "text": "これはひどいセッティングだった。"
  },
  {
    "start": 5544694,
    "end": 5547820,
    "text": "とにかく、これを消します。"
  },
  {
    "start": 5549950,
    "end": 5555342,
    "text": "私がここでやっていることは、ランダムにパラメーターを割り当てて、ネットワークが良好かどうかを推測してチェックしているだけだ。"
  },
  {
    "start": 5555476,
    "end": 5557294,
    "text": "それはアマチュアのやることだ。"
  },
  {
    "start": 5557332,
    "end": 5559006,
    "text": "それはニューラルネットを最適化する方法ではない。"
  },
  {
    "start": 5559108,
    "end": 5565010,
    "text": "ニューラルネットを最適化する方法は、ランダムに推測することから始める。"
  },
  {
    "start": 5565160,
    "end": 5567700,
    "text": "ここで重要なのは、損失関数があるということだ。"
  },
  {
    "start": 5568230,
    "end": 5573410,
    "text": "この損失は、微分可能な操作のみで構成されている。"
  },
  {
    "start": 5574150,
    "end": 5584150,
    "text": "これらのw行列に関して損失の勾配を計算することにより、wを調整することで損失を最小化することができる。"
  },
  {
    "start": 5584970,
    "end": 5591430,
    "text": "そこで、損失を最小化するようにwを調整し、勾配ベースの最適化を使ってwの良い設定を見つけることができる。"
  },
  {
    "start": 5591590,
    "end": 5593018,
    "text": "どうなるか見てみよう。"
  },
  {
    "start": 5593104,
    "end": 5596982,
    "text": "これで、マイクログラッドの時とほぼ同じような展開になるだろう。"
  },
  {
    "start": 5597126,
    "end": 5601958,
    "text": "ここで私は、マイクログラッドの講義をノートにまとめた。"
  },
  {
    "start": 5602054,
    "end": 5603558,
    "text": "このリポジトリのものだ。"
  },
  {
    "start": 5603734,
    "end": 5608366,
    "text": "マイクログラッドと同じように、最後までスクロールすると、似たようなことが書いてあった。"
  },
  {
    "start": 5608548,
    "end": 5610906,
    "text": "インプットの例はいくつもあった。"
  },
  {
    "start": 5610938,
    "end": 5623950,
    "text": "この場合、xの中に4つの入力例があり、それぞれのターゲット、希望するターゲットがあった。"
  },
  {
    "start": 5624110,
    "end": 5630920,
    "text": "整数をベクトルに変換する。ただし、ベクトルは3倍ではなく27倍になる。"
  },
  {
    "start": 5631930,
    "end": 5639590,
    "text": "そこでまず、すべての入力に対してニューラルネットを実行し、予測を得るというフォワードパスを行った。"
  },
  {
    "start": 5640170,
    "end": 5644902,
    "text": "私たちのニューラルネットは、このxのnの時点では多層パーセプトロンであった。"
  },
  {
    "start": 5645046,
    "end": 5653290,
    "text": "このニューラルネットは、単層、単線型層、ソフトマックス層で構成されている。"
  },
  {
    "start": 5653710,
    "end": 5655280,
    "text": "それが我々のニューラルネットだ"
  },
  {
    "start": 5655730,
    "end": 5658330,
    "text": "ここでの損失は平均二乗誤差である。"
  },
  {
    "start": 5658410,
    "end": 5662910,
    "text": "私たちは単純に、地上真実から予測値を引き、それを二乗して、すべてを合計した。"
  },
  {
    "start": 5662980,
    "end": 5664038,
    "text": "それが敗因だった。"
  },
  {
    "start": 5664154,
    "end": 5668418,
    "text": "の損失は、ニューラルネットの品質を要約する唯一の数字だった。"
  },
  {
    "start": 5668504,
    "end": 5675250,
    "text": "損失がほとんどゼロのように低い場合は、ニューラルネットが正しく予測していることを意味する。"
  },
  {
    "start": 5676170,
    "end": 5681894,
    "text": "ニューラルネットの性能を要約する数値が1つあった。"
  },
  {
    "start": 5682012,
    "end": 5685910,
    "text": "ここではすべてが微分可能で、巨大な計算グラフに保存されていた。"
  },
  {
    "start": 5686730,
    "end": 5691562,
    "text": "その後、すべてのパラメーターを繰り返し、勾配がゼロに設定されていることを確認した。"
  },
  {
    "start": 5691696,
    "end": 5699642,
    "text": "をlost backwardと呼び、lost backwardはlossの最終出力ノードでバックプロパゲーションを開始するんだよね？"
  },
  {
    "start": 5699696,
    "end": 5703582,
    "text": "ああ、こういう表現を覚えておいてほしい。"
  },
  {
    "start": 5703636,
    "end": 5710570,
    "text": "バックプロパゲーションを開始し、過去にさかのぼって、すべてのパラメータにグラッドが設定されていることを確認した。"
  },
  {
    "start": 5710730,
    "end": 5714386,
    "text": "gradはゼロから始まったが、バックプロパゲーションで埋められた。"
  },
  {
    "start": 5714488,
    "end": 5726870,
    "text": "その後、すべてのパラメーターを反復して更新し、パラメーターの各要素を勾配の逆方向に移動させるという単純なパラメーターの更新を行った。"
  },
  {
    "start": 5727530,
    "end": 5731320,
    "text": "だから、ここでもまったく同じことをするつもりだ。"
  },
  {
    "start": 5731690,
    "end": 5741962,
    "text": "これをサイドに寄せて、使えるようにしておく。"
  },
  {
    "start": 5742096,
    "end": 5743994,
    "text": "これがフォワードパスだった。"
  },
  {
    "start": 5744112,
    "end": 5748838,
    "text": "このようなことをしたのは、私たちのYpreadである。"
  },
  {
    "start": 5748934,
    "end": 5750502,
    "text": "今は損失を評価しなければならない。"
  },
  {
    "start": 5750566,
    "end": 5758160,
    "text": "平均二乗誤差ではなく、負の対数尤度を使っているのは、分類をしているのであって、いわゆる回帰をしているわけではないからだ。"
  },
  {
    "start": 5759010,
    "end": 5761390,
    "text": "ここでは損失を計算したい。"
  },
  {
    "start": 5762450,
    "end": 5766926,
    "text": "今、私たちが計算しているのは、この平均負対数尤度である。"
  },
  {
    "start": 5767118,
    "end": 5772900,
    "text": "さて、この問題は5×27の形をしている。"
  },
  {
    "start": 5773270,
    "end": 5779862,
    "text": "そのため、すべての確率を得るためには、基本的にここで正しい指標を抜き出したい。"
  },
  {
    "start": 5779996,
    "end": 5789530,
    "text": "特に、ラベルは配列y'sに格納されているので、基本的には、最初の例では、5つの確率を見ていることになる。"
  },
  {
    "start": 5789600,
    "end": 5790730,
    "text": "インデックス5で。"
  },
  {
    "start": 5790880,
    "end": 5799690,
    "text": "2つ目の例では、2行目のインデックス1において、インデックス13に割り当てられた確率に興味がある。"
  },
  {
    "start": 5800190,
    "end": 5802830,
    "text": "番目の例でも13番がある。"
  },
  {
    "start": 5803410,
    "end": 5806320,
    "text": "3列目には1つ欲しい。"
  },
  {
    "start": 5807250,
    "end": 5811134,
    "text": "最後の行、つまり4行目では、ゼロを求める。"
  },
  {
    "start": 5811252,
    "end": 5813870,
    "text": "これが私たちが興味を持っている確率ですよね？"
  },
  {
    "start": 5814020,
    "end": 5817650,
    "text": "を見ると、上で見たような驚くべきものではないことがわかる。"
  },
  {
    "start": 5818550,
    "end": 5826958,
    "text": "しかし、このようなタプルに列挙するだけでなく、もっと効率的に確率にアクセスする方法が欲しい。"
  },
  {
    "start": 5827064,
    "end": 5841878,
    "text": "pytorchでこれを行う方法、少なくともそのひとつは、基本的にこれらの整数をすべてベクトルで渡すことだ。"
  },
  {
    "start": 5842054,
    "end": 5849094,
    "text": "これらは01234だが、これはmpを使って作成できる。"
  },
  {
    "start": 5849222,
    "end": 5854238,
    "text": "mpではなく、501234のトーチアレンジ。"
  },
  {
    "start": 5854404,
    "end": 5857840,
    "text": "トーチを5本並べれば、ここにインデックスができる。"
  },
  {
    "start": 5858210,
    "end": 5860590,
    "text": "ここではY'sでインデックスを作る。"
  },
  {
    "start": 5861090,
    "end": 5875810,
    "text": "ニューラルネットワークが正しい次の文字に割り当てる確率を抜き出している。"
  },
  {
    "start": 5876230,
    "end": 5880450,
    "text": "さて、これらの確率を実際に対数確率で見てみよう。"
  },
  {
    "start": 5880610,
    "end": 5886694,
    "text": "ドットログを作成し、それを平均化する。"
  },
  {
    "start": 5886732,
    "end": 5893050,
    "text": "その平均を取り、負の平均対数尤度が損失となる。"
  },
  {
    "start": 5894110,
    "end": 5897834,
    "text": "ここでの損失は3.70ドルだ。"
  },
  {
    "start": 5897952,
    "end": 5906298,
    "text": "この損失、3.76, 3.76は、前に求めたものとまったく同じだが、これはその式をベクトル化したものであることがわかるだろう。"
  },
  {
    "start": 5906474,
    "end": 5913870,
    "text": "このフォワード・パスの一部として、同じロスと同じロスを考えることができる。"
  },
  {
    "start": 5914020,
    "end": 5916202,
    "text": "私たちは今、ここで損失を達成した。"
  },
  {
    "start": 5916346,
    "end": 5918366,
    "text": "さて、私たちはロスまでやってきた。"
  },
  {
    "start": 5918398,
    "end": 5922062,
    "text": "フォワードパスを定義し、ネットワークとロスを転送した。"
  },
  {
    "start": 5922126,
    "end": 5924130,
    "text": "これでバックワードパスの準備は整った。"
  },
  {
    "start": 5924280,
    "end": 5932230,
    "text": "そこでバックワードパスでは、まずすべてのグラデーションがゼロになるようにリセットする。"
  },
  {
    "start": 5932380,
    "end": 5938018,
    "text": "さて、Pytorchではグラデーションをゼロに設定することもできるが、単にゼロに設定することもできる。"
  },
  {
    "start": 5938114,
    "end": 5940118,
    "text": "を \"なし \"に設定する方が効率的だ。"
  },
  {
    "start": 5940214,
    "end": 5945654,
    "text": "Pytorchは \"none \"を \"勾配がない \"と解釈する。"
  },
  {
    "start": 5945782,
    "end": 5948890,
    "text": "これは、グラデーションをゼロに設定する方法である。"
  },
  {
    "start": 5950370,
    "end": 5952510,
    "text": "今、私たちは後方に失われている。"
  },
  {
    "start": 5954610,
    "end": 5956942,
    "text": "ロスト・バックする前に、もうひとつ必要なことがある。"
  },
  {
    "start": 5956996,
    "end": 5964260,
    "text": "Microgradを思い出してほしいのだが、Pytorchはgradが真であることを要求するパスが必要なのだ。"
  },
  {
    "start": 5965110,
    "end": 5971422,
    "text": "この葉テンソルの勾配を計算することに興味があることをPytorchに伝える。"
  },
  {
    "start": 5971486,
    "end": 5973262,
    "text": "デフォルトではfalseである。"
  },
  {
    "start": 5973406,
    "end": 5975606,
    "text": "それで計算し直してみよう。"
  },
  {
    "start": 5975788,
    "end": 5978870,
    "text": "その後、「なし」に設定され、後方へ失点した。"
  },
  {
    "start": 5980650,
    "end": 5992310,
    "text": "Pytorchは、Microgradと同じように、フォワード・パスを実行すると、ボンネットの中のすべてのオペレーションを記録する。"
  },
  {
    "start": 5992390,
    "end": 6000410,
    "text": "Microgradで作成したグラフと同じように、完全な計算グラフを構築し、それらのグラフはPytorchの中に存在する。"
  },
  {
    "start": 6000750,
    "end": 6004798,
    "text": "そのため、すべての依存関係とすべての数学的操作を知っている。"
  },
  {
    "start": 6004964,
    "end": 6019842,
    "text": "ドット・バックワードでは、ニューラルネットのパラメータであるwまで、すべての中間値の勾配を埋めることができる。"
  },
  {
    "start": 6019976,
    "end": 6023870,
    "text": "これでWfGradができるようになった。"
  },
  {
    "start": 6023950,
    "end": 6025560,
    "text": "中身があるんだ。"
  },
  {
    "start": 6028970,
    "end": 6032600,
    "text": "このグラデーション、ここにあるすべての要素。"
  },
  {
    "start": 6033290,
    "end": 6036678,
    "text": "ドットシェイプは27×27。"
  },
  {
    "start": 6036844,
    "end": 6040220,
    "text": "Wグラッドの形は同じ27×27。"
  },
  {
    "start": 6040590,
    "end": 6048474,
    "text": "Wドットグラッドの各要素は、その重みが損失関数に与える影響を示している。"
  },
  {
    "start": 6048672,
    "end": 6057386,
    "text": "つまり、例えば、この数字がずっとここにある場合、この要素、つまりwの要素が、勾配が正である場合だ。"
  },
  {
    "start": 6057498,
    "end": 6061274,
    "text": "それが敗戦に良い影響を与えていることを物語っている。"
  },
  {
    "start": 6061402,
    "end": 6075170,
    "text": "この勾配は正であるため、wをわずかになだめ、wをゼロ、ゼロとし、それに小さなhを加えれば、損失はマイルドに増加する。"
  },
  {
    "start": 6075590,
    "end": 6077590,
    "text": "これらのグラデーションの中には負のものもある。"
  },
  {
    "start": 6078490,
    "end": 6080998,
    "text": "これはグラデーションの情報を教えてくれている。"
  },
  {
    "start": 6081164,
    "end": 6086546,
    "text": "この勾配情報を使って、ニューラルネットワークの重みを更新することができる。"
  },
  {
    "start": 6086658,
    "end": 6088194,
    "text": "では、アップデートを行おう。"
  },
  {
    "start": 6088322,
    "end": 6090758,
    "text": "マイクログラッドの時とよく似たものになるだろう。"
  },
  {
    "start": 6090854,
    "end": 6096890,
    "text": "すべてのパラメータをループする必要はない。なぜなら、パラメータはテンソル、つまりwだけだからだ。"
  },
  {
    "start": 6097040,
    "end": 6100830,
    "text": "を単純にドット・データ＋イコールとするだけである。"
  },
  {
    "start": 6102130,
    "end": 6107390,
    "text": "実際には、これをほぼ正確にコピーすることができる。"
  },
  {
    "start": 6109410,
    "end": 6113330,
    "text": "それがテンソルの更新となる。"
  },
  {
    "start": 6114390,
    "end": 6117090,
    "text": "テンソルを更新する。"
  },
  {
    "start": 6118630,
    "end": 6123970,
    "text": "テンソルが更新されたのだから、ロスは減るはずだ。"
  },
  {
    "start": 6124310,
    "end": 6132950,
    "text": "ここで、損失項目を印刷すると、3.76でしたね？"
  },
  {
    "start": 6133100,
    "end": 6135782,
    "text": "こちらで更新しました。"
  },
  {
    "start": 6135916,
    "end": 6141274,
    "text": "フォワードパスを再計算すると、ロスはわずかに少なくなっているはずだ。"
  },
  {
    "start": 6141392,
    "end": 6151710,
    "text": "3.76が3.74になり、再びグラフをなしにして後方更新することができる。"
  },
  {
    "start": 6152450,
    "end": 6154606,
    "text": "今、パラメーターがまた変わった。"
  },
  {
    "start": 6154788,
    "end": 6160320,
    "text": "フォワードパスを再計算すると、やはり3.72と損失は少なくなる。"
  },
  {
    "start": 6162130,
    "end": 6162686,
    "text": "オーケー。"
  },
  {
    "start": 6162788,
    "end": 6166130,
    "text": "これはまた、勾配降下を行っているところだ。"
  },
  {
    "start": 6168470,
    "end": 6175198,
    "text": "損失が少ないということは、ネットワークが正しい次の文字を高い確率で割り当てていることを意味する。"
  },
  {
    "start": 6175294,
    "end": 6179186,
    "text": "それで、すべてをアレンジして、一から組み立てたんだ。"
  },
  {
    "start": 6179378,
    "end": 6182870,
    "text": "ここでビグラムのデータセットを構築する。"
  },
  {
    "start": 6183210,
    "end": 6185782,
    "text": "まだ最初の単語だけを反復しているのがわかるだろう。"
  },
  {
    "start": 6185836,
    "end": 6186706,
    "text": "エマ"
  },
  {
    "start": 6186898,
    "end": 6188950,
    "text": "すぐに変えるつもりだ。"
  },
  {
    "start": 6189100,
    "end": 6196714,
    "text": "軸の要素数をカウントする数字を追加し、例数が5であることを明示した。"
  },
  {
    "start": 6196912,
    "end": 6200314,
    "text": "現在、私たちはエマと仕事をしていて、そこには5つの図がある。"
  },
  {
    "start": 6200512,
    "end": 6203582,
    "text": "ここで、以前とまったく同じループを追加した。"
  },
  {
    "start": 6203716,
    "end": 6208794,
    "text": "フォワードパス、バックワードパス、アップデートの勾配降下を10回繰り返した。"
  },
  {
    "start": 6208922,
    "end": 6217220,
    "text": "つまり、初期化と勾配降下という2つのセルを実行することで、損失関数をある程度改善することができる。"
  },
  {
    "start": 6218150,
    "end": 6225534,
    "text": "今、私はすべての単語を使いたいと思っている。"
  },
  {
    "start": 6225582,
    "end": 6229502,
    "text": "しかし、今は何の修正も必要ないはずだ。"
  },
  {
    "start": 6229566,
    "end": 6235794,
    "text": "私たちが書いたコードは、バイグラムが5個だろうが22万8000個だろうが関係ないのだから。"
  },
  {
    "start": 6235842,
    "end": 6237382,
    "text": "何事も、ただ働くべきだ。"
  },
  {
    "start": 6237436,
    "end": 6240150,
    "text": "このまま実行されることがわかるだろう。"
  },
  {
    "start": 6240300,
    "end": 6244438,
    "text": "現在では、すべてのビグラムからなる訓練セット全体に対して最適化を行っている。"
  },
  {
    "start": 6244614,
    "end": 6250460,
    "text": "そのため、学習率を上げる余裕があるのだろう。"
  },
  {
    "start": 6252270,
    "end": 6254800,
    "text": "おそらく、さらに大きな学習率を確保できるだろう。"
  },
  {
    "start": 6260610,
    "end": 6263458,
    "text": "このとてもとてもシンプルな例では、50でも機能するようだ。"
  },
  {
    "start": 6263544,
    "end": 6263890,
    "text": "そうだね。"
  },
  {
    "start": 6263960,
    "end": 6270420,
    "text": "再初期化して、100回反復してみよう。"
  },
  {
    "start": 6272870,
    "end": 6281094,
    "text": "さて、ここにきてかなり良い負け方が続いているようだ。"
  },
  {
    "start": 6281212,
    "end": 6282598,
    "text": "2.47."
  },
  {
    "start": 6282764,
    "end": 6284438,
    "text": "あと100回走らせてくれ。"
  },
  {
    "start": 6284604,
    "end": 6287186,
    "text": "ところで、予想される損失額は？"
  },
  {
    "start": 6287298,
    "end": 6291020,
    "text": "実際には、当初持っていたものと同じようなものが得られると期待している。"
  },
  {
    "start": 6291950,
    "end": 6303258,
    "text": "このビデオの冒頭で、数えるだけで最適化したとき、スムージングを加えた後の損失はおよそ2.47だったことを思い出してほしい。"
  },
  {
    "start": 6303434,
    "end": 6309498,
    "text": "平滑化する前に、我々はおよそ2.45の尤度を得た。"
  },
  {
    "start": 6309674,
    "end": 6315838,
    "text": "というわけで、実際には、私たちが達成すると予想している値にほぼ近い。"
  },
  {
    "start": 6315934,
    "end": 6320686,
    "text": "ここでは、勾配ベースの最適化によって、ほぼ同じ結果を得ている。"
  },
  {
    "start": 6320878,
    "end": 6326062,
    "text": "2.46、2.45といったところだ。"
  },
  {
    "start": 6326206,
    "end": 6329814,
    "text": "というのも、基本的に追加情報を取り入れることはないからだ。"
  },
  {
    "start": 6329932,
    "end": 6333494,
    "text": "私たちはまだ、前のキャラクターを受け止め、次のキャラクターを予測しようとしているだけなのだ。"
  },
  {
    "start": 6333612,
    "end": 6339946,
    "text": "私たちはそれを、カウントや正規化によって明示的に行うのではなく、勾配ベースの学習によって行っている。"
  },
  {
    "start": 6340048,
    "end": 6348342,
    "text": "勾配に基づく最適化をしなくても、陽解法で損失関数を最適化することができるのだ。"
  },
  {
    "start": 6348486,
    "end": 6358190,
    "text": "ビグラム言語モデルのセットアップは非常に簡単で単純なので、これらの確率を直接推定し、テーブルで管理する余裕がある。"
  },
  {
    "start": 6358770,
    "end": 6362698,
    "text": "勾配ベースのアプローチは、より柔軟性が高い。"
  },
  {
    "start": 6362874,
    "end": 6372674,
    "text": "今できることは、このアプローチを拡張し、ニューラルネットを複雑化することだ。"
  },
  {
    "start": 6372792,
    "end": 6375954,
    "text": "現在、私たちは一人のキャラクターをニューラルネットに入力しているだけだ。"
  },
  {
    "start": 6375992,
    "end": 6380274,
    "text": "ニューラルネットは極めてシンプルだが、これからこれを大幅に反復していく。"
  },
  {
    "start": 6380402,
    "end": 6387410,
    "text": "私たちは、過去の複数のキャラクターをどんどん複雑なニューラルネットに送り込んでいくつもりだ。"
  },
  {
    "start": 6387490,
    "end": 6395446,
    "text": "基本的に、ニューラルネットの出力は常に対数であり、その対数はまったく同じ変換を経る。"
  },
  {
    "start": 6395558,
    "end": 6403338,
    "text": "それらをソフトマックスにかけ、損失関数と負の対数尤度を計算し、勾配ベースの最適化を行う。"
  },
  {
    "start": 6403514,
    "end": 6411950,
    "text": "だから、ニューラルネットを複雑化し、トランスフォーマーに至るまで、根本的には何も変わらない。"
  },
  {
    "start": 6412020,
    "end": 6413566,
    "text": "どれも根本的には変わらない。"
  },
  {
    "start": 6413668,
    "end": 6428758,
    "text": "変わるのはフォワードパスの方法だけで、前のキャラクターをいくつか取り出して、次のキャラクターのロジットを計算する。"
  },
  {
    "start": 6428924,
    "end": 6447180,
    "text": "というのも、入力される文字がもっとたくさんある場合に、このビグラム・アプローチをどのように拡張したかは明らかではないからだ。"
  },
  {
    "start": 6447710,
    "end": 6451990,
    "text": "前のキャラクターが1人しかいないのであれば、すべてをカウント表にまとめておけばいい。"
  },
  {
    "start": 6452070,
    "end": 6457370,
    "text": "入力された最後の10文字がある場合、すべてをテーブルで管理することはできません。"
  },
  {
    "start": 6457450,
    "end": 6466626,
    "text": "ニューラルネットワークのアプローチは、よりスケーラブルであり、時間をかけて改良していくことができる。"
  },
  {
    "start": 6466728,
    "end": 6468498,
    "text": "次に掘るのはそこだ。"
  },
  {
    "start": 6468584,
    "end": 6470914,
    "text": "もう2つ指摘したいことがある。"
  },
  {
    "start": 6471112,
    "end": 6482710,
    "text": "その1、このx nは1つのホット・ベクトルで構成され、その1つのホット・ベクトルにこのw行列が掛け合わされていることに注目してほしい。"
  },
  {
    "start": 6483130,
    "end": 6488310,
    "text": "私たちはこれを、複数のニューロンが完全につながった形で転送されていると考える。"
  },
  {
    "start": 6488650,
    "end": 6510974,
    "text": "実際にここで起こっていることは、例えば、5次元目に1を持つ1ホットベクトルがあるとすると、行列の乗算の仕組み上、その1ホットベクトルにwを乗算すると、wの5行目のlogitsが抜き取られ、wの5行目だけになるということだ。"
  },
  {
    "start": 6511172,
    "end": 6514740,
    "text": "それは、行列の掛け算が機能するからだ。"
  },
  {
    "start": 6516950,
    "end": 6519620,
    "text": "ということになる。"
  },
  {
    "start": 6520470,
    "end": 6527602,
    "text": "というのも、実は以前にも同じようなことがあったのだ。"
  },
  {
    "start": 6527666,
    "end": 6538558,
    "text": "私たちは最初の文字を取り出し、その最初の文字がこの配列の行にインデックスされ、その行から次の文字の確率分布が得られる。"
  },
  {
    "start": 6538674,
    "end": 6546118,
    "text": "最初の文字は、確率分布を得るために、ここで行列へのルックアップとして使われた。"
  },
  {
    "start": 6546294,
    "end": 6553310,
    "text": "インデックスを1つのホットとしてエンコードし、それにwを掛けているのだから。"
  },
  {
    "start": 6553460,
    "end": 6567090,
    "text": "logitsは文字通りwの適切な行になり、前と同じように指数化してカウントを作成し、正規化して確率になる。"
  },
  {
    "start": 6567430,
    "end": 6574260,
    "text": "これは文字通り、この配列と同じものだ。"
  },
  {
    "start": 6575050,
    "end": 6578882,
    "text": "wはログ数であって、カウント数ではないことをお忘れなく。"
  },
  {
    "start": 6578946,
    "end": 6586018,
    "text": "を指数化したwドットxがこの配列であると言った方がより正確である。"
  },
  {
    "start": 6586194,
    "end": 6593878,
    "text": "この配列は、基本的にバイグラムのカウントを数えることによって埋められている。"
  },
  {
    "start": 6593974,
    "end": 6603018,
    "text": "一方、勾配ベースのフレームワークでは、ランダムに初期化し、損失によってまったく同じ配列になるように導く。"
  },
  {
    "start": 6603194,
    "end": 6614350,
    "text": "この配列は基本的に最適化の最後の配列である。"
  },
  {
    "start": 6614930,
    "end": 6617890,
    "text": "だから、最後に同じ損失関数が得られたのだ。"
  },
  {
    "start": 6617960,
    "end": 6630738,
    "text": "2つ目の注意点は、ここで、これらの確率の分布を平滑化し、より均一にするために、カウントに偽のカウントを加えた平滑化を思い出してほしい。"
  },
  {
    "start": 6630914,
    "end": 6636870,
    "text": "そのため、グラム単位の確率をゼロとすることはできなかった。"
  },
  {
    "start": 6637390,
    "end": 6642250,
    "text": "さて、ここでカウントを増やしたら、確率はどうなるだろうか？"
  },
  {
    "start": 6642750,
    "end": 6651562,
    "text": "数を増やせば増やすほど、確率は均一になっていく。"
  },
  {
    "start": 6651616,
    "end": 6664370,
    "text": "ここにあるすべての数字にプラス100万を足すとすると、行とその確率が、割り算をしたときにどのように均等な確率分布に近づいていくかがわかるだろう。"
  },
  {
    "start": 6665110,
    "end": 6670130,
    "text": "勾配ベースのフレームワークには、スムージングと同等の効果があることがわかった。"
  },
  {
    "start": 6670710,
    "end": 6677990,
    "text": "特に、ここでランダムに初期化したWについて考えてみよう。"
  },
  {
    "start": 6678490,
    "end": 6681720,
    "text": "wをゼロに初期化することも考えられる。"
  },
  {
    "start": 6682090,
    "end": 6688486,
    "text": "wのエントリーがすべてゼロであれば、ロジットがすべてゼロになることがわかるだろう。"
  },
  {
    "start": 6688668,
    "end": 6691500,
    "text": "となると、これらのロジットを指数化すると、すべて1になる。"
  },
  {
    "start": 6692030,
    "end": 6695290,
    "text": "とすると、確率は正確に一様であることがわかる。"
  },
  {
    "start": 6695710,
    "end": 6703870,
    "text": "基本的に、wがすべて互いに等しいか、特にゼロである場合、確率は完全に一様になる。"
  },
  {
    "start": 6704370,
    "end": 6712810,
    "text": "wをゼロに近づけようとするインセンティブは、基本的にラベル・スムージングと同じである。"
  },
  {
    "start": 6712890,
    "end": 6718638,
    "text": "損失関数でそのようなインセンティブを与えれば与えるほど、よりスムーズな分配が達成されることになる。"
  },
  {
    "start": 6718814,
    "end": 6728150,
    "text": "正則化と呼ばれるもので、損失関数に正則化損失と呼ばれる小さな要素を加えることができる。"
  },
  {
    "start": 6728890,
    "end": 6738842,
    "text": "具体的には、例えば、wを取り出し、そのすべてのエントリーを二乗にする。"
  },
  {
    "start": 6738976,
    "end": 6741980,
    "text": "wのすべてのエントリーを取って、それらを合計することができる。"
  },
  {
    "start": 6743550,
    "end": 6747610,
    "text": "スクウェアだから、もうサインはない。"
  },
  {
    "start": 6748190,
    "end": 6751258,
    "text": "マイナスもプラスもすべてつぶしてプラスの数字にする。"
  },
  {
    "start": 6751424,
    "end": 6756942,
    "text": "ということは、wがちょうどゼロであれば、ロスはゼロということになる。"
  },
  {
    "start": 6757076,
    "end": 6760750,
    "text": "wがゼロでない場合、損失が蓄積される。"
  },
  {
    "start": 6761090,
    "end": 6764706,
    "text": "だから、これを実際にここに追加することができる。"
  },
  {
    "start": 6764888,
    "end": 6771570,
    "text": "ロス＋Wの平方和のようなことができる。"
  },
  {
    "start": 6771910,
    "end": 6776740,
    "text": "そうしないと合計が大きくなりすぎるからだ。"
  },
  {
    "start": 6777430,
    "end": 6779910,
    "text": "というのは、もう少し扱いやすいような気がする。"
  },
  {
    "start": 6781210,
    "end": 6786454,
    "text": "ということは、正則化の損失は0.1倍といったところか。"
  },
  {
    "start": 6786492,
    "end": 6791340,
    "text": "正則化の強さを選択し、あとはこれを最適化するだけだ。"
  },
  {
    "start": 6792030,
    "end": 6795238,
    "text": "この最適化には2つの要素がある。"
  },
  {
    "start": 6795334,
    "end": 6803722,
    "text": "すべての確率をうまくいかせようとするだけでなく、それに加えて、同時にすべてのwをゼロにしようとする要素が加わっている。"
  },
  {
    "start": 6803856,
    "end": 6806202,
    "text": "Wがゼロでなければ、損失を感じるからだ。"
  },
  {
    "start": 6806266,
    "end": 6810446,
    "text": "だから、これを最小化するには、wをゼロにするしかない。"
  },
  {
    "start": 6810628,
    "end": 6817602,
    "text": "つまり、これはバネの力、あるいはwをゼロにする重力のようなものを加えていると考えることができる。"
  },
  {
    "start": 6817736,
    "end": 6819282,
    "text": "wはゼロでありたい。"
  },
  {
    "start": 6819336,
    "end": 6827174,
    "text": "確率は均一であることを望むが、同時にデータが示すように、あなたの確率と一致することも望む。"
  },
  {
    "start": 6827372,
    "end": 6835960,
    "text": "つまり、この正則化の強みは、まさにここに追加するカウントの量をコントロールすることなのだ。"
  },
  {
    "start": 6837310,
    "end": 6857790,
    "text": "なぜなら、この数を増やせば増やすほど、損失関数のこの部分が支配的になり、これらのウェイトが成長できなくなるからだ。"
  },
  {
    "start": 6858370,
    "end": 6864990,
    "text": "だから、もしこれが十分に強いとしたら、私たちはこの損失の力に打ち勝つことはできない。"
  },
  {
    "start": 6866690,
    "end": 6869118,
    "text": "基本的にすべてが一様な予測になる。"
  },
  {
    "start": 6869294,
    "end": 6870706,
    "text": "ちょっとクールだと思った。"
  },
  {
    "start": 6870808,
    "end": 6871074,
    "text": "オーケー。"
  },
  {
    "start": 6871112,
    "end": 6876440,
    "text": "最後に、このニューラルネット・モデルからどのようにサンプリングするかをお見せしよう。"
  },
  {
    "start": 6876810,
    "end": 6886166,
    "text": "さっきのサンプリングコードをコピーペーストしたんだけど、5回サンプリングしたのを覚えている？"
  },
  {
    "start": 6886268,
    "end": 6898460,
    "text": "pの現在のix行を取得し、それが確率行となり、そこから次のインデックスをサンプリングし、それを累積してゼロになったらブレークする。"
  },
  {
    "start": 6898830,
    "end": 6902350,
    "text": "これを実行したところ、次のような結果が出た。"
  },
  {
    "start": 6903810,
    "end": 6907438,
    "text": "Pはまだメモリーに残っているから、これでいいんだ。"
  },
  {
    "start": 6907604,
    "end": 6911806,
    "text": "さて、このpはpの行から来たものではない。"
  },
  {
    "start": 6911908,
    "end": 6913940,
    "text": "その代わりに、このニューラルネットから来ている。"
  },
  {
    "start": 6914870,
    "end": 6921730,
    "text": "まず、ixをxankの1ホット行にエンコードする。"
  },
  {
    "start": 6922390,
    "end": 6928978,
    "text": "このxankはRwを乗算し、実際にはixに対応するwの行を抜き出すだけである。"
  },
  {
    "start": 6929074,
    "end": 6930262,
    "text": "本当に、そういうことなんだ。"
  },
  {
    "start": 6930396,
    "end": 6938834,
    "text": "そして、そのロジットを正規化し、指数化してカウントを求め、正規化して分布を求める。"
  },
  {
    "start": 6938962,
    "end": 6941110,
    "text": "であれば、分布からサンプリングすることができる。"
  },
  {
    "start": 6941270,
    "end": 6959866,
    "text": "もし私がこのような拍子抜けするような、あるいはクライマックスのような、見方にもよるが、まったく同じ結果を得るとしたら、それはこれが同一のモデルだからであり、同じ損失を達成しているだけでなく、申し上げたように、これらは同一のモデルなのだ。"
  },
  {
    "start": 6959898,
    "end": 6964110,
    "text": "このwは、以前に推定した値の対数である。"
  },
  {
    "start": 6964260,
    "end": 6969326,
    "text": "私たちはまったく違う方法でこの答えにたどり着いたし、まったく違う解釈をしている。"
  },
  {
    "start": 6969438,
    "end": 6972994,
    "text": "基本的に、これは基本的に同じモデルで、ここでも同じサンプルが得られる。"
  },
  {
    "start": 6973112,
    "end": 6975682,
    "text": "それはちょっとクールだね。"
  },
  {
    "start": 6975736,
    "end": 6977854,
    "text": "オーケー、それで、私たちは実際に多くの分野をカバーしてきた。"
  },
  {
    "start": 6977982,
    "end": 6981560,
    "text": "ビグラム文字レベルの言語モデルを紹介した。"
  },
  {
    "start": 6981930,
    "end": 6990082,
    "text": "モデルの訓練方法、モデルからのサンプリング方法、そして負の対数尤度損失を使ってモデルの品質を評価する方法を見ました。"
  },
  {
    "start": 6990226,
    "end": 6996074,
    "text": "そこで、2つのまったく異なる方法でモデルをトレーニングした。"
  },
  {
    "start": 6996272,
    "end": 7001258,
    "text": "最初の方法では、すべてのbigramの頻度を数え上げ、正規化した。"
  },
  {
    "start": 7001434,
    "end": 7015678,
    "text": "2つ目の方法は、勾配ベースのフレームワークで損失が最小化されるように、計数行列または計数配列を最適化するためのガイドとして負の対数尤度損失を使用した。"
  },
  {
    "start": 7015774,
    "end": 7018100,
    "text": "どちらも同じ結果であることがわかった。"
  },
  {
    "start": 7018470,
    "end": 7021282,
    "text": "それだけだ。"
  },
  {
    "start": 7021416,
    "end": 7024782,
    "text": "さて、2つ目のグラデーションベースのフレームワークは、より柔軟性がある。"
  },
  {
    "start": 7024926,
    "end": 7027582,
    "text": "今、私たちのニューラルネットワークは超シンプルだ。"
  },
  {
    "start": 7027656,
    "end": 7033938,
    "text": "ロジットを計算するために、1つ前の文字を1つのリニアレイヤーに通している。"
  },
  {
    "start": 7034114,
    "end": 7035714,
    "text": "これは複雑になりそうだ。"
  },
  {
    "start": 7035842,
    "end": 7042774,
    "text": "だから、次のビデオでは、これらのキャラクターをもっともっと取り上げて、ニューラルネットに送り込んでいくつもりだ。"
  },
  {
    "start": 7042892,
    "end": 7045302,
    "text": "このニューラルネットは、まったく同じものを出力する。"
  },
  {
    "start": 7045356,
    "end": 7050654,
    "text": "ニューラルネットはロジットを出力し、これらのロジットはまったく同じように正規化される。"
  },
  {
    "start": 7050692,
    "end": 7055242,
    "text": "ロスも何もかも、グラディエント・ベースのフレームワークも、すべて同じままだ。"
  },
  {
    "start": 7055386,
    "end": 7060298,
    "text": "ただ、このニューラルネットはトランスフォーマーによって複雑化する。"
  },
  {
    "start": 7060474,
    "end": 7064030,
    "text": "それはかなりすごいことになりそうで、今のところ楽しみにしている。"
  },
  {
    "start": 7064100,
    "end": 7064250,
    "text": "さようなら。"
  }
]