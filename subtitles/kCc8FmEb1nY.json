[
  {
    "start": 0,
    "end": 784,
    "text": "皆さん、こんにちは。"
  },
  {
    "start": 784,
    "end": 3419,
    "text": "ChatGPTをご存じでしょうか。"
  },
  {
    "start": 3419,
    "end": 6654,
    "text": "世界とAI界に旋風を巻き起こした。"
  },
  {
    "start": 6654,
    "end": 12228,
    "text": "AIと対話し、テキストベースのタスクを与えることができるシステムです。"
  },
  {
    "start": 12228,
    "end": 20800,
    "text": "例えば、ChatGPTに、AIを理解することがいかに重要か、そしてそれを使って世界を良くし、より豊かにすることができるかということを、小さな俳句にしてもらうことができるのです。"
  },
  {
    "start": 21440,
    "end": 27040,
    "text": "これを実行すると、AIの知識が繁栄をもたらし、誰もがその力を受け入れることができます。"
  },
  {
    "start": 28000,
    "end": 28760,
    "text": "なるほど、悪くないですね。"
  },
  {
    "start": 28760,
    "end": 35045,
    "text": "ChatGPTは左から右へ、これらの単語を順次生成していることがわかりました。"
  },
  {
    "start": 35045,
    "end": 42295,
    "text": "先ほど、まったく同じ質問をしたところ、少し違う結果が出ました。"
  },
  {
    "start": 42295,
    "end": 46680,
    "text": "AIの力で成長する、無知が足かせになる、学ぶ、繁栄が待っている。"
  },
  {
    "start": 46680,
    "end": 50000,
    "text": "どちらもかなり良いし、微妙に違う。"
  },
  {
    "start": 50000,
    "end": 52731,
    "text": "見ると、ChatGPTが確率論的なシステムであることがわかります。"
  },
  {
    "start": 52731,
    "end": 57440,
    "text": "一つのプロンプトに対して、複数の答えを返すような形で出すことができます。"
  },
  {
    "start": 57440,
    "end": 60240,
    "text": "これはプロンプトの一例に過ぎません。"
  },
  {
    "start": 60240,
    "end": 68957,
    "text": "ChatGPTとのやり取りをまとめたサイトもあるほどで、ユーモラスな事例が多いですね。"
  },
  {
    "start": 68957,
    "end": 77891,
    "text": "HTMLを犬みたいに説明したり、チェスのリリースノートも書いたり、イーロン・マスクがツイッターを買ったことを書いたり、いろいろです。"
  },
  {
    "start": 77891,
    "end": 85394,
    "text": "例として、木から葉っぱが落ちて衝撃的な展開になったという速報記事を書いてください。"
  },
  {
    "start": 85394,
    "end": 88480,
    "text": "近所の公園の木から葉っぱが落ちてきた。"
  },
  {
    "start": 88480,
    "end": 94611,
    "text": "目撃者の話によると、それまで木の枝に付いていた葉っぱが外れて地面に落ちてきたそうです。"
  },
  {
    "start": 94611,
    "end": 95482,
    "text": "とてもドラマチックですね。"
  },
  {
    "start": 95482,
    "end": 98951,
    "text": "ご覧いただくと、これが非常に優れたシステムであることがおわかりいただけると思います。"
  },
  {
    "start": 98951,
    "end": 107444,
    "text": "単語や文字、トークンの並びをモデル化するため、言語モデルと呼ばれるものです。"
  },
  {
    "start": 107444,
    "end": 111904,
    "text": "英語では、ある単語がどのように続いているのかを知っているのです。"
  },
  {
    "start": 111904,
    "end": 116960,
    "text": "その視点からは、シーケンスを完成させていることになります。"
  },
  {
    "start": 116960,
    "end": 121794,
    "text": "シーケンスの開始を与えると、その結果でシーケンスを完成させるんだ。"
  },
  {
    "start": 121794,
    "end": 124964,
    "text": "その意味では言語モデルですね。"
  },
  {
    "start": 124964,
    "end": 130883,
    "text": "ChatGPTを機能させるための裏技的な要素に焦点を当てたいと思います。"
  },
  {
    "start": 130883,
    "end": 135823,
    "text": "この言葉の並びをモデル化したニューラルネットワークは、どのようなものなのでしょうか。"
  },
  {
    "start": 135823,
    "end": 140752,
    "text": "この注目という名の紙から生まれるものは、必要なものばかりです。"
  },
  {
    "start": 140752,
    "end": 147680,
    "text": "2017年、トランスフォーマーアーキテクチャを生み出し、提案したAIのランドマーク的な論文。"
  },
  {
    "start": 148720,
    "end": 153309,
    "text": "GPTとは、Generatively Pre-trained Transformerの略で、日本語に訳すと「世代別学習済み変換器」となります。"
  },
  {
    "start": 153309,
    "end": 159100,
    "text": "トランスフォーマーは、ボンネットの下で実際にすべての重労働をこなすニューラルネットです。"
  },
  {
    "start": 159100,
    "end": 161040,
    "text": "2017年の本紙からです。"
  },
  {
    "start": 161040,
    "end": 170400,
    "text": "この論文を読むと、かなり適当に機械翻訳した論文のように読めますが、それは著者たちがトランスが現場に与える影響を十分に予想していなかったためだと思います。"
  },
  {
    "start": 170400,
    "end": 180565,
    "text": "機械翻訳の文脈で彼らが生み出したこのアーキテクチャは、実はその後5年間で、AIの残りの部分を支配することになったのです。"
  },
  {
    "start": 180565,
    "end": 189040,
    "text": "このアーキテクチャをマイナーチェンジしたものが、近年ではAIの膨大なアプリケーションにコピーペーストされています。"
  },
  {
    "start": 189040,
    "end": 191864,
    "text": "ChatGPTのコアに含まれるものです。"
  },
  {
    "start": 191864,
    "end": 201279,
    "text": "今やりたいことは、ChatGPTのようなものを作りたいのですが、ChatGPTを再現することはもちろんできません。"
  },
  {
    "start": 201279,
    "end": 204870,
    "text": "これは、非常に本格的なプロダクショングレードシステムです。"
  },
  {
    "start": 204870,
    "end": 213040,
    "text": "インターネット上の膨大な量のデータを使って訓練し、さらに事前訓練と微調整の段階を踏むので、非常に複雑なのです。"
  },
  {
    "start": 213040,
    "end": 218305,
    "text": "今回注目したいのは、あくまでもトランスフォーマーベースの言語モデルを学習させることです。"
  },
  {
    "start": 218305,
    "end": 222181,
    "text": "この場合、文字レベルの言語モデルになります。"
  },
  {
    "start": 222181,
    "end": 226346,
    "text": "このようなシステムの仕組みに関しては、やはりとても勉強になると思います。"
  },
  {
    "start": 226346,
    "end": 228706,
    "text": "インターネットのかたまりでトレーニングするのは嫌だ。"
  },
  {
    "start": 228706,
    "end": 230164,
    "text": "もっと小さなデータセットが必要だ。"
  },
  {
    "start": 230164,
    "end": 233850,
    "text": "今回は、私の大好きなおもちゃのデータセットで作業することを提案します。"
  },
  {
    "start": 233850,
    "end": 244817,
    "text": "Tiny Shakespeareと呼ばれるもので、基本的には私の理解ではシェイクスピアの全作品を連結したもので、シェイクスピアの全てを1つのファイルにまとめたものです。"
  },
  {
    "start": 244817,
    "end": 248815,
    "text": "このファイルは約1メガバイトで、シェイクスピアの全部を収録しています。"
  },
  {
    "start": 248815,
    "end": 254798,
    "text": "これから行うのは、キャラクターがお互いにどのようにフォローし合うかをモデル化することです。"
  },
  {
    "start": 254798,
    "end": 269408,
    "text": "例えば、このようなキャラクターの塊があり、過去のキャラクターの文脈があると、トランスフォーマー・ニューラルネットワークは、私がハイライトしたキャラクターを見て、次に来るであろうキャラクターを予測します。"
  },
  {
    "start": 269408,
    "end": 273826,
    "text": "シェイクスピアのトランスフォーマーを鍛えるからそうなるんだよ。"
  },
  {
    "start": 273826,
    "end": 282240,
    "text": "このような文字列を生成しようとするだけで、その過程で、このデータ内のすべてのパターンをモデル化することになります。"
  },
  {
    "start": 282960,
    "end": 293280,
    "text": "システムを訓練したら、プレビューをお見せしましょう。無限のシェイクスピアを生成することができます。もちろん、シェイクスピアのように見える偽物のようなものです。"
  },
  {
    "start": 296880,
    "end": 301440,
    "text": "このページでは、私が解決することができないジャンクがあることをお詫びします。"
  },
  {
    "start": 301440,
    "end": 306216,
    "text": "ご覧いただくと、一文字一文字がどうなっているかがおわかりいただけると思います。"
  },
  {
    "start": 306216,
    "end": 310671,
    "text": "シェイクスピア的な言葉を予測するようなものですね。"
  },
  {
    "start": 310671,
    "end": 326179,
    "text": "トラニオがまた何か言っているのです。"
  },
  {
    "start": 326179,
    "end": 329204,
    "text": "我々の場合はChatGPTで一文字ずつ。"
  },
  {
    "start": 329204,
    "end": 332228,
    "text": "トークン・バイ・トークンのレベルで出てきます。"
  },
  {
    "start": 332228,
    "end": 338480,
    "text": "トークンは、小さなサブワードのようなもので、単語レベルではなく、単語の塊のようなレベルです。"
  },
  {
    "start": 340560,
    "end": 352112,
    "text": "このトランスフォーマーを訓練するためのコード全体をすでに書き上げ、GitHubのレポジトリに置いてあります（NanoGPTと呼ばれています）。"
  },
  {
    "start": 352112,
    "end": 355847,
    "text": "NanoGPTは、私のGitHubにあるリポジトリです。"
  },
  {
    "start": 355847,
    "end": 360953,
    "text": "任意のテキストに関するトランスフォーマーを訓練するためのリポジトリです。"
  },
  {
    "start": 360953,
    "end": 364711,
    "text": "トランスフォーマーのトレーニングにはいろいろなやり方があるからです。"
  },
  {
    "start": 364711,
    "end": 367040,
    "text": "という非常にシンプルな実装になっています。"
  },
  {
    "start": 367040,
    "end": 369840,
    "text": "300行のコードを2つのファイルにまとめただけです。"
  },
  {
    "start": 369840,
    "end": 375901,
    "text": "各ファイルは，変換器であるGPTモデルを定義し，1ファイルは与えられたテキストデータセットに対して変換器を学習させるものである．"
  },
  {
    "start": 375901,
    "end": 381840,
    "text": "ここでは、ウェブページのかなり大きなデータセットであるオープンウェブテキストデータセットで学習させた場合の結果を示しています。"
  },
  {
    "start": 381840,
    "end": 385996,
    "text": "で、GPT2の性能を再現しています。"
  },
  {
    "start": 385996,
    "end": 406769,
    "text": "GPT2は確か2017年のOpenAIのGPTの初期バージョンで、今のところ最小の1億2400万パラメータモデルしか再現していませんが、基本的にはコードベースが正しく配置されていることの証明で、OpenAIが後から公開しているニューラルネットワークの重みをロードできている程度ですね。"
  },
  {
    "start": 406769,
    "end": 410431,
    "text": "NanoGPTでは、完成したコードを見ることができます。"
  },
  {
    "start": 410431,
    "end": 416624,
    "text": "この講義では、基本的にこのリポジトリをゼロから書きたいと思います。"
  },
  {
    "start": 416624,
    "end": 422777,
    "text": "空のファイルから始めて、トランスフォーマーを少しずつ定義していきます。"
  },
  {
    "start": 422777,
    "end": 430016,
    "text": "小さなシェイクスピアのデータセットで訓練し、無限のシェイクスピアを生成できるかを確認します。"
  },
  {
    "start": 430016,
    "end": 434467,
    "text": "もちろん、これは好きな任意のテキストデータセットにコピーペーストすることができます。"
  },
  {
    "start": 434467,
    "end": 441343,
    "text": "私の目標は、GPTの仕組みを理解してもらうことです。"
  },
  {
    "start": 441343,
    "end": 454484,
    "text": "Pythonが使えることと、微積分と統計の基本的な知識があれば大丈夫です。また、同じYouTubeチャンネルにある私の過去のビデオもご覧ください、"
  },
  {
    "start": 454484,
    "end": 462880,
    "text": "特に、「make more」シリーズでは、より小さくシンプルなニューラルネットワーク言語モデルを定義しています。"
  },
  {
    "start": 462880,
    "end": 464920,
    "text": "多層パーセプトロンなど。"
  },
  {
    "start": 464920,
    "end": 471680,
    "text": "このビデオでは、言語モデリングのフレームワークを紹介し、さらにこのビデオでは、トランスフォーマーニューラルネットワークそのものに焦点を当てます。"
  },
  {
    "start": 472640,
    "end": 476500,
    "text": "ここで新しいGoogle Colab Jupyterノートブックを作成しました。"
  },
  {
    "start": 476500,
    "end": 482621,
    "text": "これによって、これから一緒に開発するこのコードを後で簡単に共有することができ、みなさんが一緒についていけるようになります。"
  },
  {
    "start": 482621,
    "end": 485435,
    "text": "後ほど動画説明でご紹介します。"
  },
  {
    "start": 485435,
    "end": 488210,
    "text": "ここで私は前準備をしたところです。"
  },
  {
    "start": 488210,
    "end": 491432,
    "text": "このURLで、ちっちゃなシェイクスピアのデータセットをダウンロードしたんだ。"
  },
  {
    "start": 491432,
    "end": 493785,
    "text": "見ると、約1メガバイトのファイルであることがわかります。"
  },
  {
    "start": 493785,
    "end": 507829,
    "text": "ここでは、input.txtファイルを開いて、すべてのテキストを文字列として読み込んでいます。およそ100万文字を扱っていることがわかりますが、最初の1000文字をプリントアウトすると、基本的に期待通りの結果になります。"
  },
  {
    "start": 507829,
    "end": 514325,
    "text": "これは、シェイクスピアの小さなデータセットの最初の1000文字で、およそここまでのところ、今のところ良好です。"
  },
  {
    "start": 514325,
    "end": 520038,
    "text": "次にこのテキストを取り上げますが、テキストとはPythonの文字の並びのことです。"
  },
  {
    "start": 520038,
    "end": 528027,
    "text": "で、そのセットコンストラクタを呼び出すと、このテキストに登場するすべての文字のセットを取得することになるんだ。"
  },
  {
    "start": 528027,
    "end": 537043,
    "text": "その上でlistを呼び出すと、単なる集合ではなく、それらの文字のリストが作成され、任意の順序で並び替えが行われます。"
  },
  {
    "start": 537043,
    "end": 542511,
    "text": "データセット全体に出現するすべての文字を取得し、それらをソートしています。"
  },
  {
    "start": 542511,
    "end": 545453,
    "text": "これで、その数が語彙のサイズになります。"
  },
  {
    "start": 545453,
    "end": 553193,
    "text": "これが配列の可能な要素で、ここに文字を印刷すると全部で65個あることがわかります。"
  },
  {
    "start": 553193,
    "end": 560639,
    "text": "スペース文字があって、それからいろいろな特殊文字があって、大文字と小文字がある。"
  },
  {
    "start": 560639,
    "end": 568188,
    "text": "これが我々のボキャブラリーで、モデルが見たり発したりできる可能性のあるキャラクターのようなものです。"
  },
  {
    "start": 568188,
    "end": 573721,
    "text": "それでは次に、入力テキストをトークン化するための戦略を立てたいと思います。"
  },
  {
    "start": 573721,
    "end": 585252,
    "text": "トークン化とは、文字列としての生テキストを、可能な要素の語彙に従ったノートブックに従って、整数の列に変換することを意味する言葉である。"
  },
  {
    "start": 585252,
    "end": 589427,
    "text": "ここでは例として、文字レベルの言語モデルを構築することにします。"
  },
  {
    "start": 589427,
    "end": 592895,
    "text": "単に個々の文字を整数に変換しているに過ぎません。"
  },
  {
    "start": 592895,
    "end": 596529,
    "text": "それを実現するコードの一部をお見せしましょう。"
  },
  {
    "start": 596529,
    "end": 603200,
    "text": "エンコーダーとデコーダーの両方を構築しているのですが、ここで何が起こっているのかを説明しましょう。"
  },
  {
    "start": 603200,
    "end": 611440,
    "text": "hi thereのような任意のテキストをエンコードすると、その文字列を表す整数のリストを受け取ることになります。"
  },
  {
    "start": 611440,
    "end": 613603,
    "text": "例えば46、47など。"
  },
  {
    "start": 613603,
    "end": 621631,
    "text": "このリストをデコードして全く同じ文字列を返すリバースマッピングもあります。"
  },
  {
    "start": 621631,
    "end": 629419,
    "text": "任意の文字列に対して2つの整数を往復させるようなもので、私たちはそれを文字レベルで行っています。"
  },
  {
    "start": 629419,
    "end": 637552,
    "text": "この方法は、すべての文字を繰り返し処理し、文字から整数へのルックアップテーブルを作成し、その逆もまた同様です。"
  },
  {
    "start": 637552,
    "end": 646800,
    "text": "ある文字列をエンコードするには、すべての文字を個別に変換し、デコードして戻すには、逆のマッピングを使用して、すべての文字を連結することになります。"
  },
  {
    "start": 647440,
    "end": 652389,
    "text": "これは、可能性のあるエンコーディングや可能性のあるトークナイザーのひとつにすぎません。"
  },
  {
    "start": 652389,
    "end": 657400,
    "text": "これは非常にシンプルなものですが、実際には他にもさまざまなスキーマがあります。"
  },
  {
    "start": 657400,
    "end": 660320,
    "text": "、例えばGoogleはセンテンスピースを使用しています。"
  },
  {
    "start": 660320,
    "end": 672612,
    "text": "sentence pieceもテキストを整数にエンコードしますが、異なるスキーマで異なる語彙を使用し、sentence pieceはサブワード的なトークナイザーとなります。"
  },
  {
    "start": 672612,
    "end": 676630,
    "text": "単語全体をエンコードしているわけではない、ということです。"
  },
  {
    "start": 676630,
    "end": 679988,
    "text": "個々の文字をエンコードしているわけではありません。"
  },
  {
    "start": 679988,
    "end": 684295,
    "text": "サブワード単位のレベルなので、実際にはそれが採用されるのが普通です。"
  },
  {
    "start": 684295,
    "end": 690452,
    "text": "例えば、OpenAIにもTickTokenというライブラリがあり、バイトペアエンコーディングのトークナイザーを使っています。"
  },
  {
    "start": 690452,
    "end": 692384,
    "text": "GPTが使っているものです。"
  },
  {
    "start": 692384,
    "end": 698187,
    "text": "hello worldのような単語を整数のリストにエンコードすることも可能です。"
  },
  {
    "start": 698187,
    "end": 705872,
    "text": "例として、TickTokenライブラリを使用していますが、GPT2用のエンコーディングまたはGPT2で使用されたエンコーディングを取得しています。"
  },
  {
    "start": 705872,
    "end": 709856,
    "text": "65個のキャラクターやトークンの可能性があるのではなく"
  },
  {
    "start": 709856,
    "end": 711819,
    "text": "50,000トークンを持っているそうです。"
  },
  {
    "start": 711819,
    "end": 718520,
    "text": "全く同じ文字列を上位にエンコードしても、3つの整数のリストしか得られないのです。"
  },
  {
    "start": 718520,
    "end": 721520,
    "text": "これらの整数は、0から64の間ではない。"
  },
  {
    "start": 721520,
    "end": 724555,
    "text": "0から50,256の間である。"
  },
  {
    "start": 724555,
    "end": 742570,
    "text": "コードブックのサイズと配列の長さをトレードオフすることで、非常に長い整数の配列で非常に小さな語彙を持つことも、短い整数の配列で非常に大きな語彙を持つことも可能です。"
  },
  {
    "start": 742570,
    "end": 747414,
    "text": "実際にはこのようなサブワード符号化を使うのが一般的です。"
  },
  {
    "start": 747414,
    "end": 749999,
    "text": "トークナイザーは非常にシンプルにしたいと思います。"
  },
  {
    "start": 749999,
    "end": 752338,
    "text": "文字レベルのトークナイザーを使用しています。"
  },
  {
    "start": 752338,
    "end": 754963,
    "text": "非常に小さなコードブックを持っていることになります。"
  },
  {
    "start": 754963,
    "end": 758100,
    "text": "非常にシンプルなエンコードとデコードの機能を備えています。"
  },
  {
    "start": 758100,
    "end": 761208,
    "text": "その結果、非常に長いシーケンスを得ることができます。"
  },
  {
    "start": 761208,
    "end": 765655,
    "text": "というのが一番シンプルなので、この講義ではそのレベルにこだわることにしています。"
  },
  {
    "start": 765655,
    "end": 769534,
    "text": "これでエンコーダーとデコーダー、そしてトークナイザーが揃ったことになります。"
  },
  {
    "start": 769534,
    "end": 772705,
    "text": "シェイクスピアの訓練セット全体をトークン化することができます。"
  },
  {
    "start": 772705,
    "end": 780625,
    "text": "ここでは、PyTorchライブラリ、特にPyTorchライブラリのtorch.tensorを使用するようにします。"
  },
  {
    "start": 780625,
    "end": 788699,
    "text": "小さなシェイクスピアのテキストをすべてエンコードし、torch.tensorに包んでデータテンソルを得ます。"
  },
  {
    "start": 788699,
    "end": 794945,
    "text": "最初の1000文字、1000個の要素だけを見ると、データテンソルはこんな感じです。"
  },
  {
    "start": 794945,
    "end": 803760,
    "text": "見ると、大量の整数列があり、この整数列は基本的にこの最初の1000文字を同一に翻訳したものであることがわかります。"
  },
  {
    "start": 804319,
    "end": 809200,
    "text": "例えば、0は改行文字で、多分1はスペースだと思うのですが。"
  },
  {
    "start": 809200,
    "end": 813995,
    "text": "100％ではないですが、これからはテキストの全データセットがjustとして再表現されます。"
  },
  {
    "start": 813995,
    "end": 818400,
    "text": "非常に大きな1つの整数の列として引き伸ばされるだけです。"
  },
  {
    "start": 818400,
    "end": 821520,
    "text": "ここで話を進める前に、もうひとつだけさせてください。"
  },
  {
    "start": 821520,
    "end": 825764,
    "text": "データセットを訓練と検証の2つに分けたいのですが。"
  },
  {
    "start": 825764,
    "end": 838160,
    "text": "特に、データセットの最初の90％をトランスのトレーニングデータとし、最後の10％を検証データとして保留することにします。"
  },
  {
    "start": 838160,
    "end": 841409,
    "text": "これにより、モデルがどの程度オーバーフィットしているのかを把握することができます。"
  },
  {
    "start": 841409,
    "end": 848865,
    "text": "この正確なシェイクスピアを完璧に覚えてもらうため、バリデーションデータは基本的に隠して、横に置いておくことにしているからです。"
  },
  {
    "start": 848865,
    "end": 862407,
    "text": "シェイクスピアのような文章を作成するニューラルネットワークが欲しいのですが、そうすれば、実際の「密航」「密航」のようなシェイクスピアの文章を作成できる可能性はかなり高いはずです。"
  },
  {
    "start": 862407,
    "end": 867280,
    "text": "これを使ってオーバーフィッティングの感覚をつかむことにします。"
  },
  {
    "start": 867280,
    "end": 875430,
    "text": "それでは、テキストシーケンスや整数シーケンスをトランスフォーマーに挿入して、パターンを学習させたいと思います。"
  },
  {
    "start": 875430,
    "end": 881584,
    "text": "ここで重要なのは、実際にテキスト全体を一度にトランスフォーマーに送り込むことはないということです。"
  },
  {
    "start": 881584,
    "end": 884776,
    "text": "それは計算上非常に高価であり、禁止されています。"
  },
  {
    "start": 884776,
    "end": 889949,
    "text": "実際に多くのデータセットで変換器を学習させる場合、データセットの塊だけを扱うことになります。"
  },
  {
    "start": 889949,
    "end": 897162,
    "text": "Transformerの学習では、基本的に学習セットからランダムに小さな塊を取り出し、その塊だけを一度に学習させることにしています。"
  },
  {
    "start": 897162,
    "end": 902640,
    "text": "これらの塊は基本的に何らかの長さを持っており、それは最大長である。"
  },
  {
    "start": 903440,
    "end": 907600,
    "text": "少なくとも私が普段書いているコードでは、一般的に最大長はブロックサイズと呼ばれています。"
  },
  {
    "start": 908400,
    "end": 913280,
    "text": "コンテキストの長さとか、違う名前で出ていますね。"
  },
  {
    "start": 913280,
    "end": 920511,
    "text": "まずはブロックサイズを8だけにして、最初に訓練されたデータ文字、最初のブロックサイズにプラス1文字を見てみますね。"
  },
  {
    "start": 920511,
    "end": 922319,
    "text": "なぜプラスワンなのか、その理由は後ほど説明します。"
  },
  {
    "start": 923680,
    "end": 928480,
    "text": "トレーニングセットに含まれる配列の最初の9文字である。"
  },
  {
    "start": 928480,
    "end": 939520,
    "text": "ここで注目したいのは、このようにデータの塊をサンプリングする場合、例えばトレーニングセットからこの9文字を取り出すと、実際には複数の例が詰め込まれていることです。"
  },
  {
    "start": 939520,
    "end": 943852,
    "text": "このキャラクターたちは皆、互いにフォローし合っているんです。"
  },
  {
    "start": 943852,
    "end": 954114,
    "text": "つまり、トランスに接続することで、これらのすべての位置で予測を行うよう同時に訓練することになります。"
  },
  {
    "start": 954114,
    "end": 961301,
    "text": "9文字の中に、実は8つの事例が詰まっているんです。"
  },
  {
    "start": 961301,
    "end": 968224,
    "text": "という例もありますし、18の文脈では47が次に来る可能性が高いです。"
  },
  {
    "start": 968224,
    "end": 972138,
    "text": "18と47の文脈で56が次に来る。"
  },
  {
    "start": 972138,
    "end": 976848,
    "text": "18 47 56 57 の文脈では、次に来ることができる、といった具合です。"
  },
  {
    "start": 976848,
    "end": 979160,
    "text": "8つの個別例です。"
  },
  {
    "start": 979160,
    "end": 981280,
    "text": "実際にコードで綴ってみます。"
  },
  {
    "start": 982560,
    "end": 984805,
    "text": "ここに、説明のためのコードの塊があります。"
  },
  {
    "start": 984805,
    "end": 990383,
    "text": "Xはトランスフォーマーへの入力で、最初のブロックサイズの文字だけとなります。"
  },
  {
    "start": 990383,
    "end": 1000480,
    "text": "Yは次のブロックサイズの文字になるので、1つオフセットされ、それはYが入力の各位置のターゲットになるためです。"
  },
  {
    "start": 1000480,
    "end": 1015648,
    "text": "ここではブロックサイズが8であるものをすべて繰り返し、コンテキストは常にxのtまでの文字とtを含むすべての文字、ターゲットは常にt番目の文字であるがターゲット配列yにあるものである。"
  },
  {
    "start": 1015648,
    "end": 1020540,
    "text": "私が言ったことを言葉で表現してくれています。"
  },
  {
    "start": 1020540,
    "end": 1028736,
    "text": "これらは、トレーニングセットからサンプリングした9文字の塊に隠された8つの例である。"
  },
  {
    "start": 1028736,
    "end": 1031384,
    "text": "もうひとつ、触れておきたいことがあります。"
  },
  {
    "start": 1031384,
    "end": 1038199,
    "text": "ここでは8つの例で、1つの間のコンテキストからブロックサイズのコンテキストまで、すべてのコンテキストでトレーニングを行っています。"
  },
  {
    "start": 1038199,
    "end": 1044736,
    "text": "計算上の理由だけでなく、たまたま配列が決まっているからとか、そういう理由でトレーニングしています。"
  },
  {
    "start": 1044736,
    "end": 1046825,
    "text": "ただ効率を考えてやっているわけではありません。"
  },
  {
    "start": 1046825,
    "end": 1054959,
    "text": "また、トランスネットワークは、1つのコンテキストからブロックサイズまで、すべてのコンテキストを見ることができるようにするために行われます。"
  },
  {
    "start": 1054959,
    "end": 1059296,
    "text": "トランスフォーマーには、その中間を見ることに慣れ親しんでもらいたいと思います。"
  },
  {
    "start": 1059296,
    "end": 1067247,
    "text": "サンプリング中に、わずか1文字のコンテキストでサンプリング生成を開始することができるので、推論中に後で役に立つことになる。"
  },
  {
    "start": 1067247,
    "end": 1072510,
    "text": "トランスフォーマーは、1つの文脈から次のキャラクターを予測する方法を知っています。"
  },
  {
    "start": 1072510,
    "end": 1075258,
    "text": "ブロックサイズまでの全てを予測することができます。"
  },
  {
    "start": 1075258,
    "end": 1085040,
    "text": "変換器が次の文字を予測する際に、ブロックサイズ以上の入力を受けることはないからです。"
  },
  {
    "start": 1085040,
    "end": 1089440,
    "text": "トランスフォーマーに投入されるテンソルの時間次元について見てきました。"
  },
  {
    "start": 1089440,
    "end": 1091348,
    "text": "気になる次元が一つ増えましたね。"
  },
  {
    "start": 1091348,
    "end": 1092820,
    "text": "バッチの次元です。"
  },
  {
    "start": 1092820,
    "end": 1099079,
    "text": "テキストの塊をサンプリングしながら、実際には毎回、トランスフォーマーに送り込んでいくことになります。"
  },
  {
    "start": 1099079,
    "end": 1104113,
    "text": "複数のテキストの塊が一つのテンソルに積み重なっているようなバッチがたくさんあるはずです。"
  },
  {
    "start": 1104113,
    "end": 1112475,
    "text": "GPUはデータの並列処理に非常に優れているので、GPUを忙しくさせるために、効率化のために行っているのです。"
  },
  {
    "start": 1112475,
    "end": 1116319,
    "text": "複数のチャンクを同時に処理したいだけなのです。"
  },
  {
    "start": 1116319,
    "end": 1118754,
    "text": "これらのチャンクは完全に独立して処理されます。"
  },
  {
    "start": 1118754,
    "end": 1120718,
    "text": "お互いに話をしないのです。"
  },
  {
    "start": 1120718,
    "end": 1124844,
    "text": "基本的には、これを一般化し、バッチの次元を導入することにしましょう。"
  },
  {
    "start": 1124844,
    "end": 1129440,
    "text": "ここにコードの塊があります。これを実行して、何をするのか説明します。"
  },
  {
    "start": 1129680,
    "end": 1138566,
    "text": "ここでは、データセット内のランダムな場所をサンプリングして、そこからチャンクを取り出します。 乱数発生器にシードを設定します、"
  },
  {
    "start": 1138566,
    "end": 1144422,
    "text": "再現しようとすると、ここで見た数字が、後で見た数字と同じになるということです。"
  },
  {
    "start": 1144422,
    "end": 1150720,
    "text": "ここでいうバッチサイズとは、トランスフォーマーのフォワード・バックワード・パスごとに処理する独立したシーケンスの数です。"
  },
  {
    "start": 1152160,
    "end": 1156673,
    "text": "説明したブロックサイズは、その予測をするための最大文脈長です。"
  },
  {
    "start": 1156673,
    "end": 1162870,
    "text": "バッチサイズ4ブロックサイズ8とし、任意の分割の場合のバッチの求め方を説明します。"
  },
  {
    "start": 1162870,
    "end": 1168470,
    "text": "もし、その分割がトレーニングの分割であれば、トレーニングのデータを、そうでなければ、バルのデータを見ることになります。"
  },
  {
    "start": 1168470,
    "end": 1170586,
    "text": "これでデータ配列が出来上がりました。"
  },
  {
    "start": 1170586,
    "end": 1181840,
    "text": "そのため、ランダムな位置を生成し、そこからチャンクを取り出す際に、実際にバッチサイズ数のランダムなオフセットを生成することになります。"
  },
  {
    "start": 1181840,
    "end": 1191892,
    "text": "ixは0からデータのlenからブロックサイズを引いた値の間でランダムに生成される4つの数字ということになります。"
  },
  {
    "start": 1191892,
    "end": 1195683,
    "text": "トレーニングセットへのランダムなオフセットに過ぎないのです。"
  },
  {
    "start": 1195683,
    "end": 1205554,
    "text": "で、xは説明したようにiから始まる最初のブロックサイズの文字で、yはその1つ前のオフセットです。"
  },
  {
    "start": 1205554,
    "end": 1220364,
    "text": "足すだけで、iからixまでの整数ごとにチャンクを取得し、torch.stackを使って、ここで見たような1次元テンソルをすべて取得します、"
  },
  {
    "start": 1220364,
    "end": 1228400,
    "text": "行として積み重ね、4×8テンソルの1行になるようにする。"
  },
  {
    "start": 1228400,
    "end": 1234125,
    "text": "バッチxbとybをサンプリングしたときに印刷する場所はここです。"
  },
  {
    "start": 1234125,
    "end": 1236751,
    "text": "トランスの入力になります。"
  },
  {
    "start": 1236751,
    "end": 1238266,
    "text": "インプットされました。"
  },
  {
    "start": 1238266,
    "end": 1243335,
    "text": "xは4×8テンソル、4行8列である。"
  },
  {
    "start": 1243335,
    "end": 1258715,
    "text": "これらはそれぞれトレーニングセットの塊で、ターゲットは関連する配列yにあり、最後に変換器に入って損失関数を作成することになるのです。"
  },
  {
    "start": 1258715,
    "end": 1267280,
    "text": "xの中のすべての位置について正しい答えを与えてくれるでしょうし、これらは4つの独立した行です。"
  },
  {
    "start": 1268880,
    "end": 1280080,
    "text": "この4×8配列には32の例が含まれており、トランスフォーマーに関しては完全に独立しています。"
  },
  {
    "start": 1280960,
    "end": 1299880,
    "text": "入力が24のとき、ターゲットは43というか、y配列のここで43入力が24 43のとき、ターゲットは58入力が24 43 58のとき、ターゲットは5など、あるいは52 58 1のとき、ターゲットは58というように、正しい。"
  },
  {
    "start": 1299880,
    "end": 1302824,
    "text": "ご覧いただくと、なんとなくわかると思うのですが、このように綴られているのです。"
  },
  {
    "start": 1302824,
    "end": 1313399,
    "text": "32の独立した例を1つのバッチに詰め込んだもので、入力xに、そして、目的のターゲットがyにあります。"
  },
  {
    "start": 1313399,
    "end": 1320328,
    "text": "このxの整数テンソルは、変換器に送り込まれることになります。"
  },
  {
    "start": 1320328,
    "end": 1335734,
    "text": "変換器はこれらすべての例を同時に処理し、テンソルの各位置に予測すべき正しい整数を調べます。 さて、変換器に送り込みたい入力のバッチができました。"
  },
  {
    "start": 1335734,
    "end": 1338658,
    "text": "これを基本的にニューラルネットワークに投入することから始めましょう。"
  },
  {
    "start": 1338658,
    "end": 1345064,
    "text": "まず、最もシンプルなニューラルネットワークから始めましょう。言語モデリングの場合、私の考えでは、ビグラム言語モデルです。"
  },
  {
    "start": 1345064,
    "end": 1349251,
    "text": "ビグラム言語モデルについては、make moreのシリーズでかなり深く取り上げています。"
  },
  {
    "start": 1349251,
    "end": 1357784,
    "text": "ここではより早く、bigram言語モデルを実装するpytorchモジュールを直接実装することにします。"
  },
  {
    "start": 1357784,
    "end": 1362260,
    "text": "再現性のために、pytorchのnnモジュールをインポートしています。"
  },
  {
    "start": 1362260,
    "end": 1368478,
    "text": "nnモジュールのサブクラスであるbigram言語モデルを構築しています。"
  },
  {
    "start": 1368478,
    "end": 1374297,
    "text": "それを呼び出して、入力とターゲットを渡して、プリントしています。"
  },
  {
    "start": 1374297,
    "end": 1381961,
    "text": "入力と目標がここに来るとき、私はただインデックスを取るだけであることがわかるでしょう。"
  },
  {
    "start": 1381961,
    "end": 1385812,
    "text": "このトークンエンベディングテーブルに渡すだけです。"
  },
  {
    "start": 1385812,
    "end": 1392535,
    "text": "ここでは、コンストラクタの中で、トークン埋め込みテーブルを作成しています。"
  },
  {
    "start": 1392535,
    "end": 1395786,
    "text": "それは、ボキャブラリーサイズのボキャブラリーサイズによるものです。"
  },
  {
    "start": 1395786,
    "end": 1403548,
    "text": "これは、ボキャブラリーの大きさを表すテンソルを、ボキャブラリーの大きさごとに包んだものです。"
  },
  {
    "start": 1403548,
    "end": 1416724,
    "text": "ここで起こっているのは、idxを渡すと、入力のすべての整数がこの埋め込みテーブルを参照し、そのインデックスに対応する埋め込みテーブルの行を抜き出すということです。"
  },
  {
    "start": 1416724,
    "end": 1425003,
    "text": "24はここでエンベッディングテーブルに行き、24番目の行を抜き出し、43はここで43番目の行を抜き出すなどです。"
  },
  {
    "start": 1425003,
    "end": 1439155,
    "text": "この場合、バッチは4、タイムは8、チャンネルはボキャブラリーのサイズ、つまり65になります。"
  },
  {
    "start": 1439155,
    "end": 1444556,
    "text": "これらの行をすべて抜き出し、b×t×cのように並べます。"
  },
  {
    "start": 1444556,
    "end": 1451920,
    "text": "これは、基本的に次の文字に対する得点であるロジットと解釈することにします。"
  },
  {
    "start": 1451920,
    "end": 1459687,
    "text": "つまり、1つのトークンのアイデンティティに基づいて、次に何が起こるかを予測しているのです。"
  },
  {
    "start": 1459687,
    "end": 1471189,
    "text": "現在トークンたちは互いに会話しておらず、自分自身しか見ていないので、私はトークン5号なのです。"
  },
  {
    "start": 1471189,
    "end": 1482560,
    "text": "私の知っているキャラクターは、典型的なシナリオの中で他のキャラクターを追いかけるようなところがあるからです。"
  },
  {
    "start": 1482560,
    "end": 1485935,
    "text": "このあたりは、「MAKE MORE」シリーズで、もっと深く掘り下げて見てきました。"
  },
  {
    "start": 1485935,
    "end": 1497370,
    "text": "これを実行すると、4×8の各ポジションの予測値、スコア、ロジットが得られ、次に何が起こるかを予測することができました、"
  },
  {
    "start": 1497370,
    "end": 1507424,
    "text": "損失関数を評価したいと思います。make more seriesでは、予測値の損失や品質を測定する良い方法として、負の対数尤度損失を使用することを確認しました、"
  },
  {
    "start": 1507424,
    "end": 1512036,
    "text": "これは、pytorchでもcross entropyという名前で実装されています。"
  },
  {
    "start": 1512036,
    "end": 1518304,
    "text": "ここでは、予測値と目標値のクロスエントロピーの損失を計算したいと思います。"
  },
  {
    "start": 1518304,
    "end": 1522324,
    "text": "ターゲットに対するロジットの品質を測ることができます。"
  },
  {
    "start": 1522324,
    "end": 1525884,
    "text": "つまり、次の登場人物の正体がわかったのです。"
  },
  {
    "start": 1525884,
    "end": 1534848,
    "text": "ロジットをもとに、次のキャラクターをどれだけ予測できるか、直感的に正しい、うーん正しい次元のロジットを予測できるか。"
  },
  {
    "start": 1534848,
    "end": 1542552,
    "text": "ターゲットによっては非常に高い数値になり、他の次元では非常に低い数値になるはずです。"
  },
  {
    "start": 1542552,
    "end": 1545663,
    "text": "問題は、これが実際に私たちが望んでいることではないことです。"
  },
  {
    "start": 1545663,
    "end": 1549973,
    "text": "基本的にはロジットとロスを出力したいのですが、このような要望があります。"
  },
  {
    "start": 1549973,
    "end": 1552852,
    "text": "残念ながら、これは実際には実行されません。"
  },
  {
    "start": 1552852,
    "end": 1554327,
    "text": "というエラーメッセージが表示されます。"
  },
  {
    "start": 1554327,
    "end": 1568058,
    "text": "直感的には、これを測定したいのですが、pytorchのクロスエントロピーのドキュメントを見ると、クロスエントロピーをその関数形式で呼び出そうとしていますね。"
  },
  {
    "start": 1568058,
    "end": 1571650,
    "text": "そのためのモジュールのようなものを作る必要はない、ということです。"
  },
  {
    "start": 1571650,
    "end": 1584923,
    "text": "ここで、ドキュメントを見ると、pytorchがどのようにこれらの入力を期待しているかを詳細に調べる必要があります。基本的に、ここで問題となるのは、pytorchは多次元入力がある場合に期待しています、"
  },
  {
    "start": 1584923,
    "end": 1588242,
    "text": "これは、b×t×cのテンソルがあるためです。"
  },
  {
    "start": 1588242,
    "end": 1594706,
    "text": "実際にはチャンネルが2次元になることを望んでいることになります。"
  },
  {
    "start": 1594706,
    "end": 1610682,
    "text": "基本的には、b by t by cの代わりに、b by c by tを求めるということで、pytorchがこの種の入力をどう扱うかの詳細であり、実際にはそれを扱いたくありません。"
  },
  {
    "start": 1610682,
    "end": 1613878,
    "text": "その代わりに、ロジットの形状を変える必要があります。"
  },
  {
    "start": 1613878,
    "end": 1615274,
    "text": "私が好きなのは、こんな感じです。"
  },
  {
    "start": 1615274,
    "end": 1622575,
    "text": "私は、ロジスドットシェイプがb×t×cであるように、基本的に寸法に名前をつけて、その数字を解きほぐすのが好きです。"
  },
  {
    "start": 1622575,
    "end": 1626667,
    "text": "ロジットはロジットのドットビューに等しいとしましょう。"
  },
  {
    "start": 1626667,
    "end": 1629699,
    "text": "a b times c b times t by c となるようにしたい。"
  },
  {
    "start": 1629699,
    "end": 1640742,
    "text": "2次元配列にしたもので、ここにあるすべての位置を取り出します、"
  },
  {
    "start": 1640742,
    "end": 1646806,
    "text": "一次元に引き伸ばし、チャネル次元を二次元として保存します。"
  },
  {
    "start": 1646806,
    "end": 1649322,
    "text": "配列の引き伸ばしのようなものです。"
  },
  {
    "start": 1649322,
    "end": 1650684,
    "text": "二次元である。"
  },
  {
    "start": 1650684,
    "end": 1656329,
    "text": "その場合、ピートルクが期待する寸法に、より適合するようになります。"
  },
  {
    "start": 1656329,
    "end": 1665795,
    "text": "現在、ターゲットはb×tの形をしていますが、これをb×tの形にしたいからです。"
  },
  {
    "start": 1665795,
    "end": 1667490,
    "text": "一次元"
  },
  {
    "start": 1667490,
    "end": 1673660,
    "text": "Pytorchがこの値を推測してくれるからです。"
  },
  {
    "start": 1673660,
    "end": 1675067,
    "text": "並べるなら。"
  },
  {
    "start": 1675067,
    "end": 1698199,
    "text": "このように、クロスエントロピーのケースと一致するように形を変えれば、損失を評価することができるようになるはずです。"
  },
  {
    "start": 1698199,
    "end": 1705662,
    "text": "実際に推測することができ、特に負対数尤度についてはかなり詳しく取り上げました。"
  },
  {
    "start": 1705662,
    "end": 1712220,
    "text": "65歳以上では対数かロンを、それ以下ではマイナスを想定している。"
  },
  {
    "start": 1712220,
    "end": 1718585,
    "text": "4.17程度の損失が予想されるが、4.87の損失が発生する。"
  },
  {
    "start": 1718585,
    "end": 1725760,
    "text": "初期予測は超拡散的ではなく、少しエントロピーがあるため、推測が誤っているということです。"
  },
  {
    "start": 1727120,
    "end": 1731808,
    "text": "しかし、実際には、損失を評価することができるのです。"
  },
  {
    "start": 1731808,
    "end": 1736104,
    "text": "これで、いくつかのデータでモデルの品質を評価できるようになりましたね。"
  },
  {
    "start": 1736104,
    "end": 1740348,
    "text": "モデルから生成することもできるようにしたいので、生成を行いましょう。"
  },
  {
    "start": 1740348,
    "end": 1746641,
    "text": "以前のビデオですでに説明したとおりだからです。"
  },
  {
    "start": 1746641,
    "end": 1749920,
    "text": "このモデルのジェネレート機能はこちらです。"
  },
  {
    "start": 1752240,
    "end": 1753705,
    "text": "取ることがあります。"
  },
  {
    "start": 1753705,
    "end": 1757312,
    "text": "ここでは、同じような入力のidxを取ります。"
  },
  {
    "start": 1757312,
    "end": 1765581,
    "text": "基本的にこれは、あるバッチの中のあるキャラクターの現在の文脈を表しています。"
  },
  {
    "start": 1765581,
    "end": 1767135,
    "text": "それはTによるbです。"
  },
  {
    "start": 1767135,
    "end": 1773260,
    "text": "generateの仕事は、基本的にこのb by tを、b by t＋1＋2＋3というように拡張することである。"
  },
  {
    "start": 1773260,
    "end": 1778996,
    "text": "そのため、基本的には時間次元のすべてのバッチ次元で世代交代を続けることになるのです。"
  },
  {
    "start": 1778996,
    "end": 1781984,
    "text": "それが仕事であり、最大限の新しいトークンのためにそれを行うのです。"
  },
  {
    "start": 1781984,
    "end": 1785888,
    "text": "この下の方に、いくつかのものがあるのがお分かりいただけると思います。"
  },
  {
    "start": 1785888,
    "end": 1796160,
    "text": "下は、予測されたものが、1次元目の時間軸に沿って、前のidxの上に連結され、tプラス1によってbを作成します。"
  },
  {
    "start": 1796160,
    "end": 1797840,
    "text": "新しいidxになります。"
  },
  {
    "start": 1797840,
    "end": 1805548,
    "text": "generateの仕事は、b by tをb by t＋1＋2＋3として、最大数だけ新しいトークンを作ることです。"
  },
  {
    "start": 1805548,
    "end": 1808002,
    "text": "これは、モデルからの世代です。"
  },
  {
    "start": 1808002,
    "end": 1810795,
    "text": "今、世代を超えて、私たちは何をしているのだろう。"
  },
  {
    "start": 1810795,
    "end": 1812429,
    "text": "現在の指標をとっています。"
  },
  {
    "start": 1812429,
    "end": 1816577,
    "text": "予測値が得られるので、それらがロジットの中にあることがわかります。"
  },
  {
    "start": 1816577,
    "end": 1822080,
    "text": "ここでの損失は無視されることになりますね。"
  },
  {
    "start": 1822080,
    "end": 1829600,
    "text": "ロジットを取得した後に比較する、ある種の真実の目標がないのです、"
  },
  {
    "start": 1829600,
    "end": 1837056,
    "text": "私たちは最後のステップにしか注目していないので、b by t by cの代わりに、ネガティブなものを抜き取るのです、"
  },
  {
    "start": 1837056,
    "end": 1842326,
    "text": "時間軸の最後の要素は、次に来るものを予測するものだからです。"
  },
  {
    "start": 1842326,
    "end": 1846817,
    "text": "で、ロジットが得られるので、それをソフトマックスで確率に変換する。"
  },
  {
    "start": 1846817,
    "end": 1851390,
    "text": "トーション多項式を使用してこれらの確率からサンプリングする。"
  },
  {
    "start": 1851390,
    "end": 1854063,
    "text": "ピートルクにサンプルを1つ渡してもらう。"
  },
  {
    "start": 1854063,
    "end": 1862784,
    "text": "idx nextはb by oneになるわけで、バッチ次元のそれぞれで、次に来るものを1つだけ予測することになります。"
  },
  {
    "start": 1862784,
    "end": 1865760,
    "text": "この数字のサンプルは、1が1になるようにします。"
  },
  {
    "start": 1866240,
    "end": 1872374,
    "text": "で、サンプリングで得られた整数を、ここで与えられた確率分布にしたがって"
  },
  {
    "start": 1872374,
    "end": 1878281,
    "text": "これらの整数は、現在の整数の流れのようなものの上に連結されました。"
  },
  {
    "start": 1878281,
    "end": 1880468,
    "text": "これにより、B by Tプラス1が得られます。"
  },
  {
    "start": 1880468,
    "end": 1889755,
    "text": "それを返すことができます。ここで一つ、idxのselfを呼び出しているのがわかりますが、これは結局forward関数に行くことになります。"
  },
  {
    "start": 1889755,
    "end": 1891431,
    "text": "私はターゲットを提供していない。"
  },
  {
    "start": 1891431,
    "end": 1899688,
    "text": "targetsはnot givenのようなものなので、targetsはオプションでなければならないからです。"
  },
  {
    "start": 1899688,
    "end": 1901930,
    "text": "targets は、デフォルトで none です。"
  },
  {
    "start": 1901930,
    "end": 1906202,
    "text": "ターゲットが存在しないのであれば、作成しても損はない。"
  },
  {
    "start": 1906202,
    "end": 1908080,
    "text": "ただ、損失はゼロではありません。"
  },
  {
    "start": 1908640,
    "end": 1912969,
    "text": "でなければ、このようなことがすべて起こり、損失を生み出すことができるのです。"
  },
  {
    "start": 1912969,
    "end": 1914681,
    "text": "作ることができます。"
  },
  {
    "start": 1914681,
    "end": 1921396,
    "text": "目標があればそれを提供して損失を得るが、目標がなければロジットを得るだけである。"
  },
  {
    "start": 1921396,
    "end": 1928040,
    "text": "モデルから生成されるので、それに乗ってみましょう。"
  },
  {
    "start": 1928040,
    "end": 1929000,
    "text": "今おっと。"
  },
  {
    "start": 1929000,
    "end": 1936016,
    "text": "ここに別のコードがあり、モデルからモデルを生成します。"
  },
  {
    "start": 1936016,
    "end": 1939691,
    "text": "多分、これを分解してみますね。"
  },
  {
    "start": 1939691,
    "end": 1942888,
    "text": "これらはidxの権利です。"
  },
  {
    "start": 1942888,
    "end": 1949166,
    "text": "バッチのwill be just one time will be just oneを作成しています。"
  },
  {
    "start": 1949166,
    "end": 1956559,
    "text": "私は小さな1つ1つのテンソルを作成し、それはゼロを保持し、データ型はuh整数です。"
  },
  {
    "start": 1956559,
    "end": 1959698,
    "text": "zeroは、世代交代をどうキックオフするかということになりますね。"
  },
  {
    "start": 1959698,
    "end": 1964573,
    "text": "0がuhが改行文字を表す要素であることを忘れないでください。"
  },
  {
    "start": 1964573,
    "end": 1972262,
    "text": "一連の流れの中で一番最初の文字を改行として送り込むのは、ある意味合理的なことだと思います。"
  },
  {
    "start": 1972262,
    "end": 1975944,
    "text": "ここに入力するidxになります。"
  },
  {
    "start": 1975944,
    "end": 1986360,
    "text": "generateはバッチ単位で動作するからです。"
  },
  {
    "start": 1986360,
    "end": 1997460,
    "text": "0行目にインデックスを付けて、基本的に存在する単一バッチ次元を削除し、時間ステップを得ます。"
  },
  {
    "start": 1997460,
    "end": 2012710,
    "text": "これは1次元の配列で、pytorchのテンソルから単純なpythonのリストに変換し、デコード関数に送り込んで整数をテキストに変換することができるようにします。"
  },
  {
    "start": 2012710,
    "end": 2020980,
    "text": "これを戻して、100トークンを生成して、実行し、達成した生成はこちらです。"
  },
  {
    "start": 2020980,
    "end": 2026314,
    "text": "明らかにゴミです。ゴミである理由は、これが完全にランダムなモデルであるからです。"
  },
  {
    "start": 2026314,
    "end": 2028845,
    "text": "次に、このモデルをトレーニングすることにします。"
  },
  {
    "start": 2028845,
    "end": 2034056,
    "text": "もうひとつ、この機能は一般的なものであることをお伝えしておきます。"
  },
  {
    "start": 2034056,
    "end": 2039048,
    "text": "今、私たちが食べているのは、とんでもないことなんです。"
  },
  {
    "start": 2039048,
    "end": 2041098,
    "text": "この文脈を構築しているのです。"
  },
  {
    "start": 2041098,
    "end": 2049879,
    "text": "しかし、これは単なる単純なビグラムモデルなので、それはちょっとおかしいです。"
  },
  {
    "start": 2049879,
    "end": 2053578,
    "text": "例えばkについてこのような予測をするためには、このwがあればいいのです。"
  },
  {
    "start": 2053578,
    "end": 2061359,
    "text": "実際にモデルに投入したのは、全シーケンスを投入し、最後の1枚だけを見てkを予測したものです。"
  },
  {
    "start": 2062159,
    "end": 2066960,
    "text": "このように書いているのは、今はこれがビグラムモデルであるからです。"
  },
  {
    "start": 2066960,
    "end": 2078641,
    "text": "この機能は固定で、後々、キャラクターが基本的に履歴を見るときに使えるようにしたいので、今は履歴を使わないようにしています。"
  },
  {
    "start": 2078641,
    "end": 2079759,
    "text": "バカバカしい感じがします。"
  },
  {
    "start": 2080320,
    "end": 2085457,
    "text": "履歴が残るからこそ、この方法でやりたいんです。"
  },
  {
    "start": 2085457,
    "end": 2087440,
    "text": "ひとこと。"
  },
  {
    "start": 2087440,
    "end": 2089484,
    "text": "これがランダムであることがわかります。"
  },
  {
    "start": 2089484,
    "end": 2091075,
    "text": "モデルをトレーニングしよう。"
  },
  {
    "start": 2091075,
    "end": 2092865,
    "text": "少しランダム性がなくなります。"
  },
  {
    "start": 2092865,
    "end": 2094471,
    "text": "モデルをトレーニングしてみましょう。"
  },
  {
    "start": 2094471,
    "end": 2100113,
    "text": "まず最初に、Pytorch最適化オブジェクトを作成します。"
  },
  {
    "start": 2100113,
    "end": 2103688,
    "text": "ここでは、オプティマイザーのadam wを使用しています。"
  },
  {
    "start": 2103688,
    "end": 2105485,
    "text": "make moreシリーズに登場しました。"
  },
  {
    "start": 2105485,
    "end": 2107539,
    "text": "ストキャスティック・グラディエント・ディセントしか使ったことがないんです。"
  },
  {
    "start": 2107539,
    "end": 2110951,
    "text": "代わりにsgdを使用することで、最もシンプルなオプティマイザを得ることができます。"
  },
  {
    "start": 2110951,
    "end": 2115083,
    "text": "adamを使いたいのですが、adamはもっと高度で人気のあるオプティマイザーです。"
  },
  {
    "start": 2115083,
    "end": 2124319,
    "text": "学習率がおよそ3eマイナス4という典型的な良い設定では非常にうまくいくのですが、今回のような非常に小さなネットワークではどうでしょうか。"
  },
  {
    "start": 2124319,
    "end": 2129365,
    "text": "その場合、学習率はマイナス3分の1、あるいはそれ以上となる可能性があります。"
  },
  {
    "start": 2129365,
    "end": 2139972,
    "text": "オプティマイザーオブジェクトを作成します。このオブジェクトは基本的に勾配を取り、勾配を利用してパラメータを更新します。"
  },
  {
    "start": 2139972,
    "end": 2142800,
    "text": "もっと大きなものを実際に使わせてください。"
  },
  {
    "start": 2142800,
    "end": 2143688,
    "text": "32とします。"
  },
  {
    "start": 2143688,
    "end": 2148290,
    "text": "で、あるステップ数の間、新しいデータのバッチをサンプリングしています。"
  },
  {
    "start": 2148290,
    "end": 2150043,
    "text": "損失を評価しているところです。"
  },
  {
    "start": 2150043,
    "end": 2158882,
    "text": "前のステップのグラデーションをすべてゼロにして、すべてのパラメーターのグラデーションを取得し、そのグラデーションを使ってパラメーターを更新しているのです。"
  },
  {
    "start": 2158882,
    "end": 2162805,
    "text": "make moreシリーズに見られるような、典型的なトレーニングループです。"
  },
  {
    "start": 2162805,
    "end": 2170407,
    "text": "これを100回繰り返して、どのような損失が出るか見てみましょう。"
  },
  {
    "start": 2170407,
    "end": 2180638,
    "text": "4.7くらいから始めて、今は4.6、4.5などになっているので、最適化は確実に行われています。"
  },
  {
    "start": 2180638,
    "end": 2188320,
    "text": "繰り返し回数を増やし、最後に印刷するだけにしましょう。"
  },
  {
    "start": 2190320,
    "end": 2203040,
    "text": "3.6になったわけですが、3.6になるまでのざっくりとした流れは、最も雑な最適化です。"
  },
  {
    "start": 2207200,
    "end": 2208120,
    "text": "OK 動作しています。"
  },
  {
    "start": 2208120,
    "end": 2220662,
    "text": "10,000を実行し、ここからこれをコピーして、うまくいけば合理的なものが得られるでしょう。"
  },
  {
    "start": 2220662,
    "end": 2231591,
    "text": "少なくとも損失が改善されていることは確認できたので、もう少しリーズナブルなものを期待したいところですね さて、2.5程度の損失ですが、どうなるでしょうか。"
  },
  {
    "start": 2231591,
    "end": 2235468,
    "text": "このたびは劇的な進化を遂げました。"
  },
  {
    "start": 2235468,
    "end": 2238240,
    "text": "トークンの数を増やさせてください。"
  },
  {
    "start": 2239040,
    "end": 2249529,
    "text": "少なくとも、「シェイクスピア」のような合理的なものを手に入れ始めていることがわかります。"
  },
  {
    "start": 2249529,
    "end": 2261893,
    "text": "トークンは互いに会話していないので、これは非常に単純なモデルであることは明らかです。"
  },
  {
    "start": 2261893,
    "end": 2268720,
    "text": "生成されたものの前の文脈を考慮すると、次に来るものを予測するためには、一番最後の文字しか見ていないことになります。"
  },
  {
    "start": 2268720,
    "end": 2277068,
    "text": "そのため、トークンたちは互いに会話し、文脈の中に何があるのかを理解し、次に起こることをより的確に予測できるようにしなければなりません。"
  },
  {
    "start": 2277068,
    "end": 2293280,
    "text": "次に、このjupyterノートブックで開発したコードを、スクリプトに変換してみました。"
  },
  {
    "start": 2293280,
    "end": 2296726,
    "text": "定義したすべてのパラメータを配置します。"
  },
  {
    "start": 2296726,
    "end": 2306991,
    "text": "また、エンコーダーとデコーダーで再現性のあるデータを読み取ることも重要です、"
  },
  {
    "start": 2306991,
    "end": 2315280,
    "text": "TrainとTestの分割は、入力とターゲットのバッチを取得するデータローダーのようなものを使用します。"
  },
  {
    "start": 2315280,
    "end": 2316080,
    "text": "新品です。"
  },
  {
    "start": 2316080,
    "end": 2318319,
    "text": "それについては、また後日お話します。"
  },
  {
    "start": 2318319,
    "end": 2321536,
    "text": "これが、私たちが開発したビグラム言語モデルです。"
  },
  {
    "start": 2321536,
    "end": 2323925,
    "text": "ロジットを転送して、損失を与えることができます。"
  },
  {
    "start": 2323925,
    "end": 2329840,
    "text": "生成し、オプティマイザーを作成します。"
  },
  {
    "start": 2331840,
    "end": 2340058,
    "text": "この辺は見慣れた感じだと思いますが、いくつか追加した点があります。その1 Gpuをお持ちの方は、Gpu上で動作させる機能を追加しました。"
  },
  {
    "start": 2340058,
    "end": 2347032,
    "text": "Gpuをお持ちの方は、Cpuの代わりにCudaを使用することで、より高速に動作することができます。"
  },
  {
    "start": 2347032,
    "end": 2353502,
    "text": "デバイスがキューダになったら、データをロードするときにデバイスに移動させるようにする必要があります。"
  },
  {
    "start": 2353502,
    "end": 2358870,
    "text": "モデルを作成する際に、モデルのパラメータをデバイスに移動させたいと考えています。"
  },
  {
    "start": 2358870,
    "end": 2366996,
    "text": "この例では、nnエンベッディングテーブルがあり、その中にルックアップテーブルのようなものを格納するドットウエイトがあります。"
  },
  {
    "start": 2366996,
    "end": 2368786,
    "text": "gpuに移動させることになります。"
  },
  {
    "start": 2368786,
    "end": 2371541,
    "text": "ここでの計算がすべてgpuで行われること。"
  },
  {
    "start": 2371541,
    "end": 2373090,
    "text": "より速くなることができます。"
  },
  {
    "start": 2373090,
    "end": 2382070,
    "text": "そして最後に、生成させるためのコンテキストを作成する際、デバイス番号2に作成することを確認する必要があります、"
  },
  {
    "start": 2382070,
    "end": 2392730,
    "text": "今回紹介したのは、トレーニングループの中で、ロスドットアイテムを出力しているだけという事実です。"
  },
  {
    "start": 2392730,
    "end": 2412985,
    "text": "なぜなら、すべてのバッチは多かれ少なかれ幸運だからです。そこで、通常やりたいことは、推定損失関数を持っていて、推定損失は基本的にここに上がり、複数のバッチの損失を平均化することです。"
  },
  {
    "start": 2412985,
    "end": 2418838,
    "text": "特に、evalを何度も繰り返し、基本的に損失を得ることになります。"
  },
  {
    "start": 2418838,
    "end": 2424664,
    "text": "両方の分割の平均損失が得られるので、ノイズが少なくなります。"
  },
  {
    "start": 2424664,
    "end": 2433880,
    "text": "ここで、推定損失を呼び出すと、かなり正確な列車と検証の損失が報告されることになります。"
  },
  {
    "start": 2433880,
    "end": 2450266,
    "text": "モデルを評価フェーズに設定し、この下はトレーニングフェーズに戻しています、"
  },
  {
    "start": 2450266,
    "end": 2455473,
    "text": "評価モードでもトレーニングモードでも、どちらも同じ動作をすることになります。"
  },
  {
    "start": 2455473,
    "end": 2456883,
    "text": "ドロップアウト層はありません。"
  },
  {
    "start": 2456883,
    "end": 2469382,
    "text": "しかし、ニューラルネットワークがどのようなモードであるかは、推論時や学習時に異なる動作をする層があるため、よく考えておく必要があります。"
  },
  {
    "start": 2469382,
    "end": 2473397,
    "text": "このコンテクストマネージャーのトーチ・ドット・ノー・グラードもあります。"
  },
  {
    "start": 2473397,
    "end": 2479352,
    "text": "これは、この関数の中で起こるすべてのことを、ドットバックで呼び出さないということをpytorchに伝えているだけです。"
  },
  {
    "start": 2479352,
    "end": 2491680,
    "text": "そのため、Pytorchはメモリ使用量を大幅に削減することができます。中間変数をすべて保存する必要がないため、後方から呼び出すことがないためです。"
  },
  {
    "start": 2491680,
    "end": 2496400,
    "text": "また、バックプロパゲーションを行うつもりがない場合は、pytorchに伝えるのが良い習慣です。"
  },
  {
    "start": 2497600,
    "end": 2504320,
    "text": "現在、このスクリプトは約120行のコードで構成されており、これがスターターコードのようなものです。"
  },
  {
    "start": 2505200,
    "end": 2508488,
    "text": "gram.pyという名前で、後日公開する予定です。"
  },
  {
    "start": 2508488,
    "end": 2519100,
    "text": "このスクリプトを実行すると、ターミナルに次のような出力が表示されます。基本的にこのコードを実行すると、列車の損失とバルスの損失が表示されました。"
  },
  {
    "start": 2519100,
    "end": 2523606,
    "text": "見ると、ビグラムモデルでは2.5程度に変換されることがわかります。"
  },
  {
    "start": 2523606,
    "end": 2526982,
    "text": "最後に制作したサンプルがこちらです。"
  },
  {
    "start": 2526982,
    "end": 2542975,
    "text": "スクリプトにすべてをパッケージングして、これを反復するのに適した状態になりました、"
  },
  {
    "start": 2542975,
    "end": 2558120,
    "text": "ここでは、トランスフォーマー内の自己アテンションに使われる数学的なトリックに慣れてもらい、自己アテンションの効率的な実装の中核となるような、このおもちゃの例を使って、この操作に慣れてもらいたいと思います。"
  },
  {
    "start": 2558120,
    "end": 2565050,
    "text": "実際に台本に書き込むと、より明確になりますね。"
  },
  {
    "start": 2565050,
    "end": 2570663,
    "text": "物語の例でb tとcがちょうど4 8と2であるb by t by cを作成しましょう。"
  },
  {
    "start": 2570663,
    "end": 2584849,
    "text": "これは基本的にチャネルで、バッチがあり、時間的な要素があり、シーケンスの各ポイントで何らかの情報を持っています。"
  },
  {
    "start": 2584849,
    "end": 2590760,
    "text": "ここでは、最大8つのトークンを一括して使用していますが、この8つのトークンは現在、互いに会話していません。"
  },
  {
    "start": 2590760,
    "end": 2593545,
    "text": "お互いに話し合ってもらいたいと思います。"
  },
  {
    "start": 2593545,
    "end": 2600023,
    "text": "カップルにしたいのですが、特にそうではなく、非常に具体的な方法でカップルにしたいのです。"
  },
  {
    "start": 2600023,
    "end": 2610475,
    "text": "例えば、5番目の場所にあるトークンは、6番目と7番目と8番目の場所にあるトークンとは通信しないはずである。"
  },
  {
    "start": 2610475,
    "end": 2635609,
    "text": "5番目の場所にいるトークンは、4番目、3番目、2番目、1番目の場所にいるトークンとだけ話す必要があります、"
  },
  {
    "start": 2635609,
    "end": 2652826,
    "text": "例えば、私が5番目のトークンであるならば、私のステップの情報を構成するチャンネルを取りたいのです。"
  },
  {
    "start": 2652826,
    "end": 2655708,
    "text": "第4ステップのチャンネルも含まれます。"
  },
  {
    "start": 2655708,
    "end": 2659295,
    "text": "第3ステップ 第2ステップ 第1ステップで、それらを平均化したいと思います。"
  },
  {
    "start": 2659295,
    "end": 2665380,
    "text": "それはある種の特徴ベクトルになり、自分の歴史の中で自分を要約することになります。"
  },
  {
    "start": 2665380,
    "end": 2676160,
    "text": "もちろん、合計や平均を出すだけでは相互作用は極めて弱く、このコミュニケーションは極めてロッシーです。"
  },
  {
    "start": 2677040,
    "end": 2678000,
    "text": "今はいいんです。"
  },
  {
    "start": 2678000,
    "end": 2688970,
    "text": "この情報をどのように持ち帰るかは後述しますが、今は、バッチ要素ごとに、そのシーケンス内の歯牙トークンごとに独立させたいと考えています。"
  },
  {
    "start": 2688970,
    "end": 2697442,
    "text": "ここで、前のトークンとこのトークンにおけるすべてのベクトルの平均を計算したいと思います。"
  },
  {
    "start": 2697442,
    "end": 2699266,
    "text": "書き出しましょう。"
  },
  {
    "start": 2699266,
    "end": 2704960,
    "text": "私はここに小さなスニペットを持っています。"
  },
  {
    "start": 2706400,
    "end": 2718640,
    "text": "B-o-WはBag of wordsの略で、Bag of wordsとは、つまり、物事を平均化するときに使われる言葉です。"
  },
  {
    "start": 2718640,
    "end": 2724038,
    "text": "基本的にこの8カ所に単語が格納されているのです。"
  },
  {
    "start": 2724038,
    "end": 2725560,
    "text": "やっているのですが、言葉の袋を"
  },
  {
    "start": 2725560,
    "end": 2726700,
    "text": "平均しているに過ぎない。"
  },
  {
    "start": 2726700,
    "end": 2730211,
    "text": "最初からゼロで初期化されていることにします。"
  },
  {
    "start": 2730211,
    "end": 2731702,
    "text": "ここでforループをしています。"
  },
  {
    "start": 2731702,
    "end": 2733200,
    "text": "まだ効率的とは言えない。"
  },
  {
    "start": 2733200,
    "end": 2733986,
    "text": "というのが来ています。"
  },
  {
    "start": 2733986,
    "end": 2739914,
    "text": "今のところ、すべてのバッチ次元を独立に繰り返し、時間をかけて反復しているだけです。"
  },
  {
    "start": 2739914,
    "end": 2745230,
    "text": "前のトークンはこのバッチ次元にある。"
  },
  {
    "start": 2745230,
    "end": 2750274,
    "text": "歯型まですべて。"
  },
  {
    "start": 2750274,
    "end": 2760513,
    "text": "このようにxを切り出すと、xprevは、過去にいくつのt要素があったかという形になり、もちろんcにもなります。"
  },
  {
    "start": 2760513,
    "end": 2765399,
    "text": "この小さなトークンから、あらゆる二次元情報を得ることができる。"
  },
  {
    "start": 2765399,
    "end": 2775440,
    "text": "これは、現在のシーケンスの前のトークンの塊で、0番目の次元の平均または平均を計算しているのです。"
  },
  {
    "start": 2775440,
    "end": 2794998,
    "text": "ここでは時間を平均化し、小さなc個の一次元ベクトルを得ます。これをx個の単語袋に格納し、これを実行します。"
  },
  {
    "start": 2794998,
    "end": 2807806,
    "text": "ゼロにすると、最初の場所で2つが等しくなっているのがわかると思います。"
  },
  {
    "start": 2807806,
    "end": 2811798,
    "text": "この2つの平均値になっています。"
  },
  {
    "start": 2811798,
    "end": 2821824,
    "text": "この最後の1つは、これら3つの要素の平均値である。"
  },
  {
    "start": 2821824,
    "end": 2836480,
    "text": "垂直方向の平均は、すべてのトークンを平均化すると、このような結果になります。これはこれでいいのですが、非常に非効率です。"
  },
  {
    "start": 2837360,
    "end": 2838984,
    "text": "数学的なトリックです。"
  },
  {
    "start": 2838984,
    "end": 2840384,
    "text": "お見せしましょう。"
  },
  {
    "start": 2840384,
    "end": 2843260,
    "text": "このおもちゃの例を使って、実行してみましょう。"
  },
  {
    "start": 2843260,
    "end": 2844682,
    "text": "説明します。"
  },
  {
    "start": 2844682,
    "end": 2851416,
    "text": "ここに、3×3の単純な行列があり、すべての1の行列と、乱数だけの行列bがあります。"
  },
  {
    "start": 2851416,
    "end": 2859053,
    "text": "3×2とマトリックスCで、3×3と3×2の掛け算で、3×2が出ます。"
  },
  {
    "start": 2859053,
    "end": 2871664,
    "text": "ここでは、行列の掛け算を使っているので、aにbをかけるとcになります、"
  },
  {
    "start": 2871664,
    "end": 2879052,
    "text": "この左上の数字は、bの1列目とのドットプロダクトの1行目である。"
  },
  {
    "start": 2879052,
    "end": 2890449,
    "text": "今のaの行はすべて1なので、このbの列とのドットプロダクトは、この列のこれらの和になるだけです。"
  },
  {
    "start": 2890449,
    "end": 2892288,
    "text": "2＋6＋6で14です。"
  },
  {
    "start": 2892288,
    "end": 2901541,
    "text": "cの出力に含まれる要素は、aの1列目とbの2列目を掛け合わせたものでもあります、"
  },
  {
    "start": 2901541,
    "end": 2906369,
    "text": "7と4と5を足すと16になる、ここには繰り返しの要素があることがわかるだろう。"
  },
  {
    "start": 2906369,
    "end": 2912211,
    "text": "この14は、この行が再びすべて1であり、bの最初の列と掛け合わされているためです。"
  },
  {
    "start": 2912211,
    "end": 2920169,
    "text": "14、これは、と続くので、この最後の数字は、最後の行のドット積最後の列となります。"
  },
  {
    "start": 2920169,
    "end": 2928400,
    "text": "ここでのトリックは次のようなものです。これは単なるつまらない数の、つまらないoneの配列です。"
  },
  {
    "start": 2928960,
    "end": 2951759,
    "text": "torchにはtrillという関数があります。これは三角形のようなものの略で、これをtorchedで一度にラップすると、この三角形の下の部分を返すだけです。"
  },
  {
    "start": 2954960,
    "end": 2961810,
    "text": "で、aはこう、bはこう、そしてcはどうなっているかというと、この数字は何でしょう。"
  },
  {
    "start": 2961810,
    "end": 2964875,
    "text": "これは、1行目×1列目です。"
  },
  {
    "start": 2964875,
    "end": 2975831,
    "text": "これはゼロで、ここにある要素は無視されるので、2が得られ、そしてこの数字は、最初の行×2番目の列です。"
  },
  {
    "start": 2975831,
    "end": 2981556,
    "text": "これらはゼロなので無視され、7がこの1を掛け合わせるだけだからです。"
  },
  {
    "start": 2981556,
    "end": 2989503,
    "text": "見ると、これは1であり、次に0であるため、結局のところ、このbの行の列を抜き取っただけである、"
  },
  {
    "start": 2989503,
    "end": 2993461,
    "text": "それが今、ここにある1.0.0.0.0.0.0です。"
  },
  {
    "start": 2993461,
    "end": 3012476,
    "text": "この2つの列の1.1.0ドットプロダクトは、2＋6で8、7＋4で11となり、これが1.1.1なので、結局はすべての足し算となり、ここにある1と0の数に応じて、基本的に1.0.0となります。"
  },
  {
    "start": 3012476,
    "end": 3020457,
    "text": "基本的には、これらの行の可変数の現在和を行い、それがcに預けられる。"
  },
  {
    "start": 3020457,
    "end": 3026229,
    "text": "現在、私たちは、これらが1つであるため、和算をしていますが、平均値を正しくすることも可能です。"
  },
  {
    "start": 3026229,
    "end": 3034607,
    "text": "行う必要がないため、bの行の平均をインクリメンタル方式で行うことができることがわかると思います。"
  },
  {
    "start": 3034607,
    "end": 3040749,
    "text": "これらの行の合計が1になるように正規化し、平均を求めることになります。"
  },
  {
    "start": 3040749,
    "end": 3055759,
    "text": "取り、aの1/3次元のumでa equals a divide a torch dot sumを行い、それらをtrueとして保持するようにします。"
  },
  {
    "start": 3056400,
    "end": 3059478,
    "text": "そのため、放送はうまくいくでしょう。"
  },
  {
    "start": 3059478,
    "end": 3064560,
    "text": "これを再実行すると、これらの行の合計が1になり、この行が1であることがわかります。"
  },
  {
    "start": 3064560,
    "end": 3065652,
    "text": "この行は0.5である。"
  },
  {
    "start": 3065652,
    "end": 3066492,
    "text": "0.5はゼロです。"
  },
  {
    "start": 3066492,
    "end": 3068425,
    "text": "3分の1になります。"
  },
  {
    "start": 3068425,
    "end": 3072313,
    "text": "A×Bを行うと、どうなるのでしょう？"
  },
  {
    "start": 3072313,
    "end": 3075794,
    "text": "ここでは、最初の行の1行目を取得しているだけです。"
  },
  {
    "start": 3075794,
    "end": 3079877,
    "text": "で、最初の2行の平均を取得しています。"
  },
  {
    "start": 3079877,
    "end": 3085291,
    "text": "2と6の平均は4、4と7の平均は5.5ということになりますね。"
  },
  {
    "start": 3085291,
    "end": 3091251,
    "text": "下に、この3つの行の平均が表示されるようになりました。"
  },
  {
    "start": 3091251,
    "end": 3095635,
    "text": "bの全要素の平均値がここに預けられるようになった。"
  },
  {
    "start": 3095635,
    "end": 3105256,
    "text": "この乗算行列の要素を操作して、任意の行列と乗算することでわかると思います。"
  },
  {
    "start": 3105256,
    "end": 3115975,
    "text": "得るだけなので、このようなインクリメンタルな方法で平均を取ることができ、それをOKの要素に基づいて操作できるので、とても便利です。"
  },
  {
    "start": 3115975,
    "end": 3122069,
    "text": "ここでもう一度、学んだことを活かして、これをベクトル化し、より効率的なものにする方法を考えてみましょう。"
  },
  {
    "start": 3122069,
    "end": 3128000,
    "text": "特に配列aを作るのですが、ここではweightsの略でwayと呼ぶことにします。"
  },
  {
    "start": 3129040,
    "end": 3130384,
    "text": "これは、私たちのア"
  },
  {
    "start": 3130384,
    "end": 3134610,
    "text": "これは、すべての行を平均化したい量です。"
  },
  {
    "start": 3134610,
    "end": 3140175,
    "text": "これらの行の合計が1であることがわかるので、平均値になるのです。"
  },
  {
    "start": 3140175,
    "end": 3152450,
    "text": "これがaで、この例ではbはもちろんxです。つまり、これから起こることは、xbow2ができることです、"
  },
  {
    "start": 3152450,
    "end": 3157942,
    "text": "このXbow2は、RXの倍率が高くなりそうです。"
  },
  {
    "start": 3157942,
    "end": 3169216,
    "text": "このように考えてみると、t×tはマトリックス、a×b×t×cはピトルヒの乗算で、どのような形になるのかがわかります。"
  },
  {
    "start": 3169216,
    "end": 3177563,
    "text": "pytorchがここに来て、これらの形が同じでないことがわかるので、ここにバッチ次元を作ることができるのです。"
  },
  {
    "start": 3177563,
    "end": 3189569,
    "text": "これはバッチ行列の乗算なので、この行列の乗算をすべてのバッチ要素に並行して個別に適用し、さらに各バッチ要素に適用することになります、"
  },
  {
    "start": 3189569,
    "end": 3195769,
    "text": "以下のように、tにcを掛けたt by tが正確に存在することになります。"
  },
  {
    "start": 3195769,
    "end": 3205331,
    "text": "これにより、b by t by c が作成され、xbow2 は xbow と同一になります。"
  },
  {
    "start": 3205331,
    "end": 3214400,
    "text": "xbowとxbow2のすべてのクローズを焼き切ったことが確認できるはずです。"
  },
  {
    "start": 3216240,
    "end": 3239175,
    "text": "このようにすることで、これらが実際に同じものであることを確信させることができます。"
  },
  {
    "start": 3239175,
    "end": 3246592,
    "text": "最初のバッチだけで、これとこれは同一であるべきで、それは正しいのですが、ここで何が起こったのでしょうか。"
  },
  {
    "start": 3246592,
    "end": 3253389,
    "text": "その結果、バッチのマトリックス乗算を使用して、このような集約を行うことができました。"
  },
  {
    "start": 3253389,
    "end": 3256150,
    "text": "重み付けされた集計である。"
  },
  {
    "start": 3256150,
    "end": 3260331,
    "text": "このt×tの配列で重みが指定される。"
  },
  {
    "start": 3260331,
    "end": 3279586,
    "text": "つまり、t番目の次元のトークンは、その前のトークンからしか情報を得られないということです。"
  },
  {
    "start": 3279586,
    "end": 3281449,
    "text": "それこそが、私たちが望んでいることなのです。"
  },
  {
    "start": 3281449,
    "end": 3287452,
    "text": "最後に、もう1つの方法で書き換えて、それがなぜ役に立つかを見てみたいと思います。"
  },
  {
    "start": 3287452,
    "end": 3293462,
    "text": "第3弾で、これも第1弾、第2弾と同じです。"
  },
  {
    "start": 3293462,
    "end": 3295905,
    "text": "通して話をさせてください。"
  },
  {
    "start": 3295905,
    "end": 3297460,
    "text": "ソフトマックスを使用しています。"
  },
  {
    "start": 3297460,
    "end": 3310106,
    "text": "トリル、この行列の下三角形のものは、すべてゼロとして始まる、だから、最初のほうの方法を印刷すると、すべてゼロになる。"
  },
  {
    "start": 3310106,
    "end": 3312416,
    "text": "その後、マスクドフィルを使用しました。"
  },
  {
    "start": 3312416,
    "end": 3316574,
    "text": "ドットマスクフィルの方法です。"
  },
  {
    "start": 3316574,
    "end": 3324640,
    "text": "トリルがゼロに等しい要素については、負の無限大になるように言っているのです。"
  },
  {
    "start": 3325359,
    "end": 3331745,
    "text": "トリルがゼロの要素はすべて負の無限大になるので、このような結果になります。"
  },
  {
    "start": 3331745,
    "end": 3334799,
    "text": "最終ラインはソフトマックスになります。"
  },
  {
    "start": 3337200,
    "end": 3340613,
    "text": "ソフトマックスを使用すると、すべてのso dimが負の値になります。"
  },
  {
    "start": 3340613,
    "end": 3343815,
    "text": "ソフトマックスの場合、すべての行に沿う。"
  },
  {
    "start": 3343815,
    "end": 3351445,
    "text": "ソフトマックスは、正規化操作のようなものでもあるんですね。"
  },
  {
    "start": 3351445,
    "end": 3355638,
    "text": "ネタバレになりますが、全く同じマトリックスを手に入れることができます。"
  },
  {
    "start": 3355638,
    "end": 3363519,
    "text": "ソフトマックスを復活させ、ソフトマックスでは、これらの一つ一つを指数化することを思い出してください。"
  },
  {
    "start": 3363519,
    "end": 3366237,
    "text": "その和で割ることになる。"
  },
  {
    "start": 3366237,
    "end": 3377013,
    "text": "つまり、すべての要素を指数化すると、ここでは1が得られ、ここでは1が得られ、ここでは1が得られ、ここでは1が得られ、それ以外の場所では基本的にゼロゼロゼロゼロになります。"
  },
  {
    "start": 3377013,
    "end": 3380082,
    "text": "正規化するときには、ここに1つだけ用意します。"
  },
  {
    "start": 3380082,
    "end": 3382700,
    "text": "1つ、そして0を1つ獲得するのです。"
  },
  {
    "start": 3382700,
    "end": 3388058,
    "text": "ソフトマックスが再び分割し、0.5 0.5となり、これが繰り返される。"
  },
  {
    "start": 3388058,
    "end": 3402678,
    "text": "このマスクがもう少し面白いのは、この重みがゼロから始まるからで、最終的に自己アテンションで使うことになります。"
  },
  {
    "start": 3402678,
    "end": 3408240,
    "text": "これは相互作用の強さのようなもの、あるいは親和性のようなものだと考えることができます。"
  },
  {
    "start": 3408240,
    "end": 3416267,
    "text": "過去の各トークンをどれだけ集計して平均化するかということです。"
  },
  {
    "start": 3416267,
    "end": 3423538,
    "text": "このセリフは、過去のトークンを負の無限大に設定することで、通信できないと言っていることになります。"
  },
  {
    "start": 3423538,
    "end": 3427519,
    "text": "そのトークンから何かを集約することはないと言っているのです。"
  },
  {
    "start": 3427519,
    "end": 3450680,
    "text": "基本的に、これはソフトマックスと重み付けを経て、行列の乗算で集約されます。つまり、これは現在、これらのゼロは私たちがゼロに設定したと考えることができます。"
  },
  {
    "start": 3450680,
    "end": 3459286,
    "text": "あるトークンは他のトークンに興味を持ち、あるトークンは他のトークンに興味を持ち、あるトークンは他のトークンに興味を持ち、あるトークンは他のトークンに興味を持ち、あるトークンは自分の価値観に依存しているのである、"
  },
  {
    "start": 3459286,
    "end": 3469268,
    "text": "「未来は過去とコミュニケーションできない」ということになるのでしょう、"
  },
  {
    "start": 3469268,
    "end": 3470957,
    "text": "クランプする予定です。"
  },
  {
    "start": 3470957,
    "end": 3494721,
    "text": "正規化と合計をするときに、互いがどの程度興味深いかによって、その値を集約することになります。このセクション全体から言えることは、下三角形の行列乗算を使えば、過去の要素の重み付き集約ができるということです。"
  },
  {
    "start": 3494721,
    "end": 3502568,
    "text": "この下の三角形の部分の素子は、それぞれの素子がこの位置にどれだけ融合しているかを教えてくれているのです。"
  },
  {
    "start": 3502568,
    "end": 3506435,
    "text": "今回は、このトリックを使って、「セルフアテンションブロック」を開発することにします。"
  },
  {
    "start": 3506435,
    "end": 3514823,
    "text": "まず、前置きが長くなりましたが、気になるのは、コンストラクタにボキャブラリーのサイズを渡しているところです。"
  },
  {
    "start": 3514823,
    "end": 3519535,
    "text": "ボキャブラリーのサイズはグローバル変数としてすでに定義されているので、その必要はない。"
  },
  {
    "start": 3519535,
    "end": 3525605,
    "text": "このようなものを渡す必要はないのですが、次に私がやりたいことは、実際に作成することではありません。"
  },
  {
    "start": 3525605,
    "end": 3531007,
    "text": "ロジットのエンベッディングに直接行かないような、間接的なレベルを作りたいですね。"
  },
  {
    "start": 3531007,
    "end": 3537135,
    "text": "大きくしていくので、この中間段階を経ることになるのです。"
  },
  {
    "start": 3537135,
    "end": 3539680,
    "text": "新しい変数nembedを紹介しましょう。"
  },
  {
    "start": 3540800,
    "end": 3545206,
    "text": "埋め込み次元数の略です。"
  },
  {
    "start": 3545206,
    "end": 3547847,
    "text": "nembedはここでは32とします。"
  },
  {
    "start": 3547847,
    "end": 3551842,
    "text": "github copilotからの提案でした。"
  },
  {
    "start": 3551842,
    "end": 3554614,
    "text": "また、32という数字も示唆されました。"
  },
  {
    "start": 3554614,
    "end": 3559168,
    "text": "これはエンベッディングテーブルで、32次元のエンベッディングのみです。"
  },
  {
    "start": 3559168,
    "end": 3565472,
    "text": "これはロジットを直接与えるのではなく、トークン埋め込みを与えることになります。"
  },
  {
    "start": 3565472,
    "end": 3566984,
    "text": "ということにしています。"
  },
  {
    "start": 3566984,
    "end": 3570102,
    "text": "で、トークン埋め込みからロジットに移行する。"
  },
  {
    "start": 3570102,
    "end": 3572236,
    "text": "リニアレイヤーが必要になってきます。"
  },
  {
    "start": 3572236,
    "end": 3575958,
    "text": "self.lmheadは、言語モデリングの略とでも言いましょうか。"
  },
  {
    "start": 3575958,
    "end": 3585451,
    "text": "headはネムベッドからボキャブラリーサイズまでnnlinearで、こちらに振ると実際にコパイロットの言うとおりにロジットを取得することになります、"
  },
  {
    "start": 3585451,
    "end": 3590459,
    "text": "ここで注意しなければならないのは、このcとこのcは等しくないということです。"
  },
  {
    "start": 3590459,
    "end": 3593920,
    "text": "ネムベッドC、こちらはボキャブラリーサイズです。"
  },
  {
    "start": 3594960,
    "end": 3598126,
    "text": "ネムベットがcに等しいとでも言っておこうか。"
  },
  {
    "start": 3598126,
    "end": 3603583,
    "text": "リニアなレイヤーを通して、1つのスプリアスなインダイレクトのレイヤーを作るだけになってしまいます。"
  },
  {
    "start": 3603583,
    "end": 3605279,
    "text": "基本的に実行されるはずです。"
  },
  {
    "start": 3605280,
    "end": 3619200,
    "text": "実行され、現在のところ異常な状態に見えますが、次はこの上に構築するつもりです。"
  },
  {
    "start": 3619200,
    "end": 3626367,
    "text": "idxの中にあるトークンのIDに基づき、これらのインデックスをエンコードしています。"
  },
  {
    "start": 3626367,
    "end": 3634270,
    "text": "次に、よくあることですが、トークンの識別だけでなく、その位置も符号化します。"
  },
  {
    "start": 3634270,
    "end": 3638262,
    "text": "ここでは、2番目の位置の埋め込みテーブルを用意することにします。"
  },
  {
    "start": 3638262,
    "end": 3643431,
    "text": "self.position.embedding.tableは、nembedによるブロックサイズのエンベッディングです。"
  },
  {
    "start": 3643431,
    "end": 3649473,
    "text": "0からブロックサイズマイナス1までの各位置には、それぞれ埋め込みベクトルも入ります。"
  },
  {
    "start": 3649473,
    "end": 3654288,
    "text": "まずここで、idx.shapeからb by tをデコードしてみます。"
  },
  {
    "start": 3654288,
    "end": 3659168,
    "text": "ここではプラスアルファのエンベッディング、つまり位置のエンベッディングもあることになります。"
  },
  {
    "start": 3659168,
    "end": 3661160,
    "text": "これらはこれです。"
  },
  {
    "start": 3661160,
    "end": 3668680,
    "text": "基本的に0からtマイナス1までの整数と、0からtマイナス1までの整数のみになります。"
  },
  {
    "start": 3668680,
    "end": 3672366,
    "text": "テーブルを貫通して埋め込まれ、T by Cを作成します。"
  },
  {
    "start": 3672366,
    "end": 3681337,
    "text": "ここでは「x」と改名し、「x」はトークン埋め込みと位置埋め込みの足し算とします。"
  },
  {
    "start": 3681337,
    "end": 3684331,
    "text": "ここで、放送局をうまく利用することになります。"
  },
  {
    "start": 3684331,
    "end": 3691360,
    "text": "b by t by c plus t by c これは右寄せになり、新しい次元の1が追加され、バッチ全体に放送されます。"
  },
  {
    "start": 3692640,
    "end": 3703116,
    "text": "この時点で、xはトークンのアイデンティティだけでなく、これらのトークンが出現する位置も保持しています。もちろん、単純なビグラムモデルを持っているだけなので、これは現在それほど有用ではありません。"
  },
  {
    "start": 3703116,
    "end": 3709145,
    "text": "この段階では、5番目の位置でも2番目の位置でも、どこでも翻訳不変なのです。"
  },
  {
    "start": 3709145,
    "end": 3721756,
    "text": "しかし、「セルフ・アテンション・ブロック」に取り組むと、この情報が重要になることがわかります。"
  },
  {
    "start": 3721756,
    "end": 3730680,
    "text": "このビデオでは、おそらく最も重要な部分を理解するために、私たちは、彼らが呼ばれるように、単一の個々の頭のための小さな自己注意を実装するつもりです。"
  },
  {
    "start": 3730680,
    "end": 3734400,
    "text": "このコードはすべて慣れ親しんだものであるため、まずは元の場所から始めます。"
  },
  {
    "start": 3735280,
    "end": 3739677,
    "text": "今、チャンネル数を2から32に変更する例で作業しています。"
  },
  {
    "start": 3739677,
    "end": 3748560,
    "text": "トークンを4×8で配置し、各トークンと各トークンの情報は現在32次元である。"
  },
  {
    "start": 3748560,
    "end": 3762009,
    "text": "乱数を扱うだけなのに、このコードでは、過去のすべてのトークンと現在のトークンの単純加重平均が行われていることがわかりました。"
  },
  {
    "start": 3762009,
    "end": 3766767,
    "text": "前の情報と今の情報が平均的に混ざっているだけなんです。"
  },
  {
    "start": 3766767,
    "end": 3769086,
    "text": "現在このコードで実現していることです。"
  },
  {
    "start": 3769086,
    "end": 3769680,
    "text": "そうですね"
  },
  {
    "start": 3769680,
    "end": 3776960,
    "text": "この低三角形構造を作ることで、作成したウェイ行列をマスクアウトすることができます。"
  },
  {
    "start": 3777680,
    "end": 3780722,
    "text": "マスクして、それを正常化するのです。"
  },
  {
    "start": 3780722,
    "end": 3787176,
    "text": "現在では、すべての異なる種類のトークンやノード間の親和性を初期化するときに使用されます。"
  },
  {
    "start": 3787176,
    "end": 3789760,
    "text": "私はこの言葉を同じ意味で使うつもりです。"
  },
  {
    "start": 3790560,
    "end": 3801870,
    "text": "すべての異なるトークン間の親和性をゼロに初期化すると、すべての行が一様な数字を持つという構造ができることがわかります。"
  },
  {
    "start": 3801870,
    "end": 3809566,
    "text": "この行列の乗算で単純な平均を取るようになります。"
  },
  {
    "start": 3809566,
    "end": 3821471,
    "text": "なぜなら、トークンによって、他のトークンをより面白く感じたり、面白く感じなかったりするからです。"
  },
  {
    "start": 3821471,
    "end": 3822976,
    "text": "例えば私が母音であれば"
  },
  {
    "start": 3822976,
    "end": 3825671,
    "text": "私は自分の過去に子音を探しているのかもしれません。"
  },
  {
    "start": 3825671,
    "end": 3830822,
    "text": "その子音が何なのか知りたい、その情報が流れてきてほしいということなのかもしれません。"
  },
  {
    "start": 3830822,
    "end": 3833813,
    "text": "これから過去の情報を集めたいと思います。"
  },
  {
    "start": 3833813,
    "end": 3835735,
    "text": "データ依存で行いたい。"
  },
  {
    "start": 3835735,
    "end": 3848408,
    "text": "この問題を解決するのが「自己注意」なのですが、「自己注意」がこれを解決する方法は以下の通りです。 各位置のノードやトークンが2つのベクトルを発する。"
  },
  {
    "start": 3848408,
    "end": 3852000,
    "text": "クエリを発行し、キーを発行します。"
  },
  {
    "start": 3853520,
    "end": 3857720,
    "text": "クエリベクトルは大雑把に言うと「何を探しているのか」ということです。"
  },
  {
    "start": 3857720,
    "end": 3873280,
    "text": "キーベクターとは、大まかに言うと「私は何を含んでいるか」ということで、これらのトークン間の親和性を得るには、シーケンスでは、基本的にキーとクエリーの間のドットプロダクトを行うだけです。"
  },
  {
    "start": 3873280,
    "end": 3883228,
    "text": "私のクエリは、他のすべてのトークンのすべてのキーとドットプロダクトし、そのドットプロダクトは、今では方法になります。"
  },
  {
    "start": 3883228,
    "end": 3886924,
    "text": "キーとクエリがある種一致している場合。"
  },
  {
    "start": 3886924,
    "end": 3889311,
    "text": "非常に高い量まで対話することになります。"
  },
  {
    "start": 3889311,
    "end": 3896080,
    "text": "私はその特定のトークンについて、他のトークンとは対照的に、より多くを学ぶことができる。"
  },
  {
    "start": 3896080,
    "end": 3907478,
    "text": "これを実装してみましょう！今回は、いわゆる「自己アテンションの頭」を1つ実装してみます。"
  },
  {
    "start": 3907478,
    "end": 3909440,
    "text": "これは1つの頭だけです。"
  },
  {
    "start": 3909440,
    "end": 3913285,
    "text": "この頭にはハイパーパラメーターがあり、それは頭の大きさである。"
  },
  {
    "start": 3913285,
    "end": 3922590,
    "text": "ここでは、線形モジュールを初期化し、bias equals falseを使用しているので、固定された重みでマトリックス乗算を適用するだけです。"
  },
  {
    "start": 3922590,
    "end": 3929478,
    "text": "これらのモジュールをxに転送することで、鍵とqのkとqを生成してみます。"
  },
  {
    "start": 3929478,
    "end": 3940000,
    "text": "このサイズは、ヘッドサイズなのでb by t by 16となり、こちらも同様にb by t by 16となります。"
  },
  {
    "start": 3940960,
    "end": 3942641,
    "text": "これがヘッドサイズです。"
  },
  {
    "start": 3942641,
    "end": 3950652,
    "text": "この線形を私のXの上に進めると、B by Tの配置のすべての位置にすべてのトークンがあることがわかります、"
  },
  {
    "start": 3950652,
    "end": 3955040,
    "text": "並列に並べ、それぞれ独立してキーとクエリを生成します。"
  },
  {
    "start": 3955040,
    "end": 3957600,
    "text": "まだコミュニケーションは起きていません。"
  },
  {
    "start": 3957600,
    "end": 3962994,
    "text": "通信は、今、すべての問い合わせは、すべてのキーとドット製品になります。"
  },
  {
    "start": 3962994,
    "end": 3971200,
    "text": "私たちが望んでいるのは、今すぐにでも、あるいはこれらの間の親和性を、クエリ増殖の鍵にしたいということです。"
  },
  {
    "start": 3971920,
    "end": 3974291,
    "text": "注意が必要で、行列を作るわけにはいきません。"
  },
  {
    "start": 3974291,
    "end": 3981772,
    "text": "転置する必要があるのですが、バッシュ次元がある場合なので、注意が必要です。"
  },
  {
    "start": 3981772,
    "end": 3993807,
    "text": "特に、最後の2次元のマイナス1次元とマイナス2次元を、最後の2次元のマイナス1次元とマイナス2次元に転置したい。"
  },
  {
    "start": 3993807,
    "end": 4014378,
    "text": "負の2負の1この行列の乗算は基本的に次のようになります。 b by t by 16行列はb by 16にtを掛けてb by t by tを得ます、"
  },
  {
    "start": 4014378,
    "end": 4022166,
    "text": "親和性を示すT字型正方形行列ができあがり、これがゼロにならないようにするための方法です。"
  },
  {
    "start": 4022166,
    "end": 4026816,
    "text": "キーとクエリの間のこのドット積に由来しています。"
  },
  {
    "start": 4026816,
    "end": 4029316,
    "text": "これはもう、走れる走れる。"
  },
  {
    "start": 4029316,
    "end": 4036623,
    "text": "重み付けされた集約は、これらのノードのキーとクエリの間のデータ放棄された方法で関数となります。"
  },
  {
    "start": 4036623,
    "end": 4045212,
    "text": "ここで起きたことを検証すると、wayがこのような形になり、以前はwayがただの定数だったことがわかります。"
  },
  {
    "start": 4045212,
    "end": 4048114,
    "text": "すべてのバッチ要素に同じように適用された。"
  },
  {
    "start": 4048114,
    "end": 4059104,
    "text": "バッチエレメントはそれぞれ異なる位置に異なるトークンを含んでいるからです。"
  },
  {
    "start": 4059104,
    "end": 4064935,
    "text": "例えば、入力の0番目だけを見ると、このような重みが出ています。"
  },
  {
    "start": 4064935,
    "end": 4068440,
    "text": "厳密には統一されていないことがおわかりいただけたかと思います。"
  },
  {
    "start": 4068440,
    "end": 4084880,
    "text": "特に最後の行の例では、これは8番目のトークンです。8番目のトークンは、どのようなコンテンツを持っているか、どの位置にあるかを知っています。そして、8番目のトークンは、それに基づいてクエリを作成します。"
  },
  {
    "start": 4084960,
    "end": 4085581,
    "text": "私は母音です。"
  },
  {
    "start": 4085581,
    "end": 4087010,
    "text": "私は8番目のポジションにいます。"
  },
  {
    "start": 4087010,
    "end": 4092469,
    "text": "4つまでの位置に子音を探し、すべてのノードがキーを発するようにします。"
  },
  {
    "start": 4092469,
    "end": 4102960,
    "text": "例えば、あるチャンネルで「私は子音で、私は4までのポジションにいる」とすると、そのキーはその特定のチャンネルで高い数値になります。"
  },
  {
    "start": 4102960,
    "end": 4107723,
    "text": "このように、クエリとキーがドットプロダクトになることで、お互いを見つけ、高い親和性を生み出すことができるのです。"
  },
  {
    "start": 4107723,
    "end": 4114081,
    "text": "そうすることで、お互いがお互いを見つけ出し、高い親和性を生み出すことができるのです。"
  },
  {
    "start": 4114081,
    "end": 4125043,
    "text": "高い親和性を持っている場合、ソフトマックスを通じて、この8番目のトークンはかなり興味深いものでした。"
  },
  {
    "start": 4125043,
    "end": 4138090,
    "text": "その情報を自分のポジションに集約することで、多くのことを学ぶことができます。"
  },
  {
    "start": 4138090,
    "end": 4140585,
    "text": "この操作についても、消去させてください。"
  },
  {
    "start": 4140585,
    "end": 4145519,
    "text": "マスキングとソフトマックスを消して、内部構造とその仕組みをお見せします。"
  },
  {
    "start": 4146399,
    "end": 4150330,
    "text": "マスキングなしで、ソフトマックスのやり方だと、こうなりますよね。"
  },
  {
    "start": 4150330,
    "end": 4154270,
    "text": "これがドットプロダクトの出力で、これが生の出力です。"
  },
  {
    "start": 4154270,
    "end": 4164246,
    "text": "このように、すべてのノード間の相互作用と親和性を生で見ることができます。"
  },
  {
    "start": 4164246,
    "end": 4169233,
    "text": "もし、私が第5のノードであれば、第6のノードから何も集約したくないでしょう。"
  },
  {
    "start": 4169233,
    "end": 4171276,
    "text": "第7のノードと第8のノードのことです。"
  },
  {
    "start": 4171276,
    "end": 4177192,
    "text": "私たちが使っているのは上三角形のマスキングなので、それらが通信できないようになっているのです。"
  },
  {
    "start": 4177192,
    "end": 4181437,
    "text": "今、私たちは実際に素敵なディストリビューションを持ちたいと考えています。"
  },
  {
    "start": 4181437,
    "end": 4186140,
    "text": "このノードのマイナス0.11を集計するのは、おかしいと思います。"
  },
  {
    "start": 4186140,
    "end": 4191099,
    "text": "その代わりに、指数と正規化を行い、和が1になるようなきれいな分布を得ることができます。"
  },
  {
    "start": 4191099,
    "end": 4197280,
    "text": "これは、過去のどのトークンからどれだけの情報を集約するかということを、データに依存した形で今教えてくれているのです。"
  },
  {
    "start": 4199440,
    "end": 4208822,
    "text": "もうゼロではないのですが、こうして計算すると、1つの自己アテンション・ヘッドに1つのパーツが増えるのです。"
  },
  {
    "start": 4208822,
    "end": 4215439,
    "text": "集計を行う際に、実際にトークンを正確に集計するわけではないのです。"
  },
  {
    "start": 4215439,
    "end": 4217461,
    "text": "ここでもう1つの価値を生み出します。"
  },
  {
    "start": 4217461,
    "end": 4219280,
    "text": "それを価値と呼んでいます。"
  },
  {
    "start": 4219280,
    "end": 4229893,
    "text": "、キーとクエリを作ったのと同じように、値も作って、ここではxを集約しないようにします、"
  },
  {
    "start": 4229893,
    "end": 4249531,
    "text": "この線形をxの上に再度伝播させることでvを計算し、wayにvを掛けたものを出力します。つまりvは、生のxの代わりに集約した要素、または集約したベクトルです。"
  },
  {
    "start": 4249531,
    "end": 4261448,
    "text": "もちろん、これによって1つのヘッドの出力は16次元になります。これがヘッドのサイズですから、xはこのトークンのプライベートな情報のようなものだと考えてください。"
  },
  {
    "start": 4261448,
    "end": 4263791,
    "text": "そのように考えれば"
  },
  {
    "start": 4263791,
    "end": 4265996,
    "text": "xはこのトークンのプライベートのようなものです。"
  },
  {
    "start": 4265996,
    "end": 4272152,
    "text": "私はどこかの第5トークンであり、私はあるアイデンティティを持っていて、私の情報はベクトルxに保持されています。"
  },
  {
    "start": 4272152,
    "end": 4277364,
    "text": "今回のシングルヘッドの目的ですが、私が興味を持っているのはこちらです。"
  },
  {
    "start": 4277364,
    "end": 4278688,
    "text": "これが私の持っているものです。"
  },
  {
    "start": 4278688,
    "end": 4284500,
    "text": "もしあなたが私のことを面白いと思ってくれたら、私があなたに伝えることはこうで、それはVに保存されています。"
  },
  {
    "start": 4284500,
    "end": 4291885,
    "text": "vは異なるノード間でこの一人頭の目的のために集約されるものである。"
  },
  {
    "start": 4291885,
    "end": 4314627,
    "text": "これは、基本的に自己注意のメカニズムです。"
  },
  {
    "start": 4314627,
    "end": 4318301,
    "text": "各ノードが何らかの情報のベクトルを持っていることです。"
  },
  {
    "start": 4318301,
    "end": 4326239,
    "text": "指すすべてのノードからの加重和を介して情報を集約することができ、これはデータ依存の方法で行われます。"
  },
  {
    "start": 4326239,
    "end": 4331423,
    "text": "各ノードで実際に保存されているデータがどの時点であっても、そのデータに応じて"
  },
  {
    "start": 4331423,
    "end": 4333834,
    "text": "これでグラフはこうはいきませんね。"
  },
  {
    "start": 4333834,
    "end": 4335704,
    "text": "グラフは、別の構造を持っています。"
  },
  {
    "start": 4335704,
    "end": 4343768,
    "text": "ブロックサイズが8であるため、8つのノードがあり、常に8つのトークンが存在し、最初のノードはそれ自身によってのみ指し示されます。"
  },
  {
    "start": 4343768,
    "end": 4352807,
    "text": "2番目のノードは1番目のノードとそれ自身によって指され、8番目のノードに至るまで、前のすべてのノードとそれ自身によって指されるのです。"
  },
  {
    "start": 4352807,
    "end": 4360799,
    "text": "これが、言語モデリングのような自己回帰的なシナリオにおいて、有向グラフが持つ、あるいは偶然持つ構造なのです。"
  },
  {
    "start": 4360799,
    "end": 4364052,
    "text": "原理的に任意の有向グラフに適用することができます。"
  },
  {
    "start": 4364052,
    "end": 4370665,
    "text": "2つ目の注意点は、空間という概念がないことに気づくことです。"
  },
  {
    "start": 4370665,
    "end": 4374700,
    "text": "アテンションは、このグラフの中でベクトルの集合のように作用するだけです。"
  },
  {
    "start": 4374700,
    "end": 4379296,
    "text": "そのため、デフォルトでは、これらのノードは、自分が空間のどこに位置しているのかを知ることができません。"
  },
  {
    "start": 4379296,
    "end": 4394338,
    "text": "そのため、位置情報をエンコードして、特定の位置に固定された情報を与えることで、自分の位置を把握できるようにする必要があります。"
  },
  {
    "start": 4394338,
    "end": 4401928,
    "text": "空間における情報のレイアウトは非常に特殊で、畳み込みフィルターが空間上で作用するようなものです。"
  },
  {
    "start": 4401928,
    "end": 4407526,
    "text": "だから、アテンションは、空間にあるベクトルの集合体というわけではないんです。"
  },
  {
    "start": 4407526,
    "end": 4408502,
    "text": "伝えるのです。"
  },
  {
    "start": 4408502,
    "end": 4410688,
    "text": "スペースという概念を持たせたいのであれば。"
  },
  {
    "start": 4410688,
    "end": 4412235,
    "text": "特に追加する必要があります。"
  },
  {
    "start": 4412235,
    "end": 4420021,
    "text": "これは、相対的な位置エンコードを計算し、その情報をベクターに追加したものです。"
  },
  {
    "start": 4420021,
    "end": 4426724,
    "text": "次に、非常にわかりやすいと思うのですが、バッチ次元のエレメントは、独立した例であり、決して互いに話すことはありません。"
  },
  {
    "start": 4426724,
    "end": 4428457,
    "text": "常に独立して処理されます。"
  },
  {
    "start": 4428457,
    "end": 4435193,
    "text": "これは，基本的に行列の掛け算をバッチ次元で並列に適用するバッチ行列の掛け算です．"
  },
  {
    "start": 4435193,
    "end": 4441847,
    "text": "この有向グラフの例では、バッチサイズが4なので、本当にそうなっていると言った方が正確かもしれませんね、"
  },
  {
    "start": 4441847,
    "end": 4447087,
    "text": "8つのノードからなる4つの別々のプールがあり、その8つのノードは互いにしか会話しません。"
  },
  {
    "start": 4447087,
    "end": 4453519,
    "text": "合計で32のノードが処理されていますが、8つのプールが4つに分かれているような感じです。"
  },
  {
    "start": 4453519,
    "end": 4455125,
    "text": "というような見方もできます。"
  },
  {
    "start": 4455125,
    "end": 4466091,
    "text": "次に、言語モデリングでは、有向グラフという特殊な構造を持ち、未来のトークンが過去のトークンに伝達されることはない、ということです。"
  },
  {
    "start": 4466091,
    "end": 4470240,
    "text": "一般的なケースでは必ずしも制約になるとは限りません。"
  },
  {
    "start": 4470240,
    "end": 4476270,
    "text": "実際、多くの場合、すべてのノードが互いに完全に対話できるようにすることが望ましいでしょう。"
  },
  {
    "start": 4476270,
    "end": 4488848,
    "text": "例えば、トランスフォーマーで感情分析などを行う場合、多くのトークンがあり、後で文の感情などを予測するため、すべてのトークンが互いに完全に会話できるようにしたいと思うかもしれません。"
  },
  {
    "start": 4488848,
    "end": 4491908,
    "text": "これらのノードが互いに会話しても問題ありません。"
  },
  {
    "start": 4491908,
    "end": 4499210,
    "text": "そのような場合には、自己の注意を喚起するエンコーダーブロックを使うことになりますが、それがエンコーダーブロックであるということは、すべての意味があります、"
  },
  {
    "start": 4499210,
    "end": 4508184,
    "text": "このコード行を削除することで、すべてのノードが完全に会話できるようになるのです。この実装は、デコーダーブロックと呼ばれることもあります。"
  },
  {
    "start": 4508184,
    "end": 4519360,
    "text": "デコーダーと呼ばれるのは、デコード言語のようなもので、三角行列でマスクする自己回帰形式をとっているからです。"
  },
  {
    "start": 4519360,
    "end": 4527330,
    "text": "エンコーダーブロックでは、未来からのメモが答えを教えてしまうので、過去とは話さないというのが基本です、"
  },
  {
    "start": 4527330,
    "end": 4532363,
    "text": "削除して、すべてのノードがデコーダーブロックで会話できるようにすれば、常に存在することになります。"
  },
  {
    "start": 4532363,
    "end": 4534685,
    "text": "このような三角形の構造をしているということです。"
  },
  {
    "start": 4534685,
    "end": 4537045,
    "text": "両方が許され、注目は気にならない。"
  },
  {
    "start": 4537045,
    "end": 4539777,
    "text": "attentionは、ノード間の任意の接続をサポートします。"
  },
  {
    "start": 4539777,
    "end": 4546912,
    "text": "次に、私がコメントしたいのは、「注意」「自己注意」などという言葉をよく耳にしますが、実は「交差注意」というものもあるのです。"
  },
  {
    "start": 4546912,
    "end": 4547680,
    "text": "何が違うのでしょうか？"
  },
  {
    "start": 4548800,
    "end": 4560379,
    "text": "このアテンションがセルフアテンションである理由は、キークエリーとバリューがすべてxから同じソースから来ているためです。"
  },
  {
    "start": 4560379,
    "end": 4566136,
    "text": "同じソースxがキーのクエリと値を生成するため、これらのノードはセルフアテンディングである。"
  },
  {
    "start": 4566136,
    "end": 4575842,
    "text": "例えば、エンコーダ・デコーダ・トランスでは、クエリがxから生成されるようなケースもあり得ます。"
  },
  {
    "start": 4575842,
    "end": 4579798,
    "text": "キーとバリューは全く別の外部ソースから来る。"
  },
  {
    "start": 4579798,
    "end": 4585148,
    "text": "エンコーダーブロックから、条件となる文脈をエンコードすることもあります。"
  },
  {
    "start": 4585148,
    "end": 4589058,
    "text": "そのため、キーとバリューは実際には全く別のソースから来ることになります。"
  },
  {
    "start": 4589058,
    "end": 4590415,
    "text": "サイドのノードです。"
  },
  {
    "start": 4590415,
    "end": 4595419,
    "text": "ここでは、クエリーを作るだけで、横から情報を読み上げています。"
  },
  {
    "start": 4595419,
    "end": 4599890,
    "text": "クロスアテンションは、ノードのソースが別にある場合に使用します。"
  },
  {
    "start": 4599890,
    "end": 4603544,
    "text": "からの情報をノードに取り込みたい。"
  },
  {
    "start": 4603544,
    "end": 4604782,
    "text": "それは自己アテンションです。"
  },
  {
    "start": 4604782,
    "end": 4607519,
    "text": "お互いに顔を見合わせたり、話をしたりしたいノードがあればいいのです。"
  },
  {
    "start": 4608240,
    "end": 4612261,
    "text": "ここにある注意は、たまたま自己の注意である。"
  },
  {
    "start": 4612261,
    "end": 4623865,
    "text": "この段階での最後の注意点は、もし「attention is all you need paper」（attentionは必要なものだけです）が出てきたら、私たちはすでにattentionを実装しています。"
  },
  {
    "start": 4623865,
    "end": 4627651,
    "text": "クエリのキーと値が与えられたら、キーにクエリを掛け合わせました。"
  },
  {
    "start": 4627651,
    "end": 4630800,
    "text": "ソフトマックスにし、その値を集計しています。"
  },
  {
    "start": 4631600,
    "end": 4642629,
    "text": "もうひとつ、頭の大きさの平方根を1倍した値で割るということが抜けています。"
  },
  {
    "start": 4642629,
    "end": 4653600,
    "text": "単位ガウス型の入力がある場合に、基本的に重要な正規化のようなもので、ゼロ平均単位分散のkとqは単位ガウス型になります、"
  },
  {
    "start": 4653600,
    "end": 4661280,
    "text": "素朴にやってみると、実際にはヘッドサイズ（この場合は16）のオーダーで分散が発生することがわかります。"
  },
  {
    "start": 4662320,
    "end": 4677923,
    "text": "これは平方根で、これは1オーバーということになります。そうすると、wayの分散は1になって保存されます。"
  },
  {
    "start": 4677923,
    "end": 4684714,
    "text": "そのため、特に初期化時には、かなり拡散させることが重要です。"
  },
  {
    "start": 4684714,
    "end": 4692333,
    "text": "私たちの場合は、運良くこの場所で、かなり拡散した数字が出ました。"
  },
  {
    "start": 4692333,
    "end": 4700251,
    "text": "このように、ソフトマックスの場合、重みが非常に正負の値をとることが問題です。"
  },
  {
    "start": 4700251,
    "end": 4703537,
    "text": "softmaxは実際に1つのホットベクトルに向かって収束します。"
  },
  {
    "start": 4703537,
    "end": 4710480,
    "text": "ここでは、ゼロに非常に近い値のテンソルにソフトマックスを適用する場合を例にとって説明します。"
  },
  {
    "start": 4710480,
    "end": 4713344,
    "text": "で、ソフトマックスからディフューズを出します。"
  },
  {
    "start": 4713344,
    "end": 4719435,
    "text": "8倍して大きくした瞬間、全く同じものをシャープにし始める。"
  },
  {
    "start": 4719435,
    "end": 4725200,
    "text": "例えば、ソフトマックスがシャープになり始め、実際にはマックスに向かってシャープになるのがわかると思います。"
  },
  {
    "start": 4725200,
    "end": 4729000,
    "text": "この数字が一番高いところに向かって研ぎ澄まされていきます。"
  },
  {
    "start": 4729000,
    "end": 4734553,
    "text": "基本的には、特に初期化時にこれらの値が極端に大きくなることは避けたいところです。"
  },
  {
    "start": 4734553,
    "end": 4737052,
    "text": "そうでないと、ソフトマックスがピーキーになりすぎてしまいます。"
  },
  {
    "start": 4737052,
    "end": 4744640,
    "text": "基本的に1つのノードから情報を集約しているわけで、どのノードも他の1つのノードから情報を集約しているだけです。"
  },
  {
    "start": 4744640,
    "end": 4745600,
    "text": "私たちが望んでいることではありません。"
  },
  {
    "start": 4745600,
    "end": 4751840,
    "text": "特に初期化時には、スケーリングは初期化時の分散を抑制するためだけに使用されます。"
  },
  {
    "start": 4751840,
    "end": 4763539,
    "text": "このコードでは、headモジュールを作成し、self-attentionの1つのheadを実装しています。"
  },
  {
    "start": 4763539,
    "end": 4771316,
    "text": "ここでキークエリーとバリューリニアレイヤーを作成するのですが、通常、この中でバイアスを使うことはありません。"
  },
  {
    "start": 4771316,
    "end": 4776245,
    "text": "すべてのノードに適用される線形投影です。"
  },
  {
    "start": 4776245,
    "end": 4781157,
    "text": "ここで、このtrill変数を作成しているのですが、trillはモジュールのパラメータではありません。"
  },
  {
    "start": 4781157,
    "end": 4784541,
    "text": "ピートルクの命名規則では、これをバッファと呼んでいます。"
  },
  {
    "start": 4784541,
    "end": 4786708,
    "text": "パラメータではないので、呼び出さなければならない。"
  },
  {
    "start": 4786708,
    "end": 4789520,
    "text": "レジスタバッファを使用してモジュールに割り当てる必要があります。"
  },
  {
    "start": 4789520,
    "end": 4793523,
    "text": "トリルを生み出す、試行錯誤の下三角形のマトリックス。"
  },
  {
    "start": 4793523,
    "end": 4799353,
    "text": "入力 x が与えられると、このように非常に見慣れたものになるはずです。今度は t のクエリを計算します。"
  },
  {
    "start": 4799353,
    "end": 4802119,
    "text": "サイドウェイでアテンションスコアを算出します。"
  },
  {
    "start": 4802119,
    "end": 4803126,
    "text": "正常化します。"
  },
  {
    "start": 4803126,
    "end": 4809454,
    "text": "ここではスケーラブル・アテンションを使っているので、未来が過去と交信しないようにするのです。"
  },
  {
    "start": 4809454,
    "end": 4823520,
    "text": "これは、デコーダブロックとソフトマックスを作り、その値を集計して出力するもので、言語モデルのここでは、コンストラクタでヘッドを作り、自己注意ヘッドとヘッドサイズと呼んでいます。"
  },
  {
    "start": 4823520,
    "end": 4834027,
    "text": "トークン埋め込みと位置埋め込みで情報をエンコードしたら、今はそのままで、ここに埋め込むつもりです。"
  },
  {
    "start": 4834027,
    "end": 4837058,
    "text": "、単にセルフアテンションヘッドに送り込むだけです。"
  },
  {
    "start": 4837058,
    "end": 4844082,
    "text": "その出力がデコーダー言語モデリングヘッドに入り、ロジットを作成する。"
  },
  {
    "start": 4844082,
    "end": 4850621,
    "text": "これは、今、私たちのネットワークにセルフアテンションコンポーネントを組み込む最も簡単な方法です。"
  },
  {
    "start": 4850621,
    "end": 4862992,
    "text": "もう1つ変更しなければならなかったのは、generateの中で、モデルに送り込むidxを確認する必要があったことです。"
  },
  {
    "start": 4862992,
    "end": 4874267,
    "text": "idxがブロックサイズ以上だと、ブロックサイズまでの埋め込みしかできないので、位置埋め込みテーブルの範囲が狭くなるからです。"
  },
  {
    "start": 4874267,
    "end": 4887889,
    "text": "selfに入力するコンテキストを切り抜くコードを追加し、ブロックサイズ以上の要素を決して渡さないようにしました。"
  },
  {
    "start": 4887889,
    "end": 4889995,
    "text": "私もここのスクリプトにたどり着いたんです。"
  },
  {
    "start": 4889995,
    "end": 4895043,
    "text": "学習率を下げたのは、自己アテンションが非常に高い学習率に耐えられなくなったからです。"
  },
  {
    "start": 4895043,
    "end": 4900080,
    "text": "その後、学習率が低くなったので反復回数を増やし、トレーニングしました。"
  },
  {
    "start": 4900080,
    "end": 4902923,
    "text": "以前は2.5までしか出せませんでした。"
  },
  {
    "start": 4902923,
    "end": 4904500,
    "text": "現在、2.4まで下がっています。"
  },
  {
    "start": 4904500,
    "end": 4909420,
    "text": "2.5から2.4にかけて、少しずつ改善されているのがよくわかります。"
  },
  {
    "start": 4909420,
    "end": 4917522,
    "text": "この文章は、まだ驚くべきものではありませんが、明らかに自己注意の頭が有用なコミュニケーションを行っています。"
  },
  {
    "start": 4917522,
    "end": 4924437,
    "text": "しかし、まだ先は長いですから、今、私たちはスケール・ドット・プロダクトを実装しました。"
  },
  {
    "start": 4924437,
    "end": 4928801,
    "text": "マルチヘッドアテンションというものがありますが、マルチヘッドアテンションとは何でしょうか？"
  },
  {
    "start": 4928801,
    "end": 4935680,
    "text": "これは、複数の注意を並行して行い、その結果を連結することで、ここにちょっとした図ができるのです。"
  },
  {
    "start": 4935680,
    "end": 4940865,
    "text": "これは非常にわかりやすいかどうかわかりませんが、本当に複数の注意を並行して行うことです。"
  },
  {
    "start": 4940865,
    "end": 4944080,
    "text": "それをかなりストレートに実装してみましょう。"
  },
  {
    "start": 4945280,
    "end": 4946811,
    "text": "マルチヘッドアテンションが必要な場合"
  },
  {
    "start": 4946811,
    "end": 4955813,
    "text": "そのため、Pytorchでは、複数のヘッドを作成することでこれを実現することができます。"
  },
  {
    "start": 4955813,
    "end": 4959317,
    "text": "何頭でもいい。"
  },
  {
    "start": 4959317,
    "end": 4961571,
    "text": "それぞれのヘッドサイズはどうなっているのでしょうか。"
  },
  {
    "start": 4961571,
    "end": 4967505,
    "text": "で、そのすべてをリストに並列に実行し、すべての出力を単純に連結する。"
  },
  {
    "start": 4967505,
    "end": 4984334,
    "text": "1つの通信チャネルを持つ代わりに、32のヘッドサイズを持つ1つのアテンションがあるわけではありません、"
  },
  {
    "start": 4984334,
    "end": 4987383,
    "text": "これで、4つの通信チャンネルが並列になりました。"
  },
  {
    "start": 4987383,
    "end": 4993291,
    "text": "これらの通信チャネルは、通常、それぞれ対応するように小さくなる。"
  },
  {
    "start": 4993291,
    "end": 4998866,
    "text": "私たちは4つのコミュニケーションチャンネルを持っているので、8次元の自己アテンションを求めています。"
  },
  {
    "start": 4998866,
    "end": 5003294,
    "text": "各通信チャンネルから8次元のベクトルを集めています。"
  },
  {
    "start": 5003294,
    "end": 5004763,
    "text": "4人いることになりますね。"
  },
  {
    "start": 5004763,
    "end": 5008350,
    "text": "連結して32を出す、これがオリジナルとエンベッドです。"
  },
  {
    "start": 5008350,
    "end": 5022535,
    "text": "これは、畳み込みに慣れている人なら、グループ畳み込みのようなもので、基本的に1つの大きな畳み込みではなく、グループで畳み込みを行うので、多頭の自己アテンションになります。"
  },
  {
    "start": 5022535,
    "end": 5039838,
    "text": "ここでは代わりにSAヘッドのセルフアテンションヘッドを使うことにしました。"
  },
  {
    "start": 5039838,
    "end": 5050051,
    "text": "そのため、複数のコミュニケーション・チャンネルを持つことが有効です。"
  },
  {
    "start": 5050051,
    "end": 5055119,
    "text": "子音を見つけたい、母音を見つけたい、ある位置からだけ母音を見つけたい。"
  },
  {
    "start": 5055119,
    "end": 5057939,
    "text": "いろいろなものを探したいと思っているのです。"
  },
  {
    "start": 5057939,
    "end": 5067386,
    "text": "そのため、複数の独立した通信チャネルを作り、さまざまな種類のデータを収集し、その出力をデコードするのに役立ちます、"
  },
  {
    "start": 5067386,
    "end": 5073061,
    "text": "もちろん、この図の詳細を説明したわけではありませんが、私たちがすでに実装しているコンポーネントの一部が見えてきました。"
  },
  {
    "start": 5073061,
    "end": 5081901,
    "text": "位置エンコーディング、トークン・エンコーディングを追加し、マスクされた多頭の注意を実装し、ここに別の多頭の注意を実装しました、"
  },
  {
    "start": 5081901,
    "end": 5088472,
    "text": "これはエンコーダーへのクロスアテンションですが、今回は実装していませんので、また後ほど。"
  },
  {
    "start": 5088472,
    "end": 5091905,
    "text": "ここにフィードフォワードの部分があることに気づいてほしいです。"
  },
  {
    "start": 5091905,
    "end": 5100148,
    "text": "これがブロックにまとめられ、何度も繰り返される。ここでのフィードフォワード部分は、単なる多層パーセプトロンである。"
  },
  {
    "start": 5100148,
    "end": 5101888,
    "text": "多頭飼いの"
  },
  {
    "start": 5101888,
    "end": 5107295,
    "text": "ここでは、位置ワイズフィードフォワードネットワークは、単純な小さなmlpに過ぎません。"
  },
  {
    "start": 5107295,
    "end": 5116236,
    "text": "私は、基本的に同様の方法で、ネットワークに計算を追加し、この計算をノード単位で行うことを始めたいと考えています。"
  },
  {
    "start": 5116236,
    "end": 5126720,
    "text": "私はすでにこれを導入しており、コミュニケーションを行う自己多頭型のセルフアテンションシステムを導入する前に、私が何かを追加したり変更したりしたときに、この左側に表示される差分を確認することができるのです。"
  },
  {
    "start": 5127280,
    "end": 5130059,
    "text": "ロジットを計算するのが早すぎたんです。"
  },
  {
    "start": 5130059,
    "end": 5137775,
    "text": "トークンは互いに顔を見合わせたが、他のトークンから何を発見したのか、考える時間はあまりなかった。"
  },
  {
    "start": 5137775,
    "end": 5142134,
    "text": "今回はフィードフォワードのシングルレイヤーを実装してみました。"
  },
  {
    "start": 5142134,
    "end": 5146612,
    "text": "この小さなレイヤーは、リニアに続いてリルー・ノンリニアがあるだけです。"
  },
  {
    "start": 5146612,
    "end": 5148084,
    "text": "というのがそれです。"
  },
  {
    "start": 5148084,
    "end": 5149964,
    "text": "ちょっとしたレイヤーなんです。"
  },
  {
    "start": 5149964,
    "end": 5152818,
    "text": "フィードフォワードとエンベッドと呼んでいます。"
  },
  {
    "start": 5152818,
    "end": 5158660,
    "text": "このフィードフォワードは、セルフアテンションの直後に順次呼び出されるだけです。"
  },
  {
    "start": 5158660,
    "end": 5168560,
    "text": "フィードフォワードは、リニアを適用した場合、トークン単位で行われることがわかります。"
  },
  {
    "start": 5168560,
    "end": 5170808,
    "text": "自己アテンションは、コミュニケーションです。"
  },
  {
    "start": 5170808,
    "end": 5177780,
    "text": "すべてのデータを集めたら、そのデータをもとに個別に考える必要があります。"
  },
  {
    "start": 5177780,
    "end": 5183520,
    "text": "ここに追加しました。これをトレーニングすると、実際に検証の損失が減り続けるんです。"
  },
  {
    "start": 5183520,
    "end": 5186014,
    "text": "2.28から2.24に引き下げました。"
  },
  {
    "start": 5186014,
    "end": 5188939,
    "text": "出力されると、なんだかひどいことになる。"
  },
  {
    "start": 5188939,
    "end": 5192175,
    "text": "少なくとも改善した。"
  },
  {
    "start": 5192175,
    "end": 5199373,
    "text": "これからプレビューとして、通信と計算を交互に行うようにします。"
  },
  {
    "start": 5199373,
    "end": 5201822,
    "text": "トランスの役割です。"
  },
  {
    "start": 5201822,
    "end": 5210585,
    "text": "通信するブロックと計算するブロックがあり、それらをグループ化し、複製するのです。"
  },
  {
    "start": 5210585,
    "end": 5212490,
    "text": "というようなことをやりたいと思います。"
  },
  {
    "start": 5212490,
    "end": 5226087,
    "text": "このブロックは基本的に、クロスアテンションを除いたこの部分です。このブロックは基本的に、コミュニケーションと計算を散りばめています。"
  },
  {
    "start": 5226087,
    "end": 5231914,
    "text": "すべてのトークンに独立したフィードフォワードネットワークを用いて計算を行う。"
  },
  {
    "start": 5231914,
    "end": 5243846,
    "text": "ここで追加したのは、埋め込み次元の埋め込み数と、グループ畳み込みのグループサイズのような頭数を取ることです。"
  },
  {
    "start": 5243846,
    "end": 5246865,
    "text": "私たちが欲しいのは4つの頭だと言っているんです。"
  },
  {
    "start": 5246865,
    "end": 5252786,
    "text": "これが32だから、頭の数は4であるべきだと計算されるのです。"
  },
  {
    "start": 5252786,
    "end": 5254636,
    "text": "ヘッドサイズは8インチであるべきです。"
  },
  {
    "start": 5254636,
    "end": 5258112,
    "text": "チャンネル的にはすべてうまくいっていると思います。"
  },
  {
    "start": 5258112,
    "end": 5263901,
    "text": "このように、トランスの構造は、一般的な大きさを分類しています。"
  },
  {
    "start": 5263901,
    "end": 5265775,
    "text": "ヘッドサイズは8となります。"
  },
  {
    "start": 5265775,
    "end": 5268066,
    "text": "このように挟み込みたい。"
  },
  {
    "start": 5268066,
    "end": 5277962,
    "text": "ブロックを作ることにしました。これは、ブロック・ブロック・ブロックの連続的な応用で、通信のフィードフォワードを何度も挟み込むことになります。"
  },
  {
    "start": 5277962,
    "end": 5281424,
    "text": "最後にデコードして、実際に実行してみます。"
  },
  {
    "start": 5281424,
    "end": 5293306,
    "text": "その理由は、かなり深いニューラルネットのようなものを作り始めていて、深いニューラルネットは最適化の問題に悩まされているからです。"
  },
  {
    "start": 5293306,
    "end": 5296967,
    "text": "少しづつ気付き始めているような気がします。"
  },
  {
    "start": 5296967,
    "end": 5309709,
    "text": "このような問題を解決するために、トランスフォーマーの論文からもう1つアイデアを借りる必要があります。これは、ネットワークの深さを劇的に改善し、ネットワークが最適化可能であることを確認するための2つの最適化です。"
  },
  {
    "start": 5309709,
    "end": 5321072,
    "text": "この図の最初のものは、この矢印、そしてこの矢印、この矢印がスキップ接続、または残留接続と呼ばれるものです。"
  },
  {
    "start": 5321072,
    "end": 5328960,
    "text": "このコンセプトを紹介した2015年頃の論文「the procedural learning for image recognition」から来ているそうです。"
  },
  {
    "start": 5329920,
    "end": 5338645,
    "text": "これらは、基本的にはデータを変換するのですが、前の機能から追加でスキップ接続を行うという意味です。"
  },
  {
    "start": 5338645,
    "end": 5347093,
    "text": "私が好む視覚化の方法は、次のようなものです。ここでは、計算が上から下へ行われます。"
  },
  {
    "start": 5347093,
    "end": 5350000,
    "text": "このような経路は、基本的に残されています。"
  },
  {
    "start": 5350000,
    "end": 5353150,
    "text": "残っている経路から自由に分岐することができます。"
  },
  {
    "start": 5353150,
    "end": 5358233,
    "text": "計算し、加算によって残差経路に投影する。"
  },
  {
    "start": 5358233,
    "end": 5364113,
    "text": "入力からターゲットまで、プラスとプラス、プラスとプラスを経由するだけなのです。"
  },
  {
    "start": 5364113,
    "end": 5389695,
    "text": "なぜこれが便利かというと、先ほどのマイクログラッドのビデオで思い出したのですが、ドット伝搬の際、加算は入力として供給される分岐の両方に均等に勾配を分配するので、損失からの監督や勾配は基本的にすべての加算ノードを通って入力まで飛び、さらに残余ブロックに分岐していきます。"
  },
  {
    "start": 5389695,
    "end": 5397340,
    "text": "この勾配のあるスーパーハイウェイは、監督から入力に至るまで、邪魔されることなく直進するのです。"
  },
  {
    "start": 5397340,
    "end": 5401049,
    "text": "これらの残差ブロックは、通常、最初に初期化されます。"
  },
  {
    "start": 5401049,
    "end": 5406284,
    "text": "そのように初期化された場合、残留経路への寄与はほとんどない。"
  },
  {
    "start": 5406284,
    "end": 5410099,
    "text": "最初のうちは、ほとんど存在しないようなものです。"
  },
  {
    "start": 5410099,
    "end": 5415830,
    "text": "最適化の過程でオンライン化され、貢献するようになるのです。"
  },
  {
    "start": 5415830,
    "end": 5425631,
    "text": "少なくとも初期化では、直接監視している状態から、入力勾配が阻害されず、ただ流れていくだけで、時間の経過とともにブロックが効いてくるということがある。"
  },
  {
    "start": 5425631,
    "end": 5428620,
    "text": "最適化に劇的に貢献します。"
  },
  {
    "start": 5428620,
    "end": 5429880,
    "text": "実装してみましょう。"
  },
  {
    "start": 5429880,
    "end": 5440066,
    "text": "このブロックに戻ると、基本的には、「xイコールx＋自己注意」と「xイコールx＋self.feedforward」を行いたいのです。"
  },
  {
    "start": 5440066,
    "end": 5448801,
    "text": "xで、その後フォークオフして通信を行い、戻ってくる。"
  },
  {
    "start": 5448801,
    "end": 5457096,
    "text": "このプロジェクションを導入する必要があるのです。"
  },
  {
    "start": 5457096,
    "end": 5464137,
    "text": "nn.linearで、これを連結した後のサイズと埋め込みになります。"
  },
  {
    "start": 5464137,
    "end": 5467740,
    "text": "自己アテンションそのもののアウトプットです。"
  },
  {
    "start": 5467740,
    "end": 5472960,
    "text": "で、実際に投影を適用させ、その結果を得る。"
  },
  {
    "start": 5474160,
    "end": 5477120,
    "text": "投影は、このレイヤーの結果を線形変換したものに過ぎません。"
  },
  {
    "start": 5478720,
    "end": 5481527,
    "text": "残像経路への投影です。"
  },
  {
    "start": 5481527,
    "end": 5484892,
    "text": "フィードフォワードでは同じことになります。"
  },
  {
    "start": 5484892,
    "end": 5487881,
    "text": "ここでもself.projectionができるかもしれませんね。"
  },
  {
    "start": 5487881,
    "end": 5493635,
    "text": "これを単純化して、同じ連続したコンテナの中で結合させてみます。"
  },
  {
    "start": 5493635,
    "end": 5498844,
    "text": "これは投影層が残存経路に戻るということですね。"
  },
  {
    "start": 5498844,
    "end": 5501059,
    "text": "まあそれはそれとして。"
  },
  {
    "start": 5501059,
    "end": 5502903,
    "text": "今なら、これを鍛えることができる。"
  },
  {
    "start": 5502903,
    "end": 5505709,
    "text": "もう1つ、小さな変更を加えました。"
  },
  {
    "start": 5505709,
    "end": 5511815,
    "text": "見ると、入力と出力の次元が512であることがわかります。"
  },
  {
    "start": 5511815,
    "end": 5516912,
    "text": "フィードフォワードの内層は2048の次元を持つということです。"
  },
  {
    "start": 5516912,
    "end": 5518784,
    "text": "4倍の倍率があります。"
  },
  {
    "start": 5518784,
    "end": 5524797,
    "text": "フィードフォワードネットワークの内層は、チャンネルサイズに換算して4倍する必要があります。"
  },
  {
    "start": 5524797,
    "end": 5529118,
    "text": "私はここに来て、フィードフォワードのためにここに4回埋め込むと、私は乗算しました。"
  },
  {
    "start": 5529118,
    "end": 5535105,
    "text": "4回のネムベットから、プロジェクトに戻るとネムベットに戻ってきます。"
  },
  {
    "start": 5535105,
    "end": 5542606,
    "text": "ここで少し計算を追加し、残留経路の側で残留ブロックにあるその層を成長させます。"
  },
  {
    "start": 5542606,
    "end": 5552853,
    "text": "また、ネットワークが十分に大きくなってきているため、訓練による損失が検証による損失よりも大きくなってきていることがわかります。"
  },
  {
    "start": 5552853,
    "end": 5561275,
    "text": "その結果、少しオーバーフィッティングが見られるようになり、この世代はまだ素晴らしいとは言えません。"
  },
  {
    "start": 5561275,
    "end": 5576504,
    "text": "2つ目のイノベーションは、非常に深いニューラルネットワークを最適化するのに非常に役立つものです。"
  },
  {
    "start": 5576504,
    "end": 5579000,
    "text": "追加したのですが、これが残留部分です。"
  },
  {
    "start": 5579000,
    "end": 5581599,
    "text": "このノルムはレイヤーノルムと呼ばれるものを指しています。"
  },
  {
    "start": 5582400,
    "end": 5584763,
    "text": "レイヤーノームはpytorchで実装されています。"
  },
  {
    "start": 5584763,
    "end": 5592939,
    "text": "レイヤーのノルムはバッシュのノルムと非常によく似ているんです。"
  },
  {
    "start": 5592939,
    "end": 5604472,
    "text": "Make Moreシリーズ第3回では、Bashの正規化を実施しました。Bashの正規化では、基本的にバッチ次元全体で確認することができます。"
  },
  {
    "start": 5604472,
    "end": 5609695,
    "text": "個々のニューロンは単位ガウス分布を持っていた。"
  },
  {
    "start": 5609695,
    "end": 5621350,
    "text": "平均値ゼロ、単位標準偏差1標準偏差の出力でした。そこで、make moreシリーズで開発したbash norm 1dをコピーペーストしてみました。"
  },
  {
    "start": 5621350,
    "end": 5629873,
    "text": "ここでは、例えばこのモジュールを初期化し、32個の100次元ベクトルを一括してbashのノルム層に送り込むことができます。"
  },
  {
    "start": 5629873,
    "end": 5639108,
    "text": "これは、0番目の列だけを見たときに、平均値1標準偏差がゼロであることを保証するものです。"
  },
  {
    "start": 5639108,
    "end": 5642880,
    "text": "この入力のすべての列を正規化しています。"
  },
  {
    "start": 5643840,
    "end": 5648880,
    "text": "列を正規化するだけなので、デフォルトでは行は正規化されないからです。"
  },
  {
    "start": 5649600,
    "end": 5652012,
    "text": "レイヤーノームを実装しないようにしましょう。"
  },
  {
    "start": 5652012,
    "end": 5657076,
    "text": "非常に複雑なのですが、ここに来て、これを0から1に変えるのです。"
  },
  {
    "start": 5657076,
    "end": 5661595,
    "text": "列を正規化するのではなく、行を正規化するのです。"
  },
  {
    "start": 5661595,
    "end": 5664892,
    "text": "レイヤーノームを実装しました。"
  },
  {
    "start": 5664892,
    "end": 5673781,
    "text": "このとき、列は正規化されませんが、行は個々の例で正規化されます。"
  },
  {
    "start": 5673781,
    "end": 5676560,
    "text": "その100次元のベクトルは、このように正規化される。"
  },
  {
    "start": 5677200,
    "end": 5681546,
    "text": "今の計算では例題をまたがないからです。"
  },
  {
    "start": 5681546,
    "end": 5690000,
    "text": "この操作は常に適用することができ、実行中のバッファを維持する必要がないためです。"
  },
  {
    "start": 5690560,
    "end": 5693261,
    "text": "バッファーは必要ない、必要ない。"
  },
  {
    "start": 5693261,
    "end": 5701724,
    "text": "トレーニングタイムとテストタイムの区別がないので、このようなランニングバッファは必要ありません。"
  },
  {
    "start": 5701724,
    "end": 5703184,
    "text": "ガンマとベータは残しています。"
  },
  {
    "start": 5703184,
    "end": 5707946,
    "text": "勢いはいらない、トレーニングでもなんでもいい。"
  },
  {
    "start": 5707946,
    "end": 5710340,
    "text": "もはやレイヤーの規範と言えるでしょう。"
  },
  {
    "start": 5710340,
    "end": 5718862,
    "text": "列ではなく行を正規化するので、ここは基本的にここと同じになります。"
  },
  {
    "start": 5718862,
    "end": 5724160,
    "text": "レイヤーノルムを組み込む前に、トランスフォーマーにレイヤーノルムを実装してみましょう。"
  },
  {
    "start": 5724160,
    "end": 5729118,
    "text": "ただ、この5年間、トランスのディテールはほとんど変わっていないことをお伝えしておきます。"
  },
  {
    "start": 5729118,
    "end": 5732933,
    "text": "実は原著論文から少し外れたところにあるのです。"
  },
  {
    "start": 5732933,
    "end": 5737559,
    "text": "変換後に加算とノルムが適用されることがわかります。"
  },
  {
    "start": 5737559,
    "end": 5744468,
    "text": "現在では、レイヤーノルムを変形前に適用するのが一般的です。"
  },
  {
    "start": 5744468,
    "end": 5746667,
    "text": "レイヤーの規範の入れ替えがあるんです。"
  },
  {
    "start": 5746667,
    "end": 5750679,
    "text": "プレノルム定式化と呼ばれるもので、私たちもこれを実施することにしています。"
  },
  {
    "start": 5750679,
    "end": 5753258,
    "text": "原紙と若干のずれがあります。"
  },
  {
    "start": 5753258,
    "end": 5756868,
    "text": "基本的には、ノルムをノルムを重ねる必要があります。"
  },
  {
    "start": 5756868,
    "end": 5758818,
    "text": "nnドットレイヤーノルムです。"
  },
  {
    "start": 5758818,
    "end": 5769283,
    "text": "埋め込み次元がいくつで、2層目のノルムが必要であることを伝え、ここでは層ノルムがxに即座に適用されます。"
  },
  {
    "start": 5769283,
    "end": 5778343,
    "text": "自己注目とフィードフォワードに入る前に、xにself dot layer norm 1を適用し、xにself dot layer norm 2を適用します。"
  },
  {
    "start": 5778343,
    "end": 5782437,
    "text": "このレイヤーのノルムの大きさは、エンベデッドなので32です。"
  },
  {
    "start": 5782437,
    "end": 5786659,
    "text": "レイヤーノルムが特徴量を正規化するとき。"
  },
  {
    "start": 5786659,
    "end": 5793680,
    "text": "ここでいう正規化とは、平均と分散を32個の数値で取ることです。"
  },
  {
    "start": 5793680,
    "end": 5797760,
    "text": "バッチとタイムの両方がバッチの次元として機能します。"
  },
  {
    "start": 5797760,
    "end": 5808520,
    "text": "これは、トークンごとの変換のようなもので、初期化時に特徴量を正規化し、単位平均値の単位ガウスにするだけです。"
  },
  {
    "start": 5808520,
    "end": 5815072,
    "text": "もちろん、その中のレイヤー規範は、ガンマとベータという学習可能なパラメータを持っているからです。"
  },
  {
    "start": 5815072,
    "end": 5820516,
    "text": "レイヤーノルムは、最終的に単位ガウスでない出力を作成する可能性があります。"
  },
  {
    "start": 5820516,
    "end": 5823149,
    "text": "最適化によって決定されます。"
  },
  {
    "start": 5823149,
    "end": 5828585,
    "text": "とりあえずこれはレイヤーの規範を取り入れたもので、それを鍛え上げましょう。"
  },
  {
    "start": 5828585,
    "end": 5833631,
    "text": "その結果、2.06となり、前回の2.08より良くなりました。"
  },
  {
    "start": 5833631,
    "end": 5836387,
    "text": "レイヤーノルムを追加することで、若干の改善が見られます。"
  },
  {
    "start": 5836387,
    "end": 5840240,
    "text": "もっと大きな、もっと深いネットワークがあれば、もっと助かると思うのですが。"
  },
  {
    "start": 5840960,
    "end": 5852639,
    "text": "もう一つ付け加えるのを忘れていたのですが、レイヤーノームがここにもあるはずで、一般的にはトランスフォーマーの最後、ボキャブラリーにデコードする最終リニアレイヤーの直前です。"
  },
  {
    "start": 5852639,
    "end": 5853903,
    "text": "も追加しました。"
  },
  {
    "start": 5853903,
    "end": 5860927,
    "text": "現段階では、原著論文によると、かなり完成度の高いトランスが完成しており、デコーダのみのトランスとなっています。"
  },
  {
    "start": 5860927,
    "end": 5862559,
    "text": "それについては、また後日お話します。"
  },
  {
    "start": 5863200,
    "end": 5865481,
    "text": "この段階で主要なピースが揃ったことになります。"
  },
  {
    "start": 5865481,
    "end": 5873796,
    "text": "スケールアップして、この数字をどの程度押し出せるか試してみましょう。このモデルをスケールアップするために、私はここでいくつかの外観上の変更を行い、より美しくしました。"
  },
  {
    "start": 5873796,
    "end": 5879139,
    "text": "nlayerという変数を導入し、ブロックの層数を指定するようにしました。"
  },
  {
    "start": 5879139,
    "end": 5884840,
    "text": "ブロックの束を作り、頭の数も新たに変数として用意します。"
  },
  {
    "start": 5884840,
    "end": 5887054,
    "text": "ここでレイヤーノルムを抜いた。"
  },
  {
    "start": 5887054,
    "end": 5892602,
    "text": "これは同じですが、1つだけ簡単に変更したのは、ドロップアウトを追加したことです。"
  },
  {
    "start": 5892602,
    "end": 5900880,
    "text": "ドロップアウトは、残留経路に戻る接続の直前に追加できるものです。"
  },
  {
    "start": 5901519,
    "end": 5904479,
    "text": "最後のレイヤーとしてドロップアウトすることができます。"
  },
  {
    "start": 5904479,
    "end": 5909330,
    "text": "多頭式エクステンションの最後にも、ここでドロップアウトすることができます。"
  },
  {
    "start": 5909330,
    "end": 5916511,
    "text": "基本的に親和性を計算するときやソフトマックスの後に、ここでドロップアウトすることも可能です。"
  },
  {
    "start": 5916511,
    "end": 5917999,
    "text": "そのうちのいくつかをドロップアウトすることができます。"
  },
  {
    "start": 5917999,
    "end": 5927402,
    "text": "一部のノードの通信をランダムに遮断することができますので、ドロップアウトは2014年頃のこの論文からきています。"
  },
  {
    "start": 5927402,
    "end": 5937040,
    "text": "基本的には、ニューラルネットを使用し、前方後方の通過ごとにランダムにニューロンの一部を遮断します。"
  },
  {
    "start": 5937840,
    "end": 5941694,
    "text": "ランダムにゼロにし、それなしでトレーニングします。"
  },
  {
    "start": 5941694,
    "end": 5948531,
    "text": "ドロップアウトされるもののマスクが、フォワード・バックワード・パスのたびに変化しているからです。"
  },
  {
    "start": 5948531,
    "end": 5951763,
    "text": "その結果、サブネットワークのアンサンブルをトレーニングするような形になります。"
  },
  {
    "start": 5951763,
    "end": 5954839,
    "text": "で、テスト時にはすべてが完全に有効になっています。"
  },
  {
    "start": 5954839,
    "end": 5958178,
    "text": "ような、すべてのサブネットワークが1つのアンサンブルに統合されます。"
  },
  {
    "start": 5958178,
    "end": 5960490,
    "text": "できれば、そのように考えたいのであれば。"
  },
  {
    "start": 5960490,
    "end": 5967311,
    "text": "詳しくは論文を読んでいただきたいのですが、ここでは正則化の手法というレベルにとどめておきます。"
  },
  {
    "start": 5967311,
    "end": 5973034,
    "text": "追加したのは、これからモデルの規模を大きくしていくので、オーバーフィッティングが気になったからです。"
  },
  {
    "start": 5973034,
    "end": 5979680,
    "text": "一番上までスクロールすると、ニューラルネットのハイパーパラメータをいくつか変更したことがわかります。"
  },
  {
    "start": 5979680,
    "end": 5983101,
    "text": "バッチサイズをもっと大きくして、64にしたんです。"
  },
  {
    "start": 5983101,
    "end": 5988510,
    "text": "ブロックサイズを256に変更したため、以前は8文字のコンテキストしかありませんでしたが、現在は8文字のコンテキストがあります。"
  },
  {
    "start": 5988510,
    "end": 5992098,
    "text": "257回目を予測するために、256文字の文脈を持つようになりました。"
  },
  {
    "start": 5992098,
    "end": 5999887,
    "text": "ニューラルネットが大きくなったので、学習率を少し下げました。"
  },
  {
    "start": 5999887,
    "end": 6001927,
    "text": "埋め込み次元は384となります。"
  },
  {
    "start": 6001927,
    "end": 6010385,
    "text": "ヘッドが6つあるので、384を6で割ると、すべてのヘッドが64次元の規格となります。"
  },
  {
    "start": 6010385,
    "end": 6015380,
    "text": "その層は6層となり、ドロップアウトは0.2となります。"
  },
  {
    "start": 6015380,
    "end": 6023425,
    "text": "前方後方へのパスごとに、これらすべての中間計算の20％が無効化され、ゼロになる。"
  },
  {
    "start": 6023425,
    "end": 6028791,
    "text": "すでにトレーニングして走らせたのですが、その性能はいかがでしょうか？"
  },
  {
    "start": 6028791,
    "end": 6038934,
    "text": "検証の結果、損失は1.48となり、以前の2.07からかなり改善されました。"
  },
  {
    "start": 6038934,
    "end": 6045077,
    "text": "このニューラルネットを今あるコードでスケールアップするだけで、2.07から1.48まで下げることができたのです。"
  },
  {
    "start": 6045077,
    "end": 6047178,
    "text": "もちろん、これはもっと長い間続いた。"
  },
  {
    "start": 6047178,
    "end": 6051153,
    "text": "これは、私のA100 gpuで約15分と言いたいために訓練されるかもしれません。"
  },
  {
    "start": 6051153,
    "end": 6052872,
    "text": "かなり良いGpuですね。"
  },
  {
    "start": 6052872,
    "end": 6054542,
    "text": "gpuを持っていない場合。"
  },
  {
    "start": 6054542,
    "end": 6057911,
    "text": "cpuで再現するのは無理でしょう。"
  },
  {
    "start": 6057911,
    "end": 6059760,
    "text": "ということになり、実行に移せません。"
  },
  {
    "start": 6059920,
    "end": 6062593,
    "text": "私は、これをcpuやmacbookなどで実行することはありません。"
  },
  {
    "start": 6062593,
    "end": 6067502,
    "text": "層数とか埋込寸法とか、そういうのを分解して考える必要がある。"
  },
  {
    "start": 6067502,
    "end": 6074615,
    "text": "15分ほどでこのような結果を得ることができます。"
  },
  {
    "start": 6074615,
    "end": 6081840,
    "text": "さらに、10,000文字をプリントして、それをファイルに書き出しました。"
  },
  {
    "start": 6084240,
    "end": 6087526,
    "text": "入力テキストファイルとして認識されますね。"
  },
  {
    "start": 6087526,
    "end": 6091707,
    "text": "参考までに入力テキストファイルはこのような感じでした。"
  },
  {
    "start": 6091707,
    "end": 6107439,
    "text": "私たちの予測もそのような形になっていますが、もちろん実際に読んでみると無意味なものです。"
  },
  {
    "start": 6107439,
    "end": 6122398,
    "text": "その言葉に我々は注意を払う、ああ、その試練。えっと、あなたは知っています、ああ、主よ、あなたが私を送りました。まあ、とにかく、あなたはこれを読むことができます。えっと、もちろん、それは意味不明です。"
  },
  {
    "start": 6122398,
    "end": 6128080,
    "text": "これは、シェイクスピアから生まれる100万文字のキャラクターレベルで訓練されたトランスフォーマーに過ぎません。"
  },
  {
    "start": 6128080,
    "end": 6138080,
    "text": "しかし、この規模ではもちろん意味がありませんが、可能性を示すにはかなり良いデモンストレーションだと思います。"
  },
  {
    "start": 6138080,
    "end": 6144189,
    "text": "これで、このビデオのプログラミング編は終了です。"
  },
  {
    "start": 6144189,
    "end": 6149834,
    "text": "このトランスフォーマーの実装は、基本的にかなりうまくいったと思います。"
  },
  {
    "start": 6149834,
    "end": 6156428,
    "text": "写真と実際にやったことが一致しないので、この追加パーツはどうなっているのだろう？"
  },
  {
    "start": 6156428,
    "end": 6165091,
    "text": "このアーキテクチャの説明を終わりますが、なぜこのように奇妙に見えるのか、基本的にここで起こっていることは、デコーダのみのトランスフォーマーを実装していることです。"
  },
  {
    "start": 6165091,
    "end": 6166832,
    "text": "ここにはコンポーネントがない。"
  },
  {
    "start": 6166832,
    "end": 6168909,
    "text": "この部分をエンコーダーと呼びます。"
  },
  {
    "start": 6168909,
    "end": 6171710,
    "text": "ここにはクロスアテンションブロックはありません。"
  },
  {
    "start": 6171710,
    "end": 6175359,
    "text": "私たちのブロックは、自戒とフィードフォワードしかありません。"
  },
  {
    "start": 6175359,
    "end": 6178516,
    "text": "この3つ目の中間の部分が欠けているのです。"
  },
  {
    "start": 6178516,
    "end": 6180453,
    "text": "この作品は、クロスアテンションを行います。"
  },
  {
    "start": 6180453,
    "end": 6182936,
    "text": "エンコーダーもない。"
  },
  {
    "start": 6182936,
    "end": 6194456,
    "text": "デコーダーを搭載しているだけなのですが、デコーダーだけを搭載しているのは、テキストを生成するだけなので、何も条件付けせずにデコーダーだけを使えるようにしたいからです。"
  },
  {
    "start": 6194456,
    "end": 6197923,
    "text": "私たちはただ、与えられたデータセットに従ってベラベラしゃべっているだけなのです。"
  },
  {
    "start": 6197923,
    "end": 6202869,
    "text": "デコーダである理由は、トランスフォーマーに三角形のマスクを使用していることです。"
  },
  {
    "start": 6202869,
    "end": 6207805,
    "text": "自己回帰的な性質があり、そこからサンプリングすることができるのです。"
  },
  {
    "start": 6207805,
    "end": 6216376,
    "text": "三角形のトライアングルマスクを使ってアテンションをマスクしているので、デコーダーとなり、言語モデリングに利用できるのです。"
  },
  {
    "start": 6216376,
    "end": 6223052,
    "text": "エンコーダ・デコーダのアーキテクチャを採用しているのは、機械翻訳の論文であるためです。"
  },
  {
    "start": 6223052,
    "end": 6225905,
    "text": "別の設定に関係しています。"
  },
  {
    "start": 6225905,
    "end": 6235843,
    "text": "特に、例えばフランス語をエンコードするいくつかのトークンを期待し、次に英語での翻訳をデコードすることを期待するものである。"
  },
  {
    "start": 6235843,
    "end": 6239054,
    "text": "一般的には、ここにあるのは特別なトークンなんですね。"
  },
  {
    "start": 6239054,
    "end": 6247188,
    "text": "これを読み込んで条件をつけることが期待され、スタートという特別なトークンを使って世代をスタートさせるのです。"
  },
  {
    "start": 6247188,
    "end": 6252365,
    "text": "紹介し、必ず冒頭に置く特別な新しいトークンです。"
  },
  {
    "start": 6252365,
    "end": 6261171,
    "text": "出力することが期待され、その後、特別なエンドトークンを使って生成を終了します。"
  },
  {
    "start": 6261171,
    "end": 6268160,
    "text": "この部分は、私たちがやったのと同じようにデコードされます。 ニューラルネットワークは素晴らしいので、私たちがやったことと同じになります。"
  },
  {
    "start": 6268880,
    "end": 6279599,
    "text": "私たちが行ったのとは異なり、彼らは何らかの追加情報を条件として生成することを望んでおり、その場合、この追加情報こそが翻訳すべきフランス語の文章なのです。"
  },
  {
    "start": 6280559,
    "end": 6284265,
    "text": "今はエンコーダーを持参しています。"
  },
  {
    "start": 6284265,
    "end": 6287370,
    "text": "今、エンコーダーはこの部分をここで読みます。"
  },
  {
    "start": 6287370,
    "end": 6296578,
    "text": "部分だけを取り出して、ビデオで見たのと全く同じようにトークンを作り、それにトランスフォーマーをつけるのです、"
  },
  {
    "start": 6296578,
    "end": 6302639,
    "text": "三角形のマスクがないので、すべてのトークンが好きなだけ会話できる。"
  },
  {
    "start": 6302639,
    "end": 6307616,
    "text": "このフランス語の文の内容をエンコードしているだけなのです。"
  },
  {
    "start": 6307616,
    "end": 6313124,
    "text": "エンコードした後は、基本的にここで上位に来ることになります。"
  },
  {
    "start": 6313124,
    "end": 6317952,
    "text": "ここで起こるのは、言語モデリングを行うデコーダでの処理です。"
  },
  {
    "start": 6317952,
    "end": 6326508,
    "text": "エンコーダーの出力には、ここに追加の接続があり、それはクロスアテンションを介してもたらされます。"
  },
  {
    "start": 6326508,
    "end": 6329195,
    "text": "クエリはやはりxから生成される。"
  },
  {
    "start": 6329195,
    "end": 6332038,
    "text": "今、キーとバリューはサイドから来るんだ。"
  },
  {
    "start": 6332038,
    "end": 6339237,
    "text": "キーと値は、エンコーダーのデコードの外側に来たノードによって生成されたトップから来る。"
  },
  {
    "start": 6339237,
    "end": 6350133,
    "text": "このキーと値は、デコーダーの各ブロックに横から入力されるため、クロスアテンションが追加されています。"
  },
  {
    "start": 6350133,
    "end": 6366019,
    "text": "つまり、デコードの条件として、現在のデコードの過去だけでなく、完全にエンコードされたフランス語のプロンプトのようなものを見たことがあることを条件としているのです。"
  },
  {
    "start": 6366019,
    "end": 6373209,
    "text": "そのため、2つのトランスフォーマーや追加のブロックなどを用意しています。"
  },
  {
    "start": 6373209,
    "end": 6374349,
    "text": "コンディショニングがないんです。"
  },
  {
    "start": 6374349,
    "end": 6376951,
    "text": "テキストファイルがあるので、それを模倣するだけです。"
  },
  {
    "start": 6376951,
    "end": 6385288,
    "text": "そのため、gptと同じようにデコーダのみのトランスフォーマーを使用しています。さて、今回はnano gptの簡単なウォークスルーをしたいと思います、"
  },
  {
    "start": 6385288,
    "end": 6390785,
    "text": "私のgithubにあるnano gptは、基本的に2つのファイルが対象です。"
  },
  {
    "start": 6390785,
    "end": 6397077,
    "text": "train.pyとmodel.pyがあります。train.pyは、ネットワークをトレーニングするための定型的なコードです。"
  },
  {
    "start": 6397077,
    "end": 6400770,
    "text": "基本的にここにあったものはすべてトレーニングループです。"
  },
  {
    "start": 6400770,
    "end": 6406996,
    "text": "ただ、チェックポイントや事前に学習させた重みを保存したり読み込んだりしているので、より複雑になっています。"
  },
  {
    "start": 6406996,
    "end": 6413569,
    "text": "学習速度を低下させ、モデルをコンパイルし、複数のノードやGpusに分散して学習させる方法を採用しています。"
  },
  {
    "start": 6413569,
    "end": 6417405,
    "text": "training.pyは少し毛むくじゃらで複雑になります。"
  },
  {
    "start": 6417405,
    "end": 6427602,
    "text": "しかし、model.pyはここで行ったものと非常によく似ているはずで、実際、モデルはほとんど同じです。"
  },
  {
    "start": 6427602,
    "end": 6430735,
    "text": "まずここで、因果関係のある自意識のブロックがあります。"
  },
  {
    "start": 6430735,
    "end": 6433440,
    "text": "ご覧いただければ、一目瞭然でしょう。"
  },
  {
    "start": 6433440,
    "end": 6435936,
    "text": "クエリ・キー・バリューを生成しています。"
  },
  {
    "start": 6435936,
    "end": 6437652,
    "text": "ドットプロダクトをやっています。"
  },
  {
    "start": 6437652,
    "end": 6441519,
    "text": "ソフトマックスを適用したマスキングで、オプションでドロップアウトしています。"
  },
  {
    "start": 6441519,
    "end": 6443839,
    "text": "ここでは、値をプールしています。"
  },
  {
    "start": 6443839,
    "end": 6446696,
    "text": "ここが違うのは、私たちのコードでは、このようになります。"
  },
  {
    "start": 6446696,
    "end": 6458168,
    "text": "複数のヘッドを持つアテンションを1つのヘッドに分離し、ここでは複数のヘッドを持ち、それらを明示的に連結しています。"
  },
  {
    "start": 6458168,
    "end": 6463545,
    "text": "しかし、ここではそのすべてが一つの因果的な自己主張の中でバッチ式に実装されています。"
  },
  {
    "start": 6463545,
    "end": 6466150,
    "text": "そのため、BとTとCの次元だけではありません。"
  },
  {
    "start": 6466150,
    "end": 6475217,
    "text": "また、4次元の配列テンソルがあるため、4次元の頭部を持つことになり、さらに面倒なことになります。"
  },
  {
    "start": 6475217,
    "end": 6480719,
    "text": "数学的には等価なので、私たちが持っているものと全く同じことが起こっているのです。"
  },
  {
    "start": 6480719,
    "end": 6487886,
    "text": "多層パーセプトロンでは、すべてのヘッドを一括して1つの次元として扱うので、より効率的です。"
  },
  {
    "start": 6487886,
    "end": 6492638,
    "text": "reluの代わりに、ここで定義されているgelu non-linearityを使用しています。"
  },
  {
    "start": 6492638,
    "end": 6494731,
    "text": "これは、openaiが使ったからという理由だけでやっています。"
  },
  {
    "start": 6494731,
    "end": 6504625,
    "text": "チェックポイントをロードできるようにしたいのですが、トランスフォーマーのブロックは、通信と計算のフェーズが同じで、その後、gptは同じになります。"
  },
  {
    "start": 6504625,
    "end": 6511593,
    "text": "位置エンコーディング トークン・エンコーディング ブロック レイヤー・ノルム 最終的な線形レイヤー"
  },
  {
    "start": 6511593,
    "end": 6518602,
    "text": "チェックポイントを読み込んだりするので、もう少し多くなっています、"
  },
  {
    "start": 6518602,
    "end": 6528303,
    "text": "パラメータを減衰させるべきものとそうでないものに分けていますが、生成関数も非常によく似ているはずなので、細部は少し違っていますね。"
  },
  {
    "start": 6528303,
    "end": 6536258,
    "text": "このファイルを見て、多くの部分を理解できるようになったはずです。それでは、チャットGptに話を戻しましょう。"
  },
  {
    "start": 6536258,
    "end": 6542004,
    "text": "もし、自分たちがチャットGTPのトレーニングをしたいと思ったら、どのような形になるのか、また、今日学んだこととどのような関係があるのか。"
  },
  {
    "start": 6542004,
    "end": 6545114,
    "text": "チャットgptを鍛えるには、大きく分けて2つの段階があります。"
  },
  {
    "start": 6545114,
    "end": 6550669,
    "text": "プレトレーニングステージ、そしてプレトレーニングステージでのファインチューニングステージです。"
  },
  {
    "start": 6550669,
    "end": 6558209,
    "text": "私たちは、インターネットの大きな塊で訓練しており、最初のデコーダのみのトランスフォーマーをバブリングテキストにすることだけを目指しています。"
  },
  {
    "start": 6558209,
    "end": 6562099,
    "text": "私たちがやってきたことと非常によく似ているんです。"
  },
  {
    "start": 6562099,
    "end": 6566851,
    "text": "除いては、小さな小さな赤ちゃんの事前トレーニングのようなステップを踏んでいます。"
  },
  {
    "start": 6566851,
    "end": 6581593,
    "text": "つまり、私たちの場合、このようにしてパラメーターの数をプリントしてみると、約1000万個になりました。"
  },
  {
    "start": 6581593,
    "end": 6586039,
    "text": "データセットは約100万文字なので、約100万トークンになります。"
  },
  {
    "start": 6586039,
    "end": 6589889,
    "text": "目を開けるというのは、語彙が違うということを忘れてはいけない。"
  },
  {
    "start": 6589889,
    "end": 6592000,
    "text": "キャラクターレベルではありません。"
  },
  {
    "start": 6592000,
    "end": 6597619,
    "text": "そのため、彼らはおよそ5万語の語彙を有しています。"
  },
  {
    "start": 6597619,
    "end": 6624744,
    "text": "シェイクスピアのデータセットは、オープンAIのボキャブラリーでおよそ30万トークンなので、およそ30万トークンに対して約1000万個の一次モデルを学習させました。"
  },
  {
    "start": 6624744,
    "end": 6629920,
    "text": "ここで一番大きなトランスは1750億のパラメータを持っているので、私たちのものはまた1000万なのです。"
  },
  {
    "start": 6629920,
    "end": 6633040,
    "text": "トランスにこの層数を採用した。"
  },
  {
    "start": 6633040,
    "end": 6634489,
    "text": "これがNエンベデッドです。"
  },
  {
    "start": 6634489,
    "end": 6638384,
    "text": "ヘッド数、これはヘッドサイズです。"
  },
  {
    "start": 6638384,
    "end": 6644595,
    "text": "これはバッチサイズなので、私たちの場合は65で、学習率も同様です。"
  },
  {
    "start": 6644595,
    "end": 6650250,
    "text": "このトランスフォーマーをトレーニングする際、3,000億個のトークンを使ってトレーニングしています。"
  },
  {
    "start": 6650250,
    "end": 6659280,
    "text": "この数字は、現在の基準からすると、それほど大きくはないのですが、私たちは約30万人ですから、約100万倍になります。"
  },
  {
    "start": 6659280,
    "end": 6662199,
    "text": "1兆円以上上がることになりますね。"
  },
  {
    "start": 6662199,
    "end": 6668822,
    "text": "彼らは、インターネットのかなりの部分で、かなり大きなモデルを訓練しています。"
  },
  {
    "start": 6668822,
    "end": 6671065,
    "text": "というのがプレトレーニングの段階です。"
  },
  {
    "start": 6671065,
    "end": 6678772,
    "text": "また、これらのハイパーパラメーターは、私たちが実装したものとほぼ同じです。"
  },
  {
    "start": 6678772,
    "end": 6682161,
    "text": "もちろん、これを鍛えるには膨大なインフラが必要です。"
  },
  {
    "start": 6682161,
    "end": 6688571,
    "text": "この規模のモデルを訓練するためには、通常、何千台ものGPUが互いに通信する必要があります。"
  },
  {
    "start": 6688571,
    "end": 6690770,
    "text": "あくまでプレトレーニングの段階です。"
  },
  {
    "start": 6690770,
    "end": 6693836,
    "text": "プレトレーニングの段階を終えてから、今に至ります。"
  },
  {
    "start": 6693836,
    "end": 6702376,
    "text": "質問に対して答えが返ってくるようなものはダメだし、参考にならないし、などと思っていたら、ちゃんとドキュメントコンプリートができました。"
  },
  {
    "start": 6702376,
    "end": 6706687,
    "text": "シェイクスピアは、インターネットに接続すると、「バブリーだけど、バブリーじゃない」。"
  },
  {
    "start": 6706687,
    "end": 6712067,
    "text": "恣意的なニュース記事や文書を作成し、文書を完成させようとするのは、そのために訓練されたからです。"
  },
  {
    "start": 6712067,
    "end": 6713440,
    "text": "シーケンスを完了させようとしているのです。"
  },
  {
    "start": 6714000,
    "end": 6718664,
    "text": "質問をすると、さらに質問が出る可能性があります。"
  },
  {
    "start": 6718664,
    "end": 6720666,
    "text": "質問攻めにあう。"
  },
  {
    "start": 6720666,
    "end": 6729098,
    "text": "インターネット上の学習データから、ある閉じた文書がどのような動作をするのかがわかるので、未定義の動作のようなものが出てくるのです。"
  },
  {
    "start": 6729098,
    "end": 6732824,
    "text": "基本的に2つの質問に他の質問で回答することが多いかもしれません。"
  },
  {
    "start": 6732824,
    "end": 6737112,
    "text": "あなたの質問を無視するかもしれないし、あるニュース記事を完成させようとするかもしれない。"
  },
  {
    "start": 6737112,
    "end": 6744268,
    "text": "完全に下線が引かれているので、2回目の微調整の段階で、実際にアシスタントになるように調整することになります。"
  },
  {
    "start": 6744268,
    "end": 6751520,
    "text": "openaiのチャットgptのブログでは、このステージの達成方法について少し触れています。"
  },
  {
    "start": 6752240,
    "end": 6756688,
    "text": "この段階には、基本的に3つのステップがあります。"
  },
  {
    "start": 6756688,
    "end": 6763749,
    "text": "ここでは、アシスタントが行うようなトレーニングデータを収集することにしました。"
  },
  {
    "start": 6763749,
    "end": 6768404,
    "text": "文書には、上に質問、下に答えという形式があります。"
  },
  {
    "start": 6768404,
    "end": 6772405,
    "text": "大量に保有しているが、おそらくインターネットのオーダーにはないだろう。"
  },
  {
    "start": 6772405,
    "end": 6777094,
    "text": "おそらく何千例というオーダーになるのではないでしょうか。"
  },
  {
    "start": 6777094,
    "end": 6784178,
    "text": "基本的にそのような文書にのみ焦点を当てるよう、モデルを微調整します。"
  },
  {
    "start": 6784178,
    "end": 6786200,
    "text": "徐々に揃え始めているんですね。"
  },
  {
    "start": 6786200,
    "end": 6795447,
    "text": "このような非常に大きなモデルは、微調整をする際に非常に効率的なサンプルとなります。"
  },
  {
    "start": 6795447,
    "end": 6797040,
    "text": "実は何となくわかるんです。"
  },
  {
    "start": 6797680,
    "end": 6799919,
    "text": "あくまでステップ1であり、微調整に過ぎない。"
  },
  {
    "start": 6799919,
    "end": 6804914,
    "text": "その場合、2番目のステップでモデルに応答させるという、より多くのステップを踏むことになります。"
  },
  {
    "start": 6804914,
    "end": 6813649,
    "text": "異なる評価者がさまざまな回答を見て、どちらが優れているかを順位付けし、それを報酬モデルの学習に利用する。"
  },
  {
    "start": 6813649,
    "end": 6821404,
    "text": "どのような反応が望ましいか、基本的には別のネットワークを使って予測することができます。"
  },
  {
    "start": 6821404,
    "end": 6842733,
    "text": "報酬モデルが出来上がると、ppo（政策勾配強化学習オプティマイザ）を実行して、このサンプリング政策を微調整し、チャットgptが生成する回答が、報酬モデルに従って高い報酬を獲得することが期待できるようにします。"
  },
  {
    "start": 6842733,
    "end": 6847100,
    "text": "基本的には、ここで全体の整合性をとる段階、あるいは微調整をする段階があります。"
  },
  {
    "start": 6847100,
    "end": 6855853,
    "text": "その間に複数のステップがあり、文書を完成させるモデルから、質問に答えるモデルへと変化します。"
  },
  {
    "start": 6855853,
    "end": 6858226,
    "text": "まったく別のステージのようなものです。"
  },
  {
    "start": 6858226,
    "end": 6860670,
    "text": "このデータの多くは一般には公開されていません。"
  },
  {
    "start": 6860670,
    "end": 6865828,
    "text": "openaiの内部であり、このステージを再現するのはより困難です。"
  },
  {
    "start": 6865828,
    "end": 6886890,
    "text": "チャットgptとnano gptは学習前の段階を重視しています。"
  },
  {
    "start": 6886890,
    "end": 6896670,
    "text": "小さなシェイクスピアで学習させ、賢明な結果を得ました。学習コードはすべて、およそ200行のコードです。"
  },
  {
    "start": 6896670,
    "end": 6899606,
    "text": "このコードベースを公開する予定です。"
  },
  {
    "start": 6899606,
    "end": 6904057,
    "text": "また、途中のgitログのコミットもすべて付属しています。"
  },
  {
    "start": 6904057,
    "end": 6907777,
    "text": "、このコードに加えて作り込んでいきました。"
  },
  {
    "start": 6907777,
    "end": 6910010,
    "text": "ノートを発売します。"
  },
  {
    "start": 6910010,
    "end": 6911858,
    "text": "もちろん、google collab。"
  },
  {
    "start": 6911858,
    "end": 6916332,
    "text": "このようなモデルをどのようにトレーニングするか、ご理解いただけたでしょうか？"
  },
  {
    "start": 6916332,
    "end": 6926475,
    "text": "しかし、その大きさは、数え方にもよりますが、1万倍から100万倍にもなります。"
  },
  {
    "start": 6926475,
    "end": 6934064,
    "text": "今回は、この後に続く微調整の段階については、一切触れませんでした。"
  },
  {
    "start": 6934064,
    "end": 6937211,
    "text": "言語モデリングだけでなく、何かに興味があるのなら"
  },
  {
    "start": 6937211,
    "end": 6943078,
    "text": "実際にタスクを実行したり、特定の方法で整列させたりすることを望んでいます、"
  },
  {
    "start": 6943078,
    "end": 6949387,
    "text": "とか、センチメントを検出したいとか、基本的にはいつでも、単なるドキュメントコンプリッターのようなものは必要ないのです、"
  },
  {
    "start": 6949387,
    "end": 6953521,
    "text": "そのためには、今回説明しなかった微調整の段階を踏む必要があります。"
  },
  {
    "start": 6953521,
    "end": 6954979,
    "text": "シンプルかもしれませんね。"
  },
  {
    "start": 6954979,
    "end": 6956379,
    "text": "supervised fine-tuning。"
  },
  {
    "start": 6956379,
    "end": 6958007,
    "text": "またはもっと派手なものでもよい。"
  },
  {
    "start": 6958007,
    "end": 6959112,
    "text": "チャットGTPで見るような"
  },
  {
    "start": 6959112,
    "end": 6964724,
    "text": "実際に報酬モデルを訓練し、その後、報酬モデルに関して整合性をとるためにPPOを繰り返し行っています。"
  },
  {
    "start": 6964724,
    "end": 6967277,
    "text": "そのうえで、もっとできることがたくさんある。"
  },
  {
    "start": 6967277,
    "end": 6970240,
    "text": "今のところ、2時間くらいになりそうな気がします。"
  },
  {
    "start": 6970240,
    "end": 6973198,
    "text": "この辺で終わりにします。"
  }
]