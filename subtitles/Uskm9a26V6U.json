[
  {
    "start": 90,
    "end": 13466,
    "text": "ジェイコブ・アンドレアスの教え子で、もう一人の共同研究者であるゲイブ・グランドと、同じくジョシュとヴィカシュの共同指導を受けている学生、タンジル・スレン、そして私の指導教官であるノア・グッドマンについても言及した。"
  },
  {
    "start": 13498,
    "end": 15470,
    "text": "ヴィカシュとジェイコブ・アンドレアス"
  },
  {
    "start": 17490,
    "end": 25154,
    "text": "本日の講演では、自然言語のモデル化に関する最近の進歩の多くに基づいて、私たちの幅広い目標を考えてみたいと思います。"
  },
  {
    "start": 25242,
    "end": 48086,
    "text": "また、AIにおけるツールキットや他のツールキット、認知科学や神経科学の証拠から、言語を使用し、生成し、言語から学習する知的アーキテクチャの構築について考えるさまざまな方法、さらに一般的には、私たちが思考すると言っている計算システムにおいて、言語がどのような役割を果たすか、または果たしうるかについて、より幅広いスペクトルの証拠を引き出していると思います。"
  },
  {
    "start": 48198,
    "end": 56910,
    "text": "言語哲学者から言語学者、神経科学者に至るまで、この部屋にいる多くの人々、そしておそらくこの部屋にはいないであろう他の多くの人々が考えてきた問題であることは明らかだ。"
  },
  {
    "start": 57330,
    "end": 68660,
    "text": "私たちは、この疑問に対する根本的な答えについて考えることから始めるのが有益な練習になるのではないかと考えました。"
  },
  {
    "start": 69270,
    "end": 96314,
    "text": "もちろん、この規模で「知能における言語の役割とは何か」という問いを立てている理由のひとつは、ほんの数年前から私たち全員が知っている、この驚くべき観察によるところが大きい。つまり、次の単語を予測するタスクを実行するために、大規模で特殊な変換器ベースのニューラル・アーキテクチャを言語モデルとして訓練し、十分な言語データを与えると、言語以上の何かが存在することを示唆するような挙動を示し始めるのだ。"
  },
  {
    "start": 96352,
    "end": 97740,
    "text": "彼らは考えているように見える。"
  },
  {
    "start": 98590,
    "end": 111834,
    "text": "データ上のわずかな例文からパターンを導き出したり、ザカール・トルテのようなまったく目新しい単語の定義を文脈から読み取り、その単語の使い方を即座に理解したかのようなリアルな文章を作成することもできる。"
  },
  {
    "start": 111962,
    "end": 129650,
    "text": "言語モデルにまつわる興奮の多くは、おそらく初めて、より一般的なインテリジェント・アーキテクチャの実装に向けたスケーラブルなルートを提供するものを見たということです。"
  },
  {
    "start": 131690,
    "end": 133254,
    "text": "その根底にあるものは何なのか？"
  },
  {
    "start": 133292,
    "end": 134946,
    "text": "これは言語について何を語っているのだろうか？"
  },
  {
    "start": 135058,
    "end": 143770,
    "text": "まあ、なぜこのような行動が見られるようになったのか、その根底にある支配的な仮説のひとつは、2つの考え方のようなものにあると言ってもいいと思う。"
  },
  {
    "start": 143840,
    "end": 146906,
    "text": "ひとつは、言語そのものの性質に関することだね。"
  },
  {
    "start": 147008,
    "end": 178390,
    "text": "それは、言語が十分に多様で広範であるという示唆であり、おそらく人間がその思考と言語の多様な範囲で、自分の思考の多くを表現していると考えられるからです。このタスクを完璧に解くことができること、完璧な言語モデル、あるいは少なくとも本当に優れた言語モデルになることは、本質的にAGIの完全なタスクであり、また、他の種類のタスク（世界中のすべての動画、世界中のすべてのYouTube動画を予測するようなタスク）とは異なり、便利なことに、私たちは、おそらく効率的なアーキテクチャと十分なデータを持っています。"
  },
  {
    "start": 178540,
    "end": 195002,
    "text": "思考が計算上どのようなものなのか、あるいは、私たちが思考と呼んでいる計算過程の内部で、言語がどのように必然的に関与しているのかについて、実は特に強い仮説である必要はない、ということは注目に値すると思う。"
  },
  {
    "start": 195136,
    "end": 199660,
    "text": "むしろ、言語モデリングタスクの性質そのものについての仮説にすぎない。"
  },
  {
    "start": 201470,
    "end": 225566,
    "text": "この部屋にいる多くの人が同意するのは、言語モデリング・タスクについては原理的にはそうかもしれませんが、LMSを使った最もエキサイティングな研究の多くは、より多くのデータに拡張することで、この方法で訓練され使用されたトランスフォーマーが実際にタスクを解決したり、完璧な次の単語予測モデルになったりすることを期待しているわけではないということです。"
  },
  {
    "start": 225678,
    "end": 233350,
    "text": "多くの人が指摘していると思うが、その直感は変圧器の仕組みからきている。"
  },
  {
    "start": 233420,
    "end": 233654,
    "text": "そうだね。"
  },
  {
    "start": 233692,
    "end": 241238,
    "text": "自己回帰的言語モデルは、固定された有限の内部計算を行っており、その計算量は直前の言語的文脈に基づいてのみ変化する。"
  },
  {
    "start": 241334,
    "end": 256030,
    "text": "任意に難しい数学の問題や、直感的に複雑さが直前のトークンの文脈に線形に依存しない計算量を必要とする計画問題のように、言語で質問を投げかけることができるというのは、もちろんあまり意味がない。"
  },
  {
    "start": 256450,
    "end": 265634,
    "text": "このように次の単語を予測する課題として扱うと、どのような種類の文章を完成させるのが難しいかについては、経験的にさまざまな観察結果が一致していると思います。"
  },
  {
    "start": 265672,
    "end": 265970,
    "text": "そうだろう？"
  },
  {
    "start": 266040,
    "end": 268900,
    "text": "難しい、恣意的な数学の問題や、このような計画の問題。"
  },
  {
    "start": 273510,
    "end": 276710,
    "text": "言葉の役割について考えるなら。"
  },
  {
    "start": 277450,
    "end": 286440,
    "text": "なぜ今、言語モデルがこれほど注目されているかというと、言語には思考を訓練するのに必要な証拠がすべて揃っているからだ。"
  },
  {
    "start": 287850,
    "end": 293434,
    "text": "レオが言ったように、それは言語が思考において内部的にどのように使われるかについての仮説にコミットしているわけではない。"
  },
  {
    "start": 293472,
    "end": 300410,
    "text": "トランスフォーマーの計算は、次の単語の分布を計算しているので、必ずしも言語的なことをしているわけではない。"
  },
  {
    "start": 300560,
    "end": 307438,
    "text": "さらに最近では、言語モデルを使用して、より思考力を高めることによって、より難しい課題を解決している。"
  },
  {
    "start": 307524,
    "end": 311466,
    "text": "トランスフォーマーに多くのことを考えさせるということは、より多くのトークンを生成させるということだ。"
  },
  {
    "start": 311658,
    "end": 317970,
    "text": "この考え方では、思考は必ずしも次のトークンを予測することだけで生まれるわけではない。"
  },
  {
    "start": 319430,
    "end": 322674,
    "text": "むしろ、思考は言語の中で起こるものだろう？"
  },
  {
    "start": 322792,
    "end": 334680,
    "text": "思考を連鎖させたり、スクラッチパッドを使ったり、電卓を呼び出したり、コードを書いて実行したりする。"
  },
  {
    "start": 335530,
    "end": 354990,
    "text": "私が思うに、この研究が体現している仮説のようなものは、言語と知能アーキテクチャの役割はより大きく、言語、あるいは言語のモノローグのようなものが思考の中心的なコントローラーであり、思考は主に言語の中で行われている、というものだ。"
  },
  {
    "start": 357650,
    "end": 369246,
    "text": "これらの提案で印象的なのは、ほとんどの認知科学者や神経科学者に尋ねれば、人間の一般的な知能に関連して言語が果たす役割はこれではないと言うだろう、ということだ。"
  },
  {
    "start": 369278,
    "end": 371810,
    "text": "少なくとも、支配的な仮説ではない。"
  },
  {
    "start": 372550,
    "end": 384680,
    "text": "ほんの数年前までは、おそらく多くのAI研究者が、すべての思考を制御するモノローグの主制御装置や主基盤としての言語について、必ずしもこのような役割を想定していなかっただろう。"
  },
  {
    "start": 385130,
    "end": 395420,
    "text": "そこで私たちは、言語と思考における人間らしい役割をよりよく捉えられるようなアーキテクチャを構築するための基礎として、人間における言語がどのようなものであるかの背景をいくつか見直そうと考えた。"
  },
  {
    "start": 396590,
    "end": 396954,
    "text": "そうだね。"
  },
  {
    "start": 396992,
    "end": 403962,
    "text": "そこで今回は、神経科学的、認知学的な証拠から、人々の言語がどのようなものかを簡単に説明したいと思います。"
  },
  {
    "start": 404016,
    "end": 410158,
    "text": "私たちのゴールは、言語がどのようなパラダイムに適合しうるかについて、他の種類のパラダイムを押し返すことではありません。"
  },
  {
    "start": 410244,
    "end": 427198,
    "text": "というのも、今日この部屋に座っているような、別の著名なインテリジェント・アーキテクチャにはマッチしないように思えるからです。"
  },
  {
    "start": 427304,
    "end": 437902,
    "text": "人間の認知において言語が果たしていると思われる役割の証拠のひとつは、神経画像データ、fMRIデータから得られるもので、人間の脳において言語がどのように処理されるかを示唆している。"
  },
  {
    "start": 437986,
    "end": 455546,
    "text": "現時点では、マサチューセッツ工科大学（MIT）のエヴァ・フェデレンコ、スタン・デ・ヘイン、そして最近当学科の大学院生となったカイル・マルワルドとアーニャ・イヴァノヴァを含む多くの人々による収束画像データが、人間の脳には、我々が言語から連想する多くのタスクを処理する言語特異的ネットワークがあることを示唆している。"
  },
  {
    "start": 455578,
    "end": 461360,
    "text": "文章を聞いたり、読んだり、言葉を話したり書いたりするときに活性化する。"
  },
  {
    "start": 462050,
    "end": 464490,
    "text": "これは英語だけのネットワークではない。"
  },
  {
    "start": 464570,
    "end": 468402,
    "text": "どの言語を話していても、同じ地域が活性化される。"
  },
  {
    "start": 468536,
    "end": 475490,
    "text": "ドスラク語やクリンゴン語のような言語を学び、流暢に話せるようになるためにやってくる人々のために、構築された言語を生産しているときでさえ、それは発火する。"
  },
  {
    "start": 476710,
    "end": 477122,
    "text": "そして"
  },
  {
    "start": 477176,
    "end": 477394,
    "text": "そうだね。"
  },
  {
    "start": 477432,
    "end": 479286,
    "text": "この言語ネットワークは何をしているのか？"
  },
  {
    "start": 479388,
    "end": 485058,
    "text": "さて、次第に有力な仮説のひとつは、実際に次の単語を予測するようなことをしているというものだ。"
  },
  {
    "start": 485234,
    "end": 494262,
    "text": "実際、他の多くの種類の代替モデルの中で、人々は長い間、脳へのベクトルに言葉を合わせるようなことを試みてきた。"
  },
  {
    "start": 494406,
    "end": 506670,
    "text": "特に、NLIのような他の言語タスクではなく、次の単語を予測するタスクで学習させた場合だ。"
  },
  {
    "start": 507810,
    "end": 529666,
    "text": "この神経画像からわかることは、ヒトの言語ネットワークの役割が、おそらく認知のコントローラーや中心的な座ではなく、言語に選択的であり、他の多くの種類の思考活動、たとえば数学の問題を解くとか、論理学や物理学、社会的推論に関する推論など、記号が関与しているように見えるものにも関与していないことを浮き彫りにしていると思います。"
  },
  {
    "start": 529698,
    "end": 535334,
    "text": "それらは脳の他の推論領域を呼び起こし、言語はそれらとモジュール的にインターフェースする。"
  },
  {
    "start": 535372,
    "end": 540200,
    "text": "非常に一般的なインターフェイスではあるが、思考の大部分がそこで行われているようには見えない。"
  },
  {
    "start": 541610,
    "end": 545014,
    "text": "そう、次の単語予測について質問があったんだ。"
  },
  {
    "start": 545062,
    "end": 551098,
    "text": "例えば、数学の単語を予測して、次の単語よりこっちの方が悪いとか？"
  },
  {
    "start": 551264,
    "end": 555966,
    "text": "クローズドタスクは、彼らが探している代替案のひとつなんだ。"
  },
  {
    "start": 555988,
    "end": 559840,
    "text": "そうだね、みんな遠慮なく大声を出せばいいんだ。"
  },
  {
    "start": 562770,
    "end": 568158,
    "text": "実際、成人の脳卒中患者の中には、脳のこの領域だけを損傷している人もいる。"
  },
  {
    "start": 568334,
    "end": 577314,
    "text": "音声言語を理解することはできないが、異なる媒体で描かれたさまざまな種類の仕事について考えることはできる。"
  },
  {
    "start": 577362,
    "end": 580360,
    "text": "例えば、彼らは見ているビデオから物理的な推測をすることができる。"
  },
  {
    "start": 580810,
    "end": 586434,
    "text": "逆に言えば、局所的なダメージを受ける可能性もあるということだ。"
  },
  {
    "start": 586562,
    "end": 601546,
    "text": "そうすると、まるで初歩的で局所的な単語ベースの言語モデルのように、長い文章を比較的流暢に、そして構文的にまとまりのある文章を作ることができる。"
  },
  {
    "start": 601728,
    "end": 603822,
    "text": "カンザス・シティは気に入りましたか？"
  },
  {
    "start": 603876,
    "end": 606240,
    "text": "この人は言う。"
  },
  {
    "start": 607570,
    "end": 611982,
    "text": "あるいは、明らかに世界的に意味のある、あるいは目標に向かうような文章を作る。"
  },
  {
    "start": 612036,
    "end": 616210,
    "text": "この結果を過剰に指標化することは望まないが、より首尾一貫したイメージには合致している。"
  },
  {
    "start": 616790,
    "end": 635954,
    "text": "このような神経科学からの最近の証拠は、多くの認知科学者や言語学者が、発達科学を通して私たちが目にし、学んできたこと、子どもの思考や世界の言語を研究することで、すでに信じてきた、人間の認知とその中での言語の位置づけに関するより広い図式を、いろいろな意味で裏付けていると思う。"
  },
  {
    "start": 636002,
    "end": 636262,
    "text": "そうだろう？"
  },
  {
    "start": 636316,
    "end": 652330,
    "text": "つまり、現時点での広範な研究によれば、幼児は、何らかの言語を学んだり、すでに自立して話したりするよりもずっと前に、物理学に関する推論から計画立案、因果関係や確率的推論の引き出しに至るまで、私たちが首尾一貫した思考と結びつけて考えているタスクの多くを実行しているのである。"
  },
  {
    "start": 652830,
    "end": 658894,
    "text": "言語とは、この構造化された思考基盤の上に、人間が学び、足場を作っていくもののようだ。"
  },
  {
    "start": 659012,
    "end": 667890,
    "text": "大規模な言語モデルや、実際には言語入力にまったくさらされていない人間よりも、流暢な言語を生成することを学ぶのに必要な入力データははるかに少ない。"
  },
  {
    "start": 667960,
    "end": 694940,
    "text": "有名な話だが、ニカラグアの聴覚障害者の子供たちは、孤立した聴覚障害者の家庭で育ち、コミュニケーションを取ろうとした結果、自発的に初期の手話を生み出すようになる。"
  },
  {
    "start": 695870,
    "end": 707646,
    "text": "そしてもちろん、私たちが何らかの形で知的であると連想し、確率的推論や計画を連想させるモデルを使ってモデル化された、言語をまったく使わない動物も他にもたくさんいる。"
  },
  {
    "start": 707668,
    "end": 713280,
    "text": "私たちが思考だと思っているものにとって、言語は決して必要なものではない気がする。"
  },
  {
    "start": 714390,
    "end": 741146,
    "text": "動物や非言語的な幼児に共通し、私たちの脳内の言語ネットワークを介さない思考タスクの多くで呼び出される計算のいくつかを捉えるような、この知的システム像を形式化することは、多くの認知科学の中心的な目標であると同時に、現在のようなLLM中心の瞬間が訪れる以前は、AI内のさまざまな分野の歴史的な動機でもありました。"
  },
  {
    "start": 741328,
    "end": 788642,
    "text": "ラッセルとノーウィグの教科書「人工知能：現代的なアプローチ」を開けば、知能がどのようにコンピュータ上で動作するか、または知能の計算モデルを捉えたような方程式が見られます。そのアイデアは、知的なエージェントは内部に構造化された世界モデルを持ち、世界の観察に基づいて更新できるというものであり、また、我々が行動の結果を予測できること、エージェントがある種の価値や欲求を持ち、その価値がある種の効用関数で捉えられること、エージェントが自身の観察と可能な行動、およびそれらの期待効用に関して確率的推論を行い、行動の価値を最適化するための何らかの計画を実行できることです。"
  },
  {
    "start": 788706,
    "end": 788982,
    "text": "そうだね。"
  },
  {
    "start": 789036,
    "end": 791990,
    "text": "これは普通の発言ですか、それとも説明的な発言ですか？"
  },
  {
    "start": 792150,
    "end": 796858,
    "text": "知的エージェントとは、このような性質を持つものだと定義しているのでしょうか？"
  },
  {
    "start": 797024,
    "end": 801210,
    "text": "あるいは、インテリジェントな建築を作りたいのなら、これを持つべきだというような？"
  },
  {
    "start": 801280,
    "end": 801466,
    "text": "そうだね。"
  },
  {
    "start": 801488,
    "end": 802954,
    "text": "いい質問だと思う。"
  },
  {
    "start": 802992,
    "end": 806010,
    "text": "いくつかの仕事は間違いなく規範的な観点から取り組んでいると思う。"
  },
  {
    "start": 806090,
    "end": 809870,
    "text": "これは、合理的な行動とはどのようなものかという定義のようなものだろう？"
  },
  {
    "start": 809940,
    "end": 825650,
    "text": "計算認知科学の分野では、さまざまな領域で、人はこのような行動を規範として認識するか、計算でできることの限界に従ってこのような行動をとるという証拠をたくさん集めてきたと思う。"
  },
  {
    "start": 828090,
    "end": 844250,
    "text": "つまり、このアーキテクチャでは、思考を司るのは、主に言語的なシステムではなく、一般的な世界のモデリングや確率的推論、あるいは計画や効用最大化が可能なある種のシステムということになる。"
  },
  {
    "start": 845550,
    "end": 860218,
    "text": "現在、人工知能の分野では、この方程式のさまざまな要素に対応するコンポーネントを作り上げることによって、この方程式を直接実装しようとする試みが主に行われているが、計算効率や自然知能の一般性には及ばず、苦戦を強いられている。"
  },
  {
    "start": 860394,
    "end": 871410,
    "text": "過去数十年の間に、確率的プログラミング言語と呼ばれる、少なくとも限られた領域において確率的推論タスクを解決するソフトウェアを構築するための新しいクラスのツールが出現してきた。"
  },
  {
    "start": 873750,
    "end": 889850,
    "text": "これらの言語は、オブジェクト検出ニューラルネットよりも正確に乱雑な3Dシーンを認識することから、フェイスブックのニューラル・プロフィットなどの業界をリードするソリューションよりも正確に経済動向を解釈し予測することまで、あらゆることを行うシステムの作成に応用されている。"
  },
  {
    "start": 890510,
    "end": 895174,
    "text": "これらの確率プログラミング・システムは、2つの重要な技術的特徴によって、これらのアプリケーションを可能にしている。"
  },
  {
    "start": 895222,
    "end": 907450,
    "text": "世界の複雑な確率モデルをプログラムとして表現できるモデリング言語が特徴で、表現力豊かなプログラムライクなコンポーネントで定義されたリッチな確率モデルを簡単に書き出すことができる。"
  },
  {
    "start": 907610,
    "end": 916074,
    "text": "例えば、これらのアプリケーション例の背後にある確率モデルは、3Dレンダラ、シンボリックプランナー、科学シミュレータなどの観点から定義されている。"
  },
  {
    "start": 916212,
    "end": 931650,
    "text": "この自動化によって、ユーザーは、モンテカルロ法や変分推論法など、確率推論のための高度なアルゴリズムを簡潔に実装できるようになる。"
  },
  {
    "start": 931730,
    "end": 937810,
    "text": "これらのツールについて考えるひとつの方法は、PytorchやTensorflowのようなものだ。"
  },
  {
    "start": 937970,
    "end": 945290,
    "text": "微分可能なモデルを書いて最適化を行うのではなく、確率モデルを書いてさまざまな確率的推論を行うのだ。"
  },
  {
    "start": 946510,
    "end": 961566,
    "text": "つまり、これまでのところ、AI工学の取り組みは、言語やAIの言語モデルのような部分とはあまり接触していない。"
  },
  {
    "start": 961588,
    "end": 970078,
    "text": "今日の残りの時間では、言語がこのような図式やアプローチにどのように当てはまるかを考えるために、2つの異なるアプローチを探ります。"
  },
  {
    "start": 970174,
    "end": 985974,
    "text": "レオはまず、知的エージェントが、外部からもたらされた多くの説明や観察、質問をどのように取り入れるか、エージェントが信念を更新し、世界でどのように行動するかを決定する方法に、どのようにそれらすべての言語の形式を取り入れることができるかについて話すつもりだ。"
  },
  {
    "start": 986092,
    "end": 993590,
    "text": "それから、世界のモデルや確率的推論アルゴリズムの中で思考するためのツールとして言語を活用するシステムについて少し話す。"
  },
  {
    "start": 993670,
    "end": 997594,
    "text": "いずれもごく最近のプレプリントに相当する。"
  },
  {
    "start": 997632,
    "end": 999222,
    "text": "免責事項も述べておきたい。"
  },
  {
    "start": 999286,
    "end": 1006860,
    "text": "どちらも予備的な提案であり、ここでは建築的なスケールの大きな解決策ではなく、非常に推測的な話をしていることを強調しておきたい。"
  },
  {
    "start": 1008050,
    "end": 1008734,
    "text": "クールだろ？"
  },
  {
    "start": 1008772,
    "end": 1011614,
    "text": "このトークの最初の人物は、仕事を要約している。"
  },
  {
    "start": 1011652,
    "end": 1017114,
    "text": "もっと読みたければ、単語モデルから世界モデルへの非常に長い論文がアーカイブにアップされているので、そちらを参照してほしい。"
  },
  {
    "start": 1017162,
    "end": 1020846,
    "text": "これは主に、今日はここにいないもう一人の学生、ゲイブ・グラントと一緒にやっている。"
  },
  {
    "start": 1021038,
    "end": 1030120,
    "text": "先ほど述べたように、この仕事の背景には、私たちが耳にするすべての外的言語が、少なくとも私たち人間の思考に影響を与えていると思われる幅の広さを捉えるシステムを、どのように構築できるかを考えている。"
  },
  {
    "start": 1030730,
    "end": 1033602,
    "text": "はっきりしているのは、この役割は非常に幅広いということだ。"
  },
  {
    "start": 1033666,
    "end": 1049206,
    "text": "信念と目標を持つエージェントという基本的なモデルを考えてみると、言語で表現した観察結果から状況についての信念を更新したり、言語的に指定した質問に答えることが思考の目標になったりするような、信じられないほど多様な状況があるように思える。"
  },
  {
    "start": 1049318,
    "end": 1056014,
    "text": "このようなことは、他のエージェントについての私たちの知識、つまり、彼らが何を考え、何をするかを推論するための私たちの直感的な心理に関係しているかもしれない。"
  },
  {
    "start": 1056132,
    "end": 1064110,
    "text": "あるいは、私たちを取り巻く物理的な世界、私たちが知覚しているものについて話し、私たちが見ているものについて推論し、私たちの物理的な直感を引き出さなければならないような質問をすることもできる。"
  },
  {
    "start": 1064850,
    "end": 1070146,
    "text": "もちろん、私たちが言語に興奮する理由のひとつは、それが私たちがすでに知っていることを利用するだけではないということだ。"
  },
  {
    "start": 1070248,
    "end": 1082466,
    "text": "言葉で定義したり、言葉から学んだりする新しい概念であれ、本当に深い新しい理論や概念体系であれ、人間が深く新しい知識を学び、伝えることができるのは、この手段によるように思える。"
  },
  {
    "start": 1082498,
    "end": 1082838,
    "text": "そうだね。"
  },
  {
    "start": 1082924,
    "end": 1092860,
    "text": "私たちが世界について知っていることの多くは、戦争があるという事実や、戦争とはどのような法体系を持つ科学なのか、というような、何らかの形で言語から得ていると感じられる情報に由来している。"
  },
  {
    "start": 1093630,
    "end": 1095130,
    "text": "どうすればいいのか？"
  },
  {
    "start": 1095280,
    "end": 1109146,
    "text": "まあ、言語について考えるためのひとつの長いレンズは、LLMに基づく瞬間以前から続いているようなものですが、言語とは何かというと、人間の思考を伝達するための外的な象徴的媒体だということです。"
  },
  {
    "start": 1109258,
    "end": 1117394,
    "text": "それは、私たちの内部的な思考表現から外部的な記号体系への、ある種の一般的なマッピング機能があるからだ。"
  },
  {
    "start": 1117512,
    "end": 1118526,
    "text": "それが言葉だ。"
  },
  {
    "start": 1118638,
    "end": 1129750,
    "text": "つまり、このような古い枠組みでは、言語を理解する、あるいは意味を理解するということは、私たちが耳にした外的な文章から、構造化された内的な表現にマッピングすることを意味する。"
  },
  {
    "start": 1130250,
    "end": 1142898,
    "text": "本稿で探求するのは、言語の意味を、確率的プログラミング言語における表現上のマッピングや確率分布として投げかける一般的な提案である。"
  },
  {
    "start": 1143074,
    "end": 1156270,
    "text": "というのも、ここには実に深いつながりがあると思うのですが、このような考え方を形式化したり、より豊かなものにしたりする方法のひとつになるのではないかと思うからです。"
  },
  {
    "start": 1157010,
    "end": 1176914,
    "text": "というのも、このアーキテクチャは、アレックスが言ったように、世界を表現し、そのモデルを確率的に照会し、その上で首尾一貫した推論を導き出すことを目的とした、ある種の既存の内部モデリング言語からすでに始まっているからだ。"
  },
  {
    "start": 1177112,
    "end": 1186280,
    "text": "また、さまざまな種類の文章や言語の内容を、それらがマッピングされる可能性のある確率的プログラミング表現の種類として正式にモデル化するためのフレームワークも示唆していると思う。"
  },
  {
    "start": 1191470,
    "end": 1207034,
    "text": "この論文では、この提案がどのように推論のさまざまなドメインに関してインスタンス化されるかを探ることから始める。一般的な公共推論だけでなく、関係や物理学、社会的状況に関する推論も含む。"
  },
  {
    "start": 1207162,
    "end": 1221934,
    "text": "これらすべてにおいて、私たちは、世界についての一般的な概念的知識、定義や因果的知識を伝達する言語について、確率的生成モデルを構築するものであるこれらの確率的表現を構築するものとして考えることを提案するつもりである。"
  },
  {
    "start": 1222062,
    "end": 1243740,
    "text": "このフレームワークでは、少なくとも1つの赤いマグカップが見られるとか、チャーリーはダナの祖父であるといった言語内の観察が、この確率モデルの状態を更新する形式的な条件文を構成し、質問は、モデルに関する確率的推論の形式的な対象を指定するクエリー表現にマップされる。"
  },
  {
    "start": 1244670,
    "end": 1249366,
    "text": "この枠組みでは、私たちは基本的に確率的推論としてキャストを考えている。"
  },
  {
    "start": 1249558,
    "end": 1273970,
    "text": "私たちは、言語モデルの役割を考えるもうひとつの方法として、脳の言語ネットワークのような、より小さなものを提案します。それは、自然言語の文から、確率的プログラミング言語で意味を伝える表現上の分布への、文脈に特化した、以前の談話を条件としたマッピングを予測する、これまでになかった方法で、この意味機能をインスタンス化する手段である、ということです。"
  },
  {
    "start": 1275830,
    "end": 1290830,
    "text": "この長い免責事項の一部として、これからお見せするのは、このフレームワークの本当に最小限の実装ですが、言語とさまざまな中核的認知領域との間のより一般的なインターフェースを実装するために、このアプローチを拡張するさまざまな方向へのポインターとして意図しています。"
  },
  {
    "start": 1290930,
    "end": 1297770,
    "text": "具体的に言うと、次に見る例では、意味機能はコーデックを使って実装されますよね？"
  },
  {
    "start": 1297920,
    "end": 1304334,
    "text": "現在の技術水準よりもはるかに小さいopenamモデルは、言語とコードに関する共同分布を学習するように訓練されている。"
  },
  {
    "start": 1304532,
    "end": 1314026,
    "text": "これは非常にシンプルな確率論的プログラミング言語で、一般的な標本に基づく推論手順をサポートしている。"
  },
  {
    "start": 1314138,
    "end": 1320530,
    "text": "私たちの目標は、このフレームワークが、言語と多くの異なる認知領域との間で、どのように広範なインターフェイスになりうるかを示すことです。"
  },
  {
    "start": 1321430,
    "end": 1345142,
    "text": "そこでまず、このような提案によって、言語がエージェントの信念を更新し、世界モデルを照会できるようになるかもしれないという基本的な意味を説明するために、実に単純なおもちゃの例から始めようと思います。この例は、認知科学の先行実験の束を利用したもので、実際の人が、以前に見た綱引きゲームの勝敗に基づいて、どのチームが勝つ可能性があるかについて、さまざまな推論を行うよう求められたものです。"
  },
  {
    "start": 1345286,
    "end": 1362926,
    "text": "これはジョシュのグループによる古い研究で、この規範的モデル、つまり確率的推論のノルヴィッグ・モデルが、綱引きゲームのメカニズムに関する非常に一般的な確率的モデルを用いて、実際の人間の行動や予測を多くの点で予測することを実証したものだ。"
  },
  {
    "start": 1363028,
    "end": 1371330,
    "text": "ここでの我々の目標は、我々のフレームワークが、自然言語とこの古い実験の中核となる例のすべてとの間のインターフェースをどのように実装できるかを示すことである。"
  },
  {
    "start": 1371750,
    "end": 1388354,
    "text": "つまり、この小さなおもちゃの例で皆さんが見ているのは、画面上で定義されている世界モデルが、綱引きでプレーするさまざまなトーナメントの結果に影響を与える可能性のある、さまざまな人間の特性の基本的な因果関係を捉えているということです。"
  },
  {
    "start": 1388412,
    "end": 1398326,
    "text": "つまり、例えば、ここでは選手を内部的な強さの値としてモデル化しており、強さはほぼ普通に変化する。"
  },
  {
    "start": 1398358,
    "end": 1401574,
    "text": "この未観測の潜在変数が、さまざまな種類の選手にわたって存在するのだろうか？"
  },
  {
    "start": 1401702,
    "end": 1411230,
    "text": "私たちはまた、選手にはある種の内面的な怠惰の値があると考えます。これは、選手が実際に自分の底力に従って行動しない時間の割合を表しています。"
  },
  {
    "start": 1411810,
    "end": 1417554,
    "text": "これらの変数は、綱引きゲームの結果をどのように決定するのだろうか？"
  },
  {
    "start": 1417752,
    "end": 1423330,
    "text": "まあ、選手で構成されるチーム全体の強さは、その選手の強さの累積によって決まる。"
  },
  {
    "start": 1423830,
    "end": 1427826,
    "text": "この試合で怠けることを決め込んでいる選手は、全力で引っ張らないかもしれない。"
  },
  {
    "start": 1427928,
    "end": 1431494,
    "text": "ある試合で最も力を発揮したチームが勝つ。"
  },
  {
    "start": 1431532,
    "end": 1432166,
    "text": "そうだね。"
  },
  {
    "start": 1432348,
    "end": 1434914,
    "text": "強さと怠惰というプリミティブをデザインしたのはあなたですか？"
  },
  {
    "start": 1434962,
    "end": 1437142,
    "text": "それともコーデックス自身がプリミティブを考案したのか？"
  },
  {
    "start": 1437276,
    "end": 1440646,
    "text": "今回は、旧作から派生したモデルを見ている。"
  },
  {
    "start": 1440668,
    "end": 1441426,
    "text": "これらはデザインされている。"
  },
  {
    "start": 1441458,
    "end": 1449020,
    "text": "そう、この作品の後半では、今言った定義のように、誰かが言語を使って話しているだけで、この種のモデルを学ぶことができる例をいくつか紹介するつもりだ。"
  },
  {
    "start": 1449470,
    "end": 1449834,
    "text": "そうだね。"
  },
  {
    "start": 1449872,
    "end": 1460090,
    "text": "これは実に単純な例だが、戦争ゲームを聴くことになると告げれば、人々が持つ基本的な因果関係の知識を驚くほど多くとらえることができる。"
  },
  {
    "start": 1460170,
    "end": 1463246,
    "text": "時として、人は怠け者になり、全力で引っ張らないことがある。"
  },
  {
    "start": 1463348,
    "end": 1466450,
    "text": "この領域で言語をどのように関連づけるのか？"
  },
  {
    "start": 1467510,
    "end": 1467922,
    "text": "そうだね。"
  },
  {
    "start": 1467976,
    "end": 1487590,
    "text": "さて、私たちが今示した定義に実際に適合する意味関数の単純な概念を誘導する一つの手段は、この文脈に特化した生成的世界モデルと、言語がこの領域でサンプリングされた確率的プログラミング表現にどのようにマッピングされるかを示すいくつかの例の両方で言語モデルを条件付けることである。"
  },
  {
    "start": 1488590,
    "end": 1490154,
    "text": "私たちが今していることを。"
  },
  {
    "start": 1490192,
    "end": 1490442,
    "text": "そうだね。"
  },
  {
    "start": 1490496,
    "end": 1509840,
    "text": "それは、コーデックが言語とコードの上にある一般的な事前分布と、この状況で言語がどのように使われているかという具体的な談話思考のコンテキストの両方を条件とする、任意の新しい文から表現へのこの種の状況に特化した文脈的マッピングを効果的に誘導することである。"
  },
  {
    "start": 1510690,
    "end": 1513886,
    "text": "この方法については、後ほどいくつか紹介する。"
  },
  {
    "start": 1513988,
    "end": 1519278,
    "text": "この例は、このような最小限の実装でどれだけのことができるかを説明するために使っている。"
  },
  {
    "start": 1519374,
    "end": 1522578,
    "text": "言語からコードに変換するモデルの概念。"
  },
  {
    "start": 1522744,
    "end": 1523266,
    "text": "そうだね。"
  },
  {
    "start": 1523368,
    "end": 1526162,
    "text": "ここで私たちはどのような言葉を使うだろうか？"
  },
  {
    "start": 1526216,
    "end": 1529426,
    "text": "確率の相手表現との関連で、どのように考えることができるだろうか。"
  },
  {
    "start": 1529538,
    "end": 1536850,
    "text": "まあ、ジョシュがレオに勝ったというような一般的な命題は、マップに変換される、あるいはマップになるかもしれない。"
  },
  {
    "start": 1536930,
    "end": 1542970,
    "text": "私たちは、ジョシュがレオに勝ったという観察、つまり条件文をマッピングする、あるいはそれを意味すると考えるかもしれない。"
  },
  {
    "start": 1543550,
    "end": 1558298,
    "text": "その後、ジョシュがアレックスに勝ったというような観測をすれば、ジョシュがアレックスに勝ったという事実を捕らえる確率プログラミング言語に変えるために、私たちが誘導したこの意味関数を一般的に使い続けることができる。"
  },
  {
    "start": 1558394,
    "end": 1565300,
    "text": "チームとして協力しても、レオとアレックスはこの綱引きゲームでジョシュに勝てなかった。"
  },
  {
    "start": 1566630,
    "end": 1571970,
    "text": "この時点で、ジョシュはどれくらい強いのか？"
  },
  {
    "start": 1572710,
    "end": 1586118,
    "text": "このような状況で私たちが考えていることは、実際には、私たちが定義した生成モデルから、私たちが行った観察に従って可能な世界を事後的にサンプリングしているのです。"
  },
  {
    "start": 1586204,
    "end": 1594374,
    "text": "ということは、ジョシュの強さのような文の意味は、構造化された確率的推論クエリということになる。"
  },
  {
    "start": 1594422,
    "end": 1597446,
    "text": "ジョシュの強みである潜在変数とは何か？"
  },
  {
    "start": 1597558,
    "end": 1607930,
    "text": "つまり、ジョシュは平均よりもかなり強く、しかも首尾一貫しているということだ。"
  },
  {
    "start": 1608010,
    "end": 1614820,
    "text": "ゲイブのような見たこともない新戦力は、オプリオリには彼に勝てそうにないと予想できるかもしれない。"
  },
  {
    "start": 1615990,
    "end": 1621490,
    "text": "ゲイブがジョシュに勝つ確率はどうかと問えば、やや低いと考えていることがわかる。"
  },
  {
    "start": 1626490,
    "end": 1628242,
    "text": "ジョシュの強さは？"
  },
  {
    "start": 1628306,
    "end": 1635190,
    "text": "この文脈では、強いという言葉が何を意味するのかという暗黙の疑問がある。"
  },
  {
    "start": 1639050,
    "end": 1648214,
    "text": "これは数字ではなく、比較形容詞のようなもので、おそらく交差しない枠組みだろう。"
  },
  {
    "start": 1648262,
    "end": 1651646,
    "text": "それとも、この種の問題は無視したほうがいいのだろうか？"
  },
  {
    "start": 1651828,
    "end": 1655760,
    "text": "ええ、そうですね。では、考えられる方法はいくつかあると思います。"
  },
  {
    "start": 1658690,
    "end": 1664834,
    "text": "ひとつ言えるのは、ジョシュがどれだけ強いかという答えは数字ではないということだ。"
  },
  {
    "start": 1664952,
    "end": 1676722,
    "text": "そうではなく、ジョシュが現在持っている生成モデルに関して持っていると推測される、さまざまな基礎的な強さの値の事後分布のようなものだ。"
  },
  {
    "start": 1676856,
    "end": 1683446,
    "text": "様々な不確かな形容詞、例えば「強い」という言葉のような、もうひとつの一般的な定義があると思う。"
  },
  {
    "start": 1683548,
    "end": 1694666,
    "text": "つまり、あなたには内的な閾値があり、あるいは話している人にも内的な閾値があり、その閾値を文脈や見たものから共同で推測しなければならないのです。"
  },
  {
    "start": 1694768,
    "end": 1697482,
    "text": "後で実際に例を挙げよう。"
  },
  {
    "start": 1697536,
    "end": 1697754,
    "text": "だから"
  },
  {
    "start": 1697792,
    "end": 1698042,
    "text": "そうだね。"
  },
  {
    "start": 1698096,
    "end": 1703510,
    "text": "言語学では、それを語用論的推論のように扱う研究も含め、長い研究がある。"
  },
  {
    "start": 1703590,
    "end": 1714762,
    "text": "このマッピング関数は、語用論的推論という概念を含む一般的なものだと考えることができるかもしれません。"
  },
  {
    "start": 1714826,
    "end": 1718590,
    "text": "また、もしあなたが継続的であるならば、その感覚を捉えていると思う。"
  },
  {
    "start": 1718670,
    "end": 1730054,
    "text": "それとも、人の強さについて話すような多くの一般的な場面で、強さに関するある種の古い概念がキャッシュされ、それを引き出せるようになっているのだろうか？"
  },
  {
    "start": 1730092,
    "end": 1740780,
    "text": "大規模な言語モデルを、言語から式やコードへのマッピング関数を学習したものとみなす考え方は、知識が償却されていくという意味でも有効だと思う。"
  },
  {
    "start": 1741150,
    "end": 1742854,
    "text": "あなたはそのような推論を持っていないかもしれない。"
  },
  {
    "start": 1742902,
    "end": 1763410,
    "text": "ええ、クリス、大規模な言語モデルの意味表現や推論能力を否定したり、無視したりしています。"
  },
  {
    "start": 1764790,
    "end": 1778086,
    "text": "というのも、右手にはよりクールな確率的プログラミング言語があるが、ある意味、これは2010年から2015年にかけてのようなセマンティック・パースである。"
  },
  {
    "start": 1778108,
    "end": 1791718,
    "text": "そう、あなたは大規模な言語モデルを使用しているが、実際には大規模な言語モデルの興奮を表現システムとして使用しているわけではない。"
  },
  {
    "start": 1791884,
    "end": 1801100,
    "text": "ええ、おそらく私たち一人ひとりがこれに対して異なる答えを持っているでしょう。しかし、この講演を通して私たちが描き出したいと思っていることのひとつは、実際にどのような方法があるのかということです。"
  },
  {
    "start": 1801630,
    "end": 1802090,
    "text": "そうだね。"
  },
  {
    "start": 1802160,
    "end": 1807018,
    "text": "もちろん、意味解析のもろさのようなものに戻ろうとは誰も言わない。"
  },
  {
    "start": 1807034,
    "end": 1823378,
    "text": "大規模な言語モデルが私たちに与えてくれるもの、つまり今回の講演で提案されたことのひとつは、言語学の古典的な概念や、意味解析の古典的な概念など、私たちが本当に望んでいることの多くを規範的に捉えている理論があるということです。"
  },
  {
    "start": 1823464,
    "end": 1832866,
    "text": "AIシステムにとっての答えのひとつは、そうですね、ある意味では、大規模な言語モデルから学習したことをすべて捨てたくないということです。"
  },
  {
    "start": 1832898,
    "end": 1835906,
    "text": "それに対するひとつの答えが、私がジェイコブにした答えのようなものだと思う。"
  },
  {
    "start": 1835938,
    "end": 1836520,
    "text": "そうだね。"
  },
  {
    "start": 1837050,
    "end": 1847606,
    "text": "これらの例では必ずしもそうではないが、我々は常に言語から出発し、常にある種の確率プログラミング式にマッピングするという、非常に直接的なシステムを示している。"
  },
  {
    "start": 1847638,
    "end": 1849194,
    "text": "そこですべての思考が起こる。"
  },
  {
    "start": 1849312,
    "end": 1859600,
    "text": "というのも、あなたが言っているように、大規模な言語モデルが多くの潜在情報を学習していると信じるに足る理由がたくさんあるからです。"
  },
  {
    "start": 1861250,
    "end": 1871666,
    "text": "彼らは確かに多くの潜在的な概念情報を持っているし、ある種の限定的な償却推論や、他の人が持っていたものから学んだ古い推論を再利用することも、ある程度はできるだろう。"
  },
  {
    "start": 1871688,
    "end": 1887826,
    "text": "本講演の第2部では、この公開プログラミング言語自体が、大規模な言語モデルが学習したことから必ずしも切り離されたものである必要はない一方で、大規模な言語モデルへの呼び出しを埋め込むことで、知識の蓄積を利用することができる、さまざまな方法を紹介する。"
  },
  {
    "start": 1887938,
    "end": 1904638,
    "text": "セマンティック・パーキングの三重性に立ち戻ったのは、記号的な意味表現への変換を行っているからではないか。"
  },
  {
    "start": 1904804,
    "end": 1905742,
    "text": "そうだね。"
  },
  {
    "start": 1905796,
    "end": 1926894,
    "text": "いや、そうではなく、このような広い定義が、言語における文の意味は、確率的なプログラミング表現だけではない、と言っているのだと思う。"
  },
  {
    "start": 1926952,
    "end": 1930098,
    "text": "これは教育的な目的のためにここで見せているものだ。"
  },
  {
    "start": 1930194,
    "end": 1932134,
    "text": "と言うかもしれない。"
  },
  {
    "start": 1932172,
    "end": 1936434,
    "text": "言葉の曖昧さにどう従うのか？"
  },
  {
    "start": 1936482,
    "end": 1939554,
    "text": "文章には定義が曖昧なものがある。"
  },
  {
    "start": 1939602,
    "end": 1946454,
    "text": "ジョシュはアレックスとレオに勝った。"
  },
  {
    "start": 1946502,
    "end": 1946714,
    "text": "そうだね。"
  },
  {
    "start": 1946752,
    "end": 1950358,
    "text": "これは古典的な構文なんだ。"
  },
  {
    "start": 1950454,
    "end": 1958154,
    "text": "つまり、ジョシュがアレックスとレオを倒し、彼らが同じチームでプレーしていたのか、それともジョシュがアレックスを倒し、その後にジョシュがレオを倒したのか。"
  },
  {
    "start": 1958282,
    "end": 1967474,
    "text": "一般的に言われるのは、その文の意味はどちらか一方の表現を選ぶべきでないということだ。"
  },
  {
    "start": 1967672,
    "end": 1972638,
    "text": "それは、可能なパーズの分布のようなものであるべきだ。"
  },
  {
    "start": 1972734,
    "end": 1979046,
    "text": "配給もまた、このようなまったく文脈に左右されない繊細な方法で決められるものであってはならない。"
  },
  {
    "start": 1979148,
    "end": 1982162,
    "text": "実際には、談話におけるこれまでのすべてのパターンに依存するはずだ。"
  },
  {
    "start": 1982226,
    "end": 1992970,
    "text": "もし誰かがこの接続詞を使い続け、選手たちが一緒にプレーするチームを指しているのであれば、そのような言説の偏りを考慮に入れるべきだ。"
  },
  {
    "start": 1993120,
    "end": 2010382,
    "text": "大規模な言語モデルというのは、一般的にこのような広範な共同分布を学習しているようなものですが、この生成モデルの内容によって、かなり豊かな条件付けができるものなのです。"
  },
  {
    "start": 2010516,
    "end": 2014330,
    "text": "それは、強さの普遍的な定義を考え出そうとしているのではない。"
  },
  {
    "start": 2014410,
    "end": 2018306,
    "text": "必ずしもこれらの言葉の普遍的な定義を打ち出そうとしているわけでもない。"
  },
  {
    "start": 2018408,
    "end": 2039126,
    "text": "それは、特定の状況に対して構築された特定のローカルモデルの文脈で、それらがどのように最良の表現に文脈的にマッピングされるかを考えることであり、それは明らかに過去の意味解析のもろさの課題に対処しようとしていることと関連していると思う。"
  },
  {
    "start": 2039228,
    "end": 2057866,
    "text": "これに対するもう1つの答えは、セマンティック構文解析の問題の1つは、マッピング関数を正しく理解することが歴史的に困難であったということです。"
  },
  {
    "start": 2057898,
    "end": 2061594,
    "text": "特定のロボット工学ドメインのためのセマンティック・パーサーを持ちたい場合。"
  },
  {
    "start": 2061642,
    "end": 2075310,
    "text": "その特定のロボット工学の領域に関する1000の文例と、その特定の領域で動作する1000のプログラムの文例が必要だ。"
  },
  {
    "start": 2075470,
    "end": 2078214,
    "text": "私たちがここで見ているのは、\"ノー \"ということだと思う。"
  },
  {
    "start": 2078332,
    "end": 2085990,
    "text": "言語を一般的に学ぶということは、言語とある種の基礎的な表現との間の一般的なマッピングを学ぶということだ。"
  },
  {
    "start": 2087130,
    "end": 2113454,
    "text": "ジョシュがレオと戦ったと言われるかもしれないが、ジョシュの強さについての情報を与えてくれるビデオを見ているかもしれない。"
  },
  {
    "start": 2113502,
    "end": 2122398,
    "text": "また、以前に見た刺激の中にあるような、過去の試合結果を示す写真も見たかもしれない。"
  },
  {
    "start": 2122494,
    "end": 2140490,
    "text": "つまり、私たちは、言語からの観察も含めて、しかし言語に優先順位をつけることなく、これらの観察がどのように首尾一貫して考察されるかを考えることができる、このような一般的な基盤が欲しいのだと思います。"
  },
  {
    "start": 2143070,
    "end": 2149466,
    "text": "意味解析のどの部分かにもよると思う。"
  },
  {
    "start": 2149568,
    "end": 2159440,
    "text": "ええ、その答えは、意味解析のどの部分が、そのパラダイムに疑問を投げかける原因となった脆さの原因だと考えるかによると思います。"
  },
  {
    "start": 2161330,
    "end": 2164842,
    "text": "もうひとつだけ視点を変えてみよう。"
  },
  {
    "start": 2164906,
    "end": 2167362,
    "text": "その一端がレオの言っていることだ。"
  },
  {
    "start": 2167496,
    "end": 2169538,
    "text": "従来の意味解析は、2つの点でもろい。"
  },
  {
    "start": 2169624,
    "end": 2173410,
    "text": "ひとつは、システムに解析可能な言語を幅広くカバーしているかということだ。"
  },
  {
    "start": 2173480,
    "end": 2179074,
    "text": "2つ目は、実際に回答できるセマンティッククエリの範囲がどれだけ広いかである。"
  },
  {
    "start": 2179192,
    "end": 2187160,
    "text": "あなたが指摘しているのは、システムが特定のことにしか答えられない、特定のことについてしか推論できないという、脆さの2つ目の原因に対処していないように思えるということです。"
  },
  {
    "start": 2188190,
    "end": 2198518,
    "text": "使用している形式的表現言語のもろさ、大規模な言語モデル表現。"
  },
  {
    "start": 2198694,
    "end": 2199322,
    "text": "そうだね。"
  },
  {
    "start": 2199456,
    "end": 2212894,
    "text": "私が思うに、AIエンジニアリングの観点からは、2つの方向に枝分かれしているようなものだ。"
  },
  {
    "start": 2212942,
    "end": 2227890,
    "text": "ひとつは、限定されたドメイン内で、さまざまなクエリに対して首尾一貫した確率的推論が可能なシステムに向けて、私たちはある程度の進歩を遂げたと思います。"
  },
  {
    "start": 2227970,
    "end": 2254238,
    "text": "この推論QLシステムのようなシステムがあり、ノンパラメトリックフェーズを使って膨大なデータテーブルを分析し、そのシステム、データのモデルを導き出す。"
  },
  {
    "start": 2254324,
    "end": 2270100,
    "text": "言語モデルを使ってSQLを解析することに人々が興奮しているのと同じようにね。"
  },
  {
    "start": 2271110,
    "end": 2294150,
    "text": "なぜなら、私たちは会話の相手が世界について首尾一貫した信念を持ち、私たちが与える新しい証拠に応じてその信念を更新し、不確実性を報告し、ある種のモーダルな判断を下せることを期待しているからだ。"
  },
  {
    "start": 2294230,
    "end": 2310666,
    "text": "そのため、エンジニアリングの道のりのひとつは、この種のシステムを利用して、より知的な人間の振る舞いに近い会話型インターフェースを構築し、SQLバックエンドと会話しているだけでは引き出せないような推論を引き出せるようにすることだ。"
  },
  {
    "start": 2310778,
    "end": 2335910,
    "text": "次のパートでお話しするのは、言語モデルが学習した表現を使って、確率的推論をより面白く、よりロバストに、よりもろくなくする方法です。"
  },
  {
    "start": 2337530,
    "end": 2339206,
    "text": "その次のパートに行こう。"
  },
  {
    "start": 2339308,
    "end": 2339960,
    "text": "そうだ。"
  },
  {
    "start": 2340810,
    "end": 2341560,
    "text": "オーケー。"
  },
  {
    "start": 2345550,
    "end": 2351582,
    "text": "時間の都合上、この例の続きは省略する。"
  },
  {
    "start": 2351716,
    "end": 2376334,
    "text": "クリスに対する答えの3つ目の部分として、私は、これがやろうとしていることの一部は、私たちが知性のモデルに求めているものに対して、言語モデル自体がもろいものであることを、答えを出さずに答えられるかもしれない方法を探ることだと思う。"
  },
  {
    "start": 2376382,
    "end": 2376702,
    "text": "そうだね。"
  },
  {
    "start": 2376776,
    "end": 2392794,
    "text": "このような質問に答えるとき、私たちがやろうとしているのは、ある種の規範的なクエリを指定することなのではないだろうか。"
  },
  {
    "start": 2392912,
    "end": 2411754,
    "text": "しかし、プリミティブの種類や、確率論的プログラムが何を表現できるかを考える方法によっては、もっと詳しく説明することができると思います。"
  },
  {
    "start": 2411882,
    "end": 2412222,
    "text": "そうだろう？"
  },
  {
    "start": 2412276,
    "end": 2426574,
    "text": "それを実現するための一つの方法として、確率論的プログラムは、それ自体が他の異なるメカニズムや認知を呼び出す他の種類の手段にアクセスできる、と考えることができるだろう。"
  },
  {
    "start": 2426622,
    "end": 2426834,
    "text": "そうだろう？"
  },
  {
    "start": 2426872,
    "end": 2444474,
    "text": "私はここで、大きな言語モデルがコントローラーであり、小さなコードのスニペットを書くタイミングや実行するタイミング、小さなプランナーを呼び出して組み込むタイミング、あるいはマインド・アイ・ワークのようなものを決定するものである、という概念との対比を描くと思うんだ。"
  },
  {
    "start": 2444512,
    "end": 2448342,
    "text": "言語モデルがある場合、物理シミュレーターを呼び出すタイミングを決める。"
  },
  {
    "start": 2448406,
    "end": 2458302,
    "text": "物理シミュレーターの出力を解釈する方法は、言語モデルのコンテキストにそれらを貼り戻し、順番に推論を試みることだ。"
  },
  {
    "start": 2458436,
    "end": 2498646,
    "text": "むしろ、このようなフレームワークが指し示すであろう方向性を見てみると、我々はすでに、3次元のシーンに対して表現力豊かな生成モデルを構築するようなことを可能にする言語を持っている、 例えば、物体の形状が互いにどのように隠れる傾向にあるかについての知識や、物理学の豊富なモデルを組み込んだり、心の理論を、自分自身の内部世界モデルについての信念を持ち、プランナーとして実際に行動を選択しているエージェントについて考える中で再帰的に行われるものとしてモデル化したりすることができる。"
  },
  {
    "start": 2498758,
    "end": 2499226,
    "text": "そうだろう？"
  },
  {
    "start": 2499328,
    "end": 2513042,
    "text": "このようなフレームワークでは、私が行うかもしれない他の種類の観察と並行して、このような種類のモデルに言語を組み込むにはどうしたらいいかという、ある種のモデルへの道を指し示すことができる。"
  },
  {
    "start": 2513096,
    "end": 2523582,
    "text": "特定の制約やイマジネーションを指定したイメージの意味について、どう考えたらいいのだろう？"
  },
  {
    "start": 2523726,
    "end": 2524226,
    "text": "そうだね。"
  },
  {
    "start": 2524328,
    "end": 2525226,
    "text": "どうぞ、ジェイコブ。"
  },
  {
    "start": 2525278,
    "end": 2526950,
    "text": "ええ、はっきりさせたいことがあったんです。"
  },
  {
    "start": 2527020,
    "end": 2538182,
    "text": "先ほど、このような意味を持つ機能があるとおっしゃっていましたが、コーデックが質問を通過することについても触れていると思います。"
  },
  {
    "start": 2538236,
    "end": 2544540,
    "text": "私はただ理解しようとしているだけなのだが、それはここなのか、それとも意味機能は後からついてくるものなのか？"
  },
  {
    "start": 2545390,
    "end": 2546714,
    "text": "それが最初の質問かもしれない。"
  },
  {
    "start": 2546752,
    "end": 2554694,
    "text": "もう1つ明確にしておきたいのは、これらのステートメントは、実際には、上部にあるテキストを促すことによって、コーデックからプログラムで作成されているだけなのでしょうか？"
  },
  {
    "start": 2554752,
    "end": 2555422,
    "text": "そう、その通りだ。"
  },
  {
    "start": 2555476,
    "end": 2562782,
    "text": "このフレームワークでは、意味関数には2つの一般化がある。"
  },
  {
    "start": 2562836,
    "end": 2564842,
    "text": "一般的な共同戦線がある。"
  },
  {
    "start": 2564906,
    "end": 2567954,
    "text": "そう、コーデックスはすでに、言語とコードの間で学んでいるんだ。"
  },
  {
    "start": 2567992,
    "end": 2574782,
    "text": "ということは、プロンプトが何であれ、それを条件とするという意味で、このような文脈特有の意味機能があるということだ。"
  },
  {
    "start": 2574846,
    "end": 2581478,
    "text": "生成モデルと、言語がどのように表現に関係するかについてのいくつかの例、それがドメインであり、意味機能であるということ。"
  },
  {
    "start": 2581564,
    "end": 2585910,
    "text": "はい、あなたが見ているこれらの例はすべて、その分布からの1サンプルです。"
  },
  {
    "start": 2589370,
    "end": 2590120,
    "text": "そうだね。"
  },
  {
    "start": 2590730,
    "end": 2603774,
    "text": "私がここで指摘したいことのひとつは、言語が定義から新しい概念を構築したり、あるいは思考から新しい世界モデルを構築したりすることの意味について考えるための枠組みや別の手段を示唆しているということだ。"
  },
  {
    "start": 2603812,
    "end": 2605646,
    "text": "最初に誰かが尋ねたようにね。"
  },
  {
    "start": 2605748,
    "end": 2613758,
    "text": "例えば、既存の構造化された関係モデルを、言語から学んだ概念で豊かにすることをどう考えるか？"
  },
  {
    "start": 2613854,
    "end": 2628600,
    "text": "例えば、親族関係の形式モデルのようなものを考えてみると、この領域の生成モデルは、それ自体が確率的な病気プログラムとして表現されていると言えるかもしれない。"
  },
  {
    "start": 2629130,
    "end": 2647002,
    "text": "それは、人々が子供を生む因果的な手段と、家系図がどのように生まれるかというこの核となる概念に関して、姉妹や父親のような存在であることの定義、あるいは定義のひとつの概念の両方を捉えている。"
  },
  {
    "start": 2647136,
    "end": 2664160,
    "text": "つまり、言語の意味とは、確率的プログラミング言語において言語が作り出す表現上の分布である、というような一般的な考え方をすれば、さまざまな種類の関係用語の定義について、どのように公式に考えることができるかを考え始めることができるだろう。"
  },
  {
    "start": 2665010,
    "end": 2668350,
    "text": "叔父とは、親の兄弟や叔母の夫のことである。"
  },
  {
    "start": 2668510,
    "end": 2671326,
    "text": "シブリング（sibling）とは、性別に関係なく、叔父や叔母を指す言葉である。"
  },
  {
    "start": 2671358,
    "end": 2682498,
    "text": "それは親の兄弟であり、父親の姉妹という関係概念である。"
  },
  {
    "start": 2682594,
    "end": 2688120,
    "text": "ここで提案したいのは、なぜ定義が必要なのかということだ。"
  },
  {
    "start": 2689370,
    "end": 2705374,
    "text": "さて、この用語の定義を学んだということが何を意味するかということの一つの考え方は、その用語を使って行うすべての下流の推論を首尾一貫したものにし、すでに持っている概念的知識に接ぎ木するということである。"
  },
  {
    "start": 2705492,
    "end": 2725880,
    "text": "だから、このような状況で、誰かのパアニや自分の小石について言及する新しい文章を直接作ることを考えてもいいし、家系図があるということはどういうことなのかという既存の概念的知識と、あなたがすでに持っているかもしれない親族関係を表す他のすべての概念的用語の両方を活用することを期待してもいい。"
  },
  {
    "start": 2727930,
    "end": 2736546,
    "text": "同じフレームワークは、言語から世界モデルを学習することの意味を形式化する一つのメカニズムも示唆している。"
  },
  {
    "start": 2736658,
    "end": 2751600,
    "text": "だから、さっきも言ったように、この綱引きゲームの話の発端となったシチュエーションに戻れば、私がこの壇上に座ったときに言った定義、つまり、人によって強さのレベルが違う人たちがいる、ということについて考えることができるかもしれない。"
  },
  {
    "start": 2752210,
    "end": 2755230,
    "text": "人には怠惰な時間の割合がある。"
  },
  {
    "start": 2755810,
    "end": 2765306,
    "text": "チームの強さはそのチームのメンバーの底力に依存し、あるチームが他のチームに勝つかどうかは、どちらのチームがより強く引っ張るかにかかっている。"
  },
  {
    "start": 2765338,
    "end": 2772082,
    "text": "このような環境での試合は、実際に言語であり、生成モデルそのものを構築しているんだ。"
  },
  {
    "start": 2772216,
    "end": 2797500,
    "text": "このようなシステムは、言語からこのような種類の理論を学習し、さらにこのようなローカルな問題ベースのコンテキストに追加して、私たちが与えたような任意の質問に答えたり、私たちが考えているこの特定の問題コンテキストにおける強さの意味に関するこのようなローカルな概念に関して、ジョシュがレオより強いというような様々な観察を条件としたりすることができる。"
  },
  {
    "start": 2798030,
    "end": 2800890,
    "text": "時間の都合上、あなたのセクションに飛びませんか？"
  },
  {
    "start": 2802770,
    "end": 2817554,
    "text": "私たちは今、自然言語がどのように解釈され、意味論的に解析され、確率論的な思考言語になるのかについて話してきた。"
  },
  {
    "start": 2817752,
    "end": 2838226,
    "text": "また、推論のためのアルゴリズムやモデル表現などのツールが、最近の言語モデルの進歩からどのような恩恵を受けるかについても触れていない。"
  },
  {
    "start": 2838418,
    "end": 2851550,
    "text": "ICMLのワークショップで発表したばかりの、自然言語と言語モデルの役割についてです。"
  },
  {
    "start": 2852610,
    "end": 2866126,
    "text": "自然言語がこの部分で何らかの役割を果たすに違いないと考える理由のひとつは、私たちが推論タスクを自らに課すことがあり、その仕様、つまり推論タスクを正しく解くとはどういうことなのかが、自然言語に関係しているに違いないということだ。"
  },
  {
    "start": 2866158,
    "end": 2873678,
    "text": "例えば、iPhoneを持っている人なら、ボイスメールを自動的に、しかしやや不完全に書き起こしてくれるビジュアルボイスメール機能を使ったことがあるかもしれない。"
  },
  {
    "start": 2873774,
    "end": 2889554,
    "text": "私がときどき直面する推論タスクは、これらのトランスクリプトに目を細めながら、正しくトランスクライブされなかった部分で、その人は何を言ったのだろう？"
  },
  {
    "start": 2889682,
    "end": 2892918,
    "text": "このボイスメールを聞く価値はあるのか？"
  },
  {
    "start": 2892934,
    "end": 2898460,
    "text": "それとも、私が見た記録の一部から、関連するすべての情報を得たと確信しているのだろうか？"
  },
  {
    "start": 2899870,
    "end": 2915860,
    "text": "なぜなら、私が行っている推論の重要な部分は、隙間の長さ、隙間に入る単語、隙間の周りの単語と意味的・構文的にどのように組み合わされるかということだからだ。"
  },
  {
    "start": 2918150,
    "end": 2923966,
    "text": "このように、推論問題の仕様が何らかの形で言語に関与しなければならない仕事は、他にもたくさんある。"
  },
  {
    "start": 2923998,
    "end": 2929220,
    "text": "詩やコードのように、ある構造的な制約に従わなければならないものを書いているのかもしれない。"
  },
  {
    "start": 2929990,
    "end": 2935398,
    "text": "もしかしたら、私たちはアドバイザーからのメッセージに戸惑い、彼らが言ったことに矛盾しないよう、さまざまな意味を推し量ろうとしているのかもしれない。"
  },
  {
    "start": 2935564,
    "end": 2941850,
    "text": "私たちは、聞き手に何らかの効果をもたらすと予測される言葉を、どのように組み合わせればいいかを考えているのかもしれない。"
  },
  {
    "start": 2942910,
    "end": 2951920,
    "text": "推論問題の中には、その仕様に言語が関与しているものもある。"
  },
  {
    "start": 2952290,
    "end": 2960830,
    "text": "ラバー・ダック・デバッグとは、自分自身やラバー・ダックに話しかけるだけで、私たちを困らせている何かをデバッグすることに成功することである。"
  },
  {
    "start": 2961970,
    "end": 2971620,
    "text": "これは、言語モデルランドにおける思考の連鎖、スクラッチパッド、そういったイノベーションの背後にある直感でもあると思う。"
  },
  {
    "start": 2972230,
    "end": 2981106,
    "text": "タスク仕様とアルゴリズムを区別している理由のひとつは、確率論的モデリングと推論において、この区別が長い間本当に重要だったからだ。"
  },
  {
    "start": 2981138,
    "end": 2987942,
    "text": "言語モデルに質問し、それが正しい答えを返してくれることを期待するようになると、私たちが失ってしまうものだと思う。"
  },
  {
    "start": 2988076,
    "end": 3001126,
    "text": "私たちの研究室がモデリングと推論で行っているような仕事では、タスク仕様を事後分布としてエンコードするモデル確率論的プログラムを個別に作成するようなものだ。"
  },
  {
    "start": 3001158,
    "end": 3008074,
    "text": "私たちは、その推論タスクを解くためのある種のアルゴリズムやストラテジーを構成的にコード化した推論プログラムからサンプリングし、それを別に作りたい。"
  },
  {
    "start": 3008122,
    "end": 3014506,
    "text": "確率論的プログラミング言語を使ってこれを行う場合、モデルと推論を分離するというアプローチをとることでいくつかの利点が得られる。"
  },
  {
    "start": 3014618,
    "end": 3020542,
    "text": "計算量が増えるにつれて、推論が事後値に近づいていくことを保証する健全性の定理があることは知っている。"
  },
  {
    "start": 3020686,
    "end": 3026910,
    "text": "私たちは、有限の計算で、推論がモデルに対してどれだけ正確であるかを測定するための自動化されたツールとテストを持っています。"
  },
  {
    "start": 3026990,
    "end": 3033586,
    "text": "また、推論プログラムのパラメータをより良い推論アルゴリズムに調整するのに役立つ勾配推定量もある。"
  },
  {
    "start": 3033698,
    "end": 3038802,
    "text": "工学的に有用な特性であるだけでなく、これらの保証は人間の認知の重要な側面を反映している。"
  },
  {
    "start": 3038866,
    "end": 3041698,
    "text": "より正確な結論に到達するために、私たちはより多くのことを考えることができる。"
  },
  {
    "start": 3041794,
    "end": 3046634,
    "text": "私たちは、現在の仮説が、世界のモデルから見て実際にどの程度理にかなっているのかを批判的に評価することができる。"
  },
  {
    "start": 3046752,
    "end": 3051660,
    "text": "同じような推論課題に繰り返し直面すれば、それを解くのがうまくなるように訓練することができる。"
  },
  {
    "start": 3052030,
    "end": 3064362,
    "text": "この図式にllmsを加えることで、さまざまな言語タスクを形式的な確率モデルとして指定し、推論アルゴリズムを強化することができるのではないか。"
  },
  {
    "start": 3064436,
    "end": 3067266,
    "text": "まずはモデリング面について話そう。"
  },
  {
    "start": 3067448,
    "end": 3073730,
    "text": "自己回帰言語モデルは、トークンのシーケンスに対する確率分布を定義することは周知の通りである。"
  },
  {
    "start": 3074150,
    "end": 3083714,
    "text": "Satソルバーを使うためには、気になる問題をSat式に落とし込む必要があるのと同じように、無条件分布をサンプリングしたいだけということはめったにない。"
  },
  {
    "start": 3083762,
    "end": 3091750,
    "text": "言語モデルを使うには、気になるタスクインスタンスからプロンプトへのリダクションが必要だ。"
  },
  {
    "start": 3092250,
    "end": 3101770,
    "text": "というのは、そのプロンプトの言語モデルの条件分布が、解決したいタスクの良い仕様、あるいは解決したいタスクの良い近似であると言っているのだ。"
  },
  {
    "start": 3101850,
    "end": 3106522,
    "text": "もちろん、脂肪の減少とは異なり、この減少は損失である。"
  },
  {
    "start": 3106666,
    "end": 3114498,
    "text": "ひとつの問題は、ある種のハードな制約を言語モデルに与えると、それに従わない可能性があることだ。"
  },
  {
    "start": 3114584,
    "end": 3121394,
    "text": "この条件付き分布pのタスクは、私たちが念頭に置いている仕様ではない。"
  },
  {
    "start": 3121432,
    "end": 3123540,
    "text": "ただ、近しいものを手に入れただけなんだ。"
  },
  {
    "start": 3127130,
    "end": 3132034,
    "text": "もう一つの問題は、この分布のエントロピーが、不確実性を有意義に反映していない可能性があることだ。"
  },
  {
    "start": 3132082,
    "end": 3149014,
    "text": "GPTの4つの論文をご覧になったことがあるかもしれませんが、多言語タスク、つまり多肢選択式のタスクで、多肢選択式の問題が出題され、RlHfや命令チューニングを行う前に、言語モデルがABCかDのどちらかを出力するよう求められています。"
  },
  {
    "start": 3149142,
    "end": 3161166,
    "text": "GPTの能力が0.4％であったすべての答えをプロットしたキャリブレーション・プロットを作成した場合、どれくらいの確率でその答えが正解だったのだろうか？"
  },
  {
    "start": 3161348,
    "end": 3173090,
    "text": "GPT4は驚くほどよく較正されている。これは、言語分布が不確かなときに、次のトークンを予測し、その分布に一致させるという、非常に優れた仕事をしているモデルから期待されることであり、損失関数がそれを物語っている。"
  },
  {
    "start": 3173160,
    "end": 3177074,
    "text": "その分布に従って、その質量、その確率の質量を配分しなければならない。"
  },
  {
    "start": 3177202,
    "end": 3179698,
    "text": "一方、RHSの後はキャリブレーションがうまくいかなかった。"
  },
  {
    "start": 3179794,
    "end": 3192060,
    "text": "これは、たとえrhsで人間的なフィードバックを提供していた人間が正しい答えを好んだとしても、予想されることだ。"
  },
  {
    "start": 3192830,
    "end": 3203050,
    "text": "RLHFによく使われる目的を持ってRLHFを行った後に得られる分布は、ある種の温度低下を生み出す。"
  },
  {
    "start": 3203130,
    "end": 3213630,
    "text": "それは、人間のフィードバックがある種の正解とぴったり一致する部分の温度を下げることに等しく、そのため過信するようになる。"
  },
  {
    "start": 3214290,
    "end": 3217066,
    "text": "これは非常に迅速な対応である。"
  },
  {
    "start": 3217098,
    "end": 3225454,
    "text": "GPTを使えば必ずこうなるとは言わないが、私はGPT 3.5を使ってこの注入作業を行った。"
  },
  {
    "start": 3225502,
    "end": 3229720,
    "text": "また、温度1で生成するたびに、基本的に同じ答えが返ってきた。"
  },
  {
    "start": 3230650,
    "end": 3243926,
    "text": "この分布を、ボイスメールに自分の知らないことが含まれているかもしれないので、それを聞くかどうかの判断を下すことができる不確実性を表していると考えたいのであれば、このPタスクはそのタスクには適していない。"
  },
  {
    "start": 3244118,
    "end": 3258174,
    "text": "私たちのアイデアは、プロンプトに還元する代わりに、言語モデルを呼び出す可能性のある確率的プログラムに還元することであり、言語モデルを呼び出す可能性のある確率的プログラムに還元することである。"
  },
  {
    "start": 3258292,
    "end": 3268174,
    "text": "時間がないのは分かっているが、これらのモデルは、言語モデルへの呼び出しと条件文やその他のロジックを混在させることができるということだ。"
  },
  {
    "start": 3268222,
    "end": 3291042,
    "text": "この注入タスクのための確率的プログラムでは、注入が必要な各空白を通過し、その場所を埋めるべきトークンの乱数をサンプリングし、それらのトークンをサンプリングし、次の種類の固定断片を観察するか、次の部分が固定断片であることを条件とするループになっている。"
  },
  {
    "start": 3291106,
    "end": 3297980,
    "text": "これは、プレフィックス・プロンプトだけでなく、空白を含むプロンプトを持つモデルを指定することができる。"
  },
  {
    "start": 3298830,
    "end": 3304126,
    "text": "どのようにサンプリングするかはまだ言っていないが、これは我々が望むタスクの仕様を定義するものだ。"
  },
  {
    "start": 3304228,
    "end": 3312110,
    "text": "同じように、私たちは言語を使って考えるようなさまざまなタスクを指定するプログラムを持っていますよね？"
  },
  {
    "start": 3312260,
    "end": 3320526,
    "text": "正式な文法を解析したり、俳句を作ったりする場合は、難しい制約を条件とすることができる。複数のプロンプトを使用して、エキスパートモデルの産物のようなものを作ることができる。"
  },
  {
    "start": 3320558,
    "end": 3324302,
    "text": "ロンドンとパリの両方にまつわる楽しい事実を考えてみたい。"
  },
  {
    "start": 3324366,
    "end": 3328574,
    "text": "ロンドンとパリの両方について楽しい事実を教えてください。"
  },
  {
    "start": 3328622,
    "end": 3338502,
    "text": "ロンドンについての楽しい事実です」という文の完成形と、「パリについての楽しい事実です」という文の完成形の両方を考え出さなければならないような、専門家モデルの製品を作ることもできる。"
  },
  {
    "start": 3338556,
    "end": 3349290,
    "text": "これは一種のハイブリッドで、両者の考え方は記号化されているが、楽しい事実やこれらの都市に関する知識は言語モデルの表現を使っている。"
  },
  {
    "start": 3349710,
    "end": 3357920,
    "text": "同様に、報酬関数に対するソフトな条件付けによって、条件付けによる報酬の舵取りや分類器の誘導を表現することができる。"
  },
  {
    "start": 3359330,
    "end": 3372274,
    "text": "このコードをセマンティック・パースしてコードに戻そうとすると、最初に書いたのと同じコードが返ってくるような、そんなグロスを生成してくれ、みたいなこともできる。"
  },
  {
    "start": 3372472,
    "end": 3376398,
    "text": "これらは様々なタスクを指定するためのモデルプログラムである。"
  },
  {
    "start": 3376494,
    "end": 3380114,
    "text": "これらの分布から実際にサンプリングするための推論アルゴリズムが必要だ。"
  },
  {
    "start": 3380242,
    "end": 3390806,
    "text": "これまでのところ、私たちは逐次モンテカルロ推論アルゴリズムに焦点を当ててきた。"
  },
  {
    "start": 3390918,
    "end": 3393658,
    "text": "多くの点で、逐次モンテカルロはビームサーチに似ている。"
  },
  {
    "start": 3393744,
    "end": 3408526,
    "text": "複数の仮説を維持し、それらを拡張し、モデル特有の方法で重み付けをし直し、そして再サンプリングする。これは、ビームサーチの一部分のようなもので、大きく拡張したビームからビームサイズに戻すダウンサンプリングのようなものだ。"
  },
  {
    "start": 3408628,
    "end": 3421374,
    "text": "ビームサーチとは異なり、逐次モンテカルロは、使用する仮説の数を増やすと、目的関数のargmaxに収束するのではなく、事後分布に収束する。"
  },
  {
    "start": 3421502,
    "end": 3423486,
    "text": "事後分布からのサンプリング。"
  },
  {
    "start": 3423678,
    "end": 3430040,
    "text": "FMCのこの種のデフォルトバージョンは、私たちが試したいくつかの簡単な作業で機能した。"
  },
  {
    "start": 3430970,
    "end": 3439986,
    "text": "例えば、私の好きな物理学者はおそらく、そして私の好きな作家もおそらく同じように、次のような補完を望むなら、SMCはリチャード・ファインマンを与えてくれる。"
  },
  {
    "start": 3440018,
    "end": 3443690,
    "text": "彼の複雑な考えを明確に伝える方法には本当に感心する。"
  },
  {
    "start": 3444510,
    "end": 3451514,
    "text": "FRBはテーパリングすると言っているが、利上げはまだ続く。"
  },
  {
    "start": 3451552,
    "end": 3454062,
    "text": "利上げはまだ何年も先のことだ、とかね。"
  },
  {
    "start": 3454116,
    "end": 3463418,
    "text": "この制約を強制するためにトークン・マスキングのようなことをすれば、言語モデルが5文字以上のものを生成するのを禁止するだけであることは注目に値する。"
  },
  {
    "start": 3463514,
    "end": 3465610,
    "text": "いろいろな種類の変なコンプリートができる。"
  },
  {
    "start": 3465770,
    "end": 3467194,
    "text": "後追いとは違う。"
  },
  {
    "start": 3467242,
    "end": 3473474,
    "text": "ここでは、5文字以上の単語を使わないと完成しないイディオムを設定した完成形とかがある。"
  },
  {
    "start": 3473512,
    "end": 3473762,
    "text": "そうだろう？"
  },
  {
    "start": 3473816,
    "end": 3477490,
    "text": "そうすると、非常に混乱して、ドット、ドット、ドット、もっと読む、とか書くんだ。"
  },
  {
    "start": 3477560,
    "end": 3481480,
    "text": "文が早めに切れるような文脈を考え出そうとするのだ。"
  },
  {
    "start": 3482250,
    "end": 3491510,
    "text": "インフィリングのタスクでは、意味的にも構文的にもテキストに適合するさまざまなサンプルが得られる。"
  },
  {
    "start": 3491580,
    "end": 3494474,
    "text": "インフィリングのタスクは、これよりはるかに難しくすることができる。"
  },
  {
    "start": 3494512,
    "end": 3498246,
    "text": "私は、この方法がまだこれらの影響力のある仕事をすべて解決していると主張したいわけではない。"
  },
  {
    "start": 3498278,
    "end": 3502554,
    "text": "より難しいタスクでは、よりファンシーな逐次モンテカルロ・アルゴリズムを使う必要がありそうだ。"
  },
  {
    "start": 3502602,
    "end": 3506142,
    "text": "この2つのステップは、実際にはさまざまな方法で拡張することができる。"
  },
  {
    "start": 3506196,
    "end": 3513902,
    "text": "より良いプロポーザル分布と、推測を試みるより良い再ウェイト戦略を使うことができる。"
  },
  {
    "start": 3513956,
    "end": 3516754,
    "text": "さて、良いサンプルを手に入れる道筋は見えてきたかな？"
  },
  {
    "start": 3516872,
    "end": 3530886,
    "text": "私たちは、制約に沿った良いものを提案したり、制約に陥りそうかどうかを識別したりするために、すでに文献で開発されているテクニックが、ここに入れる良い材料になると考えています。"
  },
  {
    "start": 3530988,
    "end": 3538294,
    "text": "重要なのは、そしてこれが最後の最後なのだが、重要なのは、それらすべてが推論プログラムの一部になるということだ。"
  },
  {
    "start": 3538492,
    "end": 3545002,
    "text": "パーティクルの数を増やしても、モデルの元の仕様をターゲットにしているという保証がある。"
  },
  {
    "start": 3545136,
    "end": 3552442,
    "text": "ヒューリスティックやバイアスのすべてを盲目的に信頼するのではなく、そのテクニックの鍵を手渡すのでもない。"
  },
  {
    "start": 3552586,
    "end": 3554638,
    "text": "我々にはまだ理解できる仕様がある。"
  },
  {
    "start": 3554804,
    "end": 3556446,
    "text": "オーケー、そこまでにしよう。"
  },
  {
    "start": 3556548,
    "end": 3557200,
    "text": "ありがとう。"
  },
  {
    "start": 3562210,
    "end": 3565646,
    "text": "時間が少し遅れているので、切実な質問がなければ。"
  },
  {
    "start": 3565748,
    "end": 3566654,
    "text": "燃える質問？"
  },
  {
    "start": 3566772,
    "end": 3567710,
    "text": "燃えるような質問はない。"
  },
  {
    "start": 3567780,
    "end": 3575390,
    "text": "お茶を飲んで休憩しよう。ザカリー、トルタ、そして君たちはスピーカーと話してくれ。"
  },
  {
    "start": 3575690,
    "end": 3579460,
    "text": "1130に、私たちは行く。"
  }
]