[
  {
    "start": 250,
    "end": 4378,
    "text": "さて皆さん、Twiml AIポッドキャストの別のエピソードへようこそ。"
  },
  {
    "start": 4474,
    "end": 6730,
    "text": "私はホスト役のサム・シャリントンだ。"
  },
  {
    "start": 6890,
    "end": 9114,
    "text": "今日はラム・スリ・ハルシャに登場してもらいます。"
  },
  {
    "start": 9162,
    "end": 11962,
    "text": "ラムはパインコーンのエンジニアリング担当副社長。"
  },
  {
    "start": 12106,
    "end": 17390,
    "text": "番組を始める前に、今日の番組をお聞きの方は、ぜひ購読ボタンを押してください。"
  },
  {
    "start": 17540,
    "end": 19470,
    "text": "ラム、ポッドキャストへようこそ。"
  },
  {
    "start": 20290,
    "end": 21546,
    "text": "サム、呼んでくれてありがとう。"
  },
  {
    "start": 21578,
    "end": 22846,
    "text": "ここに来られて嬉しいよ。"
  },
  {
    "start": 23028,
    "end": 24606,
    "text": "出演してくれてありがとう。"
  },
  {
    "start": 24628,
    "end": 26614,
    "text": "あなたとおしゃべりするのを楽しみにしています。"
  },
  {
    "start": 26652,
    "end": 32790,
    "text": "ベクター・データベースや検索、拡張世代など、あらゆることについて話すつもりだ。"
  },
  {
    "start": 33690,
    "end": 39690,
    "text": "その前に、あなたの生い立ちとAIに携わるようになったきっかけを少し聞かせてください。"
  },
  {
    "start": 40190,
    "end": 40938,
    "text": "いい質問だね。"
  },
  {
    "start": 41024,
    "end": 45450,
    "text": "別の人生で、私は理論物理学の博士号を取得した。"
  },
  {
    "start": 45950,
    "end": 52090,
    "text": "そこから違う分野に移って、ゴールドマンではファイナンスの仕事をしていました。"
  },
  {
    "start": 52510,
    "end": 55626,
    "text": "そこから私は西海岸のジョン・ヤフーに移った。"
  },
  {
    "start": 55738,
    "end": 60030,
    "text": "これが2010年頃で、2014年頃までそこにいた。"
  },
  {
    "start": 60450,
    "end": 68498,
    "text": "それがビッグデータ・システム、大規模データ処理、機械学習に触れた最初のきっかけだった。"
  },
  {
    "start": 68664,
    "end": 69326,
    "text": "ヤフーで"
  },
  {
    "start": 69358,
    "end": 75490,
    "text": "私は機械学習周辺のさまざまな分野で働き、最終的にはヤフーのリサーチ部門でスケーラブルな機械学習に取り組むことになった。"
  },
  {
    "start": 75640,
    "end": 84680,
    "text": "そこから時間をかけてdatabricksに入社し、Sparkの開発に時間を費やしながら、ゲノミクスなどの新しい取り組みも始めました。"
  },
  {
    "start": 85290,
    "end": 89062,
    "text": "そこからsplunkに移って、機械学習の研究グループを率いることになりました。"
  },
  {
    "start": 89116,
    "end": 89574,
    "text": "そこだ。"
  },
  {
    "start": 89692,
    "end": 95322,
    "text": "そんなことをやっているうちに、いつしか自分の会社を立ち上げたいと思うようになった。"
  },
  {
    "start": 95456,
    "end": 107040,
    "text": "そうしようと考えていたときに、パインコーンのCEOである江戸と連絡を取り、話を始めて、私たちは非常によく似た重複する問題を解決しようとしていることに気づいた。"
  },
  {
    "start": 107410,
    "end": 113574,
    "text": "そうして、私はまたどちらかと手を組み、松ぼっくりに参加することになったんだ。"
  },
  {
    "start": 113722,
    "end": 133560,
    "text": "ヤフーがビッグデータ・インフラや、検索や広告をサポートする機械学習の初期の商業的な利用法の開発という点で、どれほど巨大な存在であったかを、おそらく多くの視聴者は知らないだろうと思う。"
  },
  {
    "start": 134090,
    "end": 134694,
    "text": "もちろんだ。"
  },
  {
    "start": 134812,
    "end": 144214,
    "text": "クラウドシステム、機械学習、オンライン機械学習などにおける最高の仕事のいくつかは、ヤフーから生まれたものだと思う。"
  },
  {
    "start": 144342,
    "end": 149546,
    "text": "ヤフーで働いていた人たちは、やがてグーグルなどに行った。"
  },
  {
    "start": 149728,
    "end": 163840,
    "text": "ビッグデータ・システムや機械学習などに取り組むために必要な基礎について多くを学んだ場所だった。"
  },
  {
    "start": 165090,
    "end": 173106,
    "text": "もちろん、ベクター・データベースについて、そしてなぜベクター・データベースが急に面白くなったかについて話すつもりだ。"
  },
  {
    "start": 173208,
    "end": 179090,
    "text": "あなたはパインコーンのCEOの創業者とつながりがあるとおっしゃっていましたね。"
  },
  {
    "start": 179930,
    "end": 184246,
    "text": "これはラグやLLMよりも前の話ですか？"
  },
  {
    "start": 184268,
    "end": 188346,
    "text": "Pineconeは以前からベクターデータベースに取り組んでいたのですか？"
  },
  {
    "start": 188528,
    "end": 188922,
    "text": "そうだ。"
  },
  {
    "start": 188976,
    "end": 192860,
    "text": "松ぼっくりは数年前からベクターデータベースに取り組んでいる。"
  },
  {
    "start": 193230,
    "end": 198058,
    "text": "私自身、2年6カ月ほど前に松ぼっくりに加入した。"
  },
  {
    "start": 198144,
    "end": 198634,
    "text": "オーケー。"
  },
  {
    "start": 198752,
    "end": 212110,
    "text": "今、ラグとllmは非常に人気が出てきているが、ラグとllmの研究開発は以前から行われていた。"
  },
  {
    "start": 212260,
    "end": 219540,
    "text": "言語モデルはしばらく前からどんどん大きくなってきているので、ここに行き着くのは当然の成り行きだった。"
  },
  {
    "start": 219910,
    "end": 223854,
    "text": "llmsのことは知っていたし、言語モデルのトレンドも知っていた。"
  },
  {
    "start": 223902,
    "end": 236630,
    "text": "ベクター・データベースがこのようなフローにどのように利用できるかなどについては、すでに考えていた。"
  },
  {
    "start": 237390,
    "end": 257790,
    "text": "ベクター・データベースが突然脚光を浴び、以前はベクター・データベースの導入など考えもしなかったような人たちが、ベクター・データベースとは何なのか、どう使えばいいのかを理解しようとしている。"
  },
  {
    "start": 257940,
    "end": 260698,
    "text": "その進化について、あなたの視点から教えてください。"
  },
  {
    "start": 260794,
    "end": 266850,
    "text": "ベクター・データベースが注目されているが、それは適切なのか？"
  },
  {
    "start": 268070,
    "end": 272674,
    "text": "ベクター・データベースの役割とは？"
  },
  {
    "start": 272872,
    "end": 274274,
    "text": "いい質問だね。"
  },
  {
    "start": 274472,
    "end": 279462,
    "text": "ベクター・データベースの話をする前に、少し文脈を整理しておく必要があるだろう。"
  },
  {
    "start": 279516,
    "end": 279830,
    "text": "そうだね。"
  },
  {
    "start": 279900,
    "end": 286962,
    "text": "chat、GPT、そしてそのようなモデルを私たちは大規模言語モデルと呼んでいる。"
  },
  {
    "start": 287106,
    "end": 290578,
    "text": "これらの大規模な言語モデルは、実際には単なる配列対配列モデルである。"
  },
  {
    "start": 290594,
    "end": 293370,
    "text": "彼らは一連のテキストを受け取り、一連のテキストを生成する。"
  },
  {
    "start": 293440,
    "end": 296166,
    "text": "今は、さまざまな仕事にこれを使うことができる。"
  },
  {
    "start": 296198,
    "end": 302474,
    "text": "要約に使うもよし、検索に使うもよし、文字通り新しい使い方ができる。"
  },
  {
    "start": 302512,
    "end": 304430,
    "text": "毎日のように事件が起きている。"
  },
  {
    "start": 304930,
    "end": 308158,
    "text": "これらのモデルを非常に強力なものにしているのは、これらのモデルが本当に大きいということだ。"
  },
  {
    "start": 308324,
    "end": 314066,
    "text": "ある意味、世界の知識をパラメータに埋め込んでいるわけだから、多くのデータで訓練されている。"
  },
  {
    "start": 314248,
    "end": 324290,
    "text": "言語の構造そのものと、推論する能力の両方をパラメータに埋め込んで、使うことができるのだ。"
  },
  {
    "start": 324440,
    "end": 330214,
    "text": "彼らが本当にパワフルなのは、特別なトレーニングを受けていない仕事にも使えるようになったことだ。"
  },
  {
    "start": 330332,
    "end": 333122,
    "text": "これこそが、彼らを強力にしたのだ。"
  },
  {
    "start": 333266,
    "end": 346134,
    "text": "さて、最近の大規模言語モデルについて考える最善の方法は、それらが新たな生成AIアプリケーションのインテリジェンス層またはオーケストレーション層であるということだ。"
  },
  {
    "start": 346262,
    "end": 352750,
    "text": "つまり、インテリジェントなAIフローを実際にオーケストレーションするために使えるということだ。"
  },
  {
    "start": 353170,
    "end": 355600,
    "text": "物事を推論するのにも使える。"
  },
  {
    "start": 357410,
    "end": 360218,
    "text": "インテリジェンス層とオーケストレーション層として機能する。"
  },
  {
    "start": 360314,
    "end": 361834,
    "text": "何かが欠けている。"
  },
  {
    "start": 361962,
    "end": 367250,
    "text": "LLMだけ、大規模な言語モデルだけでは、知識層と呼ばれるものが欠けている。"
  },
  {
    "start": 367750,
    "end": 373422,
    "text": "あなたに欠けているのは、多くの知識を必要とする仕事をこなすために必要な実際の知識である。"
  },
  {
    "start": 373566,
    "end": 376034,
    "text": "ベクターデータベースの出番だ。"
  },
  {
    "start": 376152,
    "end": 384006,
    "text": "さて、大規模な言語モデルは、ある程度世界の知識をパラメータに埋め込んでいますよね。"
  },
  {
    "start": 384108,
    "end": 385954,
    "text": "知識がないわけではない。"
  },
  {
    "start": 386082,
    "end": 390410,
    "text": "彼らは世界の知識を持っているし、その知識に基づいて推論する能力もある。"
  },
  {
    "start": 390560,
    "end": 400410,
    "text": "彼らに欠けているのは、正確で関連性のある知識にアクセスする能力であり、ベクター・データベースがそれを提供する。"
  },
  {
    "start": 400560,
    "end": 411230,
    "text": "さて、ベクターデータベースがどのようにこれを提供するのかを説明するには、少し離れて、このワークフロー全体のもう一つの部分、つまり検索について話す必要がある。"
  },
  {
    "start": 411730,
    "end": 418642,
    "text": "関連性のある正確な知識を検索するということであれば、これは以前から人々が行ってきたことだ。"
  },
  {
    "start": 418776,
    "end": 423506,
    "text": "これは、大規模な言語モデルができる以前からやっていたことだよね？"
  },
  {
    "start": 423528,
    "end": 432902,
    "text": "グーグルのような検索エンジンについて考えてみると、例えば、テキスト文書を対象としたあらゆる検索エンジンについて考えてみると、それがそうだ。"
  },
  {
    "start": 433036,
    "end": 443718,
    "text": "検索エンジンのようなものに文書を入れて、それについて質問したり、検索したりすることができる。"
  },
  {
    "start": 443814,
    "end": 453834,
    "text": "情報検索の分野は非常に豊かで、キーワードの一致に基づいてテキスト文書を検索するという古典的な意味では非常によく理解されている。"
  },
  {
    "start": 453882,
    "end": 456346,
    "text": "これは非常によく理解されていることだ。"
  },
  {
    "start": 456458,
    "end": 471006,
    "text": "この半世紀ほどの間に、新しいタイプの検索がすでに行われており、ベクトル・データベースはそのためにすでに使われている。"
  },
  {
    "start": 471198,
    "end": 481686,
    "text": "コーパスにクエリにマッチするキーワードが含まれているかどうか、そしてそれらのキーワードがクエリ自体とどの程度関連しているかを調べる。"
  },
  {
    "start": 481868,
    "end": 485510,
    "text": "これが情報検索の伝統的な焦点であった。"
  },
  {
    "start": 485930,
    "end": 510686,
    "text": "最近、つまりここ5、7年の間に起こったことは、これらの文書をニューラル・ネットワーク、いわゆる埋め込みモデルを使って埋め込み、それをベクトル・データベースのようなものに入れれば、より多くの、より優れた検索が可能になるということに人々が気づいたということだ。"
  },
  {
    "start": 510788,
    "end": 513214,
    "text": "ベクター・データベースは実際に何をするのか？"
  },
  {
    "start": 513332,
    "end": 515986,
    "text": "検索エンジンの進化形として機能しているんだろう？"
  },
  {
    "start": 516088,
    "end": 528982,
    "text": "検索エンジンは、テキストをキーワードのかたまりに分割して検索するのではなく、投稿リストを作成し、それを検索する。"
  },
  {
    "start": 529116,
    "end": 532546,
    "text": "ベクトル・データベースでは、検索は異なる方法で行われる。"
  },
  {
    "start": 532738,
    "end": 536278,
    "text": "何が起こるかというと、この同じテキストを使って、それを切り刻むのだ。"
  },
  {
    "start": 536284,
    "end": 540658,
    "text": "各チャンクをニューラルネットワークで埋め込み、エンベッディングを生成する。"
  },
  {
    "start": 540754,
    "end": 546202,
    "text": "エンベッディングは、浮動小数点数の配列と考えればいいんだよね？"
  },
  {
    "start": 546336,
    "end": 551462,
    "text": "これらの埋め込みを、ベクトル・データベースのようにデータベース化する。"
  },
  {
    "start": 551606,
    "end": 559838,
    "text": "ベクトル・データベースは、クエリが与えられたら、そのクエリもエンコードする、という次のような問いに答えることができるユニークな立場にある。"
  },
  {
    "start": 559924,
    "end": 578040,
    "text": "この特定の相互作用が、クエリに最も関連する文書を検索し、セマンティック検索と呼ばれるものを実行します。"
  },
  {
    "start": 578650,
    "end": 580150,
    "text": "意味検索。"
  },
  {
    "start": 580810,
    "end": 591670,
    "text": "つまり、現代の情報検索は、正確で関連性のある知識を検索する中核としてベクトル・データベースを使用しているのである。"
  },
  {
    "start": 591750,
    "end": 615890,
    "text": "私の観察によると、チャットやGPTなどに対するLMSや関心がAIの分野を急速に拡大するにつれ、チャットボットを機能させるためのアプローチとしてのラグと、検索や情報検索との関連性が、人々に十分に理解されていないようだ。"
  },
  {
    "start": 616550,
    "end": 632966,
    "text": "私が見てきた中で、ボロボロのデモから実際に役立つものにすることに最も成功している人たちは、検索や関連性の問題に取り組んだ経験が豊富だ。"
  },
  {
    "start": 633148,
    "end": 645354,
    "text": "適切な情報をタイムリーに提供することで、ユーザーが実際に使いたくなるようなチャットボットを作るには、こうした知識がすべて活きてくる。"
  },
  {
    "start": 645472,
    "end": 647370,
    "text": "同じようなものがいくつか見える？"
  },
  {
    "start": 647520,
    "end": 648122,
    "text": "もちろんだ。"
  },
  {
    "start": 648256,
    "end": 660794,
    "text": "実際、検索、関連性、ランキングなどに費やされてきたこれらの研究は、大規模な言語モデルとともに、知識を与えるために利用されているのだ。"
  },
  {
    "start": 660922,
    "end": 665326,
    "text": "その話をしたとき、私たちは大規模な言語モデルには知識がないと言った。"
  },
  {
    "start": 665518,
    "end": 667390,
    "text": "知識は検索から生まれる。"
  },
  {
    "start": 667550,
    "end": 679590,
    "text": "だから、リトリーバル・アーキテド・ジェネレーション・ラグ・ワークフローを最もうまく使いこなすことができるのは、最高のリトリーバルをすることに本当に投資している人たちだということがわかる。"
  },
  {
    "start": 679930,
    "end": 681880,
    "text": "別の言い方をすれば"
  },
  {
    "start": 683130,
    "end": 697222,
    "text": "ラグをベースにしたチャットボットは、クエリに基づいてドキュメントを検索し、そのクエリに最も関連性の高い結果を取得または引き出すと考えることができます。"
  },
  {
    "start": 697286,
    "end": 713930,
    "text": "LLMの役割は、それを要約してユーザーに提示することです。ですから、検索がうまくいかないと、ガーベッジ・イン、ガーベッジ・アウトのような問題が発生し、チャットボットは有益な結果を出すことができません。"
  },
  {
    "start": 714090,
    "end": 714846,
    "text": "もちろんだ。"
  },
  {
    "start": 715028,
    "end": 722638,
    "text": "それよりももっと面白いのは、llmsは大量の世界知識に基づいて訓練されていると言ったのを覚えているだろうか、ということだ。"
  },
  {
    "start": 722814,
    "end": 732806,
    "text": "同じ世界の知識を、このようなベクトル・データベースに入れれば、検索が本当にうまくいけば、すでに訓練された知識でさえもllmsをより良くすることができる。"
  },
  {
    "start": 732908,
    "end": 741450,
    "text": "彼らがアクセスできなかった知識、たとえばあなた自身の文書や企業の法的文書など、彼らが決してアクセスできないようなものは忘れてください。"
  },
  {
    "start": 741520,
    "end": 741850,
    "text": "そうだね。"
  },
  {
    "start": 741920,
    "end": 747030,
    "text": "すでにアクセスしたことのある知識であっても、検索は実際に良い結果をもたらす。"
  },
  {
    "start": 747190,
    "end": 760362,
    "text": "つまり、LLMと検索というのは、2つのアルゴリズムがあるということですね。"
  },
  {
    "start": 760426,
    "end": 771646,
    "text": "1つは、一種のシーケンスからシーケンスへの次のトークン予測アルゴリズムであり、もう1つは埋め込みベースの情報検索アルゴリズムである。"
  },
  {
    "start": 771758,
    "end": 779086,
    "text": "あなたは本質的に、エンベディングに基づく情報検索は、知識生産においてシークエンス・トゥ・シークエンスよりも優れていると言っている。"
  },
  {
    "start": 779198,
    "end": 781526,
    "text": "それは強すぎる発言だろうか。"
  },
  {
    "start": 781708,
    "end": 796314,
    "text": "つまり、llmsのようなsequence to sequenceモデルとベクトルデータベースによる情報検索を併用することは、llmsを単独で使うよりも、あるいはllmsを微調整するよりも、厳密には優れているということだ。"
  },
  {
    "start": 796352,
    "end": 806970,
    "text": "LLMをベクトル・データベースのような検索エンジンの文脈で使うほど、純粋に自分のデータでLLMを再トレーニングするだけでは価値はない。"
  },
  {
    "start": 807130,
    "end": 808522,
    "text": "これがボロというものだ。"
  },
  {
    "start": 808586,
    "end": 817250,
    "text": "ラグとは、この2つの組み合わせであり、知識を必要とするこの種の仕事に知識を提供する最良の方法なのだ。"
  },
  {
    "start": 820230,
    "end": 841946,
    "text": "言語やフレームワーク、データベースの種類に関係なく、コードのウォークスルーやブログ記事など、デモを作成するのに役立つものがたくさんある。"
  },
  {
    "start": 841968,
    "end": 847318,
    "text": "実証するのはとても簡単だけど、それはほんの始まりに過ぎない。"
  },
  {
    "start": 847414,
    "end": 854442,
    "text": "ユーザーの前に出したいものを出すのは、もっと複雑なことだ。"
  },
  {
    "start": 854506,
    "end": 859406,
    "text": "そのような複雑な問題や、人々が直面する課題についてお聞かせください。"
  },
  {
    "start": 859588,
    "end": 862170,
    "text": "ああ、いろいろな複雑な事情があると思う。"
  },
  {
    "start": 862250,
    "end": 873250,
    "text": "つまり、数百の文書や少数の文書でデモを作成し、その上で検索を行うということです。"
  },
  {
    "start": 873400,
    "end": 876822,
    "text": "何十億ものベクトルを検索するのは、まったく別の問題だ。"
  },
  {
    "start": 876956,
    "end": 888666,
    "text": "パインコーンのように、ベクター・データベースの構築と完成にほとんどの時間を費やしている人たちがいるのはそのためだ。"
  },
  {
    "start": 888848,
    "end": 895030,
    "text": "もうひとつは、単純なデモ以外では、コーパスは進化するということだ。"
  },
  {
    "start": 895110,
    "end": 908160,
    "text": "人々は文書を追加したり、削除したり、編集したり、再編集したりするわけで、最新の情報をこのようなボロいワークフローで利用できるようにするのは、これまた非常に難しい問題である。"
  },
  {
    "start": 910210,
    "end": 916930,
    "text": "ベクター・データベースのデータベースの部分は、このような流れの中で、本当にデータベースのように機能するからだ。"
  },
  {
    "start": 917350,
    "end": 924866,
    "text": "要するに、エンタープライズ・データ・ストアをこのエンベッディング・ストアに同期させるのだ。"
  },
  {
    "start": 925048,
    "end": 925780,
    "text": "その通りだ。"
  },
  {
    "start": 926150,
    "end": 928870,
    "text": "シンクは歴史的に本当に難しい。"
  },
  {
    "start": 929210,
    "end": 932050,
    "text": "非常に難しいが、特にベクターデータベースは難しい。"
  },
  {
    "start": 932130,
    "end": 937734,
    "text": "伝統的なデータベースは、インデックスを常に新鮮に保つなど、このようなことが得意になってきたと思う。"
  },
  {
    "start": 937852,
    "end": 941010,
    "text": "ベクター・データベースのインデックスは、新鮮さを保つのがとてもとても難しい。"
  },
  {
    "start": 941170,
    "end": 943418,
    "text": "実際、この問題を解決した人はほとんどいない。"
  },
  {
    "start": 943504,
    "end": 944122,
    "text": "なぜですか？"
  },
  {
    "start": 944176,
    "end": 945210,
    "text": "これはどうやって保存しているのですか？"
  },
  {
    "start": 945280,
    "end": 951578,
    "text": "最大の問題は、インデックスを段階的に構築できれば、インデックスの鮮度を保つのが簡単だということだと思う。"
  },
  {
    "start": 951674,
    "end": 952030,
    "text": "そうだろう？"
  },
  {
    "start": 952100,
    "end": 952430,
    "text": "オーケー。"
  },
  {
    "start": 952500,
    "end": 959374,
    "text": "ベクトル・データベースの場合、実際にインクリメンタルにインデックスを構築するアルゴリズムはほとんど存在しませんよね。"
  },
  {
    "start": 959492,
    "end": 979442,
    "text": "というのも、ある意味、ある文書をエンコードしてこの種のデータベースに入れたとき、空間の小さな領域にある他の文書との接続を少しずつ追加していくことはできないからです。そうすると、クエリが来たときに、クエリはこの空間の領域に新しい文書があることを知らないので、それを探らなければなりませんよね？"
  },
  {
    "start": 979576,
    "end": 985286,
    "text": "クエリがそれを知るためにすべてをスキャンしなければならないか、クエリがそのドキュメントを見逃すかのどちらかである。"
  },
  {
    "start": 985388,
    "end": 987334,
    "text": "どちらも良くないよね。"
  },
  {
    "start": 987372,
    "end": 991590,
    "text": "もしクエリーがコーパス全体をスキャンしなければならないのであれば、それは実際には計算不可能である。"
  },
  {
    "start": 991670,
    "end": 992202,
    "text": "そうだろう？"
  },
  {
    "start": 992336,
    "end": 1001002,
    "text": "一方、クエリがすでにインデックス化されているか、事前に計算されている場所しか見ていない場合、ここに新しいドキュメントが追加されたことを知ることはできない。"
  },
  {
    "start": 1001136,
    "end": 1007214,
    "text": "このようなベクトル・インデックスを新鮮に保つ作業は、基本的に難しい問題である。"
  },
  {
    "start": 1007332,
    "end": 1007662,
    "text": "いいかい？"
  },
  {
    "start": 1007716,
    "end": 1012190,
    "text": "これは、既知のベクトル探索アルゴリズムすべてにとって難しい問題である。"
  },
  {
    "start": 1012260,
    "end": 1033960,
    "text": "私たちがこの半年、実際には1年近くを費やして考え、解決し、突き止めたのは、これらの問題は、最先端のアルゴリズムや検索アルゴリズムを実装するようなフロンティア研究の問題なのか、それともエンジニアリングの問題なのか、ということだった。"
  },
  {
    "start": 1034410,
    "end": 1037670,
    "text": "これは研究とエンジニアリングの両面を兼ね備えている。"
  },
  {
    "start": 1040410,
    "end": 1062766,
    "text": "少なくともパインコーンではそうだが、一般的なやり方として、私たちは最先端を行き、いろいろなことを研究し、たくさんのデータセットでテストし、何がうまくいき、何がうまくいかないかを見極め、そして、ほとんどのユースケースでうまくいき、多くの価値を提供するものができたと十分に確信が持てるようになったら、エンジニアリングを開始し、建造を開始する。"
  },
  {
    "start": 1062868,
    "end": 1071554,
    "text": "同時に、私たちはインデックスの鮮度を保つなどの研究を続けています。"
  },
  {
    "start": 1071592,
    "end": 1078758,
    "text": "この方法で世界トップクラスのベクター・データベースを構築するためには、今後数年間でやらなければならないことがたくさんある。"
  },
  {
    "start": 1078844,
    "end": 1082790,
    "text": "研究とエンジニアリングのバランスを取ることだ。"
  },
  {
    "start": 1083290,
    "end": 1091930,
    "text": "私たちが直面している課題をまとめると、規模に伴う基本的なインフラの課題がある。"
  },
  {
    "start": 1092270,
    "end": 1097930,
    "text": "インデックスの鮮度を保つことには課題がある。"
  },
  {
    "start": 1098590,
    "end": 1099478,
    "text": "規模が大きい。"
  },
  {
    "start": 1099574,
    "end": 1101942,
    "text": "規模が大きくなれば、他にもいくつかの課題がある。"
  },
  {
    "start": 1102006,
    "end": 1105214,
    "text": "ところで、コスト面だけでも課題はありますよね？"
  },
  {
    "start": 1105252,
    "end": 1115170,
    "text": "今日のジェネレーティブAIのワークフローの多く、あるいはジェネレーティブAIアプリケーションの構築を考えている人は、コストについて心配しなければならない。"
  },
  {
    "start": 1115510,
    "end": 1120782,
    "text": "高価な推論エンドポイントにぶつかったからだ。"
  },
  {
    "start": 1120846,
    "end": 1121854,
    "text": "それとも他に理由があるのだろうか？"
  },
  {
    "start": 1121902,
    "end": 1122546,
    "text": "その通りだ。"
  },
  {
    "start": 1122728,
    "end": 1123954,
    "text": "その一部でしょう？"
  },
  {
    "start": 1123992,
    "end": 1125406,
    "text": "例えば、OpenAIのエンドポイント。"
  },
  {
    "start": 1125438,
    "end": 1130790,
    "text": "実際、OpenAIはトークンあたりのコストを下げるために多くのことを行ってきたが、それでもまだ非常に高価だ。"
  },
  {
    "start": 1131130,
    "end": 1140950,
    "text": "そして、これらのオープンソースモデルがあり、私は今日、それらを少なくとも弱小モデルと呼んでいる。"
  },
  {
    "start": 1141030,
    "end": 1141322,
    "text": "そうだろう？"
  },
  {
    "start": 1141376,
    "end": 1147260,
    "text": "知識集約型のタスクの場合、人々は当然、最高のモデルを使いたがる。"
  },
  {
    "start": 1147870,
    "end": 1154974,
    "text": "ここでもまた、より多くのコンテキストを追加したり、実際にベクターデータベースなどを活用することで、弱いモデルをより強力にすることができる。"
  },
  {
    "start": 1155092,
    "end": 1166058,
    "text": "私たちは多くの時間を費やして、コストのかかるアプリケーションをどうすれば10倍安くできるかを考え、以前はできなかったような新しいユースケースを実現できるようにした。"
  },
  {
    "start": 1166244,
    "end": 1171970,
    "text": "あなたの質問に対して私が聞き逃した最後の部分は、インフラとコストです。"
  },
  {
    "start": 1172040,
    "end": 1174980,
    "text": "さらに大きな問題がある。"
  },
  {
    "start": 1175750,
    "end": 1179042,
    "text": "さて、歴史的に見ると、イルムは幻覚と闘ってきた。"
  },
  {
    "start": 1179106,
    "end": 1181254,
    "text": "また、アトリビューションなどにも苦労しているようですね。"
  },
  {
    "start": 1181292,
    "end": 1188886,
    "text": "忠実さとは、実際に彼らが知っている文書に忠実なのか、それともあなたが彼らに提供した文脈に忠実なのか、ということだ。"
  },
  {
    "start": 1188908,
    "end": 1198970,
    "text": "このようなことを測定し、実際に正しいラグ・アプリケーションを構築し、自信を持って答えを出せるようにするのだ。"
  },
  {
    "start": 1199120,
    "end": 1201066,
    "text": "それも大きな課題のひとつだと思う。"
  },
  {
    "start": 1201088,
    "end": 1205630,
    "text": "ここでも、ベクターデータベースやラグなどが大いに役立つ。"
  },
  {
    "start": 1206290,
    "end": 1209562,
    "text": "これらはすべて、同じように重要な課題だと私は考えている。"
  },
  {
    "start": 1209706,
    "end": 1214226,
    "text": "ロン、これがみんなが直面しているハイレベルな課題なんだ。"
  },
  {
    "start": 1214328,
    "end": 1224414,
    "text": "私がよく耳にするのは、埋め込みモデルの選択とチャンキング戦略だ。"
  },
  {
    "start": 1224462,
    "end": 1234418,
    "text": "ボロ・アプリケーションを構築し、展開する際に、誰かが決断しなければならない小さなことがたくさんあるのだろう。"
  },
  {
    "start": 1234514,
    "end": 1239538,
    "text": "これらの重要性をどのように感じていますか？"
  },
  {
    "start": 1239564,
    "end": 1245610,
    "text": "これらが重要な考慮事項なのか、それとも二次的な考慮事項なのかについては、賛否両論あるようだ。"
  },
  {
    "start": 1246030,
    "end": 1247340,
    "text": "いい質問だね。"
  },
  {
    "start": 1247790,
    "end": 1250598,
    "text": "だから、まず第一に、これらは重要な考慮事項である。"
  },
  {
    "start": 1250774,
    "end": 1261870,
    "text": "今起きていることは、人々が手近にある簡単なものに手を伸ばそうとしているということだと思う。"
  },
  {
    "start": 1262690,
    "end": 1272530,
    "text": "例えば、LLMにOpenAIを使うのであれば、OpenAIのエンベッディング・モデルや最新のエンベッディング・モデルを使うのは理にかなっている。"
  },
  {
    "start": 1273830,
    "end": 1280040,
    "text": "私たちがよく目にするのは、人々はただ手近なものを使っているということだ。"
  },
  {
    "start": 1280810,
    "end": 1283654,
    "text": "しかし、埋め込みモデルの選択は非常に重要である。"
  },
  {
    "start": 1283772,
    "end": 1285622,
    "text": "実は、いろいろな意味で重要なことなんだ。"
  },
  {
    "start": 1285756,
    "end": 1292666,
    "text": "より小型で安価なエンベデッドモデルが1つあれば、実際に同じ仕事ができるかもしれない。"
  },
  {
    "start": 1292848,
    "end": 1294700,
    "text": "そういう意味で重要だ。"
  },
  {
    "start": 1295150,
    "end": 1302042,
    "text": "別の意味では、あなたがやろうとしている特定のことのために微調整されたモデルを埋め込むことは、実際に大いに役立つ。"
  },
  {
    "start": 1302096,
    "end": 1305390,
    "text": "繰り返しになるが、埋め込みモデルの選択はかなり重要である。"
  },
  {
    "start": 1305540,
    "end": 1305902,
    "text": "オーケー。"
  },
  {
    "start": 1305956,
    "end": 1308442,
    "text": "それ以上に重要なのは、チャンキング戦略だ。"
  },
  {
    "start": 1308506,
    "end": 1322450,
    "text": "テキストからベクターへどのように移行するかは、ラグワークフローを作る、あるいはラグワークフローの質を向上させる上で、おそらく最も大きな部分のひとつだろう。"
  },
  {
    "start": 1322870,
    "end": 1332518,
    "text": "つまり、ドキュメントからベクターへどのように移行するかということで、チャンキング戦略の出番となる。"
  },
  {
    "start": 1332524,
    "end": 1334246,
    "text": "いろいろな考え方がある。"
  },
  {
    "start": 1334268,
    "end": 1339878,
    "text": "基本的には、そうだ。戦略をチェックすることは、このワークフローで最も重要なことのひとつだ。"
  },
  {
    "start": 1339974,
    "end": 1349882,
    "text": "もうひとつは、関連する文章や関連する文書の一部などを手に入れたら、言語モデル自体に何を送り込むかということだ。"
  },
  {
    "start": 1349936,
    "end": 1352074,
    "text": "全部まとめて餌にするのか？"
  },
  {
    "start": 1352112,
    "end": 1354906,
    "text": "その上で何かしているのか、再ランクアップしているのか？"
  },
  {
    "start": 1354938,
    "end": 1356190,
    "text": "順位はどうやって変えるのですか？"
  },
  {
    "start": 1357010,
    "end": 1361738,
    "text": "基本的に、このワークフローでは、検索後の第2段階も非常に重要になる。"
  },
  {
    "start": 1361834,
    "end": 1362480,
    "text": "そうだね。"
  },
  {
    "start": 1363030,
    "end": 1363780,
    "text": "オーケー。"
  },
  {
    "start": 1364550,
    "end": 1385350,
    "text": "このような新しいスタイルのアプリケーションに対応するために、ベクター・データベースで何が起きているのか、また、どのような課題を克服しようとしているのか、課題の観点からお聞かせください。"
  },
  {
    "start": 1385850,
    "end": 1386502,
    "text": "もちろんだ。"
  },
  {
    "start": 1386636,
    "end": 1395270,
    "text": "まず第一に、私たちはパインコーンサーバーレスと呼ぶものをリリースしたばかりです。"
  },
  {
    "start": 1395350,
    "end": 1401174,
    "text": "これまでのベクターデータベースは、従来のデータベースとは異なり、非常に堅苦しいものだった。"
  },
  {
    "start": 1401302,
    "end": 1411486,
    "text": "今日、Pineconeでポッドベースのアーキテクチャを採用する場合、使用するポッドの数を事前に指定し、そこにどれだけのデータを入れたいかを事前に理解する必要がある。"
  },
  {
    "start": 1411588,
    "end": 1417794,
    "text": "それを超えるたびに、再シャードし、ある意味でデータを再編成しなければならない。"
  },
  {
    "start": 1417992,
    "end": 1419774,
    "text": "これらはすべて非常に高価なものだ。"
  },
  {
    "start": 1419902,
    "end": 1431750,
    "text": "このアプリでどこまでやれるか、何にぶつかるかを知る前に、このような基本的なインフラ・クラスタリングのような決断をしなければならない。"
  },
  {
    "start": 1431820,
    "end": 1432534,
    "text": "その通りだ。"
  },
  {
    "start": 1432732,
    "end": 1435750,
    "text": "ユースケースが変われば、深刻な問題になる。"
  },
  {
    "start": 1435820,
    "end": 1436054,
    "text": "そうだね。"
  },
  {
    "start": 1436092,
    "end": 1445686,
    "text": "例えば、同じデータをオンデマンドのユースケースに使おうとすると、オンデマンドでしかクエリーを行わないため、1000倍以上の料金を支払うことになる。"
  },
  {
    "start": 1445718,
    "end": 1454362,
    "text": "常にクエリをしているわけではないので、必要ないメモリやスペース、コストを消費することになる。"
  },
  {
    "start": 1454496,
    "end": 1459054,
    "text": "ユースケースにもよるが、これまでは柔軟性に欠けていた。"
  },
  {
    "start": 1459092,
    "end": 1464734,
    "text": "ベクター・データベースをどのように活用するのがベストなのでしょうか？"
  },
  {
    "start": 1464772,
    "end": 1468962,
    "text": "従来のデータベースは、過去30年の間にこの問題を解決してきた。"
  },
  {
    "start": 1469096,
    "end": 1474942,
    "text": "ベクター・データベースはちょうど追いついてきたところで、パインコーンはパインコーン・サーバーレスでこの方向に非常に大きな一歩を踏み出した。"
  },
  {
    "start": 1475086,
    "end": 1485234,
    "text": "私たちがここでやっているもう一つのことは、ベクター・データベースにその知識を入れることで、彼ら自身の知識を向上させることができるとお話ししたのを覚えていますか？"
  },
  {
    "start": 1485362,
    "end": 1490166,
    "text": "これによって、ジェネレーティブAIアプリケーションなどでは、10倍のコスト削減が可能になる。"
  },
  {
    "start": 1490268,
    "end": 1494646,
    "text": "これもまた、クエリーあたりのコストが非常に安い場合にのみ可能となる。"
  },
  {
    "start": 1494758,
    "end": 1501450,
    "text": "もし、クエリ1つを実行するのに多くの料金を支払わなければならないのであれば、私は最も重要なデータしか置かない。"
  },
  {
    "start": 1501600,
    "end": 1501962,
    "text": "その通りだ。"
  },
  {
    "start": 1502016,
    "end": 1504326,
    "text": "まさか10億ものベクトルを入れるわけじゃないだろう？"
  },
  {
    "start": 1504448,
    "end": 1514394,
    "text": "ベクター・データベースをどのように見直せば、さまざまなユースケースで10倍から100倍のコスト削減を実現できるのか。"
  },
  {
    "start": 1514522,
    "end": 1517066,
    "text": "第二に、非常にフレキシブルにできる。"
  },
  {
    "start": 1517178,
    "end": 1522846,
    "text": "なぜなら、ユースケースは進化していくことがわかっているからだ。"
  },
  {
    "start": 1522878,
    "end": 1525746,
    "text": "私たちは、人々がこのデータの新しい使用例などを見つけることを知っている。"
  },
  {
    "start": 1525848,
    "end": 1527846,
    "text": "そのようなことを前もって考えてほしくないのだ。"
  },
  {
    "start": 1527948,
    "end": 1528358,
    "text": "そうだね。"
  },
  {
    "start": 1528444,
    "end": 1532546,
    "text": "治療的なAIのワークフローは非常に阻害される。"
  },
  {
    "start": 1532658,
    "end": 1534326,
    "text": "それが私たちのやってきたことだ。"
  },
  {
    "start": 1534428,
    "end": 1537640,
    "text": "そのためには、ベクター・データベースそのものを再構築する必要があった。"
  },
  {
    "start": 1538030,
    "end": 1540154,
    "text": "従来のアプローチは通用しない。"
  },
  {
    "start": 1540352,
    "end": 1544730,
    "text": "では、その再構築には何が必要なのか？"
  },
  {
    "start": 1545870,
    "end": 1554122,
    "text": "私はアーキテクチャについていくつかの仮定をしていますが、それが伝統的なモノリシックなアーキテクチャだったと仮定して、今サーバーレスになるためにはどのようにシフトする必要があるのでしょうか？"
  },
  {
    "start": 1554266,
    "end": 1555422,
    "text": "いい質問だね。"
  },
  {
    "start": 1555556,
    "end": 1564250,
    "text": "すべてのベクター・データベースは、私たちが松のコードを公開する前に、サーバーレスで、いわゆる検索エンジン・アーキテクチャで動作していた。"
  },
  {
    "start": 1564330,
    "end": 1564574,
    "text": "そうだね。"
  },
  {
    "start": 1564612,
    "end": 1567694,
    "text": "つまり、データはたくさんの破片に分割される。"
  },
  {
    "start": 1567742,
    "end": 1578578,
    "text": "これらのシャードには、データのサブセットがインデックス化され、常に利用可能な状態で格納されています。つまり、RAMとSSDの組み合わせにより、これらのマシンは常に稼働しているような状態なのです。"
  },
  {
    "start": 1578744,
    "end": 1584610,
    "text": "1秒間に何万ものクエリーを実行し、コーパス全体に触れる必要がある場合、これは理にかなっている。"
  },
  {
    "start": 1584770,
    "end": 1587718,
    "text": "そうであるなら、そのアーキテクチャは理にかなっている。"
  },
  {
    "start": 1587884,
    "end": 1599802,
    "text": "オンデマンドでクエリを実行する場合、あるいはクエリがコーパス全体に触れない場合、つまりウェブスケールのコーパスの場合、1つのクエリがコーパス全体に触れることはない。"
  },
  {
    "start": 1599936,
    "end": 1609946,
    "text": "これは、Pineconeの顧客も感じていたことで、Pineconeもポッドベースのアーキテクチャで同じアーキテクチャを使っていたからだ。"
  },
  {
    "start": 1610138,
    "end": 1624702,
    "text": "このようなワークフロー、新しく登場したジェネレーティブAIのワークフロー、そしてPineconeの顧客が行っている他の多くのワークフローを10倍の費用対効果で実現するには、まずストレージとコンピュートを切り離す必要がある。"
  },
  {
    "start": 1624766,
    "end": 1629766,
    "text": "さて、インデックス用のストレージをすべて、常にコンピュート近くに置いておくことはできない。"
  },
  {
    "start": 1629868,
    "end": 1635174,
    "text": "現在、従来のデータベースがここ数年でうまくやっていることだ。"
  },
  {
    "start": 1635292,
    "end": 1642314,
    "text": "もしあなたがスノーフレークに詳しいなら、スパナーやこのようなシステムに詳しいなら、彼らは素晴らしい。"
  },
  {
    "start": 1642352,
    "end": 1648982,
    "text": "彼らは数十年にわたるデータベースの学習から、ストレージとコンピューティングを分離する方法を考え出した。"
  },
  {
    "start": 1649046,
    "end": 1651478,
    "text": "これらのデータベースを使用するためのコストを削減することができます。"
  },
  {
    "start": 1651574,
    "end": 1652266,
    "text": "そうだね。"
  },
  {
    "start": 1652448,
    "end": 1667138,
    "text": "というのも、ベクトル検索には、インデックスを更新しようとするたびに、ある意味でインデックスの全体像を把握しなければならないという根本的な問題があることを、少なくとも既存のアルゴリズムでは説明したはずだからだ。"
  },
  {
    "start": 1667224,
    "end": 1679050,
    "text": "同様に、HNSWであろうと、fisであろうと、今日誰でもやっているような既存のアルゴリズムはすべて、メモリとローカルSSDの間にインデックス全体を保持している。"
  },
  {
    "start": 1679230,
    "end": 1683862,
    "text": "実際、2年前まではほとんどのアルゴリズムがディスクを使わず、すべてメモリーベースだった。"
  },
  {
    "start": 1683916,
    "end": 1685990,
    "text": "クラウド上のメモリは現在でも非常に高価だ。"
  },
  {
    "start": 1686060,
    "end": 1686680,
    "text": "そうだね。"
  },
  {
    "start": 1687130,
    "end": 1693622,
    "text": "ディスクベースのシステムも、実際には非常に高価である。"
  },
  {
    "start": 1693766,
    "end": 1695974,
    "text": "ローカルのSSDですべてを管理しなければならない。"
  },
  {
    "start": 1696102,
    "end": 1702222,
    "text": "クラウド上では、コンピュートから切り離されていることを知ることができない。"
  },
  {
    "start": 1702356,
    "end": 1711418,
    "text": "アマゾンやGCPでマシンを手に入れると、大量のSSD、大量のコア、大量のRAMが手に入る。"
  },
  {
    "start": 1711514,
    "end": 1713006,
    "text": "これが箱の中身だ。"
  },
  {
    "start": 1713108,
    "end": 1715914,
    "text": "ssdsといえども換金性はない。"
  },
  {
    "start": 1716042,
    "end": 1722622,
    "text": "コスト削減を推進する最善の方法は、ストレージを本当に切り離し、ブロブ・ストレージのような安価なストレージに置くことだ。"
  },
  {
    "start": 1722766,
    "end": 1732214,
    "text": "ブロブ・ストレージにモノを置いた瞬間に、そのモノのインデックスをインクリメンタルに作成し、クエリが必要とするインデックスの部分だけをインクリメンタルに取り出すという作業を、非常に効率的に行う必要がある。"
  },
  {
    "start": 1732332,
    "end": 1737014,
    "text": "そのためには、ベクトル検索がどのように機能するのか、根本的な改革が必要だ。"
  },
  {
    "start": 1737212,
    "end": 1738920,
    "text": "僕らがやったのはそういうことだ。"
  },
  {
    "start": 1739530,
    "end": 1752538,
    "text": "イノベーションの核心は、ストレージとコンピュート（計算）を切り離し、オンデマンドでインデックスの一部をページ化することで、クエリが検索する必要がないように、ベクトル検索を再構築することだと言えるでしょう。"
  },
  {
    "start": 1752624,
    "end": 1757322,
    "text": "基本的に、クエリのコストはもはや管理下のデータ全体のコストではない。"
  },
  {
    "start": 1757466,
    "end": 1762330,
    "text": "クエリーが見る必要のあるデータ部分のコストに比例するだけだ。"
  },
  {
    "start": 1762500,
    "end": 1769794,
    "text": "おそらく、あなたはクラウド上でまだこれをやっているが、以前とは異なるプリミティブを使っているだけだろう。"
  },
  {
    "start": 1769912,
    "end": 1776820,
    "text": "以前はSSDをたくさん積んだ大型マシンを使っていたかもしれないが、今は何を使っている？"
  },
  {
    "start": 1777830,
    "end": 1782214,
    "text": "いや、だから機械が違うだけじゃなくて、アルゴリズム自体が違うんだ。"
  },
  {
    "start": 1782252,
    "end": 1782742,
    "text": "それは分かる。"
  },
  {
    "start": 1782796,
    "end": 1791906,
    "text": "以前は、すべてのインデックスを事前にロードしておいたり、新鮮なまま大きなマシンにロードしておいたりしていたんだ。"
  },
  {
    "start": 1792018,
    "end": 1792390,
    "text": "そうだね。"
  },
  {
    "start": 1792460,
    "end": 1798374,
    "text": "今日、私たちができることは、このような大きなマシンをまだ持つことができるが、今はインデックスの一部をオンデマンドでロードするだけだ。"
  },
  {
    "start": 1798502,
    "end": 1806202,
    "text": "検索され、頻繁に使用されていることがわかった部分をキャッシュしているのだ。"
  },
  {
    "start": 1806346,
    "end": 1808750,
    "text": "そのため、非常に低いレイテンシーなどを実現できる。"
  },
  {
    "start": 1808820,
    "end": 1809054,
    "text": "オーケー。"
  },
  {
    "start": 1809092,
    "end": 1812442,
    "text": "そうすれば、触れられていないすべてのデータに対してコストを支払っていないことになる。"
  },
  {
    "start": 1812586,
    "end": 1813674,
    "text": "それが革新の核心だ。"
  },
  {
    "start": 1813722,
    "end": 1814250,
    "text": "わかったか？"
  },
  {
    "start": 1814340,
    "end": 1814546,
    "text": "そうだね。"
  },
  {
    "start": 1814568,
    "end": 1823582,
    "text": "あなたがブロブ・ストレージについて言及したとき、私はデータがオンラインではなく、S3にプッシュされていると思った。"
  },
  {
    "start": 1823646,
    "end": 1825090,
    "text": "はい、その通りです。"
  },
  {
    "start": 1825160,
    "end": 1825746,
    "text": "そうだ。"
  },
  {
    "start": 1825848,
    "end": 1833554,
    "text": "コツは、クエリに答える必要があるときに、データのどの部分を計算機の近くに置いておく必要があるのかを見極めることだ。"
  },
  {
    "start": 1833682,
    "end": 1834454,
    "text": "これもそうだ。"
  },
  {
    "start": 1834492,
    "end": 1840086,
    "text": "クエリの予測最適化とか、そういうことだ。"
  },
  {
    "start": 1840268,
    "end": 1850186,
    "text": "基本的には、インデックスを作成したり、インデックスを再作成したり、インデックスを新しくしたりするときに、パーティショニングの方法が必要だと思います。"
  },
  {
    "start": 1850208,
    "end": 1853466,
    "text": "従来のデータベースでは、レンジ・パーティショニングはお馴染みだ。"
  },
  {
    "start": 1853498,
    "end": 1867398,
    "text": "例えば、タイムスタンプが100より大きいというクエリを受け取った場合、100より大きいすべての範囲を調べる必要があることがわかります。"
  },
  {
    "start": 1867424,
    "end": 1870034,
    "text": "タイムスタンプが100未満のデータを見る必要もない。"
  },
  {
    "start": 1870072,
    "end": 1870370,
    "text": "そうだね。"
  },
  {
    "start": 1870440,
    "end": 1874158,
    "text": "そのためには、データをチャンクに分割する必要がある。"
  },
  {
    "start": 1874254,
    "end": 1876258,
    "text": "各チャンクにはいくつかの統計がある。"
  },
  {
    "start": 1876344,
    "end": 1879622,
    "text": "その上で、私が何を見る必要があるのか？"
  },
  {
    "start": 1879756,
    "end": 1882562,
    "text": "その一般的な哲学をパーティショニングと呼ぶ。"
  },
  {
    "start": 1882626,
    "end": 1884200,
    "text": "データをパーティション分けしているんだ。"
  },
  {
    "start": 1884650,
    "end": 1889282,
    "text": "ベクトルを分割すること以外は同じだ。"
  },
  {
    "start": 1889346,
    "end": 1902186,
    "text": "そして、その空間を幾何学的に分解し、空間の塊に分解してしまえば、クエリーが来たときに、「あのね。"
  },
  {
    "start": 1902208,
    "end": 1903858,
    "text": "私はこれらの地域にしか興味がない。"
  },
  {
    "start": 1903974,
    "end": 1906510,
    "text": "他の地域にあるデータなんてどうでもいい。"
  },
  {
    "start": 1907410,
    "end": 1916740,
    "text": "これらの領域に属するインデックスの部分のみをフェッチしてきて、クエリが必要とする最も近い候補が何かを確認する。"
  },
  {
    "start": 1917270,
    "end": 1927478,
    "text": "これは基本的に、データベースと同じ伝統的なパーティショニングの考え方を適用している。ただし、この新しいパーティショニングの方法は、ジオメトリに基づくものである。"
  },
  {
    "start": 1927564,
    "end": 1927958,
    "text": "そうだね。"
  },
  {
    "start": 1928044,
    "end": 1928342,
    "text": "興味深い。"
  },
  {
    "start": 1928396,
    "end": 1933186,
    "text": "従来のデータベースはジオメトリを理解しないが、ベクターデータベースはジオメトリを理解する必要がある。"
  },
  {
    "start": 1933298,
    "end": 1933960,
    "text": "そうだろう？"
  },
  {
    "start": 1934570,
    "end": 1935174,
    "text": "興味深い。"
  },
  {
    "start": 1935292,
    "end": 1944300,
    "text": "ユーザーから見ると、既存の松ぼっくりデータベースがあって、その中にドキュメントがある。"
  },
  {
    "start": 1945230,
    "end": 1953270,
    "text": "先ほどお話しいただいたパーティション戦略を経て、今回の発表を聞きました。"
  },
  {
    "start": 1953350,
    "end": 1959120,
    "text": "私は興奮するし、あなたがおっしゃったようなことをすべて低コストで利用したいと思う。"
  },
  {
    "start": 1959810,
    "end": 1961006,
    "text": "私に何か変化はありますか？"
  },
  {
    "start": 1961028,
    "end": 1965534,
    "text": "スイッチを入れれば、突然サーバーレスになるのか？"
  },
  {
    "start": 1965582,
    "end": 1968834,
    "text": "それとも、データをリロードしなければならないのでしょうか？"
  },
  {
    "start": 1969032,
    "end": 1978518,
    "text": "規模が大きくなると難しいということもあるが、もっと悪いのは、自分のコードを触らなければならないのか、アプリケーションを書き直さなければならないのか、ということだ。"
  },
  {
    "start": 1978684,
    "end": 1981026,
    "text": "私の観点から変えなければならないことは何か？"
  },
  {
    "start": 1981138,
    "end": 1982070,
    "text": "いい質問だね。"
  },
  {
    "start": 1982140,
    "end": 1987702,
    "text": "まず第一に、今日はパブリック・プレビューと呼ばれるものだ。"
  },
  {
    "start": 1987836,
    "end": 1994530,
    "text": "パブリック・プレビューの段階では、サーバーレスを使いたい場合はデータを再インジェストする必要がある。"
  },
  {
    "start": 1994610,
    "end": 1995250,
    "text": "いいかい？"
  },
  {
    "start": 1995420,
    "end": 2000202,
    "text": "パブリックプレビューと一般発売の間に、少し反転するようにするつもりだ。"
  },
  {
    "start": 2000256,
    "end": 2007786,
    "text": "基本的には、今から私たちが一般的に利用可能になるまでの間に、サーバーレスの利点を活用するために何もする必要はありません。"
  },
  {
    "start": 2007978,
    "end": 2010702,
    "text": "この初期段階では、データを再調整する必要がある。"
  },
  {
    "start": 2010756,
    "end": 2017330,
    "text": "サーバーレスを使い始めるのに、コードの変更は必要ない。"
  },
  {
    "start": 2017910,
    "end": 2025890,
    "text": "現在、私たちは何十億ものベクターを所有する大手の顧客とも緊密に連携しており、そのような顧客にとっては再投資は選択肢にすら入らない。"
  },
  {
    "start": 2025960,
    "end": 2026386,
    "text": "そうだろう？"
  },
  {
    "start": 2026488,
    "end": 2036082,
    "text": "サーバーレスを試している顧客の中には、シームレスな移行を行うために、フィールドエンジニアリングチームとエンジニアが一緒になってオプションを考えている。"
  },
  {
    "start": 2036226,
    "end": 2048774,
    "text": "しかし、パブリック・プレビューでは、サーバーレスを使いたいのであれば、今日中にデータを再投資することを期待している。"
  },
  {
    "start": 2048902,
    "end": 2070100,
    "text": "開発者やこのようなシステムを設計している人の立場からすると、これは基本的な経済性の多くを変えるように聞こえます。おそらく、以前とは異なるユースケースを検討したり、前もって決断しなければならないことが少なくなったりするでしょう。"
  },
  {
    "start": 2070950,
    "end": 2073778,
    "text": "APIはこれに伴って変更されるのか？"
  },
  {
    "start": 2073864,
    "end": 2075118,
    "text": "それを狙っているんだと思う。"
  },
  {
    "start": 2075144,
    "end": 2085126,
    "text": "開発者として私が興奮するような、目に見えるものを超えた新機能はあるのだろうか？"
  },
  {
    "start": 2085308,
    "end": 2086040,
    "text": "もちろんだ。"
  },
  {
    "start": 2086970,
    "end": 2089382,
    "text": "まず第一に、あなたは100％正しい。"
  },
  {
    "start": 2089516,
    "end": 2103642,
    "text": "つまり、データを持っていて、オンデマンド・クエリなどを実行したい場合、インジェストを始めればいいのです。インジェストは安価なストレージで、クエリした分だけ料金を支払えばいいのですから。"
  },
  {
    "start": 2103706,
    "end": 2106990,
    "text": "これによって、まったく新しいユースケースが生まれる。"
  },
  {
    "start": 2107650,
    "end": 2110750,
    "text": "それと同時に、APIを壊さないようにしています。"
  },
  {
    "start": 2111410,
    "end": 2114046,
    "text": "我々はAPIの互換性をできるだけ保とうとしている。"
  },
  {
    "start": 2114228,
    "end": 2116978,
    "text": "サーバーレスでは、いくつかの追加情報を得ることができる。"
  },
  {
    "start": 2117144,
    "end": 2124514,
    "text": "例えば、サーバーレス・インデックスを使用している場合、ある意味でこのクエリにはどれだけのコストがかかったかという情報を得ることになる。"
  },
  {
    "start": 2124632,
    "end": 2127358,
    "text": "私たちはこのコストの代用として、リードユニットと呼ぶものを用意している。"
  },
  {
    "start": 2127454,
    "end": 2130598,
    "text": "すべてのクエリーは、そのクエリーのコストをあなたに返す。"
  },
  {
    "start": 2130684,
    "end": 2139290,
    "text": "これは、ある意味、予算を立てたり、ユースケースパターンにどれだけのコストがかかるかを理解したりするのに非常に役立つ。"
  },
  {
    "start": 2139360,
    "end": 2139594,
    "text": "そうだね。"
  },
  {
    "start": 2139632,
    "end": 2141018,
    "text": "それを知ることはとても重要だ。"
  },
  {
    "start": 2141104,
    "end": 2153806,
    "text": "同じサーバーレス・アーキテクチャーによって、ある意味、必要な検索品質を得るために実際に見る必要のあるデータ量をコントロールすることができる。"
  },
  {
    "start": 2153988,
    "end": 2166802,
    "text": "私が言いたいのは、ほとんどのクエリーセット、私たちが行ったほとんどの実験、私たちが行ったほとんどのベンチマークでは、クエリーの約95％は少量のデータを見ることで答えられるということです。"
  },
  {
    "start": 2166856,
    "end": 2167460,
    "text": "オーケー。"
  },
  {
    "start": 2168630,
    "end": 2173186,
    "text": "もちろん、その少量のデータとは何なのかを見極めるのがコツだ。"
  },
  {
    "start": 2173288,
    "end": 2178166,
    "text": "これも伝統的なデータベース最適化の一種ですね？"
  },
  {
    "start": 2178268,
    "end": 2178966,
    "text": "その通りだ。"
  },
  {
    "start": 2179148,
    "end": 2183586,
    "text": "ただし、ベクターデータベースはジオメトリを理解しなければならないので、より難しいチャレンジになる。"
  },
  {
    "start": 2183698,
    "end": 2187718,
    "text": "ジオメトリーはグローバルなもので、よりローカルなものだ。"
  },
  {
    "start": 2187884,
    "end": 2188600,
    "text": "その通りだ。"
  },
  {
    "start": 2189690,
    "end": 2192762,
    "text": "この5％のクエリには、もっとスキャンする必要がある。"
  },
  {
    "start": 2192816,
    "end": 2214606,
    "text": "昔のポッドベースのアーキテクチャでも、今日の他のベクターデータベースでも、すべてのクエリは最小公倍数に対してお金を払うことになる。つまり、少量のデータを見るべきクエリを実行するのも、質問に答えられるだけのデータを見るべきクエリを実行するのと同じくらい高くつくことになる。"
  },
  {
    "start": 2214708,
    "end": 2215454,
    "text": "そうだね。"
  },
  {
    "start": 2215652,
    "end": 2217666,
    "text": "サーバーレスではその必要はない。"
  },
  {
    "start": 2217768,
    "end": 2229670,
    "text": "これから一般に提供されるまでの間、私たちがやろうとしていることのひとつは、このコントロールをユーザーの手に委ねるために、どのようなAPIを提供するのがベストなのかを見極めることです。"
  },
  {
    "start": 2231050,
    "end": 2239020,
    "text": "これは非常に強力なものになると思う。なぜなら、自分が良いと思う品質に対してどれだけの金額を支払っているのか、自分でコントロールできるようになるからだ。"
  },
  {
    "start": 2241150,
    "end": 2266734,
    "text": "私たちが始めたところに一歩戻って、人々が経験するボロ雑巾のような課題や、ユーザー向けアプリケーションで本当に質の高い生産、質の高い経験を得ようとする課題に立ち戻ることで、あなたがサーバーレスで行ってきたことを、その核となる課題に結びつけることができるかもしれません。"
  },
  {
    "start": 2266862,
    "end": 2274958,
    "text": "これからはもっと簡単になるはずだ。"
  },
  {
    "start": 2275054,
    "end": 2276030,
    "text": "まだまだ難しいよ。"
  },
  {
    "start": 2276120,
    "end": 2276566,
    "text": "もちろんだ。"
  },
  {
    "start": 2276668,
    "end": 2278806,
    "text": "とても注目されているんだ。"
  },
  {
    "start": 2278908,
    "end": 2280390,
    "text": "そのうち楽になるよ。"
  },
  {
    "start": 2280810,
    "end": 2287126,
    "text": "また、ベクター・データベースはそれにどのように貢献するのでしょうか？"
  },
  {
    "start": 2287228,
    "end": 2302010,
    "text": "ええ、私たちが昨年行った多くの作業は、ベクターデータベースを本当に使いやすく、経済的なスケールで、埋め込みデータを放り込むだけで検索できるものにすることでした。"
  },
  {
    "start": 2302160,
    "end": 2305502,
    "text": "私たちはそのために多くの努力をしてきた。"
  },
  {
    "start": 2305636,
    "end": 2313914,
    "text": "Pythonのサーバーレスは、その方向への大きな一歩のようなもので、私たちは今後もその方向性をさらに強め、ワークフローを洗練させることに多くの時間を費やすことになるだろう。"
  },
  {
    "start": 2314042,
    "end": 2319218,
    "text": "とはいえ、私たちはそれを本当にうまくやると確信しています。"
  },
  {
    "start": 2319304,
    "end": 2325470,
    "text": "もうひとつは、エンベッディングを作ること自体が複雑だという話だ。"
  },
  {
    "start": 2325550,
    "end": 2331318,
    "text": "チャンキング戦略そのものが、今は科学というよりアートだと話した。"
  },
  {
    "start": 2331484,
    "end": 2338514,
    "text": "私たちは、再ランキングや情報検索そのものの分野では、これらすべてを本当にうまく結びつけることはほとんど行われていないと話した。"
  },
  {
    "start": 2338572,
    "end": 2339034,
    "text": "そうだね。"
  },
  {
    "start": 2339152,
    "end": 2345402,
    "text": "ワークフローをシームレスかつ簡単にするために、私たちは多くの時間を費やすことになるだろう。"
  },
  {
    "start": 2345536,
    "end": 2370466,
    "text": "ある意味、ベクターデータベースは、今日非常にバラバラでオーダーメイドのベクターデータベースをめぐる領域を、どのような埋め込みモデルを使っているのか、どのようにチャンキングしているのか、何を検索しているのか、LLMのコンテキストにどのように組み込んでいるのか、などなど、本当につなぎ合わせるようになるのではないかと期待している。"
  },
  {
    "start": 2370488,
    "end": 2376980,
    "text": "松ぼっくりであろうと他の場所であろうと、来年はここで多くのエンジニアリングや研究が行われることになると思う。"
  },
  {
    "start": 2377530,
    "end": 2378278,
    "text": "素晴らしい。"
  },
  {
    "start": 2378444,
    "end": 2395746,
    "text": "オーラム、お忙しい中、ベクター・データベースとボロ・トピックに関する最新・最高の情報を共有していただき、本当にありがとうございます。"
  },
  {
    "start": 2395938,
    "end": 2397350,
    "text": "素晴らしい会話だ。"
  },
  {
    "start": 2397690,
    "end": 2399800,
    "text": "お招きいただきありがとうございました。"
  }
]