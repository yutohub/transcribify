[
  {
    "start": 490,
    "end": 2190,
    "text": "やあ、みんな。"
  },
  {
    "start": 2260,
    "end": 4170,
    "text": "今日はマンバについて話そう。"
  },
  {
    "start": 4250,
    "end": 12890,
    "text": "つまり、マンバは、ちょうど1ヶ月前に発表された、選択的状態空間を用いたマンバ線形時間シーケンスモデリングという論文で発表された、シーケンスモデリングのための新しいモデルなのだ。"
  },
  {
    "start": 12970,
    "end": 14814,
    "text": "今日のトピックをおさらいしよう。"
  },
  {
    "start": 15012,
    "end": 21120,
    "text": "ビデオの前半では、シーケンスモデルとは何か、どのようなシーケンスモデリングができるかを紹介する。"
  },
  {
    "start": 22290,
    "end": 26066,
    "text": "ビデオの第2部では、状態空間モデルについてお話しします。"
  },
  {
    "start": 26178,
    "end": 31634,
    "text": "状態空間モデルを完全に理解するためには、微分方程式について少し知っておく必要がある。"
  },
  {
    "start": 31762,
    "end": 42470,
    "text": "というのも、学士号や修士号ではそのようなことを教えている場合もあるが、ほとんどの場合は教えていないからだ。"
  },
  {
    "start": 42630,
    "end": 46422,
    "text": "微分方程式を理解するために必要な背景をお話しします。"
  },
  {
    "start": 46566,
    "end": 56350,
    "text": "この後、状態空間モデルについて話し、離散化の公式を導き、畳み込み計算とリカレント計算の公式も導き出す。"
  },
  {
    "start": 57090,
    "end": 62618,
    "text": "ヒポメトリクスとは何を意味するのか、そして状態ベースのモデルにおけるaマトリクスの重要性を説明しよう。"
  },
  {
    "start": 62714,
    "end": 66298,
    "text": "ビデオの第2部3では、マンバについてお話しします。"
  },
  {
    "start": 66314,
    "end": 73054,
    "text": "マンバに至った動機と、マンバの革新性、つまり選択的スキャン・アルゴリズムとは何か？"
  },
  {
    "start": 73102,
    "end": 75966,
    "text": "まず、スキャン操作とは何を意味するのか？"
  },
  {
    "start": 76078,
    "end": 81858,
    "text": "このスキャン操作はパラレル・スキャンで並列化できることがわかるだろう。"
  },
  {
    "start": 81954,
    "end": 85346,
    "text": "カーネル・フュージョンとは何か、アクティベーションの再計算について見ていこう。"
  },
  {
    "start": 85458,
    "end": 92246,
    "text": "最後に、マンバのアーキテクチャーと、トランスフォーマーに関する性能の考察を行う。"
  },
  {
    "start": 92358,
    "end": 96854,
    "text": "前提条件として、微積分の基礎があることを願うだけだ。"
  },
  {
    "start": 96902,
    "end": 104686,
    "text": "高校数学で十二分だと思うし、変圧器モデルやニューラルネットワーク全般についての基本的な理解もあるはずだ。"
  },
  {
    "start": 104788,
    "end": 106670,
    "text": "さあ、旅を始めよう。"
  },
  {
    "start": 107650,
    "end": 112122,
    "text": "シーケンスモデルの目的は、入力シーケンスを出力シーケンスにマッピングすることである。"
  },
  {
    "start": 112186,
    "end": 114554,
    "text": "入力シーケンスは連続信号でもよい。"
  },
  {
    "start": 114602,
    "end": 123326,
    "text": "その場合、出力される連続信号に対応させるか、あるいは、離散的な入力信号で、離散的な出力信号に対応させることになる。"
  },
  {
    "start": 123438,
    "end": 128646,
    "text": "例えば、連続信号は音声、離散信号はテキストなどだ。"
  },
  {
    "start": 128748,
    "end": 135862,
    "text": "実際、オーディオの場合でも、オーディオファイルを時間的にサンプリングするため、ほとんどの場合、離散信号を扱う。"
  },
  {
    "start": 135996,
    "end": 147290,
    "text": "言語モデリングについて話すなら、私たちは離散的な入力について話している。"
  },
  {
    "start": 148830,
    "end": 153440,
    "text": "シーケンス・モデリングには多くのモデルがある。"
  },
  {
    "start": 154450,
    "end": 165374,
    "text": "リカレント・ニューラル・ネットワークは、隠れ状態を持つネットワークで、次のように出力を計算する。"
  },
  {
    "start": 165502,
    "end": 172066,
    "text": "例えば、x1、x2、x3からなる入力シーケンスがある。"
  },
  {
    "start": 172248,
    "end": 186390,
    "text": "最初に行うのは、隠れた状態（hidden sequence）をゼロで初期化することで、ネットワークに隠れた状態（hidden state）を与え、最初の入力とともにゼロを与えると、最初の出力が生成される。"
  },
  {
    "start": 187050,
    "end": 190682,
    "text": "次に、以前に生成された隠れた状態を使用する。"
  },
  {
    "start": 190736,
    "end": 200982,
    "text": "前のステップの出力は、新しい入力トークンに対して新しい隠れ状態と新しい出力トークンを生成する。"
  },
  {
    "start": 201046,
    "end": 204122,
    "text": "これはY2となり、出力は2番となる。"
  },
  {
    "start": 204256,
    "end": 214206,
    "text": "新しい出力トークンと次のトークンに使用される新しい隠れ状態を生成するために、新しい入力トークンと共に以前に生成された隠れ状態を使用します。"
  },
  {
    "start": 214398,
    "end": 223810,
    "text": "なぜなら、n番目のトークンを生成するためには、nマイナスnマイナス1のトークンが必要だからである。"
  },
  {
    "start": 224310,
    "end": 230870,
    "text": "この種のモデルのトレーニングは並列化できず、これがトランスフォーマーが成功した理由のひとつである。"
  },
  {
    "start": 232010,
    "end": 244778,
    "text": "しかし、推論時間は各トークンに対して一定である。つまり、1トークンの出力を生成するためにかける計算上の労力と、メモリ上の労力は同じである。"
  },
  {
    "start": 244864,
    "end": 249754,
    "text": "それが最初のトークンであろうと、100個目のトークンであろうと関係ない。"
  },
  {
    "start": 249792,
    "end": 253614,
    "text": "私たちが行っている演算や計算の数は常に同じである。"
  },
  {
    "start": 253732,
    "end": 258830,
    "text": "私たちは、1つの出力トークンを生成するために、前の状態と現在の入力を取っている。"
  },
  {
    "start": 259170,
    "end": 270370,
    "text": "理論的には、この数列を永遠に続けることができるため、コンテキストの長さは無限であるが、現実的には、勾配が消失し爆発してしまうため、そうすることはできない。"
  },
  {
    "start": 271350,
    "end": 276258,
    "text": "シーケンスモデリングに使える別のモデルだが、あまり使われていない。"
  },
  {
    "start": 276344,
    "end": 282802,
    "text": "最近あまり使われていないのは畳み込みニューラルネットワークで、これは主にコンピュータービジョンのタスクに使われる。"
  },
  {
    "start": 282946,
    "end": 288134,
    "text": "有限のコンテキスト・ウィンドウを持ち、実行するカーネルを構築する必要がある。"
  },
  {
    "start": 288172,
    "end": 292486,
    "text": "これは、出力特徴を生成するために入力を通して実行されるカーネルである。"
  },
  {
    "start": 292518,
    "end": 293862,
    "text": "これが出力である。"
  },
  {
    "start": 294006,
    "end": 298870,
    "text": "各出力は同じカーネルを使用するため、並列化が容易である。"
  },
  {
    "start": 298950,
    "end": 305710,
    "text": "カーネルをすべての可能な入力ウィンドウに対して並列に実行し、出力特徴を生成することができる。"
  },
  {
    "start": 306050,
    "end": 308730,
    "text": "最後のモデルはトランスフォーマーだ。"
  },
  {
    "start": 308810,
    "end": 317730,
    "text": "このトランスフォーマーは、多くのドット積を計算するセルフ・アテンション・メカニズムを持っているため、トレーニングに関しては簡単に並列化できる。"
  },
  {
    "start": 317800,
    "end": 329586,
    "text": "これは行列の乗算であるため、演算を並列化することができ、シーケンス、入力シーケンス、アテンション・マスクによって定義される有限のコンテキスト・ウィンドウを持つ。"
  },
  {
    "start": 329778,
    "end": 334562,
    "text": "しかし、この種のモデルの推論は、トークンごとに一定ではない。"
  },
  {
    "start": 334626,
    "end": 343038,
    "text": "変圧器モデルでは、出力の最初のトークンを生成する場合、KVキャッシュを使用して1つのドット積を行います。"
  },
  {
    "start": 343154,
    "end": 350730,
    "text": "もし、10番目の出力トークンを生成するのであれば、それを生成するために10個のドット積を行う必要がある。"
  },
  {
    "start": 350800,
    "end": 356814,
    "text": "100個の出力トークンを生成する場合、100個のドット積が必要になる。"
  },
  {
    "start": 357012,
    "end": 364190,
    "text": "最初のトークンを作るために費やした努力と、10個目のトークンを作るために必要な努力は同じではないのだ。"
  },
  {
    "start": 364350,
    "end": 371406,
    "text": "なぜなら、これでは非常に長い入力に対応できないからだ。"
  },
  {
    "start": 371518,
    "end": 383640,
    "text": "トレーニングについても、ご覧のように、トレーニングは入力配列の長さに対して2次関数的にスケールするので、配列の長さを2倍にすると、このモデルをトレーニングするのに4倍の計算が必要になる。"
  },
  {
    "start": 384090,
    "end": 386070,
    "text": "簡単に並列化できる。"
  },
  {
    "start": 386570,
    "end": 405146,
    "text": "理想的な世界では、変換器のように学習を並列化でき、gpuを非常にうまく利用できるので、RnNのように入力配列の長さに応じてリニアにスケールするので、長い配列に対してもリニアにスケールできるモデルが望ましい。"
  },
  {
    "start": 405338,
    "end": 421010,
    "text": "つまり、最初のトークンを生成するために必要な努力は、10個目のトークンや100個目のトークンを生成するために必要な努力と同じでなければならない。"
  },
  {
    "start": 421510,
    "end": 429458,
    "text": "では、状態空間モデルを探求し、それがリカレント・ニューラル・ネットワークとトランスフォーマーの問題を解決するのにどのように役立つかを考えてみよう。"
  },
  {
    "start": 429554,
    "end": 432070,
    "text": "まずは数学の復習だ。"
  },
  {
    "start": 433610,
    "end": 438486,
    "text": "微分方程式について、とても簡単な例で紹介しよう。"
  },
  {
    "start": 438668,
    "end": 446874,
    "text": "ウサギを何匹か飼っていて、そのウサギの個体数が飼っているウサギの数に比例してλの一定の割合で増えていくとする。"
  },
  {
    "start": 446992,
    "end": 450518,
    "text": "つまり、すべてのウサギがラムダの赤ちゃんを産むということだ。"
  },
  {
    "start": 450614,
    "end": 452846,
    "text": "また、ラムダが2に等しいとする。"
  },
  {
    "start": 452868,
    "end": 455854,
    "text": "ラムダを2に等しいと書いてみよう。"
  },
  {
    "start": 456052,
    "end": 468340,
    "text": "個体数の変化率、つまり生まれてくる赤ちゃんの数は、λに特定の時間ステップtで飼っているウサギの数を掛けたものに等しいと言えます。"
  },
  {
    "start": 468790,
    "end": 476654,
    "text": "ということは、ウサギが産む赤ん坊の数もまた、この個体数の変化率ということになる。"
  },
  {
    "start": 476702,
    "end": 481430,
    "text": "個体数がどれだけ増えているかは、λに飼っているウサギの数を掛けたものに等しい。"
  },
  {
    "start": 481580,
    "end": 486006,
    "text": "高校時代のことを思い出してほしいのだが、ある関数の変化率とは何だろう？"
  },
  {
    "start": 486108,
    "end": 489660,
    "text": "変数はこの関数の導関数である。"
  },
  {
    "start": 490110,
    "end": 501614,
    "text": "また、バニーの数を表す関数の微分値は、λに特定の時点でのバニーの数を掛けたものに等しいとも言える。"
  },
  {
    "start": 501652,
    "end": 502480,
    "text": "ステップt。"
  },
  {
    "start": 504690,
    "end": 514686,
    "text": "タイムスタンプ100の時点で、0に等しい時間に5匹のバニーで構成されている個体数をどうやって求めるか？"
  },
  {
    "start": 514868,
    "end": 517650,
    "text": "tの関数bを見つける必要がある。"
  },
  {
    "start": 517720,
    "end": 523954,
    "text": "ウサギの個体数を表すtの関数bを見つける必要がある。"
  },
  {
    "start": 523992,
    "end": 526758,
    "text": "私たちの人口が時間とともに進化していく。"
  },
  {
    "start": 526924,
    "end": 531702,
    "text": "微分方程式を解くとは、tの関数（この場合はb）を求めることである。"
  },
  {
    "start": 531836,
    "end": 534962,
    "text": "つまり、上のような表現になる。"
  },
  {
    "start": 535026,
    "end": 539830,
    "text": "この式はすべてのtについて成り立つ。"
  },
  {
    "start": 539900,
    "end": 547020,
    "text": "この式に置き換えたとき、左辺が右辺に等しくなる関数tを見つける必要がある。"
  },
  {
    "start": 547550,
    "end": 554266,
    "text": "この微分方程式は、変数分離法と呼ばれる非常に簡単な方法で解くことができる。"
  },
  {
    "start": 554298,
    "end": 570290,
    "text": "ここではお見せしませんが、この微分方程式の解は、tのbがkにtのλの指数を掛けた関数であることがはっきりとわかります。"
  },
  {
    "start": 570790,
    "end": 574814,
    "text": "この微分方程式の解が実際に解であることを、どうすれば確認できるだろうか？"
  },
  {
    "start": 574862,
    "end": 581574,
    "text": "なぜなら、この式の中でこの関数を置き換えれば、左辺と右辺が等しくなるからだ。"
  },
  {
    "start": 581612,
    "end": 584194,
    "text": "これが方程式の解を検証する方法だ。"
  },
  {
    "start": 584322,
    "end": 586054,
    "text": "その代わりをやってみよう。"
  },
  {
    "start": 586172,
    "end": 590298,
    "text": "まず左辺に、この関数のtに関する導関数がある。"
  },
  {
    "start": 590384,
    "end": 601920,
    "text": "この関数のtに関する導関数を計算してみよう。この導関数は、kにラムダを掛け、ラムダtのa乗に等しい。"
  },
  {
    "start": 602530,
    "end": 612400,
    "text": "これはラムダに関数そのものを掛けたものに等しく、kにラムダtのe乗を掛けたものである。"
  },
  {
    "start": 612930,
    "end": 616050,
    "text": "この2つの表現が同じであることがわかるだろう。"
  },
  {
    "start": 616120,
    "end": 618082,
    "text": "この関数はここにある。"
  },
  {
    "start": 618216,
    "end": 622926,
    "text": "この関数は微分方程式の解である。"
  },
  {
    "start": 623118,
    "end": 630674,
    "text": "微分方程式があるとき、微分方程式の解は二次方程式を解くときのような数ではない。"
  },
  {
    "start": 630722,
    "end": 640522,
    "text": "を解くと、左辺と右辺が等しくなるxの値がいくつか得られる。"
  },
  {
    "start": 640656,
    "end": 647866,
    "text": "微分方程式の場合、左辺を右辺に等しくする関数を求める。"
  },
  {
    "start": 648048,
    "end": 654346,
    "text": "通常、変数tを省略して微分方程式を次のように書く。"
  },
  {
    "start": 654458,
    "end": 659646,
    "text": "btだから、bドットはラムダにbを掛けたものに等しい。"
  },
  {
    "start": 659748,
    "end": 666210,
    "text": "このドットは、この関数がある変数に関して導関数であることを暗示している。"
  },
  {
    "start": 666950,
    "end": 675342,
    "text": "基本的には、この微分方程式の結果がバニーの進化を表すことになる。"
  },
  {
    "start": 675406,
    "end": 682280,
    "text": "tのbを見つけたこの関数は、ウサギの個体数がどのように増えていくかを表している。"
  },
  {
    "start": 682810,
    "end": 696874,
    "text": "通常、微分方程式を使って時間経過に伴うシステムの状態をモデル化し、システムの初期状態が与えられた任意の時間ステップにおけるシステムの状態を与える関数を見つけることを目標とする。"
  },
  {
    "start": 696992,
    "end": 699990,
    "text": "これが状態空間モデルでやることだ。"
  },
  {
    "start": 700150,
    "end": 710430,
    "text": "状態空間モデルでは、tの状態表現hによって、tの入力信号xをtの出力信号yに次のように対応付けることができる。"
  },
  {
    "start": 711010,
    "end": 725234,
    "text": "つまり、この関数h of tの時間に関する導関数は、aまたはaにh of tを掛け、さらにbにx of tを掛けた行列に等しい。"
  },
  {
    "start": 725352,
    "end": 728790,
    "text": "このシステムの出力は次のように計算される。"
  },
  {
    "start": 729370,
    "end": 733314,
    "text": "この状態空間モデルは線形で時間不変である。"
  },
  {
    "start": 733442,
    "end": 746202,
    "text": "上の式の関係が線形であるため線形であり、パラメータ行列A-B-Cとdが時間によって変化しないため時間不変であることがお分かりいただけるだろう。"
  },
  {
    "start": 746256,
    "end": 748380,
    "text": "これらは各時間ステップで常に固定されている。"
  },
  {
    "start": 749630,
    "end": 759674,
    "text": "今のところ、簡単のため、Abcとdだけでなく、入力、出力、tのhなど、これらすべてのパラメータはベクトルではなく数値であると考える。"
  },
  {
    "start": 759722,
    "end": 762510,
    "text": "後ほど、分析をベクトルへと拡大する。"
  },
  {
    "start": 763010,
    "end": 772514,
    "text": "さて、これらの式はわかったが、tの入力xが与えられたとき、このモデルの出力をどのように計算すればいいのだろうか？"
  },
  {
    "start": 772712,
    "end": 776142,
    "text": "見ての通り、最初の式は微分方程式である。"
  },
  {
    "start": 776206,
    "end": 786102,
    "text": "このように出力を計算するためには、各時間ステップtにおけるシステムの状態を記述する関数hが必要である。"
  },
  {
    "start": 786236,
    "end": 794506,
    "text": "このモデルのtの出力yを求めるには、まずこの微分方程式を解く必要がある。"
  },
  {
    "start": 794608,
    "end": 800758,
    "text": "これは、すべての時間ステップについて、このシステムの状態を記述するtの関数hを見つけることを意味する。"
  },
  {
    "start": 800944,
    "end": 806238,
    "text": "この微分方程式を解くのは、解析的に困難である。"
  },
  {
    "start": 806324,
    "end": 808890,
    "text": "また、私たちは連続的な信号は扱わない。"
  },
  {
    "start": 808970,
    "end": 817650,
    "text": "というのも、コンピュータやデジタル機器を扱う場合、常に離散系を扱うからである。"
  },
  {
    "start": 817990,
    "end": 828566,
    "text": "一つの方法は、この系を実際に離散化し、連続的な方法ではなく、離散化された方法でtのhの近似解を計算できるようにすることである。"
  },
  {
    "start": 828668,
    "end": 833880,
    "text": "では、どのようにシステムを離散化して計算するか、次にシステム自体の出力を見てみよう。"
  },
  {
    "start": 835450,
    "end": 839414,
    "text": "微分方程式を解くには、tの関数hを求める必要がある。"
  },
  {
    "start": 839452,
    "end": 841562,
    "text": "つまり、私たちの場合はこの関数だ。"
  },
  {
    "start": 841616,
    "end": 844102,
    "text": "では、ポインターに変えてみよう。"
  },
  {
    "start": 844166,
    "end": 850880,
    "text": "このtの関数hをここに置き換えると、左辺は右辺と等しくなる。"
  },
  {
    "start": 851970,
    "end": 858682,
    "text": "このtの関数hを連続領域で見つけることは、解析的に非常に難しい。"
  },
  {
    "start": 858826,
    "end": 867534,
    "text": "また、離散入力を扱うので、この系を離散化することができ、この微分方程式の解を近似することができる。"
  },
  {
    "start": 867662,
    "end": 874530,
    "text": "したがって、すべての時間ステップについてtのhを求めるのではなく、特定の時間ステップについてtのhを求めることができる。"
  },
  {
    "start": 874950,
    "end": 882002,
    "text": "例えば、tのhをゼロのh、1のh、2のh、3のh、etc......について求めることができる。"
  },
  {
    "start": 882146,
    "end": 885462,
    "text": "この場合、ステップサイズ（step size）を選んだ。"
  },
  {
    "start": 885516,
    "end": 894282,
    "text": "私の場合、ステップサイズ1で区切られた特定の時間ステップでのみtの関数hを評価するため、1つを選択した。"
  },
  {
    "start": 894336,
    "end": 900326,
    "text": "ゼロから1へ、1から2へ、2から3へ、これがデルタになる。"
  },
  {
    "start": 900518,
    "end": 903086,
    "text": "だから、バニーの問題を思い出してほしい。"
  },
  {
    "start": 903188,
    "end": 907994,
    "text": "このバニー問題の近似解をオイラーの方法を使って求めてみよう。"
  },
  {
    "start": 908042,
    "end": 924590,
    "text": "では、解析解を使ったこのウサギの問題の結果をお見せする前に、非常に簡単に計算できたが、同じ解をエベラーの方法を使って、あるいは同じではないが、オイラーの方法を使った近似解を使って作ってみよう。"
  },
  {
    "start": 924750,
    "end": 935618,
    "text": "そこでまず、バニーの個体数モデルを書き換えてみよう。これは、バニーの個体数を変数に関して記述する関数の微分である。"
  },
  {
    "start": 935714,
    "end": 939580,
    "text": "時間はラムダに関数自身を掛けたものに等しい。"
  },
  {
    "start": 940830,
    "end": 944662,
    "text": "高校時代のことを思い出してほしいのだが、微分の定義とは何だろう？"
  },
  {
    "start": 944726,
    "end": 957470,
    "text": "導関数は、ステップtで評価される関数にステップを足したものから、ステップtの時点での関数をステップサイズで割ったものを引いたもののゼロになるステップの極限に等しい。"
  },
  {
    "start": 957540,
    "end": 959562,
    "text": "これがデリバティブの定義だ。"
  },
  {
    "start": 959626,
    "end": 961520,
    "text": "ここには目新しいものは何もない。"
  },
  {
    "start": 962130,
    "end": 968766,
    "text": "さて、左辺と右辺はデルタのとき等しい。"
  },
  {
    "start": 968798,
    "end": 972754,
    "text": "私たちが使っているステップサイズはとても小さい。"
  },
  {
    "start": 972872,
    "end": 976498,
    "text": "非常に小さなステップサイズを選んだとする。"
  },
  {
    "start": 976664,
    "end": 982274,
    "text": "であれば、この等式を取り除き、近似式に変えることができる。"
  },
  {
    "start": 982322,
    "end": 986742,
    "text": "ステップサイズを非常に小さくしたらどうだろう？"
  },
  {
    "start": 986796,
    "end": 991260,
    "text": "そうすれば、左辺は右辺とほぼ等しくなる。"
  },
  {
    "start": 991870,
    "end": 1007386,
    "text": "ここで、両辺にデルタを掛け合わせ、この項を右辺に持ってくることで、タイムステップtにデルタを加えたときの関数bの値を計算することができる。"
  },
  {
    "start": 1007418,
    "end": 1016980,
    "text": "つまり、タイムステップtの関数の微分にデルタを乗じたものにタイムステップtの関数を足したものを1歩進める。"
  },
  {
    "start": 1018070,
    "end": 1020900,
    "text": "関数の微分は何ですか？"
  },
  {
    "start": 1021270,
    "end": 1030966,
    "text": "ここで、近似しようとしている関数の導関数は、ラムダに関数そのものを掛けたものに等しいことがわかる。"
  },
  {
    "start": 1031148,
    "end": 1035682,
    "text": "この式で置き換えることができる。"
  },
  {
    "start": 1035746,
    "end": 1041366,
    "text": "ここで母集団モデルを置き換えれば、以下の式が得られる。"
  },
  {
    "start": 1041478,
    "end": 1046326,
    "text": "これにより、タイムステップtにデルタを加えた時点でのバニーの個体数を計算することができる。"
  },
  {
    "start": 1046358,
    "end": 1054794,
    "text": "次の時間ステップでは、前の時間ステップでのバニーの個体数にデルタとラムダを掛けたものが与えられる。"
  },
  {
    "start": 1054922,
    "end": 1057870,
    "text": "というリカレント定式化を得た。"
  },
  {
    "start": 1059570,
    "end": 1060202,
    "text": "素晴らしい。"
  },
  {
    "start": 1060266,
    "end": 1066160,
    "text": "これで、ウサギの集団の時間的な状態を近似するリカレント定式化ができた。"
  },
  {
    "start": 1066770,
    "end": 1073074,
    "text": "ラムダを2に設定したとしよう。つまり、どのバニーも2人の子供を作り、デルタは1になる。"
  },
  {
    "start": 1073112,
    "end": 1076082,
    "text": "一歩でも前に進みたい。"
  },
  {
    "start": 1076136,
    "end": 1080082,
    "text": "tがゼロの場合、tは1に等しく、tは2に等しい。"
  },
  {
    "start": 1080226,
    "end": 1088530,
    "text": "例えば、タイムステップゼロで5人のボニーの集団からスタートした場合、集団の進化は次のように計算できる。"
  },
  {
    "start": 1088690,
    "end": 1094746,
    "text": "タイムステップゼロでの母集団がわかっているので、計算式を使ってタイムステップ1での母集団を計算することができる。"
  },
  {
    "start": 1094768,
    "end": 1102234,
    "text": "bの1は、ラムダデルタにラムダを掛け合わせたものに、前のタイムステップの母集団を掛けたものに等しい。"
  },
  {
    "start": 1102272,
    "end": 1108078,
    "text": "この式と同じように、前のタイムステップの母集団を足すと15になる。"
  },
  {
    "start": 1108164,
    "end": 1109102,
    "text": "それは理にかなっている。"
  },
  {
    "start": 1109156,
    "end": 1109374,
    "text": "なぜですか？"
  },
  {
    "start": 1109412,
    "end": 1111146,
    "text": "5羽のウサギからスタートしたからだ。"
  },
  {
    "start": 1111258,
    "end": 1114938,
    "text": "1匹のウサギに2匹の子供がいるので、10匹の赤ちゃんがいることになる。"
  },
  {
    "start": 1115114,
    "end": 1120420,
    "text": "初期人口である5人に10人の赤ん坊を加えると、人口は15人となる。"
  },
  {
    "start": 1120950,
    "end": 1127878,
    "text": "ステップ1の時点の人口がわかったので、ステップ2の時点の人口を計算することができる。"
  },
  {
    "start": 1127964,
    "end": 1132038,
    "text": "私たちは15羽のウサギを飼ったので、1羽につき2人の子供が生まれたことになる。"
  },
  {
    "start": 1132124,
    "end": 1133462,
    "text": "私たちには30人の子供がいる。"
  },
  {
    "start": 1133596,
    "end": 1136902,
    "text": "15に30を足すと45になる。"
  },
  {
    "start": 1137036,
    "end": 1143402,
    "text": "ステップ2の時点の母集団が得られたので、ステップ3の時点の母集団を計算することができる。"
  },
  {
    "start": 1143456,
    "end": 1147226,
    "text": "私たちには45の身体があり、それぞれが2人の子供を作る。"
  },
  {
    "start": 1147408,
    "end": 1150458,
    "text": "90人の子どもたちに45人を加えた135人。"
  },
  {
    "start": 1150464,
    "end": 1166450,
    "text": "この時間ステップで得られている解を、前に求めた解析解と比較すると、求め方はお見せしませんでしたが、変数分離と呼ばれる方法を使えば簡単に求めることができます。"
  },
  {
    "start": 1167670,
    "end": 1175910,
    "text": "この2つの関数のプロットを比較すると、解析関数が非常に速く成長するのに対し、近似解は非常にゆっくり成長することがわかる。"
  },
  {
    "start": 1176490,
    "end": 1186614,
    "text": "両者の変化の仕方は似ているが、私たちが選んだステップサイズが非常に大きいため、両者が重なることはない。"
  },
  {
    "start": 1186652,
    "end": 1189994,
    "text": "つまり、私たちはラムダをデルタと同じ1にした。"
  },
  {
    "start": 1190112,
    "end": 1194186,
    "text": "より良い近似値を作るには、デルタを非常に小さくする必要がある。"
  },
  {
    "start": 1194288,
    "end": 1202350,
    "text": "実際、エグラー法は、解析解の非常に良い近似という、あまり良い結果を与えてはくれない。"
  },
  {
    "start": 1203730,
    "end": 1211950,
    "text": "バニーの母集団で使ったのと同様の推論を使って、状態空間モデルのシステムを離散化する。"
  },
  {
    "start": 1212100,
    "end": 1214340,
    "text": "そのシステムをどのように離散化するか？"
  },
  {
    "start": 1215670,
    "end": 1236886,
    "text": "覚えていると思うが、微分の定義を使って、タイムステップtにデルタを加えた時間で評価される関数は、多かれ少なかれ、デルタにタイムステップtでの関数の微分とタイムステップtでの関数をかけたものに等しいことがわかった。"
  },
  {
    "start": 1236988,
    "end": 1240150,
    "text": "ステップtにおける関数の微分は？"
  },
  {
    "start": 1240220,
    "end": 1250070,
    "text": "状態空間モデルから、この関数h primeの微分は、tのaにhを掛けたものに、tのbにxを掛けたものを足したものに等しいことが分かっている。"
  },
  {
    "start": 1250160,
    "end": 1259050,
    "text": "この式をこの項に置き換えるだけで、次のような導出が得られる。"
  },
  {
    "start": 1259130,
    "end": 1263202,
    "text": "この部分をtのhプライムに置き換える。"
  },
  {
    "start": 1263336,
    "end": 1264942,
    "text": "それにデルタを掛ける。"
  },
  {
    "start": 1265006,
    "end": 1273106,
    "text": "このtの項hを集めて、離散化モデルの離散化パラメータを求めると、次のようになる。"
  },
  {
    "start": 1273208,
    "end": 1300140,
    "text": "aバーを同一行列にデルタaを足したものに、bバーをデルタbに等しいものに設定すれば、バニー問題（バニー人口問題）と同じように、前の状態が与えられたときにモデルの次の状態を計算できる再帰式が得られる。"
  },
  {
    "start": 1301410,
    "end": 1312190,
    "text": "実際にこの論文では、まず状態空間モデルの連続定式化を示している。"
  },
  {
    "start": 1312260,
    "end": 1321454,
    "text": "そして、ここにあるような離散化されたモデルを見せ、この離散化されたパラメーターを求める方法も教えてくれる。"
  },
  {
    "start": 1321502,
    "end": 1326354,
    "text": "離散化されたモデルのパラメータは、このa barとb barである。"
  },
  {
    "start": 1326472,
    "end": 1329146,
    "text": "実際、論文ではエグラー法は使っていない。"
  },
  {
    "start": 1329198,
    "end": 1339254,
    "text": "私がここで離散化したものを導き出すのに使ったのは、ゼロ次ホールドと呼ばれる方法です。"
  },
  {
    "start": 1339382,
    "end": 1354138,
    "text": "離散化のアイデアは、この微分方程式の解析解を計算する代わりに、離散的な時間ステップにおけるシステムの状態を近似的に計算することである。"
  },
  {
    "start": 1354234,
    "end": 1365220,
    "text": "そして、この近似された状態値をこの2番目の関係に差し込むことで、システムの出力を得ることができる。"
  },
  {
    "start": 1366630,
    "end": 1373810,
    "text": "実際には、前に見たように、システムを離散化するためにデルタ・パラメータを選択しなければならなかった。"
  },
  {
    "start": 1373880,
    "end": 1387640,
    "text": "実際には、離散化ステップ・デルタを選択するのではなく、勾配降下法で学習するモデルのパラメータとし、モデルが目にする入力に基づいてこのパラメータ・デルタを学習するようにする。"
  },
  {
    "start": 1388910,
    "end": 1400646,
    "text": "さて、システムの出力を逐次的に計算するリカレント定式化ができたところで、それを使ってさまざまな時間ステップのシステムの出力を計算するにはどうすればいいだろうか？"
  },
  {
    "start": 1400678,
    "end": 1401594,
    "text": "そうしよう。"
  },
  {
    "start": 1401712,
    "end": 1405920,
    "text": "簡単のため、システムの初期状態をゼロとする。"
  },
  {
    "start": 1406290,
    "end": 1408862,
    "text": "システムの最初の状態。"
  },
  {
    "start": 1408916,
    "end": 1414666,
    "text": "xゼロ、x1、x2、x3、x4からなる入力があるとする。"
  },
  {
    "start": 1414698,
    "end": 1417730,
    "text": "離散化された状態空間モデルを使うのはそのためだ。"
  },
  {
    "start": 1417880,
    "end": 1427906,
    "text": "bにゼロを掛けたものに等しいシステムの最初の状態を計算することができる。"
  },
  {
    "start": 1428008,
    "end": 1429926,
    "text": "この言葉は存在しない。"
  },
  {
    "start": 1430108,
    "end": 1436566,
    "text": "この状態を使って、システムの最初の出力を計算することができる。"
  },
  {
    "start": 1436748,
    "end": 1444314,
    "text": "これで、前の状態と次の入力を使って、システムの次の状態を計算することができる。"
  },
  {
    "start": 1444352,
    "end": 1452838,
    "text": "aに、すでに計算したシステムの前の状態を掛けたバーと、システムの次の入力を掛けたbを足したもの。"
  },
  {
    "start": 1453024,
    "end": 1459840,
    "text": "次の状態があれば、次の出力を計算することができる。yはcにhを1倍したものに等しい。"
  },
  {
    "start": 1460370,
    "end": 1463230,
    "text": "さて、前の状態を使えば、次の状態を計算できる。"
  },
  {
    "start": 1463300,
    "end": 1469310,
    "text": "h two, h twoは、aに前の状態を掛け、さらにbに次の入力を掛けたものに等しい。"
  },
  {
    "start": 1469470,
    "end": 1472290,
    "text": "h 2を使えば、y 2を計算できる。"
  },
  {
    "start": 1472360,
    "end": 1478962,
    "text": "リカレント・ニューラル・ネットワークで行っていたのは、まさにこれである。"
  },
  {
    "start": 1479026,
    "end": 1486760,
    "text": "リカレント・ニューラル・ネットワークがリカレントと呼ばれるのは、前のステップから次のステップに進むための再帰的定式化を持っているからだ。"
  },
  {
    "start": 1487210,
    "end": 1491206,
    "text": "ご覧の通り、これはまさに私たちがやったことだ。"
  },
  {
    "start": 1491228,
    "end": 1499266,
    "text": "初期状態はゼロであり、最初の入力と前の状態を使って最初の出力を計算する。"
  },
  {
    "start": 1499378,
    "end": 1503680,
    "text": "これにより、最初の出力だけでなく、システムの次の状態も生成される。"
  },
  {
    "start": 1504130,
    "end": 1510942,
    "text": "次に、システムの次の状態と次の入力を使って、次の出力とシステムの次の状態を生成する。"
  },
  {
    "start": 1511076,
    "end": 1518606,
    "text": "そして、システムの次の状態と次の出力入力を使って、次の出力とシステムの次の状態を生成する。"
  },
  {
    "start": 1518638,
    "end": 1540762,
    "text": "先ほど見たように、リカレント定式化は推論に適している。なぜなら、一定のメモリと計算量で一度に一つのトークンを計算できるからだ。"
  },
  {
    "start": 1540816,
    "end": 1553578,
    "text": "大規模言語モデルでは、前のトークンとプロンプトを使用して一度に1つのトークンを生成するため、これは大規模言語モデルの推論に適しています。"
  },
  {
    "start": 1553754,
    "end": 1560122,
    "text": "しかし、リカレント方式は訓練には向かない。なぜなら、訓練中にすでにすべてのトークンを持っているからだ。"
  },
  {
    "start": 1560186,
    "end": 1564798,
    "text": "モデルの出力を1トークンずつ生成する必要はない。"
  },
  {
    "start": 1564884,
    "end": 1567758,
    "text": "入力の順序はすでに決まっている。"
  },
  {
    "start": 1567854,
    "end": 1570142,
    "text": "ターゲットとなるシークエンスはすでにある。"
  },
  {
    "start": 1570206,
    "end": 1578518,
    "text": "損失計算とモデルのトレーニングのためにこのような逐次計算をすることなく、並列にモデルの出力を計算したい。"
  },
  {
    "start": 1578604,
    "end": 1584710,
    "text": "これはまさに変圧器でやっていることだが、このリカレント計算ではできない。"
  },
  {
    "start": 1585050,
    "end": 1590918,
    "text": "ありがたいことに、状態空間モデルには畳み込みモードもあり、並列化できる。"
  },
  {
    "start": 1591014,
    "end": 1592540,
    "text": "どう動くか見てみよう。"
  },
  {
    "start": 1593550,
    "end": 1600138,
    "text": "そこで、前に見たリカレント定式化だけを使って、ステップごとに出力を展開してみよう。"
  },
  {
    "start": 1600224,
    "end": 1608782,
    "text": "先に見たように、モデルの最初の状態を計算するためには、初期入力にbを掛ける必要がある。"
  },
  {
    "start": 1608836,
    "end": 1613380,
    "text": "この用語は最初の状態では存在しない。"
  },
  {
    "start": 1614150,
    "end": 1620258,
    "text": "最初の状態を使って、cにhゼロを掛けた最初の出力を計算することができる。"
  },
  {
    "start": 1620424,
    "end": 1633800,
    "text": "このことから、hのゼロをbにxのゼロを掛けた展開式に置き換えることができる。"
  },
  {
    "start": 1634810,
    "end": 1637702,
    "text": "そして、hゼロを使ってh1を計算することができる。"
  },
  {
    "start": 1637836,
    "end": 1642922,
    "text": "h oneは、aにh zeroを掛け、bにx oneを掛けたものに等しい。"
  },
  {
    "start": 1642976,
    "end": 1644042,
    "text": "Hゼロとは何か？"
  },
  {
    "start": 1644096,
    "end": 1647034,
    "text": "Hゼロはまさにここにあるものだ。"
  },
  {
    "start": 1647072,
    "end": 1648986,
    "text": "bにゼロを掛ける。"
  },
  {
    "start": 1649088,
    "end": 1667940,
    "text": "この計算を使ってタイムステップ1でのシステムの状態、出力を計算し、h oneを使ってタイムステップ1でのシステムの出力を計算し、h oneの式を展開してこの式を得ることができる。"
  },
  {
    "start": 1668470,
    "end": 1687400,
    "text": "同様の理由で、h2空間の計算も、入力とモデルのパラメータのみに依存するように拡張することができる。"
  },
  {
    "start": 1688330,
    "end": 1697638,
    "text": "おわかりのように、出力のケイト・トークンを計算するには、次のような和算が必要なパターンがある。"
  },
  {
    "start": 1697734,
    "end": 1703562,
    "text": "例えば、y 2を計算するには、見ての通り、x 2にcbを掛けている。"
  },
  {
    "start": 1703626,
    "end": 1727202,
    "text": "見ての通り、ykはxkにcbを掛けたものであり、次に前のトークン、つまりここではx1、つまりここではxkマイナス1にcab、cab、etcを掛けたものであり、次に前のトークンにcaを掛けたものである。"
  },
  {
    "start": 1727256,
    "end": 1728934,
    "text": "これはまさに私たちがここで言っていることだ。"
  },
  {
    "start": 1728972,
    "end": 1741978,
    "text": ""
  },
  {
    "start": 1742144,
    "end": 1748986,
    "text": "システムの出力は、入力xとカーネルkの畳み込みを使って計算できる。"
  },
  {
    "start": 1749168,
    "end": 1759600,
    "text": "この公式は、私たちがここで導き出した公式であり、基本的には前に見たすべての展開公式のパターンを表している。"
  },
  {
    "start": 1760290,
    "end": 1782306,
    "text": "この式を使えば、畳み込みニューラルネットワークで使うカーネルと同じように、カーネルの第1項をcb、第2項をcab、第3項をcaとし、kbのべき乗、エトセトラ、エトセトラ、カーネルの最後の項目まで、カーネルを構築することができ、このカーネルを入力と畳み込んで直接出力を計算することができる。"
  },
  {
    "start": 1782338,
    "end": 1787350,
    "text": "覚えているように、畳み込みは並列化できるものだ。"
  },
  {
    "start": 1787770,
    "end": 1795494,
    "text": "というのも、このカーネルを構築すれば、それを入力に畳み込んで出力を生成できるからだ。"
  },
  {
    "start": 1795622,
    "end": 1804718,
    "text": "このようにカーネルを使って計算された出力が、以前これらの式を展開して求めたものとまったく同じであることを証明したい。"
  },
  {
    "start": 1804884,
    "end": 1806320,
    "text": "そうしよう"
  },
  {
    "start": 1807410,
    "end": 1818770,
    "text": "このように、cb、cab、etcetera、ca、kbのべき乗まで、生成する配列の長さに応じてカーネルを構築する。"
  },
  {
    "start": 1819270,
    "end": 1821854,
    "text": "私はカーネルを逆にしているだけだ。"
  },
  {
    "start": 1821902,
    "end": 1825022,
    "text": "前期はCBではないが、後期はCBだ。"
  },
  {
    "start": 1825086,
    "end": 1830562,
    "text": "その方がイメージしやすいからだ。"
  },
  {
    "start": 1830706,
    "end": 1834982,
    "text": "ゼロ、1、2、3と入力する。"
  },
  {
    "start": 1835036,
    "end": 1836662,
    "text": "少しパディングを加える。"
  },
  {
    "start": 1836716,
    "end": 1837494,
    "text": "その理由をお見せしよう。"
  },
  {
    "start": 1837532,
    "end": 1841078,
    "text": "そして、この畳み込みによって得られる出力が得られる。"
  },
  {
    "start": 1841254,
    "end": 1843980,
    "text": "このコンボリューションをステップ・バイ・ステップで実行してみよう。"
  },
  {
    "start": 1845550,
    "end": 1854106,
    "text": "畳み込みの最初の出力は、入力の最初の4つのトークンにカーネルをスライドさせただけである。"
  },
  {
    "start": 1854298,
    "end": 1866142,
    "text": "カーネルのこの項と入力のこの項、カーネルのこの項と入力のこの項、カーネルのこの項と入力のこの項を掛け合わせる。"
  },
  {
    "start": 1866206,
    "end": 1873902,
    "text": "そして、これらの結果をすべて合計すると、yゼロ、xゼロにcbを掛けたものに等しいことがわかる。"
  },
  {
    "start": 1874046,
    "end": 1877202,
    "text": "これはまさに、この公式から導き出された結果である。"
  },
  {
    "start": 1877256,
    "end": 1885670,
    "text": "見ての通り、最初のトークンしかない場合、前のトークンがないので、トークン自体にcbを掛けたものにしかならない。"
  },
  {
    "start": 1886330,
    "end": 1887606,
    "text": "前へ進もう。"
  },
  {
    "start": 1887788,
    "end": 1893494,
    "text": "そしてカーネルを1ステップ前進させると、次のような出力が得られる。"
  },
  {
    "start": 1893542,
    "end": 1902946,
    "text": "カーネルのこの項目は×1倍、カーネルのこの項目は×0倍、その他の項目はすべて×0倍である。"
  },
  {
    "start": 1903078,
    "end": 1905534,
    "text": "Y1はここでこれに匹敵する。"
  },
  {
    "start": 1905652,
    "end": 1909214,
    "text": "ご覧のように、yが等しいとき、kは1に等しい。"
  },
  {
    "start": 1909252,
    "end": 1916766,
    "text": "x1にcbを掛けたものがあり、さらに前の項があるので、xゼロにcabを掛けたものがある。"
  },
  {
    "start": 1916958,
    "end": 1927346,
    "text": "そしてカーネルを前方にスライドさせると、この項がこれと掛け合わされ、この項がこれと掛け合わされ、この項がこれと掛け合わされていることがわかる。"
  },
  {
    "start": 1927368,
    "end": 1930674,
    "text": "最後の1つはゼロを掛けるので、合計には寄与しない。"
  },
  {
    "start": 1930802,
    "end": 1933206,
    "text": "y 2はこのように計算される。"
  },
  {
    "start": 1933308,
    "end": 1939162,
    "text": "ご覧のように、この式と比較すれば、y k."
  },
  {
    "start": 1939216,
    "end": 1957946,
    "text": "kが2の場合、cbにx2を掛けたもの、次にcabにx1を掛けたもの、そしてcaにaを掛けないもの、実際にはcaの2乗にbを掛けたものにxゼロを掛けたもの、つまり前のトークンとxマイナス1を比較したもの、などである。"
  },
  {
    "start": 1957978,
    "end": 1961040,
    "text": "ステップ4とこのステップについては、自分で確かめることができる。"
  },
  {
    "start": 1962610,
    "end": 1971374,
    "text": "畳み込み計算の最も優れた点は、ykの出力が前の出力に依存しないため、並列化できることである。"
  },
  {
    "start": 1971422,
    "end": 1985718,
    "text": "私が言いたいのは、私たちがここでやっているこの製品は、たとえば1つのスレッド、あるいはGPUの1つのコアで行うことができ、この製品は同時に行うこともできるということです。"
  },
  {
    "start": 1985884,
    "end": 1990326,
    "text": "畳み込み計算は並列化できる。"
  },
  {
    "start": 1990518,
    "end": 1996774,
    "text": "問題はカーネルで、コンボリューション用のカーネルを作る必要がある。"
  },
  {
    "start": 1996822,
    "end": 2002694,
    "text": "このカーネルの構築は、計算の観点からも、メモリーの観点からも、少々高くつく可能性がある。"
  },
  {
    "start": 2002832,
    "end": 2012682,
    "text": "というのも、すでに入力トークンを持っており、ターゲットも持っているからだ。"
  },
  {
    "start": 2012746,
    "end": 2020414,
    "text": "計算コストが高くても、すべての入力トークンに対して並列にモデルの出力を計算することができる。"
  },
  {
    "start": 2020542,
    "end": 2026162,
    "text": "であれば、リカレント定式化を使って、このモデルから一度に1トークンずつ推論することができる。"
  },
  {
    "start": 2026296,
    "end": 2039906,
    "text": "また、こうすることで、トークンを1つ生成するのにかかる計算コストは、それが最初のトークンであろうと、100個目のトークンであろうと、1000個目のトークンであろうと、どのトークンを生成する場合でも常に同じになることもわかっている。"
  },
  {
    "start": 2040098,
    "end": 2054794,
    "text": "これは、トークンを生成する場合、最初のトークンを生成する方が100番目のトークンを生成するよりも低コストであるトランスフォーマーとは異なる。"
  },
  {
    "start": 2054842,
    "end": 2058160,
    "text": "KVCashを使えば、100個のドットプロダクトができる。"
  },
  {
    "start": 2059410,
    "end": 2066302,
    "text": "ひとつ特筆すべきは、論文では出力を計算する際にdという用語を使用していないことだ。"
  },
  {
    "start": 2066366,
    "end": 2070110,
    "text": "というのも、これはスキップ接続と考えることができるからだ。"
  },
  {
    "start": 2070190,
    "end": 2071554,
    "text": "その理由をお見せしよう。"
  },
  {
    "start": 2071752,
    "end": 2076630,
    "text": "入力xがあるとする。"
  },
  {
    "start": 2076700,
    "end": 2078550,
    "text": "このXがあるとする。"
  },
  {
    "start": 2078700,
    "end": 2092746,
    "text": "これは、状態空間モデルと呼ぶブラックボックスに送られ、再帰的に、つまり連続的に状態計算を行い、出力yを生成する。"
  },
  {
    "start": 2092928,
    "end": 2105402,
    "text": "ということは、このTのDxは、基本的に、入力を受け取り、状態空間モデルをスキップして、ある数dを掛け合わせ、それを直接出力に送ることを意味していることがわかる。"
  },
  {
    "start": 2105546,
    "end": 2114734,
    "text": "なぜなら、このdはどの時間ステップでもシステムの状態に依存しないからである。"
  },
  {
    "start": 2114932,
    "end": 2117886,
    "text": "そのため、スキップ接続と表現することができる。"
  },
  {
    "start": 2117918,
    "end": 2121060,
    "text": "だから紙面では触れていないのだ。"
  },
  {
    "start": 2121510,
    "end": 2137814,
    "text": "前にもお話ししたように、このテスト空間モデルについてこれまで行ってきた分析では、簡単のために、すべてのパラメーターは単一の数値であり、入力も単一の数値、出力も単一の数値であると考えている。"
  },
  {
    "start": 2137932,
    "end": 2143830,
    "text": "通常、言語モデルを扱う場合、特に入力は単一の数値ではなく、ベクトルである。"
  },
  {
    "start": 2143910,
    "end": 2157934,
    "text": "トークンの列があるとすると、各トークンは、バニラ・トランスフォームの場合は512次元のベクトルで表現され、たとえばllamaの場合は4096次元のベクトルで表現される。"
  },
  {
    "start": 2158132,
    "end": 2165170,
    "text": "状態空間モデルを、ベクトル入力とベクトル出力で扱うにはどうすればいいのか？"
  },
  {
    "start": 2165590,
    "end": 2173058,
    "text": "このアイデアは、入力ベクトルの各次元が独立した状態空間モデルによって管理されるというものだ。"
  },
  {
    "start": 2173144,
    "end": 2183762,
    "text": "次元、512個のトークンのベクトルからなる入力があり、512次元の出力トークンを生成する必要があるとする。"
  },
  {
    "start": 2183826,
    "end": 2196666,
    "text": "各次元に1つずつ状態空間モデルを作成する。つまり、0次元に1つ、1次元に1つ、2次元に1つ、etc...と、すべての次元に1つずつ状態空間モデルを作成し、これらすべての状態空間筋肉は互いに独立している。"
  },
  {
    "start": 2196848,
    "end": 2212190,
    "text": "この考え方はとても奇妙に見えるかもしれないが、トランスフォーマーの多頭注目について考えれば、それほど奇妙なことではない。なぜなら、トランスフォーマーには、それぞれがトークンの埋め込みを表すベクトルからなる入力もあるからだ。"
  },
  {
    "start": 2212710,
    "end": 2216094,
    "text": "各埋め込みに512次元があるとする。"
  },
  {
    "start": 2216222,
    "end": 2220690,
    "text": "それから、マルチヘッド注目は基本的にいくつかの次元をグループ化する。"
  },
  {
    "start": 2221830,
    "end": 2224910,
    "text": "バニラ・トランスに8つのヘッドがあるとする。"
  },
  {
    "start": 2224990,
    "end": 2235560,
    "text": "これは512を8で割ったもので、各ヘッドが64次元を管理し、各ヘッドが他のヘッドから独立していることを意味する。"
  },
  {
    "start": 2237790,
    "end": 2241322,
    "text": "トランスフォーマーに有効なら、状態空間モデルにも有効だ。"
  },
  {
    "start": 2241376,
    "end": 2242700,
    "text": "実際にそうだ。"
  },
  {
    "start": 2243150,
    "end": 2264340,
    "text": "パラメータABCD、入力とYTはベクトルとなり、シーケンス次元とDモデルの次元を持つことになる。"
  },
  {
    "start": 2265190,
    "end": 2287398,
    "text": "状態空間モデルのリカレント定式化では、この行列aが非常に重要である。状態空間モデルのa行列は、直感的には過去の状態の情報を取り込む行列と考えることができるので、H Tから1を引いたものが新しい状態、TのHを構築する。"
  },
  {
    "start": 2287564,
    "end": 2292170,
    "text": "また、この情報がどのようにコピーされるかも決定される。"
  },
  {
    "start": 2292240,
    "end": 2306766,
    "text": "状態空間モデルの出力を特定のタイムステップで生成するとき、つまり特定のトークンについては、そのタイムステップの状態に依存するが、タイムステップtの状態は、それ以前のすべての状態に依存する。"
  },
  {
    "start": 2306868,
    "end": 2313614,
    "text": "基本的にこのマトリックスは、モデルの情報がどのように時間的に前方に伝達されるかを教えてくれる。"
  },
  {
    "start": 2313652,
    "end": 2320130,
    "text": "このTのhは、基本的にこれまでのモデルの入力の履歴をすべてキャプチャしている。"
  },
  {
    "start": 2320200,
    "end": 2321970,
    "text": "新しいトークンを作る。"
  },
  {
    "start": 2322630,
    "end": 2328030,
    "text": "つまり、このマトリックスの構造には細心の注意が必要なのだ。"
  },
  {
    "start": 2328110,
    "end": 2335378,
    "text": "そうでなければ、次のトークンを生成するために必要な、これまでに見たすべての入力の履歴をうまくキャプチャできないかもしれない。"
  },
  {
    "start": 2335474,
    "end": 2343750,
    "text": "というのも、モデルによって生成される次のトークンは、プロンプトである前のすべてのトークンに依存しなければならないからだ。"
  },
  {
    "start": 2343830,
    "end": 2353402,
    "text": "なぜなら、トークンのリストであるプロンプトを言語モデルに送ると、そのモデルは前のトークンをすべて見て、新しいトークンを生成するはずだからだ。"
  },
  {
    "start": 2353536,
    "end": 2358462,
    "text": "状態空間モデルの場合、これは行列aによって処理される。"
  },
  {
    "start": 2358516,
    "end": 2360974,
    "text": "このマトリックスの構造は非常に重要である。"
  },
  {
    "start": 2361172,
    "end": 2379880,
    "text": "制御工学の知識があれば、状態空間モデルにおいてa行列がシステムの安定性も決定すること、つまりシステムが安定していないとシステムの出力が発散する可能性があることを思い出すかもしれないが、制御工学の知識がなければ関係ない。"
  },
  {
    "start": 2380730,
    "end": 2386002,
    "text": "そこで著者は、行列をうまく振る舞うようにするために、ヒッポ理論を使うことにした。"
  },
  {
    "start": 2386066,
    "end": 2387560,
    "text": "どう動くか見てみよう。"
  },
  {
    "start": 2388090,
    "end": 2394790,
    "text": "ヒッポ理論が何をするのかを直感的に理解してもらうために、フーリエ変換を借りることにしよう。"
  },
  {
    "start": 2394950,
    "end": 2399542,
    "text": "フーリエ変換は信号を分解することができる。"
  },
  {
    "start": 2399606,
    "end": 2404634,
    "text": "これが最初の信号で、一連の正弦波関数に変換されているとする。"
  },
  {
    "start": 2404682,
    "end": 2423070,
    "text": "これらすべての関数をそれぞれの振幅値で合計すると、元の信号が再構築されることがおわかりいただけるだろう。"
  },
  {
    "start": 2423230,
    "end": 2427750,
    "text": "正弦関数を使う代わりに、伝説の多項式を使う。"
  },
  {
    "start": 2428490,
    "end": 2436390,
    "text": "ヒッポ理論では、これまでのすべての入力信号を近似するように行列を構築する。"
  },
  {
    "start": 2436540,
    "end": 2438182,
    "text": "ここでお見せしましょう。"
  },
  {
    "start": 2438236,
    "end": 2442282,
    "text": "これが私たちの入力信号で、時間とともに変化していく。"
  },
  {
    "start": 2442336,
    "end": 2453790,
    "text": "tのx、ここでわかるように、Aマトリックスの目的は、この入力信号から情報を取り込み、それを次の状態へと伝えることである。"
  },
  {
    "start": 2453860,
    "end": 2456590,
    "text": "これは、すべての信号の履歴をキャプチャする。"
  },
  {
    "start": 2457010,
    "end": 2459086,
    "text": "これが私たちの状態だとしよう。"
  },
  {
    "start": 2459188,
    "end": 2465358,
    "text": "その状態を使ってカバ理論で元の信号を再構築すれば、出力が得られる。"
  },
  {
    "start": 2465454,
    "end": 2477930,
    "text": "再構築された信号は、最近の時間ステップでは非常によく近似され、以前の時間ステップではあまりよく近似されない。"
  },
  {
    "start": 2477950,
    "end": 2484854,
    "text": "ですから、ご覧のように、この近似値は、元の信号と比較すると、あまり良いものではありません。"
  },
  {
    "start": 2484972,
    "end": 2487554,
    "text": "最近のものは非常によく復元されている。"
  },
  {
    "start": 2487602,
    "end": 2488330,
    "text": "こちらをご覧いただきたい。"
  },
  {
    "start": 2488400,
    "end": 2490378,
    "text": "これがヒポメトリクスの仕事である。"
  },
  {
    "start": 2490464,
    "end": 2505710,
    "text": "これは、指数移動平均に非常によく似た推論を用いて、最近のトークンの情報をよく捉え、過去のトークンの情報を減衰させる状態空間表現を再構築して構築する。"
  },
  {
    "start": 2506530,
    "end": 2514762,
    "text": "言語モデルでは、トークンで構成されたプロンプトがあり、次のトークンを生成する必要があるからだ。"
  },
  {
    "start": 2514906,
    "end": 2519810,
    "text": "なぜなら、隠れた状態がシステムの出力を決定するからである。"
  },
  {
    "start": 2519880,
    "end": 2520990,
    "text": "次のトークン"
  },
  {
    "start": 2521070,
    "end": 2529954,
    "text": "そのため、ローカルなコンテクストに関する情報を非常によく捉え、グローバルなコンテクストに関する情報を多少失っても大丈夫な隠しステートを構築する必要がある。"
  },
  {
    "start": 2530002,
    "end": 2535110,
    "text": "私たちが生産しているトークンとはかけ離れたトークンについての情報。"
  },
  {
    "start": 2536570,
    "end": 2552086,
    "text": "構造化空間を使った長いシーケンスの効率的モデリングという論文の中で、著者はa行列をカバ行列で初期化するだけで、このようになると言っている。"
  },
  {
    "start": 2552128,
    "end": 2564462,
    "text": "これは基本的にn×nの行列で、主対角線より上の値はすべてゼロであり、その他の値は以下のように計算される。"
  },
  {
    "start": 2564526,
    "end": 2571330,
    "text": "対角の場合、n＋1として計算され、nは行、kは列である。"
  },
  {
    "start": 2572150,
    "end": 2581142,
    "text": "このような行列を作るだけで、モデルの性能は大きく向上する。"
  },
  {
    "start": 2581276,
    "end": 2581670,
    "text": "なぜですか？"
  },
  {
    "start": 2581740,
    "end": 2589098,
    "text": "なぜなら、aマトリックスとは、前の状態に関する情報を捕捉し、それを新しい状態に引き継ぐ役割を担うものだからだ。"
  },
  {
    "start": 2589184,
    "end": 2598970,
    "text": "tのhはベクトルとなる。"
  },
  {
    "start": 2599630,
    "end": 2610830,
    "text": "多次元の状態空間モデルがある場合、このhのtは、次の出力を生成することができるように、前のすべての入力の情報を非常によく捉えている。"
  },
  {
    "start": 2611170,
    "end": 2617758,
    "text": "さて、そろそろマンバの話をしよう。マンバを作るに至った動機について話そう。"
  },
  {
    "start": 2617854,
    "end": 2623442,
    "text": "この論文の中で著者らは、バニラ状態空間モデルが対象とする2つのタスクについて述べている。"
  },
  {
    "start": 2623496,
    "end": 2636054,
    "text": "これまで説明してきた状態空間モデル、あるいはS4モデル（構造化状態空間モデル）。"
  },
  {
    "start": 2636172,
    "end": 2640198,
    "text": "この2つのモデルは、特定の2つのタスクではうまく機能しない。"
  },
  {
    "start": 2640294,
    "end": 2643462,
    "text": "ひとつは選択的コピーで、もうひとつはIHヘッドだ。"
  },
  {
    "start": 2643606,
    "end": 2646006,
    "text": "これらのタスクを紹介しよう。"
  },
  {
    "start": 2646118,
    "end": 2654382,
    "text": "コピー・タスクは基本的に、いくつかの入力トークンがあることを意味する。"
  },
  {
    "start": 2654516,
    "end": 2658922,
    "text": "モデルは同じ出力を出さなければならないが、時間はずれている。"
  },
  {
    "start": 2659066,
    "end": 2667534,
    "text": "これはバニラの状態空間モデルでも可能で、実際には単純な畳み込みで済むからだ。"
  },
  {
    "start": 2667582,
    "end": 2671266,
    "text": "コンボリューションは、我々が行っているタイムシフトを学習することができる。"
  },
  {
    "start": 2671448,
    "end": 2689158,
    "text": "しかし、選択的コピーとは、例えば、青、白、オレンジ、赤、そして緑のような入力トークンがあり、モデルは色のついたトークンのみを生成する必要があり、白のトークンは生成できないことを意味する。"
  },
  {
    "start": 2689254,
    "end": 2716100,
    "text": "というのも、バニラ空間モデルは内容を意識した推論ができないからだ。モデルのパラメータはどのステップでも同じなので、どのトークンに対しても同じである。"
  },
  {
    "start": 2716470,
    "end": 2728242,
    "text": "これが何を意味するのかを直感的に理解してもらうために、例えば、ツイッターのコメントが与えられたとしよう。"
  },
  {
    "start": 2728296,
    "end": 2740890,
    "text": "なぜなら、パラメータABCとdは各入力に対して同じであり、特定の入力ごとに変えることはできないからだ。"
  },
  {
    "start": 2741630,
    "end": 2754378,
    "text": "状態空間モデルが苦手とする2つ目のタスクは、誘導ヘッドである。これは、モデルが現在の入力を構築するために、以前の入力から情報を呼び出す必要があることを意味する。"
  },
  {
    "start": 2754474,
    "end": 2756350,
    "text": "例えば、こんな例がある。"
  },
  {
    "start": 2756420,
    "end": 2764802,
    "text": "例えば、モデルが黒いトークンを見るたびに、前に見たものを思い出して青いトークンを出力する。"
  },
  {
    "start": 2764936,
    "end": 2771490,
    "text": "というのも、このモデルは内容を意識した推論ができないため、このタスクをうまくこなすことができないからだ。"
  },
  {
    "start": 2771910,
    "end": 2780978,
    "text": "これがマンバ創設の動機であり、この2つのタスクについて語った論文の参考文献である。"
  },
  {
    "start": 2781074,
    "end": 2784194,
    "text": "つまり、リカレント・ビューから見ると、彼らのコスト・ダイナミクスはこうなる。"
  },
  {
    "start": 2784322,
    "end": 2795734,
    "text": "なぜなら、a行列とb行列は各入力トークンに対して同じだからである。"
  },
  {
    "start": 2795862,
    "end": 2816130,
    "text": "畳み込みの観点からも、このモデルはタイムシフトを学習することができるため、コピー課題を解くことができることが知られているが、選択的コピー課題では、モデルのパラメータが各入力に対して同じであるため、特定の入力をどのように区別して扱えばよいかがわからないという問題がある。"
  },
  {
    "start": 2817750,
    "end": 2823362,
    "text": "では、マンバの革新性と状態空間モデルとの違いについて話そう。"
  },
  {
    "start": 2823496,
    "end": 2826770,
    "text": "まず、状態空間モデルのアルゴリズムとは何かをおさらいしよう。"
  },
  {
    "start": 2826840,
    "end": 2831218,
    "text": "状態空間モデルは基本的に入力があり、出力を生成しなければならない。"
  },
  {
    "start": 2831314,
    "end": 2832982,
    "text": "インプットはこれだ。"
  },
  {
    "start": 2833036,
    "end": 2839958,
    "text": "これはbl次元とz次元のテンソルであり、バッチ次元、シーケンス次元、dモデルである。"
  },
  {
    "start": 2840044,
    "end": 2845078,
    "text": "これはトランスフォーマーとまったく同じで、プロンプトのバッチがあるからだ。"
  },
  {
    "start": 2845174,
    "end": 2857034,
    "text": "各プロンプトはl個のトークンで構成され、これはシーケンスの長さである。各トークンはd個のモデル次元のベクトルで構成され、これはここでのdであり、同じ次元の出力を生成しなければならない。"
  },
  {
    "start": 2857082,
    "end": 2866498,
    "text": "トランスフォーマーと同じように、a行列がある。a行列は、前の状態を新しい状態にコピーする方法を示すパラメーターの行列である。"
  },
  {
    "start": 2866664,
    "end": 2871454,
    "text": "ここでは、状態をn個のトークンのベクトルとしてモデル化する。"
  },
  {
    "start": 2871502,
    "end": 2875090,
    "text": "これはn個のトークンのベクトルとなる。"
  },
  {
    "start": 2876170,
    "end": 2893466,
    "text": "これは基本的に、入力ベクトルがd次元で構成されている場合、各次元に1つずつ、互いに独立したd個の状態空間モデルが存在するからである。"
  },
  {
    "start": 2893568,
    "end": 2908382,
    "text": "このパラメータ行列はd×n、b行列もn、c行列もd×nで、デルタは離散化のステップサイズである。"
  },
  {
    "start": 2908436,
    "end": 2912782,
    "text": "私たちはそれを決めず、ただモデルにこのステップサイズを学習させる。"
  },
  {
    "start": 2912916,
    "end": 2925970,
    "text": "なぜなら、入力ベクトルがd次元なので、d個のデルタがあり、離散化するには、前に見た式を適用するだけだからだ。"
  },
  {
    "start": 2926040,
    "end": 2938194,
    "text": "つまり、どの離散化ルールを使うかによって、オイラー法を使うか、ゼロ次ホールド法を使うか、マンバの場合はゼロ次ホールド法を使い、そしてこの離散化されたパラメーターができる。"
  },
  {
    "start": 2938242,
    "end": 2947350,
    "text": "つまり、aバーとbバーで、SSMを訓練するか推論するかによって、リカレントまたはコンボリューションとして実行することができる。"
  },
  {
    "start": 2947430,
    "end": 2957646,
    "text": "それをトレーニングする場合、前にお見せしたように、カーネルを構築して入力に対して実行するコンボリューションとして実行するか、あるいはここにある式を使ったリカレンスとして実行する。"
  },
  {
    "start": 2957668,
    "end": 2959038,
    "text": "だから、この公式はここにある。"
  },
  {
    "start": 2959124,
    "end": 2965890,
    "text": "そこで、前の状態を使って次の状態を計算し、各状態を使って出力を構築する。"
  },
  {
    "start": 2966790,
    "end": 2977002,
    "text": "マンバの場合、状態空間モデルを選択的にする。つまり、入力トークンごとにモデルのパラメータを変更する。"
  },
  {
    "start": 2977166,
    "end": 2979922,
    "text": "えーと、入力はまだBLDだ。"
  },
  {
    "start": 2979986,
    "end": 2983874,
    "text": "つまり、一連のプロンプトがあるわけだ。"
  },
  {
    "start": 2983922,
    "end": 2990570,
    "text": "各プロンプトはl個のトークンで構成され、各トークンはd個の次元で構成され、出力は同じ形をしている。"
  },
  {
    "start": 2990910,
    "end": 2994140,
    "text": "すると、d×nのa行列ができる。"
  },
  {
    "start": 2994590,
    "end": 2999318,
    "text": "bマトリックスは基本的に線形層によって修正される。"
  },
  {
    "start": 2999414,
    "end": 3003930,
    "text": "このSbとこのSbはd次元をn次元に投影する。"
  },
  {
    "start": 3004010,
    "end": 3005120,
    "text": "こちらをご覧いただきたい。"
  },
  {
    "start": 3005650,
    "end": 3022514,
    "text": "つまり、各バッチの各入力トークンに対して、異なるB行列を持つことになります。"
  },
  {
    "start": 3022552,
    "end": 3025574,
    "text": "デルタも同様だ。"
  },
  {
    "start": 3025692,
    "end": 3033954,
    "text": "各入力トークンに対して異なるデルタを持ち、離散化された行列はこの次元を持つ。"
  },
  {
    "start": 3034002,
    "end": 3038858,
    "text": "だから、Bldnで状態空間モデルを走らせることができる。"
  },
  {
    "start": 3039024,
    "end": 3048166,
    "text": "しかし、入力ごとにモデルのパラメータが変化するため、このモデルはもはや時間不変ではない。"
  },
  {
    "start": 3048278,
    "end": 3055002,
    "text": "各トークンについて、各ステップについて、各時間ステップについて、再帰として実行するしかない。"
  },
  {
    "start": 3055066,
    "end": 3057518,
    "text": "ここではこの定式化を適用するしかない。"
  },
  {
    "start": 3057604,
    "end": 3069982,
    "text": "以前は、モデルのパラメータはすべての入力に対して固定されていたため、カーネルを構築し、すべての入力に対してそれを実行するだけでよかった。"
  },
  {
    "start": 3070046,
    "end": 3076818,
    "text": "今、すべての入力に対して異なるカーネルを使わなければならないので、コンボリューションとして計算することはできない。"
  },
  {
    "start": 3076914,
    "end": 3080230,
    "text": "我々はそれを再帰として計算せざるを得ないだけである。"
  },
  {
    "start": 3081210,
    "end": 3087282,
    "text": "著者がこのスキャン操作について話していることにお気づきだろうか？"
  },
  {
    "start": 3087346,
    "end": 3088154,
    "text": "こちらをご覧いただきたい。"
  },
  {
    "start": 3088192,
    "end": 3092950,
    "text": "モデルを再帰として評価する際のスキャン操作とは？"
  },
  {
    "start": 3093030,
    "end": 3094460,
    "text": "それについて話そう。"
  },
  {
    "start": 3096430,
    "end": 3109722,
    "text": "競技プログラミングをやったことがある人なら、Sumarrayという接頭辞をご存じだろう。Sumarrayとは、各位置の値が前の値の総和を示すように逐次計算される配列のことである。"
  },
  {
    "start": 3109866,
    "end": 3113614,
    "text": "forループを使えば線形時間で簡単に計算できる。"
  },
  {
    "start": 3113732,
    "end": 3115822,
    "text": "この初期配列があるとする。"
  },
  {
    "start": 3115886,
    "end": 3118434,
    "text": "プレフィックス・サムは次のように計算できる。"
  },
  {
    "start": 3118552,
    "end": 3121378,
    "text": "最初の値は最初の値と等しい。"
  },
  {
    "start": 3121544,
    "end": 3125774,
    "text": "2番目の値は、この配列の前の値として計算される。"
  },
  {
    "start": 3125822,
    "end": 3129330,
    "text": "これに初期配列の現在値を加えたもの。"
  },
  {
    "start": 3129410,
    "end": 3138802,
    "text": "待って、これはこれとこれを足したものに等しく、これは前の値と初期配列の現在の値を足して計算される。"
  },
  {
    "start": 3138866,
    "end": 3143686,
    "text": "この配列は、前の値と初期配列の現在の値を足して計算される。"
  },
  {
    "start": 3143798,
    "end": 3157706,
    "text": "この接頭辞 sum の各項目は、その要素までの初期配列の全項目の合計を示す。"
  },
  {
    "start": 3157818,
    "end": 3165620,
    "text": "32という数字は、10＋7＋6＋9の結果である。"
  },
  {
    "start": 3166710,
    "end": 3194422,
    "text": "スキャン演算とは、前置和のような配列を計算することで、各値は前に計算された値と現在の入力値を用いて計算することができ、状態空間モデルのリカレント式もスキャン演算と考えることができ、各状態は前の状態にa行列を掛けたものと現在の入力にb行列を掛けたものの和となる。"
  },
  {
    "start": 3194566,
    "end": 3212406,
    "text": "モデルの入力がxゼロ、x1、x2、x3、x4、x5である場合、例えばhゼロをxゼロだけを使って計算し、次にh1を前に計算した値と現在の入力を使って計算し、それぞれにa行列とb行列を掛けることができる。"
  },
  {
    "start": 3212538,
    "end": 3226878,
    "text": "そしてh 2は、h 1にaをかけたものにx 2にbをかけたものを使って計算でき、h 3はh 2にaをかけたものにx 3にbをかけたものを使って計算できる。"
  },
  {
    "start": 3226904,
    "end": 3228498,
    "text": "エトセトラ、エトセトラ、エトセトラ。"
  },
  {
    "start": 3228674,
    "end": 3235974,
    "text": "つまり、入力を生成するために、hkとc行列を掛け合わせ、出力トークンkを生成するだけである。"
  },
  {
    "start": 3236012,
    "end": 3238946,
    "text": "このスキャン出力があれば"
  },
  {
    "start": 3238978,
    "end": 3241722,
    "text": "このような配列を作れば"
  },
  {
    "start": 3241776,
    "end": 3252074,
    "text": "この配列を見ればわかるように、それぞれの値にc行列を掛け合わせるだけで、各時間ステップにおけるモデルの出力を簡単に計算することができる。"
  },
  {
    "start": 3252122,
    "end": 3259040,
    "text": "にcを掛け、ここにcを掛けてyのゼロを計算する。"
  },
  {
    "start": 3259410,
    "end": 3264670,
    "text": "これがY1、これがY2、エトセトラ、エトセトラ、エトセトラ。"
  },
  {
    "start": 3266290,
    "end": 3271966,
    "text": "さて、ここで紹介したスキャン操作が並列化できると言ったらどうなるだろうか？"
  },
  {
    "start": 3272078,
    "end": 3277774,
    "text": "もちろん、皆さんは私の言うことを信じないだろう。なぜなら、スキャン操作は、自然にシーケンスのように見える操作のひとつだからだ。"
  },
  {
    "start": 3277822,
    "end": 3282770,
    "text": "現在の値を計算するには、前の値と現在の入力を足す必要がある。"
  },
  {
    "start": 3282850,
    "end": 3285686,
    "text": "このような処理を並列化するにはどうすればいいのですか？"
  },
  {
    "start": 3285788,
    "end": 3293914,
    "text": "実際、連想演算であれば並列化できる。"
  },
  {
    "start": 3294032,
    "end": 3313114,
    "text": "さて、小学校や中学校で足し算や掛け算の性質を教わったときのことを思い出してほしい。連想特性とは、たとえばa×b×cのように3つのオペランドに対して演算を行う場合、その演算の順番は問わないということだ。"
  },
  {
    "start": 3313242,
    "end": 3316654,
    "text": "括弧をどこに入れても結果は同じである。"
  },
  {
    "start": 3316692,
    "end": 3324178,
    "text": "aにbを掛け、その結果にcを掛けることもできるし、aにbにcを掛けた結果を掛けることもできる。"
  },
  {
    "start": 3324344,
    "end": 3325890,
    "text": "これは関連するプロパティである。"
  },
  {
    "start": 3325960,
    "end": 3332390,
    "text": "という性質を持っている限り、スキャン操作を並列化することができる。"
  },
  {
    "start": 3333130,
    "end": 3335922,
    "text": "実際にどうすればいいのかをお見せしたい。"
  },
  {
    "start": 3336066,
    "end": 3338490,
    "text": "最初の配列がこうだとする。"
  },
  {
    "start": 3338640,
    "end": 3344470,
    "text": "複数のスレッドを作成し、それぞれが並列に合計を計算することができる。"
  },
  {
    "start": 3344550,
    "end": 3354502,
    "text": "例えば、これはウィキペディアから引用した写真ですが、16入力配列で作られています。"
  },
  {
    "start": 3354646,
    "end": 3356426,
    "text": "スレッドが8本あるとする。"
  },
  {
    "start": 3356458,
    "end": 3359530,
    "text": "最初のスレッドは、最初の2つの要素の合計を計算する。"
  },
  {
    "start": 3359610,
    "end": 3367134,
    "text": "番目のスレッドは、3番目と4番目の合計、4番目の3番目、5番目と6番目の合計などを計算する。"
  },
  {
    "start": 3367182,
    "end": 3373870,
    "text": "次に、この合計の結果を用いて次のステップを計算する。"
  },
  {
    "start": 3374030,
    "end": 3380870,
    "text": "そして、それを使って次のステップを並列に計算する。"
  },
  {
    "start": 3381290,
    "end": 3383954,
    "text": "私の記憶が正しければ、これはズィップダウンと呼ばれている。"
  },
  {
    "start": 3384002,
    "end": 3389218,
    "text": "そして、計算しなかったすべての値を再構築するために、ジップアップオペレーションを行う。"
  },
  {
    "start": 3389234,
    "end": 3393194,
    "text": "例えば、この値は最後のステップまで計算されない。"
  },
  {
    "start": 3393392,
    "end": 3401846,
    "text": "さて、パラレル・スキャンを行うことで、基本的にはシーケンスからのスキャン操作の時間的複雑さを減らすことができる。"
  },
  {
    "start": 3401958,
    "end": 3410730,
    "text": "ここで、tはこの演算を行う並列スレッド数である。"
  },
  {
    "start": 3410890,
    "end": 3419874,
    "text": "私のGitHubリポジトリには、このスキャン操作がどのように計算されるかをステップごとに示したエクセルファイルも置いてある。"
  },
  {
    "start": 3419912,
    "end": 3422046,
    "text": "私は実際に、すべての中間ステップをお見せしています。"
  },
  {
    "start": 3422078,
    "end": 3431574,
    "text": "このパラレル・スキャンが並列で行えることがわかったところで、この仕組みを理解したいのであれば、実際にアウト側が行っているのは次のようなことだ。"
  },
  {
    "start": 3431612,
    "end": 3440730,
    "text": "また、モデルの出力を計算するための再帰の計算も並行して行い、時間の複雑さを軽減する。"
  },
  {
    "start": 3441070,
    "end": 3450106,
    "text": "基本的に、マンバはコンボリューション（畳み込み）を使って評価することができないので、タイムバーリングであるため、モデルのパラメータは各タイムステップで異なるということになる。"
  },
  {
    "start": 3450208,
    "end": 3453846,
    "text": "出力を計算する唯一の方法は、リカレント定式化を使うことだ。"
  },
  {
    "start": 3453878,
    "end": 3459146,
    "text": "並列アルゴリズムのおかげで、これは並列化され、時間の複雑さを減らすことができる。"
  },
  {
    "start": 3459338,
    "end": 3463866,
    "text": "アウト側はまた、このアルゴリズムをより高速にするために使ったいくつかのテクニックも示している。"
  },
  {
    "start": 3463978,
    "end": 3467470,
    "text": "カーネル・フュージョン（核融合）だ。"
  },
  {
    "start": 3467630,
    "end": 3470126,
    "text": "もうひとつは、すでに紹介したパラレルスキャンだ。"
  },
  {
    "start": 3470158,
    "end": 3474002,
    "text": "ということは、後ほど紹介するアクティベーションの計算が可能になる。"
  },
  {
    "start": 3474136,
    "end": 3476454,
    "text": "では、これらのテクニックをひとつずつ見ていこう。"
  },
  {
    "start": 3476492,
    "end": 3480360,
    "text": "まず、GPUのメモリ階層がどのように機能しているかを見てみよう。"
  },
  {
    "start": 3481050,
    "end": 3491686,
    "text": "GPUは基本的に、非常に高速な計算機で、多くの演算を並列処理できる非常に大きな計算ユニットだ。"
  },
  {
    "start": 3491878,
    "end": 3494150,
    "text": "これには主に2つの記憶がある。"
  },
  {
    "start": 3494310,
    "end": 3500774,
    "text": "GPUを購入する際に実際にチェックするのは、DRAMと呼ばれるものだ。"
  },
  {
    "start": 3500822,
    "end": 3504506,
    "text": "高帯域幅のメモリで、ギガバイトのオーダーだ。"
  },
  {
    "start": 3504618,
    "end": 3509930,
    "text": "GPUには、SRAMと呼ばれるローカル・メモリもあります。"
  },
  {
    "start": 3510090,
    "end": 3514746,
    "text": "この2つの違いは、まずSRAMの方がはるかに小さいことだ。"
  },
  {
    "start": 3514778,
    "end": 3516622,
    "text": "メガバイトのオーダーだ。"
  },
  {
    "start": 3516766,
    "end": 3520686,
    "text": "しかし、ここではGPUが計算を行う。"
  },
  {
    "start": 3520718,
    "end": 3527410,
    "text": "GPUが行列の乗算を行う必要がある場合、まず高帯域幅メモリからSRAMに情報をコピーします。"
  },
  {
    "start": 3527770,
    "end": 3537590,
    "text": "そして、GPUのコアがSRAMの情報にアクセスして計算を行い、その結果をこの高帯域幅のメモリに保存し直す。"
  },
  {
    "start": 3538410,
    "end": 3545194,
    "text": "実際、GPUのデータシェードを確認すると、この場合はNvidia A 100です。"
  },
  {
    "start": 3545392,
    "end": 3557338,
    "text": "GPUは演算処理が非常に高速だが、SRAMからDRAMへの情報のコピーはあまり高速ではなく、演算処理ほど高速ではないことがわかるだろう。"
  },
  {
    "start": 3557514,
    "end": 3565222,
    "text": "このように、例えばコピー速度はかなり遅くなっている。"
  },
  {
    "start": 3565306,
    "end": 3570370,
    "text": "これは、GPUが処理できる演算数と比較すると、1秒あたり2テラバイトのようなものだ。"
  },
  {
    "start": 3570440,
    "end": 3576774,
    "text": "この場合、32ビットで毎秒20テラの浮動小数点演算ができる。"
  },
  {
    "start": 3576892,
    "end": 3582840,
    "text": "このパラメーターは、基本的にこのパラメーターより40倍速い。"
  },
  {
    "start": 3583210,
    "end": 3600230,
    "text": "これはまた、GPU上で動作するアルゴリズム、つまりcudaカーネルを作成するとき、カーネルの動作が遅くなることがあることを意味します。多くの演算を行っているためではなく、多くのものをコピーしているためかもしれません。"
  },
  {
    "start": 3600390,
    "end": 3610030,
    "text": "というのも、GPUの計算速度ではなく、コピー速度によってI O速度が制限されるからです。"
  },
  {
    "start": 3611010,
    "end": 3618878,
    "text": "アウト側では、このGPUの異なる階層構造を利用して、選択的なスキャンをより高速にするアルゴリズムを開発した。"
  },
  {
    "start": 3618974,
    "end": 3641834,
    "text": "主なアイデアは、最新のアクセラレータの特性を活用することです。GPUは、状態hを最大化するために、メモリ階層のより効率的なレベルにのみ隠された状態を具体化します。"
  },
  {
    "start": 3641952,
    "end": 3657594,
    "text": "スキャン入力は、バッチであるもの、シーケンス長は、サイズ、dモデルなので、GPUの入力ベクトルとnのサイズは、高帯域幅のメモリです。"
  },
  {
    "start": 3657642,
    "end": 3661982,
    "text": "で、状態空間モデルのすべてのパラメータをロードする。"
  },
  {
    "start": 3662036,
    "end": 3667486,
    "text": "デルタ、a マトリックス、b マトリックス、および c マトリックスは、最も帯域幅の広いメモリから直接取得する。"
  },
  {
    "start": 3667518,
    "end": 3674354,
    "text": "ドラマを高速SRAMに移し、SRAMで離散化を行い、再帰も行う。"
  },
  {
    "start": 3674392,
    "end": 3688978,
    "text": "スキャン演算もこのSRAMで行われ、最後にこのスキャンの結果が計算されて高帯域幅メモリに戻される。"
  },
  {
    "start": 3689074,
    "end": 3690598,
    "text": "カーネル・フュージョンとは何か？"
  },
  {
    "start": 3690694,
    "end": 3697030,
    "text": "テンソル演算を実行すると、深層学習フレームワークであるpytorchはテンソルをロードする。"
  },
  {
    "start": 3697110,
    "end": 3698918,
    "text": "行列の掛け算をするとしよう。"
  },
  {
    "start": 3699014,
    "end": 3705286,
    "text": "高速メモリーからテンソルをロードし、低速メモリーから高速メモリーにロードする。"
  },
  {
    "start": 3705318,
    "end": 3714846,
    "text": "DRAMからGPUのSRAMに転送されたGPUは、例えば行列の乗算などの演算を実行し、その結果をSRAMからDRAMにセーブバックします。"
  },
  {
    "start": 3714878,
    "end": 3718450,
    "text": "をSRAMからGPUの高帯域幅メモリに変換します。"
  },
  {
    "start": 3719030,
    "end": 3724130,
    "text": "しかし、同じテンソルに対して3つの操作を順番に行うとどうなるか。"
  },
  {
    "start": 3724550,
    "end": 3732146,
    "text": "ディープラーニング・フレームワークはこのように、まずテンソルを高帯域幅メモリーからSRAMにロードする。"
  },
  {
    "start": 3732258,
    "end": 3740950,
    "text": "これは最初の演算を計算し、最初の演算に関連するcudaカーネルを呼び出し、結果を高帯域幅メモリに保存することを意味する。"
  },
  {
    "start": 3741110,
    "end": 3746854,
    "text": "そして、高帯域幅メモリーから高速メモリーに、前回の計算結果を再びロードする。"
  },
  {
    "start": 3746982,
    "end": 3756426,
    "text": "この演算に関連する2番目のCudaカーネルを起動し、この演算の結果を高帯域幅メモリに保存します。"
  },
  {
    "start": 3756538,
    "end": 3769006,
    "text": "次に、前回の計算結果を再び高帯域幅メモリから高速メモリにロードし、3回目の演算を行い、その結果を高帯域幅メモリにセーブバックする。"
  },
  {
    "start": 3769118,
    "end": 3791126,
    "text": "おわかりのように、この場合、3つのオペレーションを順番に実行する場合、総時間は、高帯域幅メモリから高速メモリへ、そして高速メモリから高帯域幅メモリへ戻るコピーオペレーションで占められています。"
  },
  {
    "start": 3791318,
    "end": 3807594,
    "text": "カーネル・フュージョンとは、一連の処理を高速化するために、これらすべてのcudaカーネルをフュージョンすることを意味します。つまり、一連の処理で行っている3つの処理を1つのカスタムcudaカーネルにフュージョンし、中間結果を高帯域幅メモリにコピーしないようにします。"
  },
  {
    "start": 3807642,
    "end": 3816430,
    "text": "この3つの計算をすべて終えるまで、高速メモリでこれらの演算を続け、最後の結果だけが高帯域幅メモリに保存される。"
  },
  {
    "start": 3816510,
    "end": 3826950,
    "text": "中間コピー演算はIOに束縛されるアルゴリズムになるため、中間コピー演算が不要になるため、全体的な計算速度が向上する。"
  },
  {
    "start": 3828650,
    "end": 3834066,
    "text": "さて、この選択的スキャン・アルゴリズムの最後の革新は、アクティベーションの再計算である。"
  },
  {
    "start": 3834178,
    "end": 3835414,
    "text": "何だろう？"
  },
  {
    "start": 3835532,
    "end": 3842518,
    "text": "ディープラーニング・モデルを学習するとき、このモデルは逆伝播を行うと計算グラフに変換される。"
  },
  {
    "start": 3842614,
    "end": 3851946,
    "text": "この計算グラフの各ノードで勾配を計算するためには、前進ステップで行った各ノードの出力値をキャッシュする必要がある。"
  },
  {
    "start": 3852048,
    "end": 3855742,
    "text": "ここでお見せするような非常にシンプルなモデルがあるとしよう。"
  },
  {
    "start": 3855876,
    "end": 3857370,
    "text": "ポインターを示してみよう。"
  },
  {
    "start": 3857450,
    "end": 3862042,
    "text": "このモデルは基本的に線形演算だけで出力を計算する。"
  },
  {
    "start": 3862106,
    "end": 3869330,
    "text": "x 1にこのパラメータw 1を掛け、さらにx 2にこのパラメータw 2を掛け、さらにバイアスを掛ける。"
  },
  {
    "start": 3870230,
    "end": 3877086,
    "text": "前方への処理を行い、後方への伝搬の間に各ノードでそれぞれの値が生成されたとする。"
  },
  {
    "start": 3877118,
    "end": 3883750,
    "text": "我々の目的は、ここで各パラメーターに関する損失関数の勾配を計算することである。"
  },
  {
    "start": 3883820,
    "end": 3895980,
    "text": "例えば、w1に対する損失関数の勾配を計算するために、w2に対する損失関数の勾配を計算するために、w1に対する損失関数の勾配を計算するために、w2に対する損失関数の勾配を計算する。"
  },
  {
    "start": 3896990,
    "end": 3903378,
    "text": "この勾配を計算するには、すべての中間ノードの勾配も計算する必要がある。"
  },
  {
    "start": 3903494,
    "end": 3912062,
    "text": "中間ノードの勾配を計算するには、前進ステップで得られた各ノードの活性度の値が必要です。"
  },
  {
    "start": 3912116,
    "end": 3928566,
    "text": "したがって、例えば、このノードypreadに関する損失関数の勾配を計算するためには、2掛けypreadマイナス2掛けtargetという式になり、前進ステップ中に持っていた値ypreadをキャッシュする必要がある。"
  },
  {
    "start": 3928748,
    "end": 3933122,
    "text": "これらのアクティベーションは、実際には非常に大きなネットワークで多くのメモリを占有する可能性がある。"
  },
  {
    "start": 3933266,
    "end": 3940038,
    "text": "そのため、論文では再計算について触れている。"
  },
  {
    "start": 3940124,
    "end": 3966542,
    "text": "つまり、アクティベーションをキャッシュし、ベットプロパゲーション中に再利用するということは、アクティベーションを高帯域幅のメモリ、つまり低速メモリに保存し、バックプロパゲーション中に低速メモリからコピーバックする必要があります。"
  },
  {
    "start": 3966676,
    "end": 3969966,
    "text": "これは、彼らがこのテクニックを説明した論文の参考文献である。"
  },
  {
    "start": 3969998,
    "end": 3975454,
    "text": "最後に、ブレーク伝搬に必要な中間状態の保存も避けなければならないという。"
  },
  {
    "start": 3975502,
    "end": 3980322,
    "text": "中間状態は、この計算グラフのすべてのノードの活性化である。"
  },
  {
    "start": 3980466,
    "end": 3985170,
    "text": "私たちは、古典的な再計算のテクニックを慎重に適用し、必要なメモリを削減します。"
  },
  {
    "start": 3985250,
    "end": 3995670,
    "text": "中間状態は、高帯域幅メモリに保存されるのではなく、入力が高帯域幅メモリから高速ラムにロードされる後方パスの間に再計算される。"
  },
  {
    "start": 3995750,
    "end": 3998630,
    "text": "基本的には、計算をやり直した方が早い。"
  },
  {
    "start": 3998710,
    "end": 4007070,
    "text": "この場合も、この情報をハイバンドメモリーにコピーし、ハイバンドメモリーから高速メモリーにリロードするのではなく、ハイバンドメモリーから高速メモリーにリロードする。"
  },
  {
    "start": 4008850,
    "end": 4013994,
    "text": "では、マンバアーキテクチャーの中でマンバを構成するブロックを見てみよう。"
  },
  {
    "start": 4014042,
    "end": 4016026,
    "text": "まず、マンバ・ブロックとは何か？"
  },
  {
    "start": 4016058,
    "end": 4018686,
    "text": "そうしたら、マンバ建築のすべてをお見せしましょう。"
  },
  {
    "start": 4018798,
    "end": 4024002,
    "text": "つまり、マンバは、ここにあるようなマンバ・ブロックを何層にも積み重ねて作られるのだ。"
  },
  {
    "start": 4024136,
    "end": 4027298,
    "text": "これはトランスのスタック層とよく似ている。"
  },
  {
    "start": 4027314,
    "end": 4031974,
    "text": "トランスフォーマーのモデルを思い出すと、エンコーダーとデコーダー、そしてエンコーダー側がある。"
  },
  {
    "start": 4032012,
    "end": 4050186,
    "text": "デコーダー側は、これらのブロックを自己の注意を払いながら積み重ね、あるブロックの出力が次のブロックの入力として送られ、最後のブロックの出力がモデルの出力に送られるように、ネットワークを複数回フィードフォワードすることによって作られる。"
  },
  {
    "start": 4050288,
    "end": 4053934,
    "text": "これこそ、私たちがマンバでやっていることであり、私たちが創造していることなのだ。"
  },
  {
    "start": 4054052,
    "end": 4065970,
    "text": "このブロックの出力が次のブロックの入力になるように、このブロックは何度も繰り返される。"
  },
  {
    "start": 4066790,
    "end": 4074766,
    "text": "このブロックの最初に、サイズDのモデルをD個のインナーに変換する2つのリニアレイヤーがある。"
  },
  {
    "start": 4074798,
    "end": 4076946,
    "text": "少なくともコードではこう呼ばれている。"
  },
  {
    "start": 4077048,
    "end": 4081122,
    "text": "Dモデルは、埋め込みベクトルのサイズである。"
  },
  {
    "start": 4081186,
    "end": 4083782,
    "text": "エンベッディング・サイズが512だとする。"
  },
  {
    "start": 4083836,
    "end": 4087202,
    "text": "これはDモデルで、Dイナートを選ぶことができる。"
  },
  {
    "start": 4087266,
    "end": 4090634,
    "text": "例えば、Dモデルの2倍のサイズを選ぶこともできる。"
  },
  {
    "start": 4090832,
    "end": 4093302,
    "text": "これは単なる線形投影である。"
  },
  {
    "start": 4093446,
    "end": 4096054,
    "text": "そして、このブランチにコンボリューションがある。"
  },
  {
    "start": 4096102,
    "end": 4105914,
    "text": "というのは、トークン同士を混ぜ合わせるためである。"
  },
  {
    "start": 4105962,
    "end": 4108890,
    "text": "この畳み込みが、これらすべての次元を構成している。"
  },
  {
    "start": 4109050,
    "end": 4111690,
    "text": "となると、この2つの擬似アクティベーションがある。"
  },
  {
    "start": 4111850,
    "end": 4120466,
    "text": "先ほどお見せしたパラレル・スキャン・アルゴリズムを使って再帰を実行する状態空間モデルがここにある。"
  },
  {
    "start": 4120648,
    "end": 4126690,
    "text": "この分岐と状態空間モデルの出力の要素ごとの積を乗算する。"
  },
  {
    "start": 4126760,
    "end": 4135222,
    "text": "次に、Dインナー、つまりこのブロックの内側の寸法を外側の寸法、つまりDモデルに投影する別のリニアレイヤーがある。"
  },
  {
    "start": 4135276,
    "end": 4139494,
    "text": "埋め込みサイズの512次元に戻る。"
  },
  {
    "start": 4139532,
    "end": 4142860,
    "text": "当初、Dモデルを512とした場合。"
  },
  {
    "start": 4144110,
    "end": 4147446,
    "text": "それでは、マンバのアーキテクチャー全体を見てみよう。"
  },
  {
    "start": 4147558,
    "end": 4150970,
    "text": "私は自分でコードを解析して、このアーキテクチャを描いている。"
  },
  {
    "start": 4151040,
    "end": 4153518,
    "text": "あまり美しく仕上げる時間がなかった。"
  },
  {
    "start": 4153604,
    "end": 4156666,
    "text": "トランスフォーマーでやっていることとよく似ている。"
  },
  {
    "start": 4156698,
    "end": 4160830,
    "text": "入力があれば、埋め込みに変換される。"
  },
  {
    "start": 4161170,
    "end": 4166786,
    "text": "トークンの列となり、各トークンは、仮に512とする。"
  },
  {
    "start": 4166808,
    "end": 4177220,
    "text": "あるブロックの出力が次のブロックの入力として送られる。"
  },
  {
    "start": 4177590,
    "end": 4188198,
    "text": "前のスライドでお見せしたママ・ブロックは基本的にこれだけですが、RMSノルムを冒頭に含み、さらにここにあるようにスキップ接続も含んでいます。"
  },
  {
    "start": 4188364,
    "end": 4190614,
    "text": "これをn回繰り返す。"
  },
  {
    "start": 4190732,
    "end": 4197002,
    "text": "最後に、ラマやミストラルのようにRMSノルムがある。"
  },
  {
    "start": 4197056,
    "end": 4213760,
    "text": "そして、言語モデルをモデル化する場合、語彙の中からどのトークンを次のトークンとして選ぶかを示すソフトマックスを選択する。"
  },
  {
    "start": 4214370,
    "end": 4217922,
    "text": "これがマンバの建築だ。"
  },
  {
    "start": 4218056,
    "end": 4220702,
    "text": "パフォーマンスも見てみよう。"
  },
  {
    "start": 4220766,
    "end": 4238178,
    "text": "つまり、マンバの話を始めた当初は、選択的コピータスクと帰納タスクの問題を解決するためにマンバが導入されたのです。コンテキストを意識した推論では、状態空間モデルがあまりうまく機能しないことがわかったからです。"
  },
  {
    "start": 4238274,
    "end": 4241562,
    "text": "彼らはマンバを使ってこの問題を解決しようとした。"
  },
  {
    "start": 4241616,
    "end": 4242246,
    "text": "彼らは紹介した。"
  },
  {
    "start": 4242278,
    "end": 4248330,
    "text": "だからこそ、彼らは選択的スキャン・アルゴリズムで選択的状態空間モデルを導入したのだ。"
  },
  {
    "start": 4248750,
    "end": 4251866,
    "text": "という状態空間モデルであることがわかる。"
  },
  {
    "start": 4251968,
    "end": 4260926,
    "text": "S fourモデル、つまり構造空間位相モデルは、この選択的コピーではかなり劣っている。"
  },
  {
    "start": 4261028,
    "end": 4264906,
    "text": "99.8％の精度を誇る。"
  },
  {
    "start": 4265018,
    "end": 4272754,
    "text": "マンバは基本的に、この層をS6層と呼び、S4層は前回の論文で説明したものである。"
  },
  {
    "start": 4272792,
    "end": 4274740,
    "text": "つまり、構造化空間モデルだ。"
  },
  {
    "start": 4275110,
    "end": 4279974,
    "text": "インダクションヘッドでは、マンバも非常に良いパフォーマンスを見せている。"
  },
  {
    "start": 4280012,
    "end": 4285030,
    "text": "マンバの精度は100％近い。"
  },
  {
    "start": 4285100,
    "end": 4289802,
    "text": "実際には、10の6乗に達するシークエンスの長さは100％だ。"
  },
  {
    "start": 4289856,
    "end": 4292678,
    "text": "配列長が非常に長い。"
  },
  {
    "start": 4292774,
    "end": 4303440,
    "text": "それに比べ、例えば絶対位置エンコーディングを用いた変換器モデルや回転位置エンコーディングは、シーケンス長があるサイズに達すると精度が劣化し始める。"
  },
  {
    "start": 4305810,
    "end": 4313934,
    "text": "数百トークンまでは非常に良いが、数千トークンになるとすぐに劣化し始める。"
  },
  {
    "start": 4313982,
    "end": 4319486,
    "text": "mambaは、非常に長いシーケンスでも非常に安定したパフォーマンスを維持する。"
  },
  {
    "start": 4319518,
    "end": 4328502,
    "text": "これは言語モデリングにとって非常に重要なことで、特に検索機能拡張世代やチャットアプリケーションなどでは、プロンプトが非常に長くなっているからだ。"
  },
  {
    "start": 4328556,
    "end": 4332870,
    "text": "我々は、非常に長いシークエンスで優れたパフォーマンスを発揮できるモデルを求めている。"
  },
  {
    "start": 4333690,
    "end": 4346966,
    "text": "また、モデルの性能、つまりある複雑度に到達するためにモデルを訓練するのに必要な操作の回数は、変換器と非常に同等であることがわかる。"
  },
  {
    "start": 4346998,
    "end": 4352294,
    "text": "マンバは実際、今ある最高のトランスフォーマーモデルと同等のパフォーマンスを持っている。"
  },
  {
    "start": 4352352,
    "end": 4362894,
    "text": "トランスフォーマーのモデルは、ラマやミストラルと同じく、トランスフォーマー・プラス・プラスです。"
  },
  {
    "start": 4362932,
    "end": 4367126,
    "text": "トランスフォーマーとの同時使用は非常に良い。"
  },
  {
    "start": 4367178,
    "end": 4374158,
    "text": "前のスライドで見たように、より長いシーケンスではより優れたスケールが可能である。"
  },
  {
    "start": 4374334,
    "end": 4375922,
    "text": "僕のビデオを見てくれてありがとう。"
  },
  {
    "start": 4375976,
    "end": 4378194,
    "text": "このビデオで多くのことを学んでいただけたと思う。"
  },
  {
    "start": 4378312,
    "end": 4386578,
    "text": "マンバの配合をすべて導き出したかったので、非常に説明的で、技術的にも詳細なビデオを作りたかった。"
  },
  {
    "start": 4386594,
    "end": 4389126,
    "text": "ただ、私は数式を人に投げつけるのが好きではない。"
  },
  {
    "start": 4389308,
    "end": 4394710,
    "text": "マンバは、それなりの限界はあるにせよ、将来的には非常に人気のあるモデルになると思う。"
  },
  {
    "start": 4394790,
    "end": 4401794,
    "text": "例えば、リカレント・ニューラル・ネットワークであることに変わりはない。"
  },
  {
    "start": 4401942,
    "end": 4410590,
    "text": "例えば、ラマやミストラに使われたデータのような膨大なデータに対して、どの程度の性能を発揮するかはまだわかっていない。"
  },
  {
    "start": 4411090,
    "end": 4428498,
    "text": "というのも、トランスフォーマーは、特に言語モデリングに非常に必要とされる、非常に長いシーケンス長へのスケーリングには限界があり、画像生成、ムービー生成、オーディオ生成などの最近のモデルでも限界があることが明らかになっているからだ。"
  },
  {
    "start": 4428674,
    "end": 4438210,
    "text": "そのため、スケーリングパワーが2次関数になるため、トランスフォーマーの計算量は膨大になる。"
  },
  {
    "start": 4438290,
    "end": 4441602,
    "text": "その結果、メモリの消費量が非常に多くなる。"
  },
  {
    "start": 4441746,
    "end": 4451950,
    "text": "だから、普通の人は、モデルシャードや非常に高度なテクニックを使わない限り、ミストラルのようなモデルをコンピュータ上で推論することさえできないのだ。"
  },
  {
    "start": 4452370,
    "end": 4456638,
    "text": "この分野でもっと研究が進むことを願っている。"
  },
  {
    "start": 4456724,
    "end": 4458142,
    "text": "ビデオを見てくれてありがとう。"
  },
  {
    "start": 4458196,
    "end": 4462126,
    "text": "このビデオを気に入っていただき、私のチャンネルに登録していただけることを願っています。"
  },
  {
    "start": 4462228,
    "end": 4465662,
    "text": "このビデオを友人と共有し、ソーシャルメディアでシェアしてください。"
  },
  {
    "start": 4465716,
    "end": 4467358,
    "text": "これが私をサポートする最善の方法だ。"
  },
  {
    "start": 4467444,
    "end": 4468620,
    "text": "ありがとう。"
  }
]