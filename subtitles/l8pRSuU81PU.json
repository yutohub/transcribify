[
  {
    "start": 240,
    "end": 884,
    "text": "皆さん、こんにちは。"
  },
  {
    "start": 1214,
    "end": 11074,
    "text": "今日は、ゼロツー・ヒーロー・シリーズの続きで、特にGPT-2モデル、その1億2400万バージョンを再現します。"
  },
  {
    "start": 11534,
    "end": 17874,
    "text": "OpenAIがGPT-2をリリースしたのは2019年のことで、このブログ記事とともにリリースされた。"
  },
  {
    "start": 18174,
    "end": 23022,
    "text": "その上、彼らはこの論文を発表し、さらにこのコードをGitHubで公開した。"
  },
  {
    "start": 23118,
    "end": 34026,
    "text": "OpenAI GPT-2は、GPT-2を再現すると言っても、特にこのビデオでは1億2400万パラメータモデルを再現することになるので、注意しなければなりません。"
  },
  {
    "start": 34210,
    "end": 39530,
    "text": "このようなリリースが行われる場合、必ずミニシリーズがあるということだ。"
  },
  {
    "start": 39642,
    "end": 43858,
    "text": "GPT-2のミニシリーズがある。"
  },
  {
    "start": 43946,
    "end": 59634,
    "text": "通常、最も大きなモデルはGPT-2と呼ばれるものですが、基本的にこのようにする理由は、このようにX軸にモデルのサイズをプロットし、Y軸に翻訳、要約、質問応答など、興味のある多くの下流メトリクスを置くことができるからです。"
  },
  {
    "start": 59754,
    "end": 61970,
    "text": "このようなスケーリングの法則を図式化することができる。"
  },
  {
    "start": 62122,
    "end": 67254,
    "text": "基本的に、モデルサイズが大きくなるにつれて、下流のメトリクスはどんどん良くなっていく。"
  },
  {
    "start": 67554,
    "end": 80882,
    "text": "特にGPT-2に関しては、論文を下にスクロールしていくと、GPT-2ミニ・シリーズには4つのモデルがあり、1億2400万から始まって、15億5000万、800万まである。"
  },
  {
    "start": 81058,
    "end": 85706,
    "text": "さて、私の数字がこの表と食い違うのは、この表が間違っているからだ。"
  },
  {
    "start": 85850,
    "end": 94426,
    "text": "実際にGPT-2のGitHubレポを見ると、パラメーターの追加方法に誤りがあったようだ。"
  },
  {
    "start": 94490,
    "end": 97898,
    "text": "基本的には、これが1億2,400万パラメータのモデルだ。"
  },
  {
    "start": 98066,
    "end": 107288,
    "text": "1億2400万パラメータはトランスフォーマーに12層あり、トランスフォーマーには768チャンネル、768次元があった。"
  },
  {
    "start": 107456,
    "end": 112504,
    "text": "これらの用語の意味については、前回のビデオですべて説明したので、ある程度知っていることを前提に話を進める。"
  },
  {
    "start": 112584,
    "end": 115952,
    "text": "GPT-2を作ろう......GPTをゼロから作ろう。"
  },
  {
    "start": 116048,
    "end": 118804,
    "text": "このプレイリストの前のビデオでも取り上げた。"
  },
  {
    "start": 119224,
    "end": 136482,
    "text": "さて、すべてを正しく行い、すべてがうまくいった場合、このビデオの終わりには、このようなものが表示されます。これは、基本的に、モデルがトレーニング中に見たことのない検証データに対して、シーケンスの次のトークンを予測するのがどの程度優れているかを測定する、検証損失を見ています。"
  },
  {
    "start": 136658,
    "end": 145866,
    "text": "ゼロから初期化するため、そのタスクがあまりうまくできない状態から、トレーニングが終わるころにはかなりうまくできるようになっているのがわかる。"
  },
  {
    "start": 146010,
    "end": 150418,
    "text": "GPTの2124 mモデルを打ち負かすことを期待している。"
  },
  {
    "start": 150586,
    "end": 154434,
    "text": "以前、彼らがこれに取り組んでいたとき、これはすでに5年前のことだった。"
  },
  {
    "start": 154554,
    "end": 160576,
    "text": "当時は、GPUもコンピュートももっと小さかったし、これはかなり複雑な最適化だったのだろう。"
  },
  {
    "start": 160680,
    "end": 172152,
    "text": "今日、このモデルを再現するのにかかる時間はおよそ1時間、あるいはもっと短いかもしれない。これをクラウド・コンピューター、つまりみんながレンタルできるコンピューターのようなものでやろうと思えば、10ドル程度でできる。"
  },
  {
    "start": 172288,
    "end": 176320,
    "text": "そのコンピューターに10ドル払えば、1時間弱待つことになる。"
  },
  {
    "start": 176432,
    "end": 180924,
    "text": "実際に、OpenAIが公開したこのモデルと同程度のモデルを実現することは可能だ。"
  },
  {
    "start": 181304,
    "end": 190820,
    "text": "もうひとつ言っておくと、他の多くのモデルとは異なり、OpenAIはGPT-2のウェイトを公開しているので、これらのウェイトはすべてこのリポジトリで入手可能です。"
  },
  {
    "start": 191012,
    "end": 195980,
    "text": "GPT-2のペーパーは、トレーニングの詳細がすべて書かれているとは限らない。"
  },
  {
    "start": 196132,
    "end": 206064,
    "text": "GPT-2の論文に加え、ハイパーパラメーターや最適化設定など、より具体的なGPT-3の論文も参照するつもりだ。"
  },
  {
    "start": 207284,
    "end": 211716,
    "text": "GPT-2バージョンのモデルからアーキテクチャが大きく変わるわけではない。"
  },
  {
    "start": 211820,
    "end": 217510,
    "text": "GPT2124mを再現するために、GPT-2とGPT-3の両方を参照するつもりだ。"
  },
  {
    "start": 217662,
    "end": 218954,
    "text": "行こう"
  },
  {
    "start": 219254,
    "end": 222902,
    "text": "まず最初にしたいことは、実際に最後、あるいはターゲットから始めることだ。"
  },
  {
    "start": 223038,
    "end": 229614,
    "text": "つまり、OpenAIがリリースしたGPT 2124 mモデルをロードして、ちょっと試してみよう。"
  },
  {
    "start": 229654,
    "end": 231398,
    "text": "その中からいくつかのトークンをサンプルしてみよう。"
  },
  {
    "start": 231566,
    "end": 241114,
    "text": "GPT-2のコードベースにアクセスして、ソースを開き、モデルPyをクリックすると、これがtensorflowを使っていることがわかります。"
  },
  {
    "start": 241434,
    "end": 250974,
    "text": "このオリジナルのGPT-2のコードはTensorflowで書かれている。"
  },
  {
    "start": 251394,
    "end": 256394,
    "text": "PyTorchを使いたいのは、より友好的で簡単で、個人的にもっと好きだからです。"
  },
  {
    "start": 256514,
    "end": 258978,
    "text": "問題は、初期コードがTensorflowにあることだ。"
  },
  {
    "start": 259066,
    "end": 260178,
    "text": "ピトーチを使いたい。"
  },
  {
    "start": 260266,
    "end": 267702,
    "text": "その代わりに、ターゲットを得るために、ハグする顔のトランスフォーマーのコードを使おう。"
  },
  {
    "start": 267838,
    "end": 277954,
    "text": "トランスフォーマーのソース、トランスフォーマーのモデル、GPT-2 モデリング GPT-2 py を開くと、このファイルにトランスフォーマーの GPT-2 実装があることがわかります。"
  },
  {
    "start": 279614,
    "end": 284234,
    "text": "中程度には読めるが、完全には読めないという感じだ。"
  },
  {
    "start": 285014,
    "end": 291974,
    "text": "これは、TensorflowからPytorchに親和性のある重みに変換するすべての作業を行うものだ。"
  },
  {
    "start": 292054,
    "end": 294206,
    "text": "だから、ロードも作業もずっと簡単だ。"
  },
  {
    "start": 294390,
    "end": 301166,
    "text": "特に、ここではGPT-2モデルを見ることができ、抱き合わせ位相変圧器を使って負荷をかけることができる。"
  },
  {
    "start": 301310,
    "end": 305646,
    "text": "だから、振りかぶると、これがトランスフォーマーのそれだ。"
  },
  {
    "start": 305710,
    "end": 312234,
    "text": "GPT-2 lmヘッドモデルをインポートし、事前にトレーニングされたGPT-2からインポートする。"
  },
  {
    "start": 313214,
    "end": 321274,
    "text": "ここで一つ厄介なのは、GPT-2をロードするモデルとして使う場合、これは実際には1億2400万パラメータ・モデルだということだ。"
  },
  {
    "start": 321614,
    "end": 327634,
    "text": "実際のGPT-2、15億が欲しいのであれば、実際にエクセルをやることになる。"
  },
  {
    "start": 328094,
    "end": 330954,
    "text": "これが124メートルの目標だ。"
  },
  {
    "start": 331654,
    "end": 338554,
    "text": "このクラスで定義されているように、pytorch nnモジュールを初期化しています。"
  },
  {
    "start": 339774,
    "end": 344062,
    "text": "そこから、生のテンソルだけのステートディクトを得たい。"
  },
  {
    "start": 344158,
    "end": 347314,
    "text": "そのファイルのテンソルがあるだけだ。"
  },
  {
    "start": 347724,
    "end": 353076,
    "text": "ところで、これはJupyterノートブックだが、これはJupyterノートブックを対コードで実行している。"
  },
  {
    "start": 353220,
    "end": 356684,
    "text": "私は、ひとつのインターフェイスですべてを操作するのが好きなんだ。"
  },
  {
    "start": 356724,
    "end": 358036,
    "text": "私は対コードを使うのが好きだ。"
  },
  {
    "start": 358060,
    "end": 362984,
    "text": "これはJupyter notebookの拡張機能で、対のコードの中にある。"
  },
  {
    "start": 364844,
    "end": 366924,
    "text": "ステート・ディクテを取得するとき、これは単なるディクテである。"
  },
  {
    "start": 367004,
    "end": 371300,
    "text": "テンソルであるキーと値を表示することができる。"
  },
  {
    "start": 371372,
    "end": 373034,
    "text": "形だけを見てみよう。"
  },
  {
    "start": 373204,
    "end": 380254,
    "text": "これらは、GPT-2モデル内のさまざまなパラメータとその形状のようなものだ。"
  },
  {
    "start": 380414,
    "end": 389798,
    "text": "トークン埋め込み用のwウェイトのサイズは50,257×768である。"
  },
  {
    "start": 389966,
    "end": 396274,
    "text": "GPT-2の語彙には50,257のトークンがある。"
  },
  {
    "start": 397214,
    "end": 404206,
    "text": "ところで、このトークンは、トークン化シリーズの前回のビデオでお話ししたトークンそのものです。"
  },
  {
    "start": 404390,
    "end": 408262,
    "text": "この直前のビデオでは、トークン化について詳しく説明しています。"
  },
  {
    "start": 408438,
    "end": 411606,
    "text": "GPT-2のトークナイザーは、たまたまこれだけのトークンを持っている。"
  },
  {
    "start": 411790,
    "end": 417166,
    "text": "各トークンについて、768次元の埋め込みがある。"
  },
  {
    "start": 417310,
    "end": 421998,
    "text": "これが、そのトークンの代わりとなる分散型表現である。"
  },
  {
    "start": 422126,
    "end": 429144,
    "text": "各トークンは小さな文字列の一部であり、768個の数字はそのトークンを表すベクトルである。"
  },
  {
    "start": 429804,
    "end": 432148,
    "text": "つまり、これはトークンのルックアップテーブルに過ぎない。"
  },
  {
    "start": 432316,
    "end": 435596,
    "text": "そして、ここにポジションのルックアップテーブルがある。"
  },
  {
    "start": 435740,
    "end": 446064,
    "text": "つまり、GPT-2の最大配列長は1024なので、各トークンが過去にアテンドできる位置は1024までということになる。"
  },
  {
    "start": 446444,
    "end": 454064,
    "text": "GPT-2のこれらのポジションはすべて、最適化によって学習された768の固定ベクトルを持っている。"
  },
  {
    "start": 455364,
    "end": 458864,
    "text": "これが位置埋め込みとトークン埋め込みである。"
  },
  {
    "start": 460004,
    "end": 465064,
    "text": "となると、ここにあるのは、このトランスの他のウェイトとバイアス、その他すべてということになる。"
  },
  {
    "start": 466124,
    "end": 478780,
    "text": "例えば、位置エンベッディングを平らにして、20個の要素だけを取り出してみると、これらは単なるパラメータであり、重みであり、浮動小数点数であることがわかります。"
  },
  {
    "start": 478892,
    "end": 481024,
    "text": "これらは位置の埋め込みである。"
  },
  {
    "start": 481494,
    "end": 498142,
    "text": "というのも、このビジュアライゼーションのすべての行は、ゼロから1024までの範囲内で固定された絶対位置であり、異なる位置だからである。"
  },
  {
    "start": 498318,
    "end": 501674,
    "text": "各行がそのポジションを表している。"
  },
  {
    "start": 502374,
    "end": 512548,
    "text": "というのも、位置埋め込みは結局、それぞれの位置を表すような正弦波と余弦波を学習することになるからだ。"
  },
  {
    "start": 512676,
    "end": 526984,
    "text": "ここにある各行がその位置を表し、トランスフォーマーによって処理されることで、相対的な位置をすべて復元し、どのトークンがどこにあるかを認識し、その内容だけでなく位置に応じて対応する。"
  },
  {
    "start": 527804,
    "end": 553644,
    "text": "例えば、ここではすべてのチャンネルに注目し、そのチャンネルが1から1023までの位置の関数としてどのような動きをしているかを見ています。"
  },
  {
    "start": 554904,
    "end": 560488,
    "text": "これらのチャンネルのいくつかは、基本的にポジションスペクトルの異なる部分に多かれ少なかれ反応することがわかる。"
  },
  {
    "start": 560576,
    "end": 564358,
    "text": "このグリーン・チャンネルは、本当に何にでも発砲したがる。"
  },
  {
    "start": 564486,
    "end": 572574,
    "text": "200を超えると800まで上がるが、それ以下にはならない。"
  },
  {
    "start": 572734,
    "end": 576094,
    "text": "これらのエンベッディングが何をしているのか、なぜそうなっているのかを知っているのは誰なのか。"
  },
  {
    "start": 576214,
    "end": 582054,
    "text": "例えば、もう少しギザギザしていて、ノイズが多いので、このモデルが十分に訓練されていないことがわかる。"
  },
  {
    "start": 582174,
    "end": 587022,
    "text": "このモデルの訓練度が高ければ高いほど、この現象はより滑らかになるはずだ。"
  },
  {
    "start": 587158,
    "end": 590394,
    "text": "つまり、これは少し訓練不足のモデルだということだ。"
  },
  {
    "start": 591164,
    "end": 594668,
    "text": "原理的には、これらの曲線は滑らかである必要はない。"
  },
  {
    "start": 594716,
    "end": 596860,
    "text": "これは完全にランダムなノイズであるべきだ。"
  },
  {
    "start": 596972,
    "end": 604308,
    "text": "というのも、この位置埋め込みテーブルは完全にランダムに初期化されるからだ。"
  },
  {
    "start": 604436,
    "end": 617756,
    "text": "なぜなら、原理的には、ここから意味のあるグラフを1つも導き出すことはできないはずだからだ。"
  },
  {
    "start": 617860,
    "end": 627116,
    "text": "少しノイジーに見えるが、大部分はオリジナルのトランスのように正弦波に見える。"
  },
  {
    "start": 627140,
    "end": 628436,
    "text": "紙、注意力があればいい。"
  },
  {
    "start": 628460,
    "end": 637052,
    "text": "紙の位置埋め込みは、私の記憶が正しければ、実際には異なる周波数の正弦波と余弦波に初期化され、固定されている。"
  },
  {
    "start": 637188,
    "end": 639220,
    "text": "これが位置エンコーディングで、固定されている。"
  },
  {
    "start": 639332,
    "end": 644392,
    "text": "GPT-2では、これらは単なるパラメーターであり、他のパラメーターと同じようにゼロからトレーニングされる。"
  },
  {
    "start": 644588,
    "end": 646592,
    "text": "これも同じように機能するようだ。"
  },
  {
    "start": 646688,
    "end": 650448,
    "text": "つまり、正弦波のような特徴を回復させるのだ。"
  },
  {
    "start": 650496,
    "end": 655744,
    "text": "最適化の間、ここにある他のマトリックスも見ることができる。"
  },
  {
    "start": 655784,
    "end": 671554,
    "text": "ここでトランスフォーマーの最初のレイヤーを取り、その重みの1つを見てみると、300×300の最初のブロックだけで、いくつかの構造が見える。"
  },
  {
    "start": 671674,
    "end": 679442,
    "text": "機械論的な解釈が好きな人なら、何が起こっているのか、どんな構造をしているのか、そしてそれが何を意味するのかを理解しようとすることに興奮を覚えるかもしれない。"
  },
  {
    "start": 679538,
    "end": 681394,
    "text": "このビデオではそれはやらない。"
  },
  {
    "start": 681514,
    "end": 685498,
    "text": "面白い構造があることは間違いないし、それはちょっとクールだね。"
  },
  {
    "start": 685666,
    "end": 689986,
    "text": "私たちが最も興味を持っているのは、OpenAIが公開したこのモデルの重みをロードしたことです。"
  },
  {
    "start": 690170,
    "end": 700920,
    "text": "今、ハギング・フェイス・トランスフォーマーを使えば、生のウェイトをすべて取得できるだけでなく、パイプラインと呼ばれるものを取得し、そこからサンプリングすることもできる。"
  },
  {
    "start": 701072,
    "end": 710608,
    "text": "これは接頭辞で、こんにちは、私は言語モデルのコンマです。30個のトークンをサンプリングして、5つのシーケンスを得ています。"
  },
  {
    "start": 710696,
    "end": 713324,
    "text": "これを実行したところ、こんな結果が出た。"
  },
  {
    "start": 714264,
    "end": 719336,
    "text": "こんにちは、私は言語モデルですが、実際にやっているのは人間が読める文書を作ることです。"
  },
  {
    "start": 719480,
    "end": 722296,
    "text": "他の言語もあるが、それはドット、ドット、ドットだ。"
  },
  {
    "start": 722440,
    "end": 723824,
    "text": "よかったら、読んでみてください。"
  },
  {
    "start": 723864,
    "end": 730074,
    "text": "基本的に、これらはこのGPT 2124 mの同じ接頭辞の5つの異なる完成形である。"
  },
  {
    "start": 730234,
    "end": 743082,
    "text": "悲しいことに、シードを修正しているにもかかわらず、スニペットから彼らが得たものとは異なる世代が得られている。"
  },
  {
    "start": 743218,
    "end": 745254,
    "text": "おそらくコードが変わったのだろう。"
  },
  {
    "start": 746194,
    "end": 751282,
    "text": "しかし、現段階で重要なのは、首尾一貫したテキストを入手しているということだ。"
  },
  {
    "start": 751418,
    "end": 753958,
    "text": "モデルのロードに成功しました。"
  },
  {
    "start": 754026,
    "end": 759394,
    "text": "すべてのパラメータを見ることができ、キーがモデルのどの部分に由来するかを教えてくれる。"
  },
  {
    "start": 760094,
    "end": 764798,
    "text": "そこで起こっていることを完全に理解するために、実際にGPT-2クラスを書きたい。"
  },
  {
    "start": 764886,
    "end": 770318,
    "text": "モデリングGPT-2パイのようなものは、複雑すぎるから使いたくないんだ。"
  },
  {
    "start": 770366,
    "end": 772254,
    "text": "自分たちでゼロから書きたい。"
  },
  {
    "start": 772414,
    "end": 775550,
    "text": "ここではGPTモデルを並行して実装していく。"
  },
  {
    "start": 775702,
    "end": 784142,
    "text": "最初のタスクとして、24mmのGPT-2を、これからゼロから開発するクラスにロードしてみよう。"
  },
  {
    "start": 784318,
    "end": 788342,
    "text": "これでOpenAIのモデルをロードできるという確信が持てるだろう。"
  },
  {
    "start": 788478,
    "end": 793046,
    "text": "従って、まさに124モデルであるウェイトの設定がある。"
  },
  {
    "start": 793190,
    "end": 804038,
    "text": "もちろん、これからやることは、モデルをゼロから初期化し、これから入手する大量の文書を使って自分たちでトレーニングし、そのモデルを上回るようにすることだ。"
  },
  {
    "start": 804166,
    "end": 808444,
    "text": "ウェイトが変われば、すべてが違って見える。"
  },
  {
    "start": 809424,
    "end": 819704,
    "text": "OpenAIのモデルを読み込むことができるのだから、モデルクラスでは同じモデルファミリーに属していることになる。"
  },
  {
    "start": 819864,
    "end": 827792,
    "text": "GPT-2モデルを書いて、重みをロードし、首尾一貫したテキストを生成できることを確認しよう。"
  },
  {
    "start": 827968,
    "end": 831576,
    "text": "さて、では次に、すべての始まりとなった「attention is all you need paper」に話を移そう。"
  },
  {
    "start": 831680,
    "end": 835424,
    "text": "モデル・アーキテクチャ、オリジナルのトランスフォーマーにスクロールしてみよう。"
  },
  {
    "start": 835964,
    "end": 839972,
    "text": "さて、GPT-2はオリジナルのトランスから少し変更されていることを覚えておいてほしい。"
  },
  {
    "start": 840068,
    "end": 843468,
    "text": "特にエンコーダーがない。"
  },
  {
    "start": 843556,
    "end": 846540,
    "text": "GPT-2は、私たちが言うところのデコーダ専用トランスです。"
  },
  {
    "start": 846612,
    "end": 848844,
    "text": "このエンコーダー全体が欠けている。"
  },
  {
    "start": 849004,
    "end": 854372,
    "text": "それに加えて、そのエンコーダーを使っていたこのクロスの注意も欠けている。"
  },
  {
    "start": 854428,
    "end": 856904,
    "text": "この部分をすべて削除する。"
  },
  {
    "start": 857324,
    "end": 859536,
    "text": "他はほとんど変わらない。"
  },
  {
    "start": 859660,
    "end": 863568,
    "text": "これから見ていく中で、いくつかの違いがある。"
  },
  {
    "start": 863736,
    "end": 866164,
    "text": "主な違いは2つある。"
  },
  {
    "start": 867664,
    "end": 876416,
    "text": "GPT-2の2.3モデルを見ると、まず、レイヤー規範の入れ替えが行われている。"
  },
  {
    "start": 876600,
    "end": 883584,
    "text": "第二に、最後の自己注意ブロックに、ここで追加のレイヤー正規化が加えられた。"
  },
  {
    "start": 883704,
    "end": 894394,
    "text": "基本的に、ここでのすべてのレイヤー・ノルムは、MLPの後や注意の後にあるのではなく、その前に振られ、最終分類器の直前に追加のレイヤー・ノルムが追加される。"
  },
  {
    "start": 895094,
    "end": 902334,
    "text": "では、GPT Nnモジュールに最初のスケルトンNNモジュールを実装してみよう。"
  },
  {
    "start": 902494,
    "end": 912346,
    "text": "特に、抱擁顔トランスフォーマーで使われるこのスキーマを一致させようと思っている。そうすることで、このステート・ディクテーターからウェイトをロードするのがより簡単になるからだ。"
  },
  {
    "start": 912480,
    "end": 916158,
    "text": "このスキーマを反映したものが欲しい。"
  },
  {
    "start": 916326,
    "end": 917914,
    "text": "私が思いついたのはこうだ。"
  },
  {
    "start": 920774,
    "end": 925910,
    "text": "基本的に、すべてのモジュールを持つメイン・コンテナはトランスフォーマーと呼ばれる。"
  },
  {
    "start": 926022,
    "end": 928374,
    "text": "それをNnモジュールのディクテーションに反映させている。"
  },
  {
    "start": 928534,
    "end": 946530,
    "text": "これは基本的に、辞書の文字列のようにキーを使ってサブモジュールにインデックスを付けることができるモジュールで、その中にトークン埋め込みwtの重みと、nn埋め込みである位置埋め込みの重みがあります。"
  },
  {
    "start": 946642,
    "end": 958346,
    "text": "覚えているだろうか、nnエンベッディングとは、数字の配列、つまりこのような数字のブロックを1つだけ取り囲む、ちょっとしゃれたラッパー・モジュールなのだ。"
  },
  {
    "start": 958370,
    "end": 959882,
    "text": "単一のテンソルだ。"
  },
  {
    "start": 960058,
    "end": 968884,
    "text": "nnエンベッディングは、テンソルの周りの見栄えの良いラッパーであり、行にインデックスを付けることでその要素にアクセスできるようにする。"
  },
  {
    "start": 969744,
    "end": 977840,
    "text": "さらに、ここではドットhと、文字列を使ったインデックスではなく数字を使ったインデックスがあることがわかる。"
  },
  {
    "start": 977952,
    "end": 983464,
    "text": "点、h、ゼロ、1、2など、11点まである。"
  },
  {
    "start": 983624,
    "end": 987200,
    "text": "それは、このトランスには12層あるからだ。"
  },
  {
    "start": 987352,
    "end": 992574,
    "text": "というのは、おそらくhiddenの略だと思う。"
  },
  {
    "start": 992874,
    "end": 1000734,
    "text": "モジュールdictの代わりに、これはモデルリストなので、ここにあるように、ドット、ゼロ、ドットオンなどの整数を使ってインデックスを作ることができる。"
  },
  {
    "start": 1001234,
    "end": 1008854,
    "text": "モジュール・リストにはn個のレイヤ・ブロックがあり、n個のモジュールでまだ定義されていないブロックが少しある。"
  },
  {
    "start": 1009434,
    "end": 1016924,
    "text": "それに加えて、GPT-2の論文に従って、最終層のノルムを追加する必要がある。"
  },
  {
    "start": 1017074,
    "end": 1031080,
    "text": "そして、最終的な分類器である言語モデル・ヘッドは、このGPTの埋め込み次元数である768から、語彙サイズである50,257までを投影します。"
  },
  {
    "start": 1031232,
    "end": 1035764,
    "text": "GPT-2はこの最終的な投影にバイアスをかけない。"
  },
  {
    "start": 1036184,
    "end": 1040584,
    "text": "これが骨格で、これが反映されているのがわかるだろう。"
  },
  {
    "start": 1040744,
    "end": 1047239,
    "text": "WTEはトークン埋め込みで、ここでは出力埋め込みと呼ばれているが、実際はトークン埋め込みである。"
  },
  {
    "start": 1047431,
    "end": 1049723,
    "text": "PEはポジションエンコーディングである。"
  },
  {
    "start": 1050183,
    "end": 1054711,
    "text": "この2つの情報は、前回見たように、足し算されてトランスフォーマーに入る。"
  },
  {
    "start": 1054887,
    "end": 1063323,
    "text": "Hはグレーのブロック、LNFはGPT-2モデルによって追加された新しいレイヤーだ。"
  },
  {
    "start": 1063623,
    "end": 1066563,
    "text": "Lmヘッドはこの直線的な部分だ。"
  },
  {
    "start": 1066943,
    "end": 1069587,
    "text": "これがGPT-2の骨格だ。"
  },
  {
    "start": 1069735,
    "end": 1071984,
    "text": "あとはブロックを実装するだけだ。"
  },
  {
    "start": 1072484,
    "end": 1075132,
    "text": "それでは、ブロックそのものを再帰検索してみよう。"
  },
  {
    "start": 1075268,
    "end": 1076784,
    "text": "ブロックを定義したい。"
  },
  {
    "start": 1078164,
    "end": 1080164,
    "text": "ここに置くことにするよ。"
  },
  {
    "start": 1080324,
    "end": 1084104,
    "text": "ブロックはこう書き出したい。"
  },
  {
    "start": 1085444,
    "end": 1089864,
    "text": "これらは初期化の一部で、次にこのブロックが計算する実際のフォワードパスである。"
  },
  {
    "start": 1090364,
    "end": 1096144,
    "text": "ここで、GPT-2の論文で言及されているトランスから再び変更があることに気づくだろう。"
  },
  {
    "start": 1096464,
    "end": 1101816,
    "text": "ここでは、レイヤーの正規化はアテンションまたはフィードフォワードの適用後に行われる。"
  },
  {
    "start": 1101960,
    "end": 1106920,
    "text": "さらに、正規化は残差ストリームの中にあることに注意。"
  },
  {
    "start": 1107032,
    "end": 1112124,
    "text": "フィードフォワードがどのように適用され、この矢印が正規化を貫通しているのがわかるだろう。"
  },
  {
    "start": 1112464,
    "end": 1116784,
    "text": "ということは、残留パスウェイの中にノーマライゼーションがあるということだ。"
  },
  {
    "start": 1116904,
    "end": 1119320,
    "text": "これはあまり良いことではないし、望ましいことでもない。"
  },
  {
    "start": 1119512,
    "end": 1127224,
    "text": "監督からインプット、トークンに至るまで、単一のクリーンな残留ストリームを持つことが望ましい。"
  },
  {
    "start": 1127564,
    "end": 1141244,
    "text": "というのも、マイクログラッドの足し算を思い出してほしいのだが、トップから流れてくるグラデーションは、後方ステージでのグラデーションを両方の枝に均等に分配するだけだからだ。"
  },
  {
    "start": 1141404,
    "end": 1144740,
    "text": "はグラデーションの分岐点である。"
  },
  {
    "start": 1144892,
    "end": 1152832,
    "text": "つまり、トップからの勾配はそのままインプットに流れ、残留経路を通るトークンは変化しない。"
  },
  {
    "start": 1152968,
    "end": 1161456,
    "text": "それに加えて、勾配はブロックを流れ、ブロックは時間と共にそれぞれの貢献をして、時間と共に最適化を変化させる。"
  },
  {
    "start": 1161640,
    "end": 1166872,
    "text": "最適化の観点からは、基本的にクリーンな残留経路が望ましい。"
  },
  {
    "start": 1167048,
    "end": 1185794,
    "text": "これは正規化前のバージョンで、Rxはまずレイヤーの正規化を経て、次にアテンション（注意）を受け、その後、線形関係2番と多層パーセプトロン（フィードフォワードネットワークまたはFFNとも呼ばれる）に戻る。"
  },
  {
    "start": 1186094,
    "end": 1188994,
    "text": "そしてそれはまた残留ストリームに入る。"
  },
  {
    "start": 1189734,
    "end": 1194998,
    "text": "もうひとつ、注意はコミュニケーションであることを思い出してほしい。"
  },
  {
    "start": 1195126,
    "end": 1201804,
    "text": "すべてのトークンと1024個のトークンが順番に並んでいて、ここでトークンがコミュニケーションを取る。"
  },
  {
    "start": 1201844,
    "end": 1203460,
    "text": "ここで彼らは情報を交換する。"
  },
  {
    "start": 1203652,
    "end": 1214332,
    "text": "つまり、attentionは集約関数であり、pooling関数であり、weighted sum関数であり、reduce操作なのだ。"
  },
  {
    "start": 1214508,
    "end": 1219540,
    "text": "一方、このMLPは、トークンひとつひとつで起こる。"
  },
  {
    "start": 1219612,
    "end": 1223196,
    "text": "トークン間で情報が収集されたり交換されたりすることはない。"
  },
  {
    "start": 1223380,
    "end": 1227158,
    "text": "注目はリデュースであり、MLPはマップである。"
  },
  {
    "start": 1227286,
    "end": 1234694,
    "text": "結局のところ、トランスフォーマーはMapreduceの単なる繰り返しアプリケーションになってしまう。"
  },
  {
    "start": 1234814,
    "end": 1240966,
    "text": "ここで彼らはコミュニケーションをとり、集めた情報について個々に考える。"
  },
  {
    "start": 1241070,
    "end": 1248194,
    "text": "これらのブロックのひとつひとつが、残差ストリーム内の表現を繰り返し改良する。"
  },
  {
    "start": 1248574,
    "end": 1252394,
    "text": "これが、この写真から少し手を加えた私たちのブロックだ。"
  },
  {
    "start": 1252944,
    "end": 1255604,
    "text": "さて、それではMLPの話に移ろう。"
  },
  {
    "start": 1255904,
    "end": 1258976,
    "text": "そこで、MLPブロックは次のように実装した。"
  },
  {
    "start": 1259080,
    "end": 1260488,
    "text": "比較的簡単だ。"
  },
  {
    "start": 1260576,
    "end": 1266724,
    "text": "私たちは基本的に、ゲルの非線形性に挟まれた2つの線形投影を持っている。"
  },
  {
    "start": 1267224,
    "end": 1270524,
    "text": "ゲル近似値は10時間。"
  },
  {
    "start": 1270984,
    "end": 1286024,
    "text": "さて、Pytorchのドキュメントに目を移すと、これはN Galuで、このようなフォーマットになっている。"
  },
  {
    "start": 1286564,
    "end": 1300704,
    "text": "このプレビューでお分かりのように、ゲルは基本的にレルーに似ているが、ゼロの位置に平らなテールがないことを除けば、それ以外はわずかに滑らかなレルーによく似ている。"
  },
  {
    "start": 1301004,
    "end": 1304872,
    "text": "ガウス誤差リニアユニットという論文から来ている。"
  },
  {
    "start": 1305028,
    "end": 1312688,
    "text": "この論文を読み進めると、数学的な推論のようなものがあり、それが具体的な定式化につながる解釈につながる。"
  },
  {
    "start": 1312856,
    "end": 1318392,
    "text": "ストキャスティック・ラジアル・ライザーと、アダプティブ・ドロップアウトの修正への期待に関係している。"
  },
  {
    "start": 1318488,
    "end": 1321004,
    "text": "よろしければ、こちらをご覧ください。"
  },
  {
    "start": 1321344,
    "end": 1329484,
    "text": "なぜゲルーの近似版が存在するのかについては少し歴史があり、私が知る限りそれはこの問題に由来している。"
  },
  {
    "start": 1330184,
    "end": 1342080,
    "text": "今号の中でDaniel Hendricksは、彼らがこの非線形性を開発した当時、正確なGeluを評価するために必要なIRF関数はtensorflowでは非常に遅かったと述べている。"
  },
  {
    "start": 1342112,
    "end": 1349640,
    "text": "彼らは基本的にこの近似型を開発することになり、この近似型はバートやGPT-2などに採用されることになった。"
  },
  {
    "start": 1349792,
    "end": 1372394,
    "text": "というのも、もう大きな違いはないというのが私の予想で、これは歴史的な一種の癖のようなものだが、我々はGPT-2を正確に再現しようとしている。"
  },
  {
    "start": 1373934,
    "end": 1379446,
    "text": "さて、Reluの代わりにGeluを直感的に使うもう一つの理由は、以前ビデオで紹介したことだ。"
  },
  {
    "start": 1379470,
    "end": 1390842,
    "text": "過去に、死んだレルニューロンの問題について話したことがあるが、このレルニューロンのテールでは、もしレルニューロンがゼロでちょうど平坦であれば、そこに落ちる活性化はちょうどゼロ勾配になる。"
  },
  {
    "start": 1390898,
    "end": 1398010,
    "text": "もしこれらの活性化がこの平坦な領域で終わるなら、変化も適応もネットワークの発展もない。"
  },
  {
    "start": 1398202,
    "end": 1401066,
    "text": "ゲルは常に局所的な勾配を与える。"
  },
  {
    "start": 1401170,
    "end": 1414534,
    "text": "この論文で実証されたように、またバート論文やGPT-2論文などで取り上げられたことでも実証されたように。"
  },
  {
    "start": 1414674,
    "end": 1420230,
    "text": "そのため、GPT-2の再生ではこの非線形性を採用している。"
  },
  {
    "start": 1420382,
    "end": 1428766,
    "text": "ラマ3など、より近代的なネットワークでは、この非直線性はさらにスイグルや他の変種にも変化する。"
  },
  {
    "start": 1428910,
    "end": 1431874,
    "text": "GPT-2には、このおおよそのギャルが使われている。"
  },
  {
    "start": 1432494,
    "end": 1435154,
    "text": "さて、最後に注目の作戦だ。"
  },
  {
    "start": 1435454,
    "end": 1437754,
    "text": "私の注意を貼り付けよう。"
  },
  {
    "start": 1440314,
    "end": 1441282,
    "text": "これが多いのは分かっている。"
  },
  {
    "start": 1441298,
    "end": 1447234,
    "text": "前のビデオでも取り上げたことなので、少し早く、少しゆっくり、でもあまり遅くならないように見ていこうと思う。"
  },
  {
    "start": 1447274,
    "end": 1448534,
    "text": "私はただ、そこを指し示すだけだ。"
  },
  {
    "start": 1449794,
    "end": 1451506,
    "text": "これが注目の作戦だ。"
  },
  {
    "start": 1451650,
    "end": 1457858,
    "text": "さて、前回のビデオでは、これは単なる注意ではなく、多角的な注意であることを思い出してほしい。"
  },
  {
    "start": 1457946,
    "end": 1458574,
    "text": "そうだろう？"
  },
  {
    "start": 1459034,
    "end": 1467592,
    "text": "以前のビデオでは、この多頭式アテンション・モジュールを使っていたが、今回の実装で、これらのヘッドが実はそれほど複雑なものではないことが明らかになった。"
  },
  {
    "start": 1467778,
    "end": 1469828,
    "text": "基本的には並列だ。"
  },
  {
    "start": 1469956,
    "end": 1481824,
    "text": "アテンション・ブロックの内部には複数のヘッドがあり、それらはすべて並列に機能している。"
  },
  {
    "start": 1482164,
    "end": 1486784,
    "text": "ヘッドは並列ストリームのようなもので、出力は連結される。"
  },
  {
    "start": 1487404,
    "end": 1493784,
    "text": "だから、とてもシンプルで、ヘッドを実装するという点ではかなり単純なものだった。"
  },
  {
    "start": 1494904,
    "end": 1505632,
    "text": "ここで起こるのは、2つの独立したモジュールや、さらに多くのモジュールが連結される代わりに、それらすべてがただ1つの自己注意モジュールに入れられるということだ。"
  },
  {
    "start": 1505768,
    "end": 1514728,
    "text": "その代わり、Pytorchで非常に効率的にするために、転置や分割テンソルの体操を慎重にやっているんだ。"
  },
  {
    "start": 1514816,
    "end": 1522914,
    "text": "根本的にもアルゴリズム的にも、以前このgitリポジトリで見た実装と何も変わらない。"
  },
  {
    "start": 1523534,
    "end": 1534394,
    "text": "簡単に説明すると、あまり長くは説明したくないのですが、トークンは順番に並んでいて、1020個あります。"
  },
  {
    "start": 1534774,
    "end": 1541434,
    "text": "となると、各トークンは、この注目の段階で、クエリーキーと値の3つのベクトルを発することになる。"
  },
  {
    "start": 1541974,
    "end": 1553634,
    "text": "まず、ここで起こることは、クエリーとキーが互いに掛け合わなければならない。"
  },
  {
    "start": 1553754,
    "end": 1555722,
    "text": "それらは乗法的に相互作用しなければならない。"
  },
  {
    "start": 1555898,
    "end": 1562122,
    "text": "ここでやっていることは、QKVを計算し、それを分割し、そしてここで述べたように、たくさんの体操をすることだ。"
  },
  {
    "start": 1562258,
    "end": 1569546,
    "text": "この仕組みは、基本的にヘッドの数nhをバッチ次元にしている。"
  },
  {
    "start": 1569730,
    "end": 1572062,
    "text": "つまり、bと同じようにバッチ次元なのだ。"
  },
  {
    "start": 1572218,
    "end": 1581374,
    "text": "というのは、Pytorchはこの後に続く操作で、Bとnhをバッチとして扱い、それらすべてに対してすべての操作を並列に適用するからだ。"
  },
  {
    "start": 1581494,
    "end": 1586638,
    "text": "バッチもヘッドも、適用されるオペレーションもナンバーワンだ。"
  },
  {
    "start": 1586686,
    "end": 1589754,
    "text": "クエリーとキーが相互作用して、私たちの注意を喚起する。"
  },
  {
    "start": 1590214,
    "end": 1598594,
    "text": "これは自己回帰的なマスクで、トークンがそれ以前のトークンにのみ反応し、未来のトークンには決して反応しないようにするものである。"
  },
  {
    "start": 1599924,
    "end": 1604424,
    "text": "ここでのソフトマックスは注目度を正規化するので、常に合計が1になる。"
  },
  {
    "start": 1604924,
    "end": 1615984,
    "text": "前のビデオで、アテンション・マトリックスに値を掛け合わせるというのは、基本的にトークンごとに面白いと思ったトークンの値を加重合計する方法だということを思い出してほしい。"
  },
  {
    "start": 1616564,
    "end": 1621396,
    "text": "そして最後のトランスポーズ、コンティニュアス、ビューは、これらすべてを再び組み立てるだけだ。"
  },
  {
    "start": 1621540,
    "end": 1624544,
    "text": "これは実際に連結操作を実行する。"
  },
  {
    "start": 1625014,
    "end": 1627634,
    "text": "お望みなら、ゆっくりステップを踏んでもいい。"
  },
  {
    "start": 1627974,
    "end": 1632230,
    "text": "これは、数学的には以前の実装と同等である。"
  },
  {
    "start": 1632302,
    "end": 1633878,
    "text": "Pytorchの方が効率的なんだ。"
  },
  {
    "start": 1633926,
    "end": 1636274,
    "text": "だから、この実装を選んだんだ。"
  },
  {
    "start": 1636934,
    "end": 1640174,
    "text": "それに加えて、変数名の付け方にも気をつけている。"
  },
  {
    "start": 1640254,
    "end": 1643806,
    "text": "例えば、cはCアテンと同じである。"
  },
  {
    "start": 1643950,
    "end": 1649038,
    "text": "だから、私たちのキーは基本的に、抱きつき顔トランスフォーマーのコードのスキーマに正確に従うべきだ。"
  },
  {
    "start": 1649126,
    "end": 1658542,
    "text": "このような命名規則があれば、すべての変数が同じ名前なので、すべてのウエイトを移植するのが非常に簡単になる。"
  },
  {
    "start": 1658718,
    "end": 1663194,
    "text": "この時点でGPT-2の実装は終了した。"
  },
  {
    "start": 1663494,
    "end": 1669314,
    "text": "これにより、基本的にハグフェイスのファイルを使う必要がなくなる。"
  },
  {
    "start": 1670814,
    "end": 1675754,
    "text": "これは2000行のコードである。"
  },
  {
    "start": 1676714,
    "end": 1679330,
    "text": "その代わり、100行以下のコードしかない。"
  },
  {
    "start": 1679402,
    "end": 1681938,
    "text": "これがGPT-2の完全な実装だ。"
  },
  {
    "start": 1682066,
    "end": 1687466,
    "text": "今の段階では、すべてのウエイトを引き継いで設定し、それからジェネレーションを行えばいい。"
  },
  {
    "start": 1687610,
    "end": 1688938,
    "text": "それがどんなものか見てみよう。"
  },
  {
    "start": 1689026,
    "end": 1696186,
    "text": "さて、ここでGPTの設定も変更し、ハイブのパラメーターがGPT 2124 mモデルと一致するようにした。"
  },
  {
    "start": 1696290,
    "end": 1700602,
    "text": "ここではブロックサイズと呼ぶが、シーケンスの最大長は124である。"
  },
  {
    "start": 1700778,
    "end": 1720564,
    "text": "トークンの数は5250で、257です。私のトークナイザービデオをご覧になれば、これは50,000のマージ、BPEのマージ、256バイトのトークン、BPEツリーの葉、そして異なる文書を区切り、同様に生成を開始することができる特別なテキスト終端トークンがあることがおわかりでしょう。"
  },
  {
    "start": 1720984,
    "end": 1723608,
    "text": "十二の層があり、十二の頭部が注目に値する。"
  },
  {
    "start": 1723696,
    "end": 1727204,
    "text": "トランスの寸法は768であった。"
  },
  {
    "start": 1727784,
    "end": 1735802,
    "text": "ハギング・フェイスのパラメータをこのコードにロードして、そのパラメータでGPTクラスを初期化する方法だ。"
  },
  {
    "start": 1735898,
    "end": 1738814,
    "text": "ここにコードの束をコピーペーストしよう。"
  },
  {
    "start": 1739834,
    "end": 1748738,
    "text": "正直なところ、それほど面白くもないし、エキサイティングでもないからだ。"
  },
  {
    "start": 1748786,
    "end": 1750882,
    "text": "ウエイトを積んでいるところだから、ちょっと乾いているんだ。"
  },
  {
    "start": 1751018,
    "end": 1759292,
    "text": "さっきも言ったように、このGPT-2のミニシリーズには4つのモデルがある。"
  },
  {
    "start": 1759388,
    "end": 1760980,
    "text": "私はそれを移植しているだけだ。"
  },
  {
    "start": 1761132,
    "end": 1763584,
    "text": "これらはGPT-2モデルのハイパーパラメーターである。"
  },
  {
    "start": 1764164,
    "end": 1767036,
    "text": "コンフィグ・オブジェクトを作成し、独自のモデルを作成する。"
  },
  {
    "start": 1767180,
    "end": 1773464,
    "text": "ここで起こっているのは、私たちのモデルとハグする顔のモデルの両方にステート・ディクトを作成することです。"
  },
  {
    "start": 1775284,
    "end": 1782784,
    "text": "ここでやっているのは、顔モデルのキーをハグして、テンソルをコピーすることだ。"
  },
  {
    "start": 1783134,
    "end": 1787046,
    "text": "その過程で、我々はいくつかのバッファーを無視しているようなものだ。"
  },
  {
    "start": 1787150,
    "end": 1788606,
    "text": "パラメータではなく、バッファだ。"
  },
  {
    "start": 1788710,
    "end": 1793038,
    "text": "例えば、アテンション・バイアスは自己回帰マスクに使われるだけだ。"
  },
  {
    "start": 1793206,
    "end": 1797702,
    "text": "だから、いくつかのマスクは無視している。"
  },
  {
    "start": 1797798,
    "end": 1802070,
    "text": "それから、もうひとつ厄介なのは、これがtensorflowのレポから来たものだということだ。"
  },
  {
    "start": 1802102,
    "end": 1807926,
    "text": "ちょっと迷惑なんだけど、ウェイトの一部がピトーチが望むものとずれているんだ。"
  },
  {
    "start": 1808030,
    "end": 1814528,
    "text": "そこで、転置されるべきウェイトを手作業でハードコーディングし、そうであれば転置するようにした。"
  },
  {
    "start": 1814696,
    "end": 1816884,
    "text": "であれば、このモデルを返す。"
  },
  {
    "start": 1817184,
    "end": 1831520,
    "text": "pythonのfrom pretrainedはコンストラクタまたはクラスメソッドで、モデルタイプを与えるとGPTオブジェクトを返します。"
  },
  {
    "start": 1831712,
    "end": 1834608,
    "text": "これがコードで、これをどう使うかだ。"
  },
  {
    "start": 1834776,
    "end": 1842334,
    "text": "ここでターミナルを開き、GPT-2 pyをpythonで訓練する。"
  },
  {
    "start": 1842914,
    "end": 1845454,
    "text": "祈っている。"
  },
  {
    "start": 1847674,
    "end": 1849134,
    "text": "そう、私たちはクラッシュしなかった。"
  },
  {
    "start": 1849594,
    "end": 1855578,
    "text": "これで重みとバイアス、その他すべてをnnモジュールにロードすることができる。"
  },
  {
    "start": 1855706,
    "end": 1860570,
    "text": "では、このモデルが機能していることをさらに確信し、実際にこのモデルから生成してみよう。"
  },
  {
    "start": 1860682,
    "end": 1864158,
    "text": "さて、このモデルから実際に生成する前に、転送できるようにしなければならない。"
  },
  {
    "start": 1864226,
    "end": 1865998,
    "text": "私たちはまだそのコードを書いていない。"
  },
  {
    "start": 1866126,
    "end": 1868034,
    "text": "これがフォワード・ファンクションだ。"
  },
  {
    "start": 1869014,
    "end": 1877382,
    "text": "フォワードへの入力は、トークン、トークンのインデックスである。"
  },
  {
    "start": 1877518,
    "end": 1884234,
    "text": "バッチ次元はbで、時間次元はtまでだ。"
  },
  {
    "start": 1884854,
    "end": 1887206,
    "text": "tはブロックサイズ以上にはできない。"
  },
  {
    "start": 1887270,
    "end": 1889434,
    "text": "ブロックサイズは最大シーケンス長。"
  },
  {
    "start": 1889874,
    "end": 1894050,
    "text": "B×Tのインデックスは、2次元のレイアウトのように配置されている。"
  },
  {
    "start": 1894202,
    "end": 1900210,
    "text": "この行は基本的にすべて、ブロック・サイズまでの大きさであることを覚えておいてほしい。"
  },
  {
    "start": 1900362,
    "end": 1903034,
    "text": "これは、一連のトークンである。"
  },
  {
    "start": 1903154,
    "end": 1908494,
    "text": "とすると、独立した2つのシーケンスが一括して積み重ねられるので、効率的である。"
  },
  {
    "start": 1909394,
    "end": 1912922,
    "text": "ここで、位置埋め込みとトークン埋め込みを転送する。"
  },
  {
    "start": 1913058,
    "end": 1916178,
    "text": "このコードは、前回の講義でよく理解できたはずだ。"
  },
  {
    "start": 1916346,
    "end": 1922586,
    "text": "基本的にはレンジを使う。レンジはレンジのバージョンのようなものだが、Pytorch用だ。"
  },
  {
    "start": 1922770,
    "end": 1929094,
    "text": "ゼロからtまで繰り返し、このような位置のインデックスを作成している。"
  },
  {
    "start": 1931034,
    "end": 1937242,
    "text": "というのも、CPUだけでトレーニングするのは効率が悪すぎるからだ。"
  },
  {
    "start": 1937298,
    "end": 1940414,
    "text": "GPUのトレーニングをしたいし、それはもう少し先になるだろう。"
  },
  {
    "start": 1941434,
    "end": 1946506,
    "text": "そして、位置の埋め込みとトークンの埋め込み、そしてこの2つの加算演算がある。"
  },
  {
    "start": 1946650,
    "end": 1951978,
    "text": "ここで、位置の埋め込みは、入力のすべての行で同じになることに注意してください。"
  },
  {
    "start": 1952106,
    "end": 1964934,
    "text": "この中に放送が隠されていて、さらにここに追加の次元を作り、この2つを足し合わせる。"
  },
  {
    "start": 1965594,
    "end": 1970174,
    "text": "次にトランスフォーマーブロックを進め、最後にノルムとエレメントを入れる。"
  },
  {
    "start": 1971114,
    "end": 1973906,
    "text": "フォワードの後に出てくるのはロジットだ。"
  },
  {
    "start": 1974090,
    "end": 1985090,
    "text": "入力がb×t個のインデックスであった場合、b×t個のシーケンスごとに、次にどのトークンが来るかのロジットを計算する。"
  },
  {
    "start": 1985202,
    "end": 1986594,
    "text": "トークンとは何ですか？"
  },
  {
    "start": 1986714,
    "end": 1990722,
    "text": "B tプラス1、このトークンの右側にあるもの。"
  },
  {
    "start": 1990858,
    "end": 1995394,
    "text": "ここでいう語彙の大きさとは、可能なトークンの数である。"
  },
  {
    "start": 1995554,
    "end": 1998674,
    "text": "したがって、これがこれから得られるテンソルである。"
  },
  {
    "start": 1998834,
    "end": 2002654,
    "text": "これらのロードジットはソフトマックスで確率になる。"
  },
  {
    "start": 2003434,
    "end": 2008050,
    "text": "これがネットワークのフォワードパスである。"
  },
  {
    "start": 2008122,
    "end": 2010854,
    "text": "だから、すぐにでもモデルから生成できるようになるだろう。"
  },
  {
    "start": 2011434,
    "end": 2017322,
    "text": "では、左のハグと右のハグを同じようにセットアップしてみよう。"
  },
  {
    "start": 2017458,
    "end": 2026066,
    "text": "ここでは、パイプラインからサンプリングし、helloama言語モデルの接頭辞で30トークンまで5回サンプリングしました。"
  },
  {
    "start": 2026170,
    "end": 2028050,
    "text": "これが我々が達成した完走である。"
  },
  {
    "start": 2028202,
    "end": 2030442,
    "text": "それを左でも再現してみよう。"
  },
  {
    "start": 2030618,
    "end": 2033266,
    "text": "最大長さは30。"
  },
  {
    "start": 2033370,
    "end": 2037930,
    "text": "最初にすることは、もちろんモデルを初期化し、評価モードにすることだ。"
  },
  {
    "start": 2038042,
    "end": 2040986,
    "text": "これはモデルをevalに入れる良い練習になる。"
  },
  {
    "start": 2041050,
    "end": 2043722,
    "text": "トレーニングするわけでもなく、ただ使うだけだというのに。"
  },
  {
    "start": 2043898,
    "end": 2048634,
    "text": "実は今、これが何かをしているのかどうか、次の理由からわからない。"
  },
  {
    "start": 2048794,
    "end": 2056586,
    "text": "上記のモデルには、トレーニング時や評価時に実際に異なる振る舞いをするモジュールやレイヤーは含まれていない。"
  },
  {
    "start": 2056690,
    "end": 2060890,
    "text": "例えば、ドロップアウト、バッチノルム、その他多くのレイヤーがこのような動作をしている。"
  },
  {
    "start": 2061002,
    "end": 2065614,
    "text": "ここで使用したこれらのレイヤーはすべて、トレーニング時間と評価時間の両方で同一でなければならない。"
  },
  {
    "start": 2067394,
    "end": 2072154,
    "text": "潜在的なモデルevalは何もしない。"
  },
  {
    "start": 2072194,
    "end": 2077654,
    "text": "多分、Pytorchの内部は、この中の評価モードによって、賢いことをするのだろう。"
  },
  {
    "start": 2078493,
    "end": 2082581,
    "text": "次にすることは、モデル全体をCUDAに移行することだ。"
  },
  {
    "start": 2082637,
    "end": 2085989,
    "text": "テンソルをすべてGPUに移す。"
  },
  {
    "start": 2086141,
    "end": 2096525,
    "text": "このクラウドボックスにGPUをたくさん置いて、モデル全体とそのメンバー、テンソルなどすべてを動かしている。"
  },
  {
    "start": 2096709,
    "end": 2102285,
    "text": "すべてがGPUの上に置かれた、基本的にまったく別のコンピューターに出荷される。"
  },
  {
    "start": 2102389,
    "end": 2113754,
    "text": "GPUはCPUに接続されており、通信が可能だが、基本的には独自のコンピューター・アーキテクチャを持つまったく別のコンピューターであり、ニューラルネットワークを実行するような並列処理タスクに非常に適している。"
  },
  {
    "start": 2113914,
    "end": 2118978,
    "text": "モデルをGPU、つまりまったく別のコンピューターに住まわせるためにこうしているんだ。"
  },
  {
    "start": 2119146,
    "end": 2125014,
    "text": "GPU上でより効率的に動作するので、コードがより効率的になる。"
  },
  {
    "start": 2125794,
    "end": 2128694,
    "text": "それはモデルそのものだ。"
  },
  {
    "start": 2129794,
    "end": 2136154,
    "text": "さて、次にやりたいことは、これを接頭辞として生成することだ。"
  },
  {
    "start": 2136574,
    "end": 2139754,
    "text": "実際にプレフィックス・トークンを作ってみよう。"
  },
  {
    "start": 2140454,
    "end": 2142070,
    "text": "これが私の書いたコードだ。"
  },
  {
    "start": 2142222,
    "end": 2147558,
    "text": "OpenAIからtech tokenライブラリをインポートし、GPT-2エンコーディングを取得する。"
  },
  {
    "start": 2147606,
    "end": 2150430,
    "text": "これがGPT-2用のトークナイザーだ。"
  },
  {
    "start": 2150622,
    "end": 2156594,
    "text": "次に、この文字列をエンコードして、トークンである整数のリストを取得する。"
  },
  {
    "start": 2157274,
    "end": 2166538,
    "text": "なぜなら、この文字列をコピー・ペーストするだけで、チック・トークナイザーでそれが何であるかを検査することができるからだ。"
  },
  {
    "start": 2166666,
    "end": 2170170,
    "text": "これを貼り付けるだけで、トークンが出てくる。"
  },
  {
    "start": 2170362,
    "end": 2174414,
    "text": "この整数のリストが、トークンに期待されるものだ。"
  },
  {
    "start": 2174754,
    "end": 2179794,
    "text": "私のビデオをご覧になった方は覚えていると思いますが、もちろんトークンはすべて、小さな文字列の塊ですよね？"
  },
  {
    "start": 2179834,
    "end": 2185074,
    "text": "これらは、この文字列をGPT-2トークンに切り詰めたものである。"
  },
  {
    "start": 2186134,
    "end": 2188790,
    "text": "トークンが揃えば、それは整数のリストになる。"
  },
  {
    "start": 2188902,
    "end": 2191246,
    "text": "そこからトーチテンソルを作ることができる。"
  },
  {
    "start": 2191350,
    "end": 2192950,
    "text": "この場合は8トークンだ。"
  },
  {
    "start": 2193062,
    "end": 2199238,
    "text": "次に、この8つのトークンを5回繰り返して、8つのトークンの列を5つ作る。"
  },
  {
    "start": 2199406,
    "end": 2206926,
    "text": "これが初期入力xで、GPU上にある。"
  },
  {
    "start": 2207110,
    "end": 2222544,
    "text": "xのIDXは、ロジットを取得するためにフォワードするためのもので、5つの行の6番目のトークン、9番目のトークンを知ることができる。"
  },
  {
    "start": 2222844,
    "end": 2223324,
    "text": "いいかい？"
  },
  {
    "start": 2223364,
    "end": 2224788,
    "text": "これで生成の準備は整った。"
  },
  {
    "start": 2224836,
    "end": 2227344,
    "text": "もうひとつコード・ブロックを貼り付けよう。"
  },
  {
    "start": 2228644,
    "end": 2234428,
    "text": "このコードブロックの中で起こっていることは、xのサイズがb×tであるということだ。"
  },
  {
    "start": 2234476,
    "end": 2248854,
    "text": "このループの反復ごとに、それぞれの行に新しいインデックスの列を追加していくことになる。"
  },
  {
    "start": 2248974,
    "end": 2255614,
    "text": "ループの反復ごとに、xに1列ずつ追加され、すべての操作はコンテキストの中で行われる。"
  },
  {
    "start": 2255654,
    "end": 2257006,
    "text": "トーチの新卒マネージャー"
  },
  {
    "start": 2257110,
    "end": 2260974,
    "text": "これはピトーチに、我々はこの件に関して後ろ向きな発言をするつもりはないと伝えているだけだ。"
  },
  {
    "start": 2261094,
    "end": 2263780,
    "text": "中間テンソルをすべてキャッシュする必要がないからだ。"
  },
  {
    "start": 2263862,
    "end": 2267240,
    "text": "後で後戻りする可能性があるからといって、何らかの準備をする必要はないだろう。"
  },
  {
    "start": 2267352,
    "end": 2271404,
    "text": "これはスペースと時間の節約になる。"
  },
  {
    "start": 2272064,
    "end": 2282364,
    "text": "私たちはロードジェットを手に入れ、最後の場所でロジットを手に入れ、他のロジットはすべて捨てる。"
  },
  {
    "start": 2282664,
    "end": 2288284,
    "text": "これは無駄なことだが、サンプリングの非効率的な実装のようなものだ。"
  },
  {
    "start": 2289344,
    "end": 2290964,
    "text": "それは正しいが、非効率的だ。"
  },
  {
    "start": 2291464,
    "end": 2295844,
    "text": "ロジットの最後の列を得、それをソフトマックスに通して確率を得る。"
  },
  {
    "start": 2296144,
    "end": 2301088,
    "text": "ここでは50のトップケースをサンプリングしている。"
  },
  {
    "start": 2301216,
    "end": 2310664,
    "text": "パイプラインのハグフェイスのドキュメントを見ると、ハグフェイスにはたくさんのクォークが使われている。"
  },
  {
    "start": 2310824,
    "end": 2314464,
    "text": "正直なところ、ちょっと多いんだ。"
  },
  {
    "start": 2314504,
    "end": 2319784,
    "text": "私が気づいた重要な点は、デフォルトでトップKを使用していることだと思う。"
  },
  {
    "start": 2319944,
    "end": 2324288,
    "text": "だから、ここでもそれが使われている。"
  },
  {
    "start": 2324416,
    "end": 2330040,
    "text": "これは、基本的に確率を取り出し、上位50位までの確率だけを残すというものだ。"
  },
  {
    "start": 2330192,
    "end": 2336352,
    "text": "50分の1の確率より低いものは、ゼロにクランプして再正規化する。"
  },
  {
    "start": 2336528,
    "end": 2340124,
    "text": "そうすることで、非常にレアなトークンをサンプリングすることがない。"
  },
  {
    "start": 2340464,
    "end": 2344618,
    "text": "これからサンプリングするトークンは、常に最も可能性の高いトークンのトップ50に入っている。"
  },
  {
    "start": 2344736,
    "end": 2351102,
    "text": "そうすることで、モデルを軌道に乗せることができ、ベラベラしゃべり続けることもなく、迷子になることもなく、簡単にレールから外れることもない。"
  },
  {
    "start": 2351278,
    "end": 2356302,
    "text": "トークンの近くに刺さった方がいいような気がする。"
  },
  {
    "start": 2356478,
    "end": 2359342,
    "text": "これがPytorchでのやり方だ。"
  },
  {
    "start": 2359398,
    "end": 2361990,
    "text": "あまり洞察に富んでいるとは思えないので、スピード感をもって読み進めることにする。"
  },
  {
    "start": 2362102,
    "end": 2372334,
    "text": "大雑把に言えば、トークンの新しい列を取得し、それをxに追加する。"
  },
  {
    "start": 2372634,
    "end": 2381210,
    "text": "そして最終的に、この例では5×30の大きさのx全体ができる。"
  },
  {
    "start": 2381362,
    "end": 2384890,
    "text": "基本的には、これらの個々の行をすべて表示すればよい。"
  },
  {
    "start": 2385002,
    "end": 2394734,
    "text": "すべての行を取得し、サンプリングされたすべてのトークンを取得し、TikTokenizerのデコード関数を使用して文字列を取得しています。"
  },
  {
    "start": 2395074,
    "end": 2397494,
    "text": "だから、ターミナル、新しいターミナル。"
  },
  {
    "start": 2399264,
    "end": 2412168,
    "text": "PythonのトレインGPT-2はOK。"
  },
  {
    "start": 2412256,
    "end": 2414844,
    "text": "こんにちは、私は言語モデルであって、プログラムではありません。"
  },
  {
    "start": 2416344,
    "end": 2418624,
    "text": "新しいライン、新しいライン、等々。"
  },
  {
    "start": 2418744,
    "end": 2420144,
    "text": "こんにちは、私は言語モデルです。"
  },
  {
    "start": 2420224,
    "end": 2428128,
    "text": "私が言語を作るときに悩むことのひとつは、つまり、このような場合、ただおしゃべりしているようにしか見えないようなものを作るのがいかに簡単かということだ。"
  },
  {
    "start": 2428256,
    "end": 2434084,
    "text": "さて、ひとつお気づきの点があるとすれば、これらの世代はここでハグをする世代ではないということだ。"
  },
  {
    "start": 2434424,
    "end": 2437448,
    "text": "正直なところ、矛盾点が見つからない。"
  },
  {
    "start": 2437536,
    "end": 2444624,
    "text": "これらの選択肢をすべて調べ尽くしたわけではないが、おそらく一番上のP以外にも何かが隠れているはずなので、それを照合することはできない。"
  },
  {
    "start": 2444704,
    "end": 2450938,
    "text": "念のため、この下のjupyterノートブックで、抱きつき顔のモデルを使っている。"
  },
  {
    "start": 2451136,
    "end": 2453114,
    "text": "こちらは抱きつき顔のモデル。"
  },
  {
    "start": 2454374,
    "end": 2461714,
    "text": "コードを再現してみたが、これを実行しても、あれを実行しても、同じ結果になる。"
  },
  {
    "start": 2462014,
    "end": 2465030,
    "text": "基本的に、モデル内部は間違っていない。"
  },
  {
    "start": 2465142,
    "end": 2470990,
    "text": "ただ、パイプラインがハグフェイスで何をするのかが100％わからないから、マッチアップできないんだ。"
  },
  {
    "start": 2471102,
    "end": 2476436,
    "text": "そうでなければ、コードは正しく、すべてのテンソルを正しくロードしたことになる。"
  },
  {
    "start": 2476550,
    "end": 2479264,
    "text": "モデルを正しく初期化している。"
  },
  {
    "start": 2479424,
    "end": 2482376,
    "text": "要するに、すべてのウエイトを移植したということだ。"
  },
  {
    "start": 2482480,
    "end": 2489044,
    "text": "GPT-2を初期化する。これはGPT-2の正確なオープニングであり、シーケンスを生成することができる。"
  },
  {
    "start": 2489504,
    "end": 2504764,
    "text": "ここで、もちろんGPT-2モデルの重みで初期化しているが、今度は乱数からゼロから初期化して、これらのモデルと同等かそれ以上の品質のシーケンスを与えるモデルを実際にトレーニングしたい。"
  },
  {
    "start": 2505064,
    "end": 2507008,
    "text": "というわけで、次はその話をしよう。"
  },
  {
    "start": 2507136,
    "end": 2514360,
    "text": "というのも、Pytorchはすでにデフォルトでモデルをランダムに初期化しているからだ。"
  },
  {
    "start": 2514512,
    "end": 2525744,
    "text": "コンストラクタでGPTモデルを作成するとき、これらのレイヤーとモジュールはすべて、デフォルトでランダムなイニシャライザを持っています。"
  },
  {
    "start": 2525864,
    "end": 2528766,
    "text": "これらのリニアレイヤーが作成されるときなどだ。"
  },
  {
    "start": 2528880,
    "end": 2536274,
    "text": "デフォルトのコンストラクターがあり、たとえば、過去に見たJavierの初期化を使って、これらのレイヤーのウェイトを構築することができる。"
  },
  {
    "start": 2536434,
    "end": 2541842,
    "text": "ですから、GPT-2モデルの代わりにランダム・モデルを作るのは、実際にはかなり簡単です。"
  },
  {
    "start": 2541938,
    "end": 2546614,
    "text": "私たちはここに来て、代わりにモデル・イコールGPTを作るだろう。"
  },
  {
    "start": 2547034,
    "end": 2550522,
    "text": "であれば、デフォルト設定のGPT設定を使いたい。"
  },
  {
    "start": 2550698,
    "end": 2553562,
    "text": "デフォルトの設定では、124個のmパラメータが使用される。"
  },
  {
    "start": 2553618,
    "end": 2566838,
    "text": "これがランダムモデルの初期化で、これを実行すれば結果が得られるはずだ。"
  },
  {
    "start": 2566966,
    "end": 2569694,
    "text": "もちろん、ここでの結果はまったくのゴミである。"
  },
  {
    "start": 2569814,
    "end": 2571374,
    "text": "それは、このランダムなモデルがあるからだ。"
  },
  {
    "start": 2571494,
    "end": 2576086,
    "text": "そのため、トークン文字列の断片がまったくランダムに切り刻まれているだけだ。"
  },
  {
    "start": 2576190,
    "end": 2577754,
    "text": "それが今あるものだ。"
  },
  {
    "start": 2578654,
    "end": 2595296,
    "text": "ところで、もうひとつ指摘しておきたいことがあります。GPUを持っていないためにCUDAが使えない場合でも、ここでやっていることにある程度はついていくことができます。"
  },
  {
    "start": 2595440,
    "end": 2597352,
    "text": "今のところ、ちゃんとついていける。"
  },
  {
    "start": 2597408,
    "end": 2603680,
    "text": "Pytorchでやりたいことのひとつは、利用可能なデバイスを自動検出することです。"
  },
  {
    "start": 2603832,
    "end": 2606164,
    "text": "特に、このようにすることができる。"
  },
  {
    "start": 2608204,
    "end": 2612748,
    "text": "ここでは、最高の計算能力を持つデバイスを検出しようとしている。"
  },
  {
    "start": 2612876,
    "end": 2614260,
    "text": "そう考えることもできる。"
  },
  {
    "start": 2614412,
    "end": 2620188,
    "text": "デフォルトでは、CPUからスタートします。もちろん、すべてのコンピュータにCPUが搭載されているので、どこでも利用可能です。"
  },
  {
    "start": 2620356,
    "end": 2622756,
    "text": "GPUをお持ちですか？"
  },
  {
    "start": 2622820,
    "end": 2624092,
    "text": "あなたはまだCUDAを使っている。"
  },
  {
    "start": 2624228,
    "end": 2628284,
    "text": "CUDAを持っていないのなら、少なくともNPは持っているのか？"
  },
  {
    "start": 2628364,
    "end": 2630796,
    "text": "NPはアップル・シリコンのバックエンドだ。"
  },
  {
    "start": 2630940,
    "end": 2635086,
    "text": "もしあなたがかなり新しいMacBookをお持ちなら、おそらく内部にはアップルのシリコンが使われているはずです。"
  },
  {
    "start": 2635230,
    "end": 2640014,
    "text": "それなら、どのMacBookを使うかにもよるが、かなり高性能なGPUを搭載していることになる。"
  },
  {
    "start": 2640054,
    "end": 2643486,
    "text": "だから、CPUより高速になる可能性のあるNPを使うことができる。"
  },
  {
    "start": 2643670,
    "end": 2645382,
    "text": "ここでデバイスをプリントすることができる。"
  },
  {
    "start": 2645518,
    "end": 2650194,
    "text": "このデバイスを手に入れたら、CUDAの代わりに実際に使うことができる。"
  },
  {
    "start": 2650574,
    "end": 2652222,
    "text": "それを入れ替えるだけだ。"
  },
  {
    "start": 2652398,
    "end": 2665274,
    "text": "Xでモデルを呼び出すとき、このXがGPUではなくCPUであれば、問題なく動作することに注意してほしい。"
  },
  {
    "start": 2665734,
    "end": 2672086,
    "text": "ポーズを作るとき、iDXのデバイスを使ってこのテンソルも作るように注意した。"
  },
  {
    "start": 2672230,
    "end": 2678126,
    "text": "そうすれば、1つのテンソルがCPU上にあり、もう1つがGPU上にあるというミスマッチがなくなり、それらを組み合わせることができなくなる。"
  },
  {
    "start": 2678230,
    "end": 2685394,
    "text": "ここでは、このモデルへの入力が示すように、正しいデバイスで慎重に初期化を行っている。"
  },
  {
    "start": 2686024,
    "end": 2690484,
    "text": "これは私のためにデバイスを自動検出します、これはもちろんGPUになります。"
  },
  {
    "start": 2691064,
    "end": 2700944,
    "text": "デバイスのCUDAを使っても、別のデバイスを使っても、それほど遅くなることはありません。"
  },
  {
    "start": 2700984,
    "end": 2711208,
    "text": "もしここでdeviceをオーバーライドすれば、もしdevice equals cpuをオーバーライドすれば、もちろんCUDAと表示される。"
  },
  {
    "start": 2711256,
    "end": 2720552,
    "text": "今、実際に使っているCPUは123456。"
  },
  {
    "start": 2720728,
    "end": 2725608,
    "text": "実際、トーチコンパイルなどは使っていない。"
  },
  {
    "start": 2725696,
    "end": 2729724,
    "text": "CPUでもそれなりにフォローできると思う。"
  },
  {
    "start": 2730504,
    "end": 2731808,
    "text": "それについてのメモだ。"
  },
  {
    "start": 2731936,
    "end": 2749264,
    "text": "PyTorchで異なるデバイスを持つということはどういうことなのか、モジュール2デバイスとか、トーチテンソルを2デバイスにするときにPytorchがバックグラウンドでやってくれることとは何なのか、何が起きてどう動くのか、などなど。"
  },
  {
    "start": 2749384,
    "end": 2753200,
    "text": "とりあえずトレーニングに入りたいし、モデルのトレーニングを始めたい。"
  },
  {
    "start": 2753312,
    "end": 2759808,
    "text": "とりあえず、このデバイスがコードを高速化するとだけ言っておいて、実際にどのようにモデルをトレーニングするかについて説明しよう。"
  },
  {
    "start": 2759976,
    "end": 2762224,
    "text": "モデルをトレーニングするには、いくつかのデータセットが必要だ。"
  },
  {
    "start": 2762344,
    "end": 2770156,
    "text": "私にとって、最もデバッグに適した、最もシンプルなデータセットは、小さなシェイクスピアのデータセットで、このURLから入手できる。"
  },
  {
    "start": 2770220,
    "end": 2773544,
    "text": "wgetしてもいいし、小さなシェイクスピアのデータセットを検索してもいい。"
  },
  {
    "start": 2774244,
    "end": 2778428,
    "text": "というわけで、私のファイルシステムにはlsinput TXTとだけ入っている。"
  },
  {
    "start": 2778556,
    "end": 2786944,
    "text": "すでにダウンロード済みで、ここでデータセットを読み込んで最初の1000文字を取得し、最初の100文字を表示している。"
  },
  {
    "start": 2787324,
    "end": 2794414,
    "text": "GPT-2の圧縮率はおよそ3対1、トークナイザーの圧縮率はおよそ3対1であることを覚えておいてほしい。"
  },
  {
    "start": 2794524,
    "end": 2801014,
    "text": "1000文字ということは、現在獲得しているスライスで、ここからおよそ300トークンが出てくることになります。"
  },
  {
    "start": 2801514,
    "end": 2804094,
    "text": "これが最初の数文字だ。"
  },
  {
    "start": 2805274,
    "end": 2820212,
    "text": "もう少し統計を取りたければ、入力TXTのワード数をカウントしてみよう。"
  },
  {
    "start": 2820378,
    "end": 2822704,
    "text": "このファイルがASCII文字だけであることを知っている。"
  },
  {
    "start": 2822744,
    "end": 2824864,
    "text": "私の知る限り、ここにはクレイジーなユニコードはない。"
  },
  {
    "start": 2824984,
    "end": 2827952,
    "text": "したがって、すべてのASCII文字は1バイトでエンコードされる。"
  },
  {
    "start": 2828088,
    "end": 2833524,
    "text": "つまり、このデータセットに含まれる文字数はおよそ100万文字ということになる。"
  },
  {
    "start": 2834064,
    "end": 2838484,
    "text": "これはデフォルトのデータセット・サイズであり、デバッグ用の非常に小さな最小限のデータセットである。"
  },
  {
    "start": 2838864,
    "end": 2840136,
    "text": "私たちを軌道に乗せるためにね。"
  },
  {
    "start": 2840280,
    "end": 2853382,
    "text": "このデータセットをトークン化するために、GPT-2のtickトークンエンコーディングを取得し、最初の1000文字をエンコードして、最初の24個のトークンのみを表示することにする。"
  },
  {
    "start": 2853558,
    "end": 2856474,
    "text": "これらは整数のリストとしてのトークンである。"
  },
  {
    "start": 2856934,
    "end": 2862766,
    "text": "GPT-2トークンが読めれば、198がスラッシング文字であることがわかるだろう。"
  },
  {
    "start": 2862870,
    "end": 2864150,
    "text": "それは新しいラインだ。"
  },
  {
    "start": 2864302,
    "end": 2866158,
    "text": "では、例えばここに2つの新しい行がある。"
  },
  {
    "start": 2866206,
    "end": 2868194,
    "text": "198がここに2回ある。"
  },
  {
    "start": 2868814,
    "end": 2871954,
    "text": "これは最初の24個のトークンをトークン化しただけである。"
  },
  {
    "start": 2872274,
    "end": 2878794,
    "text": "今やりたいことは、これらのトークン・シーケンスを実際に処理して、トランスフォーマーに送り込むことだ。"
  },
  {
    "start": 2878954,
    "end": 2887282,
    "text": "特に必要なのは、これらのトークンをIDX変数に並べ替えて、トランスフォーマーに送り込むことだ。"
  },
  {
    "start": 2887378,
    "end": 2899690,
    "text": "各シーケンスは基本的にtトークンまでで、tが最大シーケンス長より大きくなることはない。"
  },
  {
    "start": 2899882,
    "end": 2908134,
    "text": "とすると、このt個の長いトークンの並びがあり、独立したb個の並びの例があることになる。"
  },
  {
    "start": 2908714,
    "end": 2914094,
    "text": "これらの1次元配列からフォワードに入力できるb×tテンソルをどのように作成できるか？"
  },
  {
    "start": 2914954,
    "end": 2917694,
    "text": "これが私のお気に入りの方法だ。"
  },
  {
    "start": 2918274,
    "end": 2924454,
    "text": "torchを使って、整数のリストと最初の24個のトークンだけからテンソル・オブジェクトを作る。"
  },
  {
    "start": 2924814,
    "end": 2933502,
    "text": "私の好きなやり方は、基本的に、例えば4×6のドット・ビューを行い、それを24倍にすることだ。"
  },
  {
    "start": 2933598,
    "end": 2936230,
    "text": "つまり、トークンを2次元的に並べ替えただけなのだ。"
  },
  {
    "start": 2936342,
    "end": 2951960,
    "text": "この1次元の列を4×6の2次元の列として見た場合、最初の6つのトークンは1列目になり、次の6つのトークンは2列目になる。"
  },
  {
    "start": 2952072,
    "end": 2960016,
    "text": "つまり、基本的には6つのトークンを独立した列として積み上げることになる。"
  },
  {
    "start": 2960120,
    "end": 2962912,
    "text": "この場合、トークンのバッチが作成される。"
  },
  {
    "start": 2963088,
    "end": 2975844,
    "text": "例えば、トークン25がトランスフォーマーにあるとすると、これを投入してこれがIDXになると、このトークンはこれら3つのトークンを見て、次に来る198番を予測しようとする。"
  },
  {
    "start": 2976724,
    "end": 2980636,
    "text": "こうして2次元のバッチを作ることができる。"
  },
  {
    "start": 2980700,
    "end": 2982196,
    "text": "なかなかいいね。"
  },
  {
    "start": 2982380,
    "end": 2988388,
    "text": "さて、損失関数を計算するためにターゲットに必要なラベルは、どうやって手に入れるのだろうか？"
  },
  {
    "start": 2988556,
    "end": 2996420,
    "text": "ラベルであるシーケンスの次のトークンがすぐ右側にあることを知っているからだ。"
  },
  {
    "start": 2996612,
    "end": 3004798,
    "text": "このトークン、一番最後のトークン13については、次の正しいトークンがないことにお気づきだろう。"
  },
  {
    "start": 3004966,
    "end": 3008474,
    "text": "実は、ここでは十分な情報が得られなかった。"
  },
  {
    "start": 3009174,
    "end": 3013102,
    "text": "基本的にこれらのバッチを手に入れる私のお気に入りの方法を紹介しよう。"
  },
  {
    "start": 3013238,
    "end": 3026714,
    "text": "個人的には、変換器への入力（xと呼びたい）だけでなく、ラベルテンソルも作りたい。"
  },
  {
    "start": 3027054,
    "end": 3028934,
    "text": "そこで、私が好きな方法を紹介しよう。"
  },
  {
    "start": 3029054,
    "end": 3036474,
    "text": "13の最後のトークンのグランドトゥルースが必要だからだ。"
  },
  {
    "start": 3037174,
    "end": 3044398,
    "text": "そして、入力を作成するときに、最後のトークンまですべてを取り込み、それを含めず、4×6として表示する。"
  },
  {
    "start": 3044526,
    "end": 3051222,
    "text": "ターゲットを作成するときは、バッファーを作成するが、インデックスゼロではなく、インデックス1から始める。"
  },
  {
    "start": 3051278,
    "end": 3054768,
    "text": "最初の要素をスキップし、まったく同じサイズで表示します。"
  },
  {
    "start": 3054926,
    "end": 3069596,
    "text": "このトークンの例として、25というトークンのターゲットは198だった。"
  },
  {
    "start": 3069740,
    "end": 3075420,
    "text": "また、この最後のトークンである13のラベルは198となった。"
  },
  {
    "start": 3075572,
    "end": 3078504,
    "text": "それはこのプラス1を積んだからだ。"
  },
  {
    "start": 3078844,
    "end": 3080844,
    "text": "基本的に、これが私の好きなやり方だ。"
  },
  {
    "start": 3080884,
    "end": 3087454,
    "text": "長いシークエンスを2次元的に捉え、時間のバッチを得る。"
  },
  {
    "start": 3087834,
    "end": 3090714,
    "text": "その場合、トークンを1つ追加する。"
  },
  {
    "start": 3090794,
    "end": 3095666,
    "text": "基本的には、b×t＋1のトークンのバッファをロードする。"
  },
  {
    "start": 3095810,
    "end": 3097954,
    "text": "そして、オフセットして見るんだ。"
  },
  {
    "start": 3098074,
    "end": 3099962,
    "text": "とすると、2つのテンソルがある。"
  },
  {
    "start": 3100058,
    "end": 3104134,
    "text": "そのうちの1つはトランスへの入力で、もう1つは正確にはラベルである。"
  },
  {
    "start": 3104434,
    "end": 3116886,
    "text": "そこで、このコードを再編成して、基本的にこれらのトークンをロードしてトランスフォーマーに送り、損失を計算しようとする非常にシンプルなデータローダーオブジェクトを作成しよう。"
  },
  {
    "start": 3117030,
    "end": 3117366,
    "text": "いいかい？"
  },
  {
    "start": 3117390,
    "end": 3120038,
    "text": "それに伴い、コードを入れ替えました。"
  },
  {
    "start": 3120206,
    "end": 3126470,
    "text": "ここにあるように、一時的にCPUをオーバーライドし、ティックトークンをインポートしている。"
  },
  {
    "start": 3126502,
    "end": 3127726,
    "text": "どれも見覚えがあるはずだ。"
  },
  {
    "start": 3127790,
    "end": 3129430,
    "text": "1000文字を読み込んでいます。"
  },
  {
    "start": 3129582,
    "end": 3136334,
    "text": "今はBTを4と32だけにしている。デバッグ中だから、非常に小さいバッチを1つだけ作りたいんだ。"
  },
  {
    "start": 3136634,
    "end": 3140254,
    "text": "右でやったことを踏襲している。"
  },
  {
    "start": 3140634,
    "end": 3145658,
    "text": "ここでモデルを作成し、ロジットを取得する。"
  },
  {
    "start": 3145826,
    "end": 3150618,
    "text": "というわけで、ここでは、ご覧のように、すでに数秒しか実行していない。"
  },
  {
    "start": 3150746,
    "end": 3159118,
    "text": "というのも、4×32のバッチがあるので、ロジットのサイズは4×32×50,257となる。"
  },
  {
    "start": 3159306,
    "end": 3162354,
    "text": "これらはすべてのポジションで次に来るもののためのロゴである。"
  },
  {
    "start": 3162774,
    "end": 3165886,
    "text": "これで、yに格納されたラベルができた。"
  },
  {
    "start": 3166030,
    "end": 3170662,
    "text": "今こそロスを計算し、バックワードパスを行い、最適化を行う時だ。"
  },
  {
    "start": 3170798,
    "end": 3172874,
    "text": "まずは損失を計算しよう。"
  },
  {
    "start": 3173654,
    "end": 3178838,
    "text": "では、損失を計算するために、モデル内のnnモジュールのフォワード関数を調整しよう。"
  },
  {
    "start": 3178966,
    "end": 3188764,
    "text": "特に、ロジットを返すだけでなく、損失も返すつもりだ。また、入力インデックスだけでなく、Yのターゲットも渡すつもりだ。"
  },
  {
    "start": 3189424,
    "end": 3193760,
    "text": "今はもう、ロードするのではなく、ただ形を作るだけだ。"
  },
  {
    "start": 3193792,
    "end": 3199604,
    "text": "サンプリングロジックの一部をスキップするために、損失関数を表示し、シストル出口をゼロにする。"
  },
  {
    "start": 3200024,
    "end": 3207644,
    "text": "さて、そこで呼び出されるフォワード・ファンクションにスイングアップしてみよう。"
  },
  {
    "start": 3209024,
    "end": 3212640,
    "text": "目標が決まれば、損失も計算できる。"
  },
  {
    "start": 3212752,
    "end": 3216114,
    "text": "基本的にはロジストのロスを返したいということを覚えておいてほしい。"
  },
  {
    "start": 3216194,
    "end": 3218054,
    "text": "デフォルトでは「なし」。"
  },
  {
    "start": 3218394,
    "end": 3223014,
    "text": "ここに書いておこう。"
  },
  {
    "start": 3223474,
    "end": 3229298,
    "text": "もしターゲットがゼロでなければ、損失を計算したい。"
  },
  {
    "start": 3229466,
    "end": 3234722,
    "text": "副操縦士はここですでに興奮し、正しい損失を計算しているようだ。"
  },
  {
    "start": 3234858,
    "end": 3239534,
    "text": "ここに記されているように、クロスエントロピー損失を使用している。"
  },
  {
    "start": 3240034,
    "end": 3243704,
    "text": "これはPytorchの関数である。"
  },
  {
    "start": 3244404,
    "end": 3247716,
    "text": "さて、ここで実際に何が起こっているかというと、ちょっと怖く見えるからだ。"
  },
  {
    "start": 3247900,
    "end": 3251940,
    "text": "基本的にfドットクロスエントロピーは多次元入力を好まない。"
  },
  {
    "start": 3251972,
    "end": 3254468,
    "text": "ボキャブラリーの大きさではb by tは取れない。"
  },
  {
    "start": 3254596,
    "end": 3260236,
    "text": "ここで起こっているのは、3次元のテンソルを2次元に平坦化することだ。"
  },
  {
    "start": 3260380,
    "end": 3264824,
    "text": "最初の次元は自動的に計算され、b×tとなる。"
  },
  {
    "start": 3265204,
    "end": 3267864,
    "text": "そして最後の次元がボキャブラリーの大きさである。"
  },
  {
    "start": 3268004,
    "end": 3281264,
    "text": "基本的にこれは、ロジットの3次元テンソルを平坦化し、2次元のb×t、個々の例とボキャブラリーのサイズを各行の長さで表したものである。"
  },
  {
    "start": 3281424,
    "end": 3285480,
    "text": "そうなると、ターゲットも平らになってしまう。"
  },
  {
    "start": 3285632,
    "end": 3289924,
    "text": "ただ平らにするのではなく、bにtを掛けたテンソルにするのだ。"
  },
  {
    "start": 3290264,
    "end": 3293964,
    "text": "これをクロスエントロピーに渡して損失を計算し、それを返す。"
  },
  {
    "start": 3294324,
    "end": 3298996,
    "text": "これは基本的に、この時点で実行されるはずだ。"
  },
  {
    "start": 3299100,
    "end": 3304664,
    "text": "損失分を印刷すべきかどうか確認してみよう。"
  },
  {
    "start": 3310324,
    "end": 3313812,
    "text": "ここでは、ざっと11枚印刷したことがわかる。"
  },
  {
    "start": 3313948,
    "end": 3321314,
    "text": "このテンソルは1つの要素のテンソルであり、それはこの数である11である。"
  },
  {
    "start": 3321484,
    "end": 3327462,
    "text": "さて、ランダムに分析されたネットワークの合理的な出発点を計算できるようにしたい。"
  },
  {
    "start": 3327598,
    "end": 3332566,
    "text": "以前のビデオでも取り上げたが、語彙数は50,257である。"
  },
  {
    "start": 3332750,
    "end": 3346038,
    "text": "ネットワークの初期化では、どのボキャブラリー・エレメントもほぼ均一の確率になることを望むだろうから、初期化時にどのトークンも過度に優遇されることはない。"
  },
  {
    "start": 3346086,
    "end": 3348482,
    "text": "初期化の時点で自信を持って間違ってはいない。"
  },
  {
    "start": 3348678,
    "end": 3354746,
    "text": "私たちが望んでいるのは、任意のトークンの確率が50,257分の1であることです。"
  },
  {
    "start": 3354930,
    "end": 3362234,
    "text": "クロスエントロピーの損失は基本的に負の対数尤度であることを思い出してほしい。"
  },
  {
    "start": 3362394,
    "end": 3372322,
    "text": "この確率を自然対数にしてマイナスにすると、初期化時に予想される損失となる。"
  },
  {
    "start": 3372458,
    "end": 3378678,
    "text": "以前のビデオでも取り上げたが、10点2点あたりを予想していた。"
  },
  {
    "start": 3378806,
    "end": 3379950,
    "text": "決して外れてはいない。"
  },
  {
    "start": 3380022,
    "end": 3382874,
    "text": "これは、私が初期化時に期待する確率とほぼ同じである。"
  },
  {
    "start": 3383174,
    "end": 3387574,
    "text": "ということは、初期化時の確率分布はほぼ拡散しているということだ。"
  },
  {
    "start": 3387654,
    "end": 3396074,
    "text": "これで、最適化を実行し、ネットワークにどのエレメントがどの順番で正しく従うべきかを指示することができる。"
  },
  {
    "start": 3396614,
    "end": 3401120,
    "text": "この時点で、損失逆算を行い、勾配を計算し、最適化を行うことができる。"
  },
  {
    "start": 3401262,
    "end": 3402540,
    "text": "さあ、始めよう。"
  },
  {
    "start": 3402692,
    "end": 3405024,
    "text": "よし、では最適化を行おう。"
  },
  {
    "start": 3405804,
    "end": 3411504,
    "text": "この敗戦は、こうして得たものなのだ。"
  },
  {
    "start": 3411964,
    "end": 3413956,
    "text": "さて、基本的にはここでちょっとしたforループを作りたい。"
  },
  {
    "start": 3413980,
    "end": 3417652,
    "text": "範囲内のIの場合は、50ステップとか、そんな感じにしよう。"
  },
  {
    "start": 3417828,
    "end": 3420824,
    "text": "Pytorchでオプティマイザー・オブジェクトを作ってみよう。"
  },
  {
    "start": 3422324,
    "end": 3431192,
    "text": "アトム・オプティマイザーは、これまで使っていた確率的勾配降下オプティマイザーSgDに代わるものである。"
  },
  {
    "start": 3431308,
    "end": 3432552,
    "text": "HTTはもっとシンプルだ。"
  },
  {
    "start": 3432608,
    "end": 3440364,
    "text": "アダムはもう少し複雑で、私は特にアトムWのバリエーションが好きなんだ。"
  },
  {
    "start": 3441144,
    "end": 3442424,
    "text": "原子wはバグである。"
  },
  {
    "start": 3442464,
    "end": 3444608,
    "text": "アトムのフィックスというのが私の言いたいことだ。"
  },
  {
    "start": 3444776,
    "end": 3446964,
    "text": "atomwのドキュメントを見てみよう。"
  },
  {
    "start": 3447984,
    "end": 3456524,
    "text": "なんと、ハイパーパラメーターがたくさん必要で、前に見たsgdよりも少し複雑なことがわかった。"
  },
  {
    "start": 3456924,
    "end": 3469140,
    "text": "というのも、基本的に学習率でスケーリングされた勾配でパラメータを更新することに加えて、これらのバッファを保持し続けるからだ。"
  },
  {
    "start": 3469292,
    "end": 3475580,
    "text": "モメンタムに少し似ているもの、RMのプロップに少し似ているもの、ご存知の方はご存知でしょうが、そうでなくてもかまいません。"
  },
  {
    "start": 3475612,
    "end": 3483124,
    "text": "これは、それぞれの勾配要素で個別に行われる正規化のようなもので、特に言語モデルの最適化をスピードアップする。"
  },
  {
    "start": 3483244,
    "end": 3485284,
    "text": "ここで詳細を説明するつもりはない。"
  },
  {
    "start": 3485404,
    "end": 3494796,
    "text": "私たちはこれをちょっとしたブラックボックスとして扱い、これまでの講義で見てきたように、SGDよりも速く目的を最適化するだけである。"
  },
  {
    "start": 3494980,
    "end": 3496308,
    "text": "ブラックボックスとして使おう。"
  },
  {
    "start": 3496356,
    "end": 3503864,
    "text": "この例では、オプティマイザー・オブジェクトを作成し、最適化を行う。"
  },
  {
    "start": 3508624,
    "end": 3513224,
    "text": "まず、副操縦士が勾配をゼロにするのを忘れていないか常に確認すること。"
  },
  {
    "start": 3513384,
    "end": 3517728,
    "text": "グラデーションがゼロの状態から始めなければならないことを常に覚えておいてください。"
  },
  {
    "start": 3517896,
    "end": 3528964,
    "text": "ドット・バックワードではグラデーションが加算されるため、グラデーションが加算されると、グラデーションが何であれ常にプラス・イコールになる。"
  },
  {
    "start": 3529264,
    "end": 3531712,
    "text": "この損失による勾配を累積する。"
  },
  {
    "start": 3531888,
    "end": 3540834,
    "text": "次に、オプティマイザーのステップ関数を呼び出してパラメーターを更新し、損失を減らす。"
  },
  {
    "start": 3541614,
    "end": 3543254,
    "text": "そして、そのステップを印刷する。"
  },
  {
    "start": 3543294,
    "end": 3545606,
    "text": "ここでは損失項目が使われる。"
  },
  {
    "start": 3545630,
    "end": 3547942,
    "text": "なぜなら、損失は1つの要素を持つテンソルだからだ。"
  },
  {
    "start": 3548118,
    "end": 3555926,
    "text": "アイテムは実際にそれを1つの浮動小数点数に変換し、この浮動小数点数はCPU上には存在しない。"
  },
  {
    "start": 3556110,
    "end": 3558182,
    "text": "これは、デバイスの内部的な部分に触れることになる。"
  },
  {
    "start": 3558238,
    "end": 3575124,
    "text": "ロスは1つの要素を持つテンソルであり、GPU上に存在する。GPUを使っているので、Pytorchを呼び出すと、裏側でその1次元テンソルを受け取り、CPUのメモリに戻し、floatに変換して出力する。"
  },
  {
    "start": 3576144,
    "end": 3580724,
    "text": "これが最適化であり、おそらくこれでうまくいくはずだ。"
  },
  {
    "start": 3582824,
    "end": 3584124,
    "text": "どうなるか見てみよう。"
  },
  {
    "start": 3585984,
    "end": 3590276,
    "text": "実は、CPUオーバーライドを使う代わりに、それを削除させてください。"
  },
  {
    "start": 3590300,
    "end": 3591700,
    "text": "この方が僕には少し速い。"
  },
  {
    "start": 3591732,
    "end": 3593024,
    "text": "CUDA上で動作する。"
  },
  {
    "start": 3599524,
    "end": 3604664,
    "text": "すべてのテンソルが同じデバイス上にあることを期待したが、CUDaゼロとCPUという少なくとも2つのデバイスが見つかった。"
  },
  {
    "start": 3605044,
    "end": 3609704,
    "text": "CUDAゼロは0番目のGPUで、実際にはこの箱には8つのGPUが搭載されているからだ。"
  },
  {
    "start": 3610004,
    "end": 3616456,
    "text": "私のボックスの0番目のGPUとCPUとモデルは、デバイスに移動しました。"
  },
  {
    "start": 3616600,
    "end": 3621776,
    "text": "このコードを書いているとき、実はバグを発見してしまった。"
  },
  {
    "start": 3621960,
    "end": 3626804,
    "text": "気をつけなければならないのは、バフ2つだけではダメだということだ。"
  },
  {
    "start": 3628544,
    "end": 3631248,
    "text": "ステートフルではないし、デバイスに変換するわけでもない。"
  },
  {
    "start": 3631296,
    "end": 3636592,
    "text": "代わりに、デバイス上にある新しいメモリへのポインタを返す。"
  },
  {
    "start": 3636728,
    "end": 3640668,
    "text": "テンソルには適用されない装置である。"
  },
  {
    "start": 3640776,
    "end": 3646900,
    "text": "バフ・イコール・バフをデバイスにしなければならない。"
  },
  {
    "start": 3647092,
    "end": 3648676,
    "text": "なら、これでうまくいくはずだ。"
  },
  {
    "start": 3648740,
    "end": 3651988,
    "text": "さて、では何を期待しているのか？"
  },
  {
    "start": 3652036,
    "end": 3656844,
    "text": "最初のうちはそれなりのロスが予想されるが、その後は単一バッチだけの最適化を続ける。"
  },
  {
    "start": 3656964,
    "end": 3665084,
    "text": "だから、この小さなバッチをつぶして、この小さなバッチだけで指数を完璧に予測できるかどうかを確認したい。"
  },
  {
    "start": 3665244,
    "end": 3667454,
    "text": "この中で、私たちが見ているのはだいたいこんな感じだ。"
  },
  {
    "start": 3667494,
    "end": 3673374,
    "text": "このケースでは、およそ10.8 211でスタートした。"
  },
  {
    "start": 3673494,
    "end": 3681670,
    "text": "そして、新しい例をロードすることなく、この1つのバッチで最適化を続けることで、1つのバッチをオーバーフィットさせることができ、非常に低損失になることを確認している。"
  },
  {
    "start": 3681742,
    "end": 3685674,
    "text": "トランスフォーマーは、この個々のバッチを記憶している。"
  },
  {
    "start": 3686094,
    "end": 3697112,
    "text": "もうひとつ触れておかなかったが、ここでの学習率は3eマイナス4であり、これは非常に早いデバッグ段階で実行したい最適化のほとんどにとって、かなり良いデフォルトである。"
  },
  {
    "start": 3697288,
    "end": 3702552,
    "text": "これは単純な内部ループで、1つのバッチをオーバーフィットしている。"
  },
  {
    "start": 3702648,
    "end": 3703712,
    "text": "これは良さそうだ。"
  },
  {
    "start": 3703848,
    "end": 3709104,
    "text": "次に来るのは、単に1つのバッチをオーバーフィットさせるだけでなく、実際に最適化を行うことだ。"
  },
  {
    "start": 3709264,
    "end": 3718814,
    "text": "このxyバッチを繰り返し実行し、常に新鮮なバッチを取得し、合理的な目的を最適化するための小さなデータ・ローダーを作成する必要がある。"
  },
  {
    "start": 3718904,
    "end": 3720114,
    "text": "次はそうしよう。"
  },
  {
    "start": 3720274,
    "end": 3723774,
    "text": "さて、そこで思いついたのが、ちょっとしたデータ・ローダー・ライトだ。"
  },
  {
    "start": 3725074,
    "end": 3741874,
    "text": "このデータ・ローダーが行っているのは、トークンをインポートし、テキスト・ファイル全体を1つの入力テキスト・ファイルから読み込んでトークン化し、このデータ・セットを反復処理する1つのエポックで、トークンの総数とバッチ数を表示することです。"
  },
  {
    "start": 3741994,
    "end": 3748684,
    "text": "ドキュメントの最初に戻って読み始めるまでに、何回ユニークなバッチを出力するのか？"
  },
  {
    "start": 3749184,
    "end": 3755160,
    "text": "ゼロの位置からスタートし、B×T回のバッチでドキュメントを歩くだけである。"
  },
  {
    "start": 3755272,
    "end": 3759644,
    "text": "B×Tのチャンクを取り、常にB×Tだけ前進する。"
  },
  {
    "start": 3759944,
    "end": 3765976,
    "text": "ここで重要なのは、我々は常にTのちょうどB倍だけポジションを前進させているということだ。"
  },
  {
    "start": 3766160,
    "end": 3771864,
    "text": "トークンをフェッチしているとき、実際には現在位置からB×T＋1までフェッチしている。"
  },
  {
    "start": 3772024,
    "end": 3779308,
    "text": "なぜなら、現在のバッチの最後のトークンのターゲット・トークンが必要だからである。"
  },
  {
    "start": 3779476,
    "end": 3784864,
    "text": "そうすれば、Xyを以前とまったく同じように行うことができる。"
  },
  {
    "start": 3785404,
    "end": 3790784,
    "text": "データを使い果たしたら、ゼロに戻るだけだ。"
  },
  {
    "start": 3791404,
    "end": 3801644,
    "text": "これは、非常にシンプルなデータ・ローダーを書く方法のひとつで、単にファイルをチャンク単位で見ていくだけである。"
  },
  {
    "start": 3801724,
    "end": 3803384,
    "text": "後で複雑にするつもりだ。"
  },
  {
    "start": 3803684,
    "end": 3808092,
    "text": "さて、この辺りに戻って、実際にデータ・ローダーを使ってみたい。"
  },
  {
    "start": 3808188,
    "end": 3812824,
    "text": "インポートのティックトークンが上がってしまったので、実はこのすべてが無駄になってしまったのだ。"
  },
  {
    "start": 3813164,
    "end": 3820380,
    "text": "その代わり、トレーニングデータ用のトレーナーローダーが欲しいだけで、4つには同じハイパーパラメーターを使いたい。"
  },
  {
    "start": 3820492,
    "end": 3823904,
    "text": "バッチサイズは4、時間は32だった。"
  },
  {
    "start": 3824284,
    "end": 3828402,
    "text": "ここで、現在のバッチのXYを取得する必要がある。"
  },
  {
    "start": 3828588,
    "end": 3831754,
    "text": "副操縦士が理解できるかどうか見てみよう。"
  },
  {
    "start": 3832174,
    "end": 3842034,
    "text": "次のバッチを呼び出し、テンソルをCPUからデバイスに移動しなければならないことを確認する。"
  },
  {
    "start": 3842534,
    "end": 3851274,
    "text": "ここでトークンを変換したとき、これらのトークンを実際にはGPUに移動させていないことに注目してほしい。"
  },
  {
    "start": 3852014,
    "end": 3855882,
    "text": "それは、GPUのメモリをあまり浪費しないようにしているからなんだ。"
  },
  {
    "start": 3855978,
    "end": 3863814,
    "text": "この場合、これは極小のデータセットであり、フィットするだろうが、今すぐの目的にはGPUに出荷するだけでいい。"
  },
  {
    "start": 3864154,
    "end": 3872654,
    "text": "次のバッチを手に入れたら、データ・ドアはシンプルなCPUクラスにしておき、ここで実際にGPUに転送してすべての計算を行う。"
  },
  {
    "start": 3872954,
    "end": 3875826,
    "text": "これが動くかどうか見てみよう。"
  },
  {
    "start": 3875970,
    "end": 3879706,
    "text": "というわけで、Python、traingput two Python。"
  },
  {
    "start": 3879720,
    "end": 3882182,
    "text": "実際にそうなる前に、私たちは何を期待しているのだろうか？"
  },
  {
    "start": 3882318,
    "end": 3885102,
    "text": "私たちが期待しているのは、実際に次のバッチを手に入れることだ。"
  },
  {
    "start": 3885198,
    "end": 3888326,
    "text": "私たちは、1つのバッチに過剰にフィットすることはないと予想しています。"
  },
  {
    "start": 3888470,
    "end": 3893438,
    "text": "だから私は、我々の損失は減るだろうが、それほど大きくはないだろうと予想している。"
  },
  {
    "start": 3893606,
    "end": 3901550,
    "text": "というのも、50,257のトークンの中には、私たちのデータセットに登場しないトークンも多いからだ。"
  },
  {
    "start": 3901662,
    "end": 3910850,
    "text": "例えば、発生しないすべてのロジットのバイアスを取り、それらを負の無限大に追いやる。"
  },
  {
    "start": 3911042,
    "end": 3918658,
    "text": "というのは、基本的に、おかしなユニコードやさまざまな言語が存在する場合、それらのトークンは決して出現しないので、その確率は非常に低くなるはずだ。"
  },
  {
    "start": 3918786,
    "end": 3925426,
    "text": "だから、私たちが見るべき利益は、基本的に発生しないトークンの使用を削除するという線に沿ったものだ。"
  },
  {
    "start": 3925530,
    "end": 3929414,
    "text": "おそらく、今この規模で見られる損失の大半はこれだろう。"
  },
  {
    "start": 3929874,
    "end": 3936352,
    "text": "50回しか反復していないので、ゼロにはならないはずだ。"
  },
  {
    "start": 3936528,
    "end": 3938976,
    "text": "今、エポックをやるには十分ではないと思う。"
  },
  {
    "start": 3939120,
    "end": 3940364,
    "text": "何が出てくるか見てみよう。"
  },
  {
    "start": 3941424,
    "end": 3951256,
    "text": "トークンは338,000個あり、3対1の圧縮率で100万文字を圧縮しています。"
  },
  {
    "start": 3951440,
    "end": 3961024,
    "text": "現在のBとtの設定では、1エポックに2600バッチかかる。"
  },
  {
    "start": 3961884,
    "end": 3968196,
    "text": "予想通り、見慣れた領域から始まり、6.6まで下がるようだ。"
  },
  {
    "start": 3968340,
    "end": 3973324,
    "text": "基本的には、私たちの期待通り、今はうまくいっているようだ。"
  },
  {
    "start": 3973484,
    "end": 3974340,
    "text": "それはいいことだ。"
  },
  {
    "start": 3974452,
    "end": 3978264,
    "text": "さて、次は実際にコードにあるバグを修正したい。"
  },
  {
    "start": 3978724,
    "end": 3983814,
    "text": "大きなバグではないが、GPT-2のトレーニングがどのように行われるべきかという点ではバグだ。"
  },
  {
    "start": 3986194,
    "end": 3987694,
    "text": "バグは次のようなものだ。"
  },
  {
    "start": 3988434,
    "end": 3992770,
    "text": "ハグ顔からウエイトを載せるとき、私たちは十分な注意を払っていなかった。"
  },
  {
    "start": 3992962,
    "end": 4000394,
    "text": "ここで、この2つのテンソルの形が同じであることに注目してほしい。"
  },
  {
    "start": 4000514,
    "end": 4006234,
    "text": "これは、トランスフォーマーの底に埋め込まれているトークンですね？"
  },
  {
    "start": 4006274,
    "end": 4011614,
    "text": "そしてこれが、トランスフォーマーの一番上にある言語モデリング・ヘッドだ。"
  },
  {
    "start": 4012084,
    "end": 4017064,
    "text": "どちらも基本的には2次元のテンソルであり、その形状は同じである。"
  },
  {
    "start": 4017444,
    "end": 4025828,
    "text": "1つ目は出力埋め込み、トークン埋め込みで、2つ目は一番上の線形層、分類器層だ。"
  },
  {
    "start": 4025996,
    "end": 4030504,
    "text": "どちらも形は5万対57×768。"
  },
  {
    "start": 4031844,
    "end": 4045376,
    "text": "これはトークンのエンベッディングで、これはトランスフォーマーの768チャンネルを50,000から57にアップスケールして、次のトークンのロジを得ようとしている。"
  },
  {
    "start": 4045560,
    "end": 4047648,
    "text": "どちらも同じ形だ。"
  },
  {
    "start": 4047816,
    "end": 4056000,
    "text": "それ以上に、実は、Pytorchで両者のエレメントを比較すると、これはエレメント単位での等式なのだ。"
  },
  {
    "start": 4056072,
    "end": 4059992,
    "text": "次にdot allを使うと、すべての要素が同一であることがわかる。"
  },
  {
    "start": 4060168,
    "end": 4072654,
    "text": "それ以上に、データ・ポインターを実際に見てみると、これはPytorchでデータとストレージへの実際のポインターを取得する方法なのだが、実はポインターは同一であることがわかる。"
  },
  {
    "start": 4072814,
    "end": 4079874,
    "text": "これらの2つのテンソルは、たまたま同じ形と要素を持つ別々のテンソルであるだけでなく、実際には同一のテンソルを指している。"
  },
  {
    "start": 4080174,
    "end": 4087394,
    "text": "ここで起こっているのは、これはよくある待ち時間のスキームで、実はオリジナルから来ているということだ。"
  },
  {
    "start": 4089254,
    "end": 4093474,
    "text": "元の注意から、必要なすべての紙であり、実際にはそれ以前の参照であっても。"
  },
  {
    "start": 4093814,
    "end": 4103614,
    "text": "ここに来れば、エンベッディングとソフトマックスに注目するだけで、ペーパーは必要ない。"
  },
  {
    "start": 4104234,
    "end": 4112694,
    "text": "彼らは、我々のモデルでは、2つの埋め込み層の間で同じ重み行列を共有し、30と同様のプリソフトマックス線形変換を行ったと述べた。"
  },
  {
    "start": 4113914,
    "end": 4122414,
    "text": "この2つは共有され、結びついており、同じマトリックスである。"
  },
  {
    "start": 4123804,
    "end": 4130836,
    "text": "これは2017年に発表されたもので、論文の全文を読むことができるが、基本的にはこのウェイトタイの方式を主張している。"
  },
  {
    "start": 4130980,
    "end": 4137508,
    "text": "直感的に、なぜこのようなことをしたいのか、その理由はこの段落にあると思う。"
  },
  {
    "start": 4137676,
    "end": 4148184,
    "text": "基本的に、これらの2つの行列は、次のような意味で似たような振る舞いをすることを望んでいることがわかる。"
  },
  {
    "start": 4148843,
    "end": 4158251,
    "text": "2つのトークンが意味的に非常に似ている場合、たとえば一方はすべて小文字で、もう一方はすべて大文字であるとか、同じトークンを別の言語で表記しているとか、そのような場合です。"
  },
  {
    "start": 4158347,
    "end": 4176344,
    "text": "2つのトークンの間に類似性がある場合、おそらくトークン埋め込み空間において近接していると予想されますが、それとまったく同じように、意味的に類似している2つのトークンがある場合、意味的に類似しているため、変換器の出力において同じ確率になると予想されます。"
  },
  {
    "start": 4177004,
    "end": 4188464,
    "text": "したがって、一番下の変換器と一番上の変換器の両方の位置は、似たようなトークンは似たような埋め込み、または似たような重みを持つという性質を持っている。"
  },
  {
    "start": 4188764,
    "end": 4191516,
    "text": "それが、ここでの探求の動機なのだ。"
  },
  {
    "start": 4191540,
    "end": 4198684,
    "text": "論文全体は読みたくないし、あなたが読めばいいんだけど、彼らが観察しているのはこういうことなんだ。"
  },
  {
    "start": 4198844,
    "end": 4208334,
    "text": "また、出力の埋め込みを見ると、その重みを単語の埋め込みとして使おうとすると、単語の埋め込みと同じような振る舞いをすることも観察されている。"
  },
  {
    "start": 4209154,
    "end": 4216014,
    "text": "彼らはこの類似性を観察し、それを結びつけようとし、その方がはるかに良いパフォーマンスが得られることを観察する。"
  },
  {
    "start": 4216314,
    "end": 4219746,
    "text": "ということで、これが採用された。"
  },
  {
    "start": 4219850,
    "end": 4223402,
    "text": "その後、GPT-2でも再び使用された。"
  },
  {
    "start": 4223538,
    "end": 4227386,
    "text": "トランスフォーマーの実装では見つけられなかった。"
  },
  {
    "start": 4227450,
    "end": 4235578,
    "text": "OpenAIが導入したオリジナルのGPT-2コードにはない。"
  },
  {
    "start": 4235746,
    "end": 4242770,
    "text": "これはOpenAIのGPT-2ソースモデルで、ここでこのモデルを転送している。"
  },
  {
    "start": 4242802,
    "end": 4252054,
    "text": "トークンの埋め込みと位置のエンコーダーを示します。"
  },
  {
    "start": 4253114,
    "end": 4257792,
    "text": "そして、この一番下で再びWteを使ってロジットを行う。"
  },
  {
    "start": 4257928,
    "end": 4265324,
    "text": "ロディッツに到達すると、このトランスフォーマーからの出力をマトミュールし、w teテンソルを再利用する。"
  },
  {
    "start": 4266824,
    "end": 4272784,
    "text": "そのため、w teテンソルは基本的にトランスの底と頂で2回使用される。"
  },
  {
    "start": 4272944,
    "end": 4278304,
    "text": "バックワードパスでは、両方のブランチからの寄与であるグラデーションが得られる。"
  },
  {
    "start": 4278344,
    "end": 4282524,
    "text": "これらの勾配はWTEテンソル上で加算される。"
  },
  {
    "start": 4283314,
    "end": 4294242,
    "text": "分類器レイヤーからの寄与が得られ、変換器の一番最後に、w teテンソルに再びフローティングされた寄与が得られる。"
  },
  {
    "start": 4294418,
    "end": 4300454,
    "text": "今のところWTEをコードで共有していないが、そうしたい。"
  },
  {
    "start": 4302114,
    "end": 4306614,
    "text": "我々はスキームを共有している。"
  },
  {
    "start": 4307354,
    "end": 4310414,
    "text": "ゲパルがそれを手にするかどうか見てみよう。"
  },
  {
    "start": 4310594,
    "end": 4311390,
    "text": "ああ、そうだね。"
  },
  {
    "start": 4311462,
    "end": 4318894,
    "text": "オーケー、これはひとつの方法で、基本的には比較的簡単だ。"
  },
  {
    "start": 4318934,
    "end": 4327674,
    "text": "ここでやっているのは、w teのドットウエイトを、単にlmのヘッドを指すようにリダイレクトしているのだ。"
  },
  {
    "start": 4328294,
    "end": 4332550,
    "text": "これは基本的に、データ・ポインタをコピーするんだよね？"
  },
  {
    "start": 4332582,
    "end": 4339266,
    "text": "リファレンスがコピーされ、WTEウエイトは孤児となり、古い値となる。"
  },
  {
    "start": 4339410,
    "end": 4343042,
    "text": "Pytorchはそれをクリーンアップし、Pythonはそれをクリーンアップする。"
  },
  {
    "start": 4343178,
    "end": 4349850,
    "text": "そのため、テンソルは1つだけとなり、フォワードパスで2回使われることになる。"
  },
  {
    "start": 4350042,
    "end": 4353730,
    "text": "私の知る限り、これが必要なことのすべてだ。"
  },
  {
    "start": 4353802,
    "end": 4356154,
    "text": "これは使えるはずだ。"
  },
  {
    "start": 4356234,
    "end": 4357934,
    "text": "これはおそらく訓練する必要がある。"
  },
  {
    "start": 4358674,
    "end": 4362078,
    "text": "私たちは基本的に、このまったく同じセンサーを2回使うことになる。"
  },
  {
    "start": 4362266,
    "end": 4367462,
    "text": "我々は可能性を追跡することに注意を払っていなかった。"
  },
  {
    "start": 4367558,
    "end": 4372718,
    "text": "論文によれば、そしてその結果によれば、こうすることで少し良い結果が期待できる。"
  },
  {
    "start": 4372886,
    "end": 4378750,
    "text": "それに加えて、これが僕らにとって非常にありがたいもうひとつの理由は、これがトンデモないパラメーターだということだ。"
  },
  {
    "start": 4378822,
    "end": 4379462,
    "text": "そうだろう？"
  },
  {
    "start": 4379638,
    "end": 4380758,
    "text": "ここのサイズは？"
  },
  {
    "start": 4380806,
    "end": 4384774,
    "text": "768×5257だ。"
  },
  {
    "start": 4384894,
    "end": 4390270,
    "text": "これは4,000万パラメータで、これは1億2,400万パラメータのモデルである。"
  },
  {
    "start": 4390382,
    "end": 4392166,
    "text": "40、ディバイド124"
  },
  {
    "start": 4392230,
    "end": 4397750,
    "text": "ということは、この待ち時間スキームを使ってパラメーターの30％が保存されているようなものだ。"
  },
  {
    "start": 4397942,
    "end": 4401430,
    "text": "だから、これが少しうまくいっている理由のひとつかもしれない。"
  },
  {
    "start": 4401542,
    "end": 4413030,
    "text": "ウェイトの縛りによってモデルのトレーニング時間が短くなっていれば、多くのパラメーターをトレーニングする必要がなくなるので、トレーニングの効率が良くなる。"
  },
  {
    "start": 4413062,
    "end": 4419424,
    "text": "この2つのエンベッディングはトークン間で共通点があるはずだという帰納的バイアスを入れている。"
  },
  {
    "start": 4419924,
    "end": 4426972,
    "text": "これは待機時間スキームであり、パラメーターを大量に節約した。"
  },
  {
    "start": 4427148,
    "end": 4433420,
    "text": "では次に、GPT-2のモデルの初期化方法を参考にしながら、もう少し慎重に初期化を行いたい。"
  },
  {
    "start": 4433612,
    "end": 4439324,
    "text": "さて、残念ながら、GPT-2の論文とGPT-3の論文では、初期化についてあまり明示されていない。"
  },
  {
    "start": 4439404,
    "end": 4441464,
    "text": "私たちは行間を読まなければならない。"
  },
  {
    "start": 4442104,
    "end": 4448664,
    "text": "曖昧な論文の代わりに、OpenAIが公開したコードに少し情報がある。"
  },
  {
    "start": 4448704,
    "end": 4458044,
    "text": "モデル・パイを見ると、重みを初期化する際に標準偏差0.02を使用していることがわかる。"
  },
  {
    "start": 4458704,
    "end": 4465032,
    "text": "これはウェイトの正規分布であり、バイアスの標準偏差は0.02である。"
  },
  {
    "start": 4465088,
    "end": 4466884,
    "text": "それをゼロで初期化するのだ。"
  },
  {
    "start": 4467764,
    "end": 4472824,
    "text": "では、下にスクロールすると、なぜスクロールしないのか？"
  },
  {
    "start": 4474764,
    "end": 4481104,
    "text": "トークン埋め込みは0.02、位置埋め込みはなぜか0.01で初期化される。"
  },
  {
    "start": 4481524,
    "end": 4487476,
    "text": "これらは初期化で、GPT-2のモジュールに反映させたい。"
  },
  {
    "start": 4487580,
    "end": 4491984,
    "text": "これは、私がすぐに思いついたコードの断片である。"
  },
  {
    "start": 4493524,
    "end": 4508224,
    "text": "ここで起こっているのは、GPTモジュールのイニシャライザーの最後で、nnモジュールのapply関数を呼び出して、このモジュールのすべてのサブモジュールを繰り返し、それらにinit weights関数を適用している。"
  },
  {
    "start": 4508604,
    "end": 4513612,
    "text": "つまり、ここで起こっているのは、すべてのモジュールを反復している、ということだ。"
  },
  {
    "start": 4513748,
    "end": 4521924,
    "text": "もしnn個のリニアモジュールであれば、標準偏差0.02の正規値を使ってウェイトを初期化する。"
  },
  {
    "start": 4522344,
    "end": 4526080,
    "text": "このレイヤーにバイアスがある場合は、それをゼロに初期化する。"
  },
  {
    "start": 4526272,
    "end": 4530604,
    "text": "バイアスのゼロ初期化は、実際にはPytorchのデフォルトではないことに注意してください。"
  },
  {
    "start": 4531904,
    "end": 4535016,
    "text": "デフォルトでは、ここのバイアスは一様で初期化される。"
  },
  {
    "start": 4535160,
    "end": 4536864,
    "text": "それは興味深い。"
  },
  {
    "start": 4537024,
    "end": 4538640,
    "text": "必ずゼロを使う。"
  },
  {
    "start": 4538832,
    "end": 4543244,
    "text": "エンベッディングには0.02を使い、同じにする。"
  },
  {
    "start": 4543784,
    "end": 4547754,
    "text": "ポジションを0.01に変更するつもりはない。"
  },
  {
    "start": 4548254,
    "end": 4554534,
    "text": "このモデルを見ると、初期化が必要でパラメータを持つレイヤーはノルムだけだ。"
  },
  {
    "start": 4554614,
    "end": 4561150,
    "text": "pytorch のデフォルトの初期化では、レイヤーノルムのスケールは1、レイヤーノルムのオフセットは0に設定される。"
  },
  {
    "start": 4561262,
    "end": 4562598,
    "text": "それこそが私たちが望んでいることだ。"
  },
  {
    "start": 4562646,
    "end": 4565114,
    "text": "だから、このままでいい。"
  },
  {
    "start": 4565534,
    "end": 4576746,
    "text": "ということは、GPT-2のソースコードに従えば、これがデフォルトの初期化ということになる。"
  },
  {
    "start": 4576930,
    "end": 4589474,
    "text": "ところで、ハビエルの初期化に従えば、この初期化における標準偏差は、このレイヤーに入力されるフィーチャーの数の平方根を1倍したものになる。"
  },
  {
    "start": 4589634,
    "end": 4600288,
    "text": "というのも、GPT-2のトランス内部のdモデルのサイズはおよそ768 1600などだからだ。"
  },
  {
    "start": 4600466,
    "end": 4605064,
    "text": "の平方根に1を乗じると、例えば768は0.03となる。"
  },
  {
    "start": 4605404,
    "end": 4614172,
    "text": "601,600を差し込めば0.02となり、その3倍を差し込めば0.014となる。"
  },
  {
    "start": 4614268,
    "end": 4623188,
    "text": "基本的に0.02は、いずれにせよ、これらの初期化のための妥当な値のほぼ近辺である。"
  },
  {
    "start": 4623356,
    "end": 4633518,
    "text": "ここで0.02をハードコーディングするのは完全にクレイジーというわけではないが、その代わりにモデルサイズに合わせて大きくなるようなものが一般的だろう。"
  },
  {
    "start": 4633646,
    "end": 4637822,
    "text": "これはGPT-2のソースコードによる初期化なので、このままにしておきます。"
  },
  {
    "start": 4637998,
    "end": 4641750,
    "text": "というのも、もうひとつ注意点があるからだ。"
  },
  {
    "start": 4641902,
    "end": 4649734,
    "text": "ここでは、モデルの深さによる残差パス上の蓄積を考慮した修正初期化が使用されている。"
  },
  {
    "start": 4649894,
    "end": 4655894,
    "text": "残差層の初期化の重みを、nの平方根を1倍した値でスケーリングする。"
  },
  {
    "start": 4656234,
    "end": 4658386,
    "text": "これがGPT-2の論文だ。"
  },
  {
    "start": 4658570,
    "end": 4662154,
    "text": "我々はまだそれを実施していない。"
  },
  {
    "start": 4662314,
    "end": 4666734,
    "text": "さて、ここで彼らが何を意味しているのか、実際に少し動機づけをしてみたいと思う。"
  },
  {
    "start": 4667194,
    "end": 4668974,
    "text": "大まかな意味はこうだ。"
  },
  {
    "start": 4670234,
    "end": 4673570,
    "text": "もし、あなたの残留ストリームがゼロからスタートするのであれば。"
  },
  {
    "start": 4673762,
    "end": 4679330,
    "text": "各残留ストリームは、私たちがそれに追加し続けるこの形式であることを覚えておいてほしい。"
  },
  {
    "start": 4679402,
    "end": 4682722,
    "text": "XはXに何かを加えたものであり、何らかの貢献である。"
  },
  {
    "start": 4682858,
    "end": 4690174,
    "text": "残留ネットワークのすべてのブロックがいくらか貢献し、それが加算される。"
  },
  {
    "start": 4690474,
    "end": 4697534,
    "text": "その結果、残差ストリームのアクティベーションの分散が大きくなる。"
  },
  {
    "start": 4697874,
    "end": 4699130,
    "text": "ここに小さな例がある。"
  },
  {
    "start": 4699162,
    "end": 4707094,
    "text": "ゼロから始めて、100回、768個のゼロのストリームが残るとする。"
  },
  {
    "start": 4707454,
    "end": 4715830,
    "text": "これは正規分布のゼロ平均1標準偏差である。"
  },
  {
    "start": 4716022,
    "end": 4720474,
    "text": "さらにそれを加えると、最終的には標準偏差が10になる。"
  },
  {
    "start": 4720774,
    "end": 4727286,
    "text": "それはいつも数字を足しているからだ。"
  },
  {
    "start": 4727430,
    "end": 4732914,
    "text": "このスケーリングファクターは、その成長を正確に補正するものだ。"
  },
  {
    "start": 4733464,
    "end": 4742432,
    "text": "をnとすると、基本的には、これらの残留ストリームへの寄与をnの平方根の分だけ1ずつ減らしていくことになる。"
  },
  {
    "start": 4742608,
    "end": 4753984,
    "text": "nの平方根を1つ越えると、nのマイナス0.5になりますね。nの0.5が平方根なので、平方根を1つ越えると、nのマイナス0.5になります。"
  },
  {
    "start": 4754104,
    "end": 4759564,
    "text": "このようにスケーリングすると、実際には、うーん、1つ。"
  },
  {
    "start": 4760744,
    "end": 4768280,
    "text": "これは、フォワードパスで残留ストリーム内のアクティベーションの増加を抑制する方法である。"
  },
  {
    "start": 4768472,
    "end": 4774408,
    "text": "だから、各ブロックの最後にあるウェイトも同じように初期化したい。"
  },
  {
    "start": 4774496,
    "end": 4783764,
    "text": "GPTの論文では、このc projレイヤーの重みを、残差レイヤーの数の平方根で1ずつ減らすことを提案している。"
  },
  {
    "start": 4784444,
    "end": 4787304,
    "text": "これを実現する粗雑な方法として、次のようなものがある。"
  },
  {
    "start": 4787604,
    "end": 4793824,
    "text": "これがPytorchの認可を受けているかどうかはわからないが、僕の場合はうまくいった。"
  },
  {
    "start": 4794484,
    "end": 4803544,
    "text": "あのセルフドットの特別なナノグリップスケールのイニシエーションがそうだ。"
  },
  {
    "start": 4803964,
    "end": 4807460,
    "text": "このモジュールのフラグのようなものを設定している。"
  },
  {
    "start": 4807652,
    "end": 4809260,
    "text": "パイトーチよりいい方法があるはずだろう？"
  },
  {
    "start": 4809292,
    "end": 4811444,
    "text": "分からないよ。"
  },
  {
    "start": 4812264,
    "end": 4818404,
    "text": "オーケー、では基本的にこのフラグを付けて、以前の何かと衝突しないようにするんだ。"
  },
  {
    "start": 4818824,
    "end": 4823884,
    "text": "そうすると、このSTdはデフォルトで0.02になるはずだ。"
  },
  {
    "start": 4824864,
    "end": 4839146,
    "text": "ということは、ハザターがこのモジュールを使っているのであれば、コバルトのSTD倍率は正しく推測されていないことになる。"
  },
  {
    "start": 4839290,
    "end": 4842186,
    "text": "レイヤー数の平方根以上のものが欲しい。"
  },
  {
    "start": 4842330,
    "end": 4854890,
    "text": "ここでの残留レイヤーの数は、コンフィグを解くレイヤーの2倍で、さらにこの2倍がマイナスになる。"
  },
  {
    "start": 4854922,
    "end": 4862576,
    "text": ".5なので、標準偏差を縮小し、これが正しいはずなので、それを実行する。"
  },
  {
    "start": 4862770,
    "end": 4871740,
    "text": "ちなみに、レイヤーの数が2倍というのは、トランスフォーマーのすべてのレイヤーに視覚的な経路を追加するブロックが2つあることに由来する。"
  },
  {
    "start": 4871772,
    "end": 4872084,
    "text": "そうだね。"
  },
  {
    "start": 4872164,
    "end": 4874116,
    "text": "我々は注目し、そしてMLPを手に入れた。"
  },
  {
    "start": 4874220,
    "end": 4876184,
    "text": "そこが2度の出番の由来だ。"
  },
  {
    "start": 4876724,
    "end": 4893000,
    "text": "もうひとつ言っておくと、少し厄介なことだが修正するつもりはない。というのも、この反復ではすべてのサブモジュールに対してw teとLM headを重み共有しているため、実際にはこのテンソルに2回戻ってくることになるからだ。"
  },
  {
    "start": 4893152,
    "end": 4902288,
    "text": "まず、0.02でエンベッディングとして初期化し、その後、リニアに再び戻って0.02で初期化する。"
  },
  {
    "start": 4902416,
    "end": 4906040,
    "text": "ミョウバンヘッドはもちろんスケーリングされていないので、0.02になる。"
  },
  {
    "start": 4906072,
    "end": 4907968,
    "text": "ここには来ない。"
  },
  {
    "start": 4908136,
    "end": 4912544,
    "text": "ただ、基本的には同じ初期化を使って2回初期化することになる。"
  },
  {
    "start": 4912704,
    "end": 4913804,
    "text": "それでいいんだ。"
  },
  {
    "start": 4914184,
    "end": 4923408,
    "text": "それから、ここにコードを追加して、種を設定するための再現性を持たせた。"
  },
  {
    "start": 4923456,
    "end": 4928608,
    "text": "これでGPT-2 pyをpythonで訓練し、実行させることができるはずだ。"
  },
  {
    "start": 4928696,
    "end": 4934096,
    "text": "私の知る限り、これがGPT-2の初期化であり、私たちが今実装している方法だ。"
  },
  {
    "start": 4934280,
    "end": 4938524,
    "text": "私にはこれが合理的に見える。"
  },
  {
    "start": 4939084,
    "end": 4941524,
    "text": "さて、この時点でGPT-2モデルを手に入れた。"
  },
  {
    "start": 4941604,
    "end": 4943732,
    "text": "私たちは、それが正しく実施されていると確信している。"
  },
  {
    "start": 4943828,
    "end": 4949028,
    "text": "適切に初期化され、データ・ローダーがデータ・バッチを繰り返し、トレーニングができるようになった。"
  },
  {
    "start": 4949196,
    "end": 4950548,
    "text": "ここからが楽しいところだ。"
  },
  {
    "start": 4950636,
    "end": 4953028,
    "text": "トレーニングをもっとスピードアップしてほしい。"
  },
  {
    "start": 4953116,
    "end": 4960104,
    "text": "私たちは、ここで使っているハードウェアに見合うだけの価値を得ているし、トレーニングをかなりスピードアップできる。"
  },
  {
    "start": 4961324,
    "end": 4966928,
    "text": "今、どのようなハードウェアがあり、それが何を提供し、それを十分に活用しているかということから始めたい。"
  },
  {
    "start": 4967076,
    "end": 4981056,
    "text": "私の場合、Nvidia SMIにアクセスすると、8つのGPUがあり、それぞれのGPUがA 100 SXM 80GBであることがわかります。"
  },
  {
    "start": 4981240,
    "end": 4984936,
    "text": "これが、この箱の中に入っているGPUだ。"
  },
  {
    "start": 4985120,
    "end": 4993244,
    "text": "さて、私がこの種のボックスをスピンアップするとき、ちなみに私のお気に入りの場所はラムダ・ラボだ。"
  },
  {
    "start": 4993804,
    "end": 4997344,
    "text": "彼らは私の開発やプロジェクトのスポンサーでもある。"
  },
  {
    "start": 4997684,
    "end": 4999708,
    "text": "ここは私のお気に入りの場所だ。"
  },
  {
    "start": 4999836,
    "end": 5003084,
    "text": "ここでは、これらのマシンのいずれかを回転させることができ、1時間ごとに支払う。"
  },
  {
    "start": 5003164,
    "end": 5004504,
    "text": "とてもシンプルなことだ。"
  },
  {
    "start": 5004884,
    "end": 5007212,
    "text": "私はそれらをスピンアップして、それに対コードを接続するのが好きなんだ。"
  },
  {
    "start": 5007228,
    "end": 5008664,
    "text": "そうやって僕は成長していくんだ。"
  },
  {
    "start": 5009124,
    "end": 5018584,
    "text": "さて、ここで入手可能なa 100を見てみると、180ギガバイトのSXMが私の持っているGPUだ。"
  },
  {
    "start": 5018724,
    "end": 5024280,
    "text": "このGPUでどれだけの計算ができるか、ここにたくさんの数字があります。"
  },
  {
    "start": 5024472,
    "end": 5029912,
    "text": "こっちに来て、この後すぐに侵入するんだ。"
  },
  {
    "start": 5029968,
    "end": 5031724,
    "text": "パイソンはGPTを訓練する。"
  },
  {
    "start": 5032304,
    "end": 5035204,
    "text": "ロジットとロスを計算した後、すぐに慣らし運転をしているんだ。"
  },
  {
    "start": 5036504,
    "end": 5044994,
    "text": "興味深いのは、dtypeをlogitsすると、トーチフロート32と表示されることだ。"
  },
  {
    "start": 5045144,
    "end": 5054914,
    "text": "Pytorchでは、デフォルトでは、テンソルを作成するとき、そしてこれはすべての活性化、ネットワークのパラメータなどの場合であるが、デフォルトでは、すべてがfloat 32である。"
  },
  {
    "start": 5055214,
    "end": 5064034,
    "text": "つまり、アクティベーションやウェイトなどのすべての数値は、32ビットのfloat表現を使っているということだ。"
  },
  {
    "start": 5064534,
    "end": 5066478,
    "text": "実際、かなりのメモリ量だ。"
  },
  {
    "start": 5066526,
    "end": 5071020,
    "text": "経験的に、ディープラーニングの計算負荷としては、これは多すぎることがわかった。"
  },
  {
    "start": 5071142,
    "end": 5076404,
    "text": "ディープラーニングとこれらのネットワークのトレーニングは、かなり低い精度を許容することができる。"
  },
  {
    "start": 5077144,
    "end": 5080360,
    "text": "すべての計算ワークフローが小さな精度を許容できるわけではない。"
  },
  {
    "start": 5080472,
    "end": 5089040,
    "text": "例えば、データシートに戻ると、これらのGPUはFP 64までサポートしていることがわかります。"
  },
  {
    "start": 5089192,
    "end": 5095568,
    "text": "これは、多くの科学計算アプリケーションにとって非常に便利で、本当に必要なものだと私は理解している。"
  },
  {
    "start": 5095736,
    "end": 5098502,
    "text": "ディープラーニングのトレーニングにそこまでの精度は必要ない。"
  },
  {
    "start": 5098638,
    "end": 5109954,
    "text": "現在のところFP32であり、このコードをそのまま使えば、最大でも19.5テラフロップスのパフォーマンスが期待できる。"
  },
  {
    "start": 5110254,
    "end": 5114654,
    "text": "つまり、19兆5000億回の浮動小数点演算を行っていることになる。"
  },
  {
    "start": 5114774,
    "end": 5120234,
    "text": "これは浮動小数点の乗算加算であろう。"
  },
  {
    "start": 5121214,
    "end": 5123994,
    "text": "これが浮動小数点演算である。"
  },
  {
    "start": 5125434,
    "end": 5135914,
    "text": "TF32はより精度の低いフォーマットなので、精度を下げても構わない。"
  },
  {
    "start": 5136034,
    "end": 5145054,
    "text": "float16かb float16まで下げようと思えば、実際に16倍のパフォーマンスを312テラフロップスまで得ることができる。"
  },
  {
    "start": 5145514,
    "end": 5149106,
    "text": "Nvidiaはここにアスタリスクのついた数字を引用するのが好きなことがわかるだろう。"
  },
  {
    "start": 5149210,
    "end": 5158306,
    "text": "このアスタリスクはスパース性を持つという意味ですが、私たちのコードではスパース性を使うつもりはありません。"
  },
  {
    "start": 5158410,
    "end": 5165098,
    "text": "ほとんどの人は、この数字をスパースなしで見ている。"
  },
  {
    "start": 5165226,
    "end": 5193326,
    "text": "というのも、Int 8は基本的に一様な間隔を持つからである。ニューラルネットワークの学習時に発生する正規分布（活性度と重みの両方が正規分布として分布する）にうまく一致させるために、実際にはfloatが必要なのだ。"
  },
  {
    "start": 5193510,
    "end": 5198714,
    "text": "だから、浮動小数点はその表現にマッチさせるために本当に重要なのだ。"
  },
  {
    "start": 5199294,
    "end": 5204024,
    "text": "通常、int 8はトレーニングには使わないが、推論には使う。"
  },
  {
    "start": 5204214,
    "end": 5210780,
    "text": "精度を下げれば、GPUのテンソルコアからもっと多くのテラフロップスを引き出すことができる。"
  },
  {
    "start": 5210852,
    "end": 5212324,
    "text": "それについては後で話そう。"
  },
  {
    "start": 5212484,
    "end": 5220052,
    "text": "それに加えて、もしこれらの数字がすべて少ないビット数で表現されるなら、それらを移動させるのはずっと簡単になるだろう。"
  },
  {
    "start": 5220228,
    "end": 5224364,
    "text": "そこで、メモリ帯域幅とモデルのメモリに関わることになる。"
  },
  {
    "start": 5224524,
    "end": 5236848,
    "text": "GPUが記憶できるビット数には限りがあるだけでなく、メモリにアクセスできる速度やメモリ帯域幅にも限りがある。"
  },
  {
    "start": 5236936,
    "end": 5238728,
    "text": "とても貴重な資源だ。"
  },
  {
    "start": 5238896,
    "end": 5243848,
    "text": "実際、トレーニング用のディープラーニングのワークロードの多くは、メモリに縛られている。"
  },
  {
    "start": 5243976,
    "end": 5258044,
    "text": "つまり、非常に高速な乗算を行うテンソルコアは、ほとんどの時間、待機しているのだ。"
  },
  {
    "start": 5258164,
    "end": 5264664,
    "text": "ハードウェアの典型的な利用率である60％の利用率が得られているのであれば、非常にうまくいっていることになる。"
  },
  {
    "start": 5265604,
    "end": 5272104,
    "text": "よく調整されたアプリケーションでは、データが利用できないためにテンソルコアが乗算を行わないことが半分ある。"
  },
  {
    "start": 5272484,
    "end": 5275124,
    "text": "ここでのメモリ帯域幅も非常に重要だ。"
  },
  {
    "start": 5275244,
    "end": 5288272,
    "text": "すべての浮動小数点数の精度を下げれば、すべての数値、ウェイト、アクティベーションに必要なメモリが突然少なくなる。"
  },
  {
    "start": 5288408,
    "end": 5294564,
    "text": "では、そのメリットを享受するために、まずTensorflow 32フォーマットを見てみよう。"
  },
  {
    "start": 5295184,
    "end": 5303680,
    "text": "ではまず、テンソルコアとは何なのか。テンソルコアとは、100番台のアーキテクチャにおける単なる命令だよね？"
  },
  {
    "start": 5303872,
    "end": 5308384,
    "text": "これは基本的に4×4の小さな行列の乗算を行う。"
  },
  {
    "start": 5308544,
    "end": 5320576,
    "text": "これは4×4の行列の掛け算であり、どの行列がどのような精度であるかについては複数の構成がある。"
  },
  {
    "start": 5320680,
    "end": 5326944,
    "text": "内部アキュムレートがどのような精度で行われるか、そして出力精度、入力精度などはどうなるのか。"
  },
  {
    "start": 5327064,
    "end": 5330400,
    "text": "いくつかのスイッチがあるが、基本的には4×4の掛け算だ。"
  },
  {
    "start": 5330552,
    "end": 5340722,
    "text": "行列の乗算を必要とする演算があればいつでも、このような小さな4×4の乗算命令に分割される。"
  },
  {
    "start": 5340818,
    "end": 5352170,
    "text": "というのも、行列を乗算する最速の方法は行列の乗算だからだ。"
  },
  {
    "start": 5352322,
    "end": 5359942,
    "text": "計算上の作業のほとんどは、リニア・レイヤー、リニア・リニア、その他で行われる。"
  },
  {
    "start": 5360058,
    "end": 5362022,
    "text": "その間にいくつか挟まれている。"
  },
  {
    "start": 5362118,
    "end": 5367734,
    "text": "残差の追加、ガルーノン・リニアリティ、レイヤー・ノルムなどがある。"
  },
  {
    "start": 5367814,
    "end": 5380670,
    "text": "ただ時間を計ってみれば、イントラ・トランスフォーマーが行列の乗算の束に過ぎないことがわかるだろう。"
  },
  {
    "start": 5380822,
    "end": 5385866,
    "text": "実は、行列の乗算が圧倒的に大きいのは、一番上の分類器層だ。"
  },
  {
    "start": 5386030,
    "end": 5390706,
    "text": "これは、768から50,257への巨大なマトリックス倍率である。"
  },
  {
    "start": 5390730,
    "end": 5396654,
    "text": "その行列は、大雑把に言って、そのネットワークで起こる他のすべてのことを支配する。"
  },
  {
    "start": 5397194,
    "end": 5406170,
    "text": "その行列乗算は、線形レイヤーの中に隠されていて、テンソルコアによって高速化されている。"
  },
  {
    "start": 5406362,
    "end": 5416074,
    "text": "テンソル・コアに関するベスト・リファレンスは、基本的に100アーキテクチャのホワイトペーパーを読むことだ。"
  },
  {
    "start": 5416494,
    "end": 5421754,
    "text": "何が起きているのか半分理解できれば、比較的読みやすいと思う。"
  },
  {
    "start": 5423094,
    "end": 5425954,
    "text": "図9 テンソルフロー 32."
  },
  {
    "start": 5427014,
    "end": 5430514,
    "text": "これがTf32とここで起こることについての基本的な説明である。"
  },
  {
    "start": 5431174,
    "end": 5433822,
    "text": "ここには多くのコンフィギュレーション・オプションが用意されていることがわかるだろう。"
  },
  {
    "start": 5433998,
    "end": 5448904,
    "text": "入力オペランドと、アキュムレータ内のそれらの精度と、この行列乗算をアキュムレートする際の命令内の基本的な内部表現。"
  },
  {
    "start": 5449524,
    "end": 5454900,
    "text": "中間プラスイコール中間リトルベクトルはここで乗算する。"
  },
  {
    "start": 5455012,
    "end": 5457104,
    "text": "それはすべてFP32で起こる。"
  },
  {
    "start": 5458124,
    "end": 5462752,
    "text": "ということは、これは8倍の改善ということになる。"
  },
  {
    "start": 5462908,
    "end": 5465728,
    "text": "Tf232、具体的にはこの行を見ている。"
  },
  {
    "start": 5465856,
    "end": 5477576,
    "text": "この仕組みは、通常FB32は32ビットで、TF32はまったく同じビットである。"
  },
  {
    "start": 5477640,
    "end": 5485016,
    "text": "符号ビットが1つ、指数ビットが8つあるが、仮数ビットは浮動小数点数で切り取られる。"
  },
  {
    "start": 5485160,
    "end": 5494784,
    "text": "最後の13ビットは切り捨てられるので、基本的には32ビットではなく19ビットになる。"
  },
  {
    "start": 5495484,
    "end": 5498108,
    "text": "これらはすべてインストラクションに組み込まれている。"
  },
  {
    "start": 5498276,
    "end": 5501304,
    "text": "そのどれもが、私たちのパイトーチの中では何も見えない。"
  },
  {
    "start": 5501724,
    "end": 5505412,
    "text": "パイトーチのコードは何も変わらず、すべての数字が同じに見える。"
  },
  {
    "start": 5505548,
    "end": 5514048,
    "text": "ただ、ハードウェアの内部でテンソルコア命令を呼び出すと、この13ビットが切り取られる。"
  },
  {
    "start": 5514196,
    "end": 5519576,
    "text": "これにより、この小さな行列の乗算を大幅に高速化することができる。"
  },
  {
    "start": 5519720,
    "end": 5520884,
    "text": "8倍速い。"
  },
  {
    "start": 5521224,
    "end": 5526952,
    "text": "もちろん、このスピードアップには代償が伴う。"
  },
  {
    "start": 5527088,
    "end": 5528928,
    "text": "蓄積はまだFP32だ。"
  },
  {
    "start": 5528976,
    "end": 5537352,
    "text": "出力はFP32であり、入力もFP32であるが、演算をより高速に実行するために、内部的にはオペランドが切り捨てられる。"
  },
  {
    "start": 5537488,
    "end": 5540148,
    "text": "そのため、私たちの結果はもう少し近似したものになりつつある。"
  },
  {
    "start": 5540296,
    "end": 5543904,
    "text": "経験的に、実際にこれでトレーニングしてみると、基本的に違いはわからない。"
  },
  {
    "start": 5544244,
    "end": 5552028,
    "text": "私がTF32を好きな理由は、少々の精密なごまかしを許容できるなら、これは自由だからだ。"
  },
  {
    "start": 5552116,
    "end": 5560740,
    "text": "どのコードもこれを見ず、操作の内部で完結している。"
  },
  {
    "start": 5560892,
    "end": 5564300,
    "text": "最適化という点では、かなりスイートスポットだと言える。"
  },
  {
    "start": 5564492,
    "end": 5566848,
    "text": "それがどんなものか、まずは見てみよう。"
  },
  {
    "start": 5566976,
    "end": 5572044,
    "text": "反復の時間を計るようにコードを設定したので、輸入の時間がかかる。"
  },
  {
    "start": 5572464,
    "end": 5581072,
    "text": "ハイパーのパラメーターを変更し、実行したいワークロードをもう少し反映できるようにした。"
  },
  {
    "start": 5581168,
    "end": 5588924,
    "text": "バッチサイズ16を使用し、実際のGPT-2の最大シーケンス長1024トークンを使用してみましょう。"
  },
  {
    "start": 5589744,
    "end": 5591844,
    "text": "これがコンフィギュレーションだ。"
  },
  {
    "start": 5592204,
    "end": 5597252,
    "text": "ということは、50回反復することになる。"
  },
  {
    "start": 5597388,
    "end": 5602708,
    "text": "現在時刻を取得するためにtimeを実行し、これが最適化ループだ。"
  },
  {
    "start": 5602876,
    "end": 5605348,
    "text": "どのくらい時間がかかるか時間を計ってみたい。"
  },
  {
    "start": 5605476,
    "end": 5617156,
    "text": "さて、GPUで作業する際の問題点として、CPUが動作しているときは、GPUでの作業をスケジューリングしているに過ぎないということがある。"
  },
  {
    "start": 5617180,
    "end": 5619160,
    "text": "仕事を発注するだけだろう？"
  },
  {
    "start": 5619272,
    "end": 5622016,
    "text": "それでリクエストを送り、走り続ける。"
  },
  {
    "start": 5622160,
    "end": 5631280,
    "text": "そのため、GPU上で実行するカーネルを大量にキューに入れ、スピードアップを図ることもある。"
  },
  {
    "start": 5631392,
    "end": 5641004,
    "text": "しかし、GPUはまだ動いている。GPUは、実行予定だった作業を実行するのに時間がかかるからだ。"
  },
  {
    "start": 5641424,
    "end": 5644402,
    "text": "つまり、GPUのためのキューを構築しているだけなのだ。"
  },
  {
    "start": 5644568,
    "end": 5648614,
    "text": "だから、もし必要なら、トーチのゴダデータが同期するのを待つんだ。"
  },
  {
    "start": 5648734,
    "end": 5654830,
    "text": "これは、GPUがこの上で予定されていたすべての作業を終えるのを待つことになる。"
  },
  {
    "start": 5654982,
    "end": 5657070,
    "text": "そうすれば、実際に時間を取ることができる。"
  },
  {
    "start": 5657262,
    "end": 5659334,
    "text": "基本的にはGPUが止まるのを待っている。"
  },
  {
    "start": 5659414,
    "end": 5663262,
    "text": "この繰り返しには時間がかかる。"
  },
  {
    "start": 5663318,
    "end": 5670788,
    "text": "ここでトレーニングループを実行し、右側でNvidia SMIを見ている。"
  },
  {
    "start": 5670966,
    "end": 5674408,
    "text": "GPUを使用していないので、ゼロからスタートする。"
  },
  {
    "start": 5674536,
    "end": 5676656,
    "text": "であれば、デフォルトでPytorchはgpu zeroを使う。"
  },
  {
    "start": 5676720,
    "end": 5682324,
    "text": "80GBのうち35GBを使っている。"
  },
  {
    "start": 5683384,
    "end": 5693524,
    "text": "バッチサイズを大きくしたため、小さなシェイクスピアで1つのエポックを実行するのに20バッチしか必要ないことがわかる。"
  },
  {
    "start": 5693904,
    "end": 5698594,
    "text": "1回の反復でおよそ1000ミリ秒かかっていることがわかるだろう？"
  },
  {
    "start": 5698934,
    "end": 5703518,
    "text": "最初の反復が遅くなることがある。"
  },
  {
    "start": 5703646,
    "end": 5707518,
    "text": "というのも、Pytorchは最初の反復で多くの初期化を行っている可能性があるからだ。"
  },
  {
    "start": 5707686,
    "end": 5711870,
    "text": "そのため、おそらくこれらのテンソルとバッファを初期化して、すべての勾配を保持しているのだろう。"
  },
  {
    "start": 5711942,
    "end": 5717294,
    "text": "私は無害ではないし、確かにここで起こるすべての仕事は、しかし、これはより遅い反復かもしれない。"
  },
  {
    "start": 5717414,
    "end": 5720054,
    "text": "ロジックのタイミングを計るときは、常に気をつけたいものだ。"
  },
  {
    "start": 5720174,
    "end": 5723704,
    "text": "基本的には、1回の反復で1000ミリ秒の時間がかかる。"
  },
  {
    "start": 5724644,
    "end": 5728424,
    "text": "というわけで、今のままだと約50秒間実行されることになる。"
  },
  {
    "start": 5729004,
    "end": 5731624,
    "text": "これがベースラインの流入量、32だ。"
  },
  {
    "start": 5732164,
    "end": 5740756,
    "text": "もうひとつ言っておきたいのは、もしこれがGPUに収まらずメモリ不足のエラーが出るようなら、バッチサイズを小さくして収まるようにすることだ。"
  },
  {
    "start": 5740860,
    "end": 5747484,
    "text": "16個ではなく、8個や4個など、あなたのgpuにバッチを適合させるために必要なものを試してみてください。"
  },
  {
    "start": 5747644,
    "end": 5751504,
    "text": "もっと大きなGpuがあれば、32個などでも大丈夫な可能性がある。"
  },
  {
    "start": 5751884,
    "end": 5759300,
    "text": "デフォルトでは、基本的にGPUに収まるバッチサイズを最大にし、きれいな数字を維持したい。"
  },
  {
    "start": 5759412,
    "end": 5762884,
    "text": "2のべき乗をたくさん含む数字を使う。"
  },
  {
    "start": 5763004,
    "end": 5764468,
    "text": "16はいい数字だ。"
  },
  {
    "start": 5764556,
    "end": 5768868,
    "text": "824-3248これはいい番号だ。"
  },
  {
    "start": 5768956,
    "end": 5775944,
    "text": "というのも、GPU上で非常に非効率的に動作するからだ。"
  },
  {
    "start": 5776324,
    "end": 5779184,
    "text": "とりあえず、1610 24で我慢しよう。"
  },
  {
    "start": 5779624,
    "end": 5793368,
    "text": "もうひとつ追加したのは、トレーニング中の1秒あたりのスループットを計算していることです。"
  },
  {
    "start": 5793496,
    "end": 5796600,
    "text": "トークン／秒は、私たちが実際に気にしている客観的な指標である。"
  },
  {
    "start": 5796672,
    "end": 5802520,
    "text": "何トークンのデータをトレーニングしているのか、最適化で得られるトークンのスループットはどれくらいなのか。"
  },
  {
    "start": 5802712,
    "end": 5808224,
    "text": "現在、1秒間におよそ16万3000トークンを処理し、トレーニングしている。"
  },
  {
    "start": 5808304,
    "end": 5810524,
    "text": "それはもう少し客観的な指標だ。"
  },
  {
    "start": 5811184,
    "end": 5813392,
    "text": "では、TF32を有効にしてみよう。"
  },
  {
    "start": 5813448,
    "end": 5816808,
    "text": "幸運なことに、Pytorchはこれをかなり簡単にしてくれる。"
  },
  {
    "start": 5816936,
    "end": 5822496,
    "text": "tF32を有効にするには、1行だけ実行すればよい。"
  },
  {
    "start": 5822680,
    "end": 5829344,
    "text": "Pytorchのドキュメントを見ると、この関数は基本的にPytorchに実行するカーネルの種類を指示する。"
  },
  {
    "start": 5829504,
    "end": 5834624,
    "text": "デフォルトでは、Matmulの最高精度になっていると思います。"
  },
  {
    "start": 5835004,
    "end": 5839028,
    "text": "つまり、すべてが以前と同じようにフロート32で起こるということだ。"
  },
  {
    "start": 5839196,
    "end": 5846744,
    "text": "現在のようにこれをHighに設定すると、行列の乗算はTensorflow 32が利用可能なときにそれを使うようになる。"
  },
  {
    "start": 5847884,
    "end": 5853692,
    "text": "私のgpuはa 100なので、アンペアシリーズで、したがってTF 32が利用可能です。"
  },
  {
    "start": 5853868,
    "end": 5859184,
    "text": "もしあなたが古いGPUを使っていたら、これは使えないかもしれないが、私のGPUでは使える。"
  },
  {
    "start": 5859344,
    "end": 5866264,
    "text": "だから、私がPytorchに期待するのは、nn個の線形があるすべての場所で、行列の掛け算があるということだ。"
  },
  {
    "start": 5866384,
    "end": 5873964,
    "text": "現在、行列の乗算はテンソルコースで実行され、TF32の精度が利用されていると期待している。"
  },
  {
    "start": 5874664,
    "end": 5879184,
    "text": "これこそ、必要な変化である。"
  },
  {
    "start": 5879224,
    "end": 5880392,
    "text": "再放送しよう。"
  },
  {
    "start": 5880528,
    "end": 5887938,
    "text": "さて、私たちに約束されているスループットという点では、およそ8×8が得られることになっている。"
  },
  {
    "start": 5888066,
    "end": 5889534,
    "text": "どうなるかな"
  },
  {
    "start": 5891634,
    "end": 5895094,
    "text": "8×はここから来たんだろ？"
  },
  {
    "start": 5896154,
    "end": 5897162,
    "text": "8×。"
  },
  {
    "start": 5897338,
    "end": 5905186,
    "text": "ここでも、19.5ではなく156tフロップだった。"
  },
  {
    "start": 5905370,
    "end": 5907054,
    "text": "では、実際に何が起こったのか？"
  },
  {
    "start": 5907434,
    "end": 5912214,
    "text": "スループットは8倍ではなく、およそ3倍である。"
  },
  {
    "start": 5912374,
    "end": 5921246,
    "text": "1000ミリ秒から300ミリ秒に短縮され、スループットは毎秒約5万トークンになった。"
  },
  {
    "start": 5921390,
    "end": 5923478,
    "text": "の場合、8×ではなく、およそ3×となる。"
  },
  {
    "start": 5923526,
    "end": 5929790,
    "text": "基本的に、ここで起こっているのは、これらのワークロードの多くがメモリに縛られているということだ。"
  },
  {
    "start": 5929942,
    "end": 5949426,
    "text": "TF32は原理的にはかなり高速なスループットを提供するのだが、それでもこれらの数値はすべて浮動小数点32桁であり、メモリシステムを通じてあちこちに送られるのは浮動小数点32桁の数値である。"
  },
  {
    "start": 5949570,
    "end": 5959214,
    "text": "そのため、乗算自体はかなり高速になったが、メモリに制約があり、このナプキン数学から得られる恩恵をフルに享受できていない。"
  },
  {
    "start": 5959594,
    "end": 5967582,
    "text": "とはいえ、スループットは3倍も向上している。"
  },
  {
    "start": 5967718,
    "end": 5970334,
    "text": "変数がすべてfloat32のままだ。"
  },
  {
    "start": 5970454,
    "end": 5975594,
    "text": "ただ、より速く走り、若干近似しているが、基本的には気づかないだろう。"
  },
  {
    "start": 5976214,
    "end": 5978062,
    "text": "それがTF32だ。"
  },
  {
    "start": 5978238,
    "end": 5979634,
    "text": "さて、では続けよう。"
  },
  {
    "start": 5980014,
    "end": 5992202,
    "text": "この行を練習して、操作の内部で精度の一部を切り取ることができることを確認しました。しかし、まだメモリに拘束され、すべての浮動小数点数を動かしていることがわかりましたね？"
  },
  {
    "start": 5992258,
    "end": 5995094,
    "text": "そうでなければ、このために私たちはそのコストを支払うことになる。"
  },
  {
    "start": 5995394,
    "end": 6002854,
    "text": "次に、移動する量を減らし、b float 16に落とすことにしよう。"
  },
  {
    "start": 6003554,
    "end": 6008642,
    "text": "この場合、1フロートあたり16ビットしか保持しないので、b float 16を使うことになる。"
  },
  {
    "start": 6008698,
    "end": 6009978,
    "text": "少し説明しよう。"
  },
  {
    "start": 6010106,
    "end": 6013534,
    "text": "FP16人差で、この列になる。"
  },
  {
    "start": 6013954,
    "end": 6024050,
    "text": "A 100のドキュメントに戻ると、使用可能な精度が記載されている。"
  },
  {
    "start": 6024242,
    "end": 6026042,
    "text": "これがオリジナルのFP32だ。"
  },
  {
    "start": 6026098,
    "end": 6028734,
    "text": "TF32は精度を引き出す。"
  },
  {
    "start": 6029074,
    "end": 6040104,
    "text": "次にBf 16では、TF 32と非常によく似ていることがわかるが、この浮動小数点数の仮数部の精度をより積極的に切り落としている。"
  },
  {
    "start": 6040274,
    "end": 6046332,
    "text": "Bフロート16で重要なのは、指数ビットと符号ビットはもちろん変更されないということだ。"
  },
  {
    "start": 6046468,
    "end": 6053588,
    "text": "もしフロートナンバーに詳しいなら、これだけで1本のビデオになるはずだ。"
  },
  {
    "start": 6053756,
    "end": 6062060,
    "text": "指数は数値を表現できる範囲を設定し、精度は数値の精度を設定する。"
  },
  {
    "start": 6062252,
    "end": 6074884,
    "text": "しかし、仮数を切り捨てているため、その範囲内での可能性は少なくなり、その範囲での精度は低くなる。"
  },
  {
    "start": 6075464,
    "end": 6085680,
    "text": "つまり、floatで表現可能な元の数値の範囲はあるが、その精度が低いだけなので、実際にはかなり良いということだ。"
  },
  {
    "start": 6085872,
    "end": 6090600,
    "text": "FP16との違いは、実際にタッチしてレンジを変えることだ。"
  },
  {
    "start": 6090712,
    "end": 6096652,
    "text": "FP16はFP32の全範囲を表現することはできず、範囲を狭めている。"
  },
  {
    "start": 6096788,
    "end": 6103092,
    "text": "というのも、グラデーション・スカラーなどが必要になるからだ。"
  },
  {
    "start": 6103188,
    "end": 6108900,
    "text": "このビデオでその詳細を説明するつもりはない。"
  },
  {
    "start": 6109052,
    "end": 6111500,
    "text": "歴史的にはFP16が先だった。"
  },
  {
    "start": 6111612,
    "end": 6115204,
    "text": "アンペールの前のボルタ・シリーズにはそれがあった。"
  },
  {
    "start": 6115364,
    "end": 6125834,
    "text": "そのためFP16が最初に登場し、誰もがFP16でトレーニングを始めたが、誰もが勾配スケーリング演算を使わなければならなかった。"
  },
  {
    "start": 6125994,
    "end": 6130414,
    "text": "その理由は、FP16で指数範囲が縮小されたからだ。"
  },
  {
    "start": 6130794,
    "end": 6133458,
    "text": "これがIEEE FP 16の仕様だ。"
  },
  {
    "start": 6133586,
    "end": 6143186,
    "text": "というのも、仮数を切り捨てるだけなので、まったく同じ範囲があり、勾配スカラーは必要ないからだ。"
  },
  {
    "start": 6143290,
    "end": 6145134,
    "text": "すべてがずっとシンプルになった。"
  },
  {
    "start": 6145474,
    "end": 6152584,
    "text": "しかし、Bf16を使用すると、Pytorchのコードに表示される数字に影響を与えることになる。"
  },
  {
    "start": 6152884,
    "end": 6156676,
    "text": "しかし、この変化は手術そのものに限ったことではない。"
  },
  {
    "start": 6156820,
    "end": 6158624,
    "text": "どうなるか見てみよう。"
  },
  {
    "start": 6160844,
    "end": 6174490,
    "text": "Pytorchのドキュメントの中にも、もっとわかりにくいチュートリアルなどがたくさんあるので。"
  },
  {
    "start": 6174612,
    "end": 6180154,
    "text": "というわけで、私は特にこの1枚を薦める。"
  },
  {
    "start": 6180534,
    "end": 6191154,
    "text": "それなら、ここに来たとき、すべてのことを無視し、グラディエント・スカラーのこともすべて無視して、トーチ・オートキャストだけを見ていればいい。"
  },
  {
    "start": 6191814,
    "end": 6195222,
    "text": "基本的には、これも最後に1行のコードになる。"
  },
  {
    "start": 6195318,
    "end": 6202270,
    "text": "このコンテキスト・マネージャーをネットワークで使いたい。"
  },
  {
    "start": 6202422,
    "end": 6210390,
    "text": "トーチのオートキャストをクリックすると、もう少しガイドラインが表示されます。"
  },
  {
    "start": 6210462,
    "end": 6221430,
    "text": "Bfloat16をテンソルのいずれにも呼び出さず、オートキャストを使用し、モデルのフォワードパスと損失計算だけを囲む。"
  },
  {
    "start": 6221582,
    "end": 6224174,
    "text": "この2つだけだ。"
  },
  {
    "start": 6224294,
    "end": 6226870,
    "text": "バックワードとオプティマイザーのステップは放っておく。"
  },
  {
    "start": 6227062,
    "end": 6229758,
    "text": "それがピトーチ・チームからのガイダンスだ。"
  },
  {
    "start": 6229886,
    "end": 6231374,
    "text": "我々はその指導に従うつもりだ。"
  },
  {
    "start": 6231454,
    "end": 6244686,
    "text": "トーチ・フロート16を使うと、勾配スカラーも使う必要があるからだ。"
  },
  {
    "start": 6244790,
    "end": 6246794,
    "text": "ここではb float 16を使用する。"
  },
  {
    "start": 6247134,
    "end": 6254874,
    "text": "これはアンペアしかできないが、基本的にはこの1行のコードだけというように、変更は極めて最小限ということになる。"
  },
  {
    "start": 6256784,
    "end": 6262368,
    "text": "実際に実行する前に、まずここに入らせてもらおう。"
  },
  {
    "start": 6262536,
    "end": 6270964,
    "text": "ロジットの直後に、TF32とは異なる、テンソルに影響を与えることをお見せしたい。"
  },
  {
    "start": 6271824,
    "end": 6280684,
    "text": "このlogitテンソルを見て、dタイプを見てみると、これがb float 16になっていることがわかる。"
  },
  {
    "start": 6281834,
    "end": 6283378,
    "text": "もうフロート32ではない。"
  },
  {
    "start": 6283426,
    "end": 6285298,
    "text": "アクティベーションが変更された。"
  },
  {
    "start": 6285346,
    "end": 6289530,
    "text": "活性化テンソルはpflow 16になったが、すべてが変わったわけではない。"
  },
  {
    "start": 6289562,
    "end": 6298674,
    "text": "これはウェイトトークンの埋め込みテーブルである。"
  },
  {
    "start": 6298794,
    "end": 6306046,
    "text": "その中にドットウエイトがあり、このウエイトのdタイプ、このパラメーターはトーチフロート32のままである。"
  },
  {
    "start": 6306210,
    "end": 6312034,
    "text": "パラメータはまだインフロー32のようだが、アクティベーション、ロードジェットはpflow16になっている。"
  },
  {
    "start": 6312334,
    "end": 6315462,
    "text": "これが、精度がまちまちな理由であることは明らかだ。"
  },
  {
    "start": 6315598,
    "end": 6321314,
    "text": "あるものはPytorchがインフロー32を維持し、あるものはPytorchが低精度に変換している。"
  },
  {
    "start": 6323134,
    "end": 6327074,
    "text": "何がどの時点で変換されるのかは、あまり明確ではない。"
  },
  {
    "start": 6327414,
    "end": 6332314,
    "text": "スクロールした覚えがあるんだけど、ここかな？"
  },
  {
    "start": 6334354,
    "end": 6335250,
    "text": "ブラッ。"
  },
  {
    "start": 6335442,
    "end": 6336934,
    "text": "見つからないんだ。"
  },
  {
    "start": 6338434,
    "end": 6339890,
    "text": "ここだと思った。"
  },
  {
    "start": 6340082,
    "end": 6341214,
    "text": "よし、行くぞ。"
  },
  {
    "start": 6341994,
    "end": 6345894,
    "text": "このオートキャストを使用する際のドキュメントがいくつかある。"
  },
  {
    "start": 6346194,
    "end": 6348986,
    "text": "何がいつb float 16に変換されるのか。"
  },
  {
    "start": 6349090,
    "end": 6353774,
    "text": "例えば、これらの行列乗算のような演算だけが、b float 16に変換される。"
  },
  {
    "start": 6354154,
    "end": 6356674,
    "text": "多くのオペレーションがフロー32に残っている。"
  },
  {
    "start": 6356754,
    "end": 6362544,
    "text": "特に、レイヤー規範などの正規化の多くは、すべてのレイヤーが変換されるとは限らない。"
  },
  {
    "start": 6363484,
    "end": 6367212,
    "text": "一部のレイヤーだけが選択的にb float 16を実行することになる。"
  },
  {
    "start": 6367308,
    "end": 6379732,
    "text": "ソフトマックス、レイヤー・ノルム、ログ・ソフトマックス、損失関数の計算のようなものは、精度の変化の影響を受けやすいので、float32のままかもしれない。"
  },
  {
    "start": 6379868,
    "end": 6385832,
    "text": "行列乗算は精度の変化にかなり強い。"
  },
  {
    "start": 6386008,
    "end": 6391364,
    "text": "ネットワークの一部は、精度変更の影響を多かれ少なかれ受けている。"
  },
  {
    "start": 6393264,
    "end": 6396968,
    "text": "基本的に、モデルの一部だけが精度を落として実行されている。"
  },
  {
    "start": 6397136,
    "end": 6403244,
    "text": "実際に使ってみて、どのような改善が見られるか見てみよう。"
  },
  {
    "start": 6409084,
    "end": 6411948,
    "text": "さて、以前は333ミリ秒だった。"
  },
  {
    "start": 6411996,
    "end": 6415980,
    "text": "今は300ですが、以前は毎秒5万トークンほどでした。"
  },
  {
    "start": 6416012,
    "end": 6421652,
    "text": "僕たちはもう55歳だから、確かに速く走れるようになったけれど、それほど速くはないかもしれない。"
  },
  {
    "start": 6421828,
    "end": 6429324,
    "text": "というのも、GPT-2にはまだ多くのボトルネックがあるからだ。"
  },
  {
    "start": 6429404,
    "end": 6434092,
    "text": "現在のGPUは100で、Pytorch Autocastを使っている。"
  },
  {
    "start": 6434268,
    "end": 6439138,
    "text": "残念ながら、ピトーチ・オートキャストが何をするのか正確には知らない。"
  },
  {
    "start": 6439266,
    "end": 6443154,
    "text": "フロート16に何が入っているのか、フロート32に何が入っているのか、正確には知らない。"
  },
  {
    "start": 6443274,
    "end": 6451922,
    "text": "しかし、これらのルールはPytorchが内部的に持っているもので、残念ながらあまり文書化されていない。"
  },
  {
    "start": 6452098,
    "end": 6455978,
    "text": "それについては、あまり詳しく説明するつもりはない。"
  },
  {
    "start": 6456106,
    "end": 6458298,
    "text": "今のところ、bflow16でトレーニングしている。"
  },
  {
    "start": 6458426,
    "end": 6460250,
    "text": "グラデーションスケーラーは必要ない。"
  },
  {
    "start": 6460362,
    "end": 6467648,
    "text": "より高速に動作しているのは、テンソルコアをB float 16で動作させることができるからだ。"
  },
  {
    "start": 6467696,
    "end": 6474440,
    "text": "今、私たちはこの列の中にいるが、このために精密さも払っているということだ。"
  },
  {
    "start": 6474632,
    "end": 6480184,
    "text": "オリジナルのFP 32と比較すると、若干精度が落ちることが予想される。"
  },
  {
    "start": 6480344,
    "end": 6491916,
    "text": "経験的に、多くの場合、このトレードオフは価値がある。なぜなら、より速く走れるようになり、例えば、より長くトレーニングして、精度の低下を補うことができるからだ。"
  },
  {
    "start": 6492060,
    "end": 6495404,
    "text": "今のところ、ビーフロー16だ。"
  },
  {
    "start": 6495524,
    "end": 6500484,
    "text": "さて、見ての通り、現在1反復あたり約300ミリ秒である。"
  },
  {
    "start": 6500564,
    "end": 6504316,
    "text": "私たちはこれから、パイトーチの武器庫にある本当に重い武器に手を伸ばすことになる。"
  },
  {
    "start": 6504420,
    "end": 6507144,
    "text": "特にトーチ・コンパイルを紹介する。"
  },
  {
    "start": 6507524,
    "end": 6514500,
    "text": "torch compileは、Pytorchチームによる非常に素晴らしいインフラで、基本的にはニューラルネットワーク用のコンパイラだ。"
  },
  {
    "start": 6514612,
    "end": 6517890,
    "text": "CとCコードのためのGCCのようなものだ。"
  },
  {
    "start": 6518012,
    "end": 6520766,
    "text": "これはニューラルネットのGCCに過ぎない。"
  },
  {
    "start": 6520950,
    "end": 6525634,
    "text": "少し前に発売されたもので、使い方は極めて簡単だ。"
  },
  {
    "start": 6526534,
    "end": 6528994,
    "text": "ストレージコンパイルの使い方はこうだ。"
  },
  {
    "start": 6529534,
    "end": 6533686,
    "text": "モデルをコンパイルしてそれを返すのは、たった1行のコードだ。"
  },
  {
    "start": 6533830,
    "end": 6539358,
    "text": "さて、このコード行はコンパイル時間を犠牲にするが、お察しの通り、コードをより速くすることができる。"
  },
  {
    "start": 6539526,
    "end": 6543006,
    "text": "実行には時間がかかるので、実際に実行してみよう。"
  },
  {
    "start": 6543110,
    "end": 6546690,
    "text": "現在、300ミリ秒であることをお忘れなく。"
  },
  {
    "start": 6546842,
    "end": 6551490,
    "text": "さて、これを実行している間に、トーチ・コンパイルが何をするのかを少し説明したい。"
  },
  {
    "start": 6551522,
    "end": 6552374,
    "text": "ボンネットの中"
  },
  {
    "start": 6552794,
    "end": 6555974,
    "text": "Pytorchのこのページを自由にお読みください。"
  },
  {
    "start": 6556394,
    "end": 6560842,
    "text": "基本的に、pytorchでtorchコンパイルを使わない本当の理由はない。"
  },
  {
    "start": 6560898,
    "end": 6567374,
    "text": "デバッグをしていて、コードを本当に速く走らせたいのでなければ、ほとんどデフォルトで使うべきだと思う。"
  },
  {
    "start": 6568074,
    "end": 6573244,
    "text": "トーチ・コンパイルで、なぜこの方が速いのかがよくわかる1行を見つけた。"
  },
  {
    "start": 6573744,
    "end": 6578304,
    "text": "スピードアップは主に、pythonのオーバーヘッドとGPUの読み込み書き込みの削減によるものです。"
  },
  {
    "start": 6578424,
    "end": 6580204,
    "text": "それを少し紐解いてみよう。"
  },
  {
    "start": 6581224,
    "end": 6582336,
    "text": "よし、着いたぞ。"
  },
  {
    "start": 6582440,
    "end": 6584464,
    "text": "300ミリ秒から300ミリ秒へ。"
  },
  {
    "start": 6584584,
    "end": 6586924,
    "text": "現在129ミリ秒。"
  },
  {
    "start": 6587744,
    "end": 6594524,
    "text": "これはPytorchの1行のコードから300÷129、約2.3倍の改善だ。"
  },
  {
    "start": 6595344,
    "end": 6596496,
    "text": "とても信じられない。"
  },
  {
    "start": 6596600,
    "end": 6597456,
    "text": "何が起こっているのか？"
  },
  {
    "start": 6597480,
    "end": 6598728,
    "text": "ボンネットの下で何が起きているのか？"
  },
  {
    "start": 6598896,
    "end": 6610112,
    "text": "トーチ・コンパイルにモデルを渡すと、このNnモジュールにあるのは、ネットワークで何が起こるかをアルゴリズムで記述したものだ。"
  },
  {
    "start": 6610288,
    "end": 6616720,
    "text": "トーチ・コンパイルは全体を分析し、あなたがどのような操作を使いたいかを調べます。"
  },
  {
    "start": 6616872,
    "end": 6623288,
    "text": "何が起こるかを正確に知っているという利点があるため、イーガー・モードと呼ばれるモードで実行する必要はない。"
  },
  {
    "start": 6623376,
    "end": 6637300,
    "text": "Pythonインタープリターが通常前方から始めて、Pythonインタープリターが、よし、このオペレーションをやろう、そしてあのオペレーションをやろう、とするように、レイヤーごとに進んでいく必要はない。"
  },
  {
    "start": 6637452,
    "end": 6641024,
    "text": "すべてのオペレーションを具体化するようなものだ。"
  },
  {
    "start": 6641324,
    "end": 6645260,
    "text": "これらの計算は、この順序でディスパッチされ、実行される。"
  },
  {
    "start": 6645412,
    "end": 6650348,
    "text": "Pythonインタープリターとこのコードは、後でどのような操作が行われるかを知らない。"
  },
  {
    "start": 6650516,
    "end": 6657020,
    "text": "トーチ・コンパイルはあなたのコード全体を同時に見ることができ、あなたがどのような操作を実行しようとしているのかを知ることができる。"
  },
  {
    "start": 6657132,
    "end": 6659544,
    "text": "そのプロセスを最適化する。"
  },
  {
    "start": 6659844,
    "end": 6670228,
    "text": "最初に行うのは、フォワードパスからPythonインタープリターを完全に取り除き、Pythonインタープリターを介さずにニューラルネット全体を1つのオブジェクトとしてコンパイルすることだ。"
  },
  {
    "start": 6670356,
    "end": 6675184,
    "text": "何が実行されるかを正確に把握しているので、それを実行するだけで、すべてが効率的なコードで実行されることになる。"
  },
  {
    "start": 6676584,
    "end": 6682648,
    "text": "もうひとつは、ごく簡単に触れたリード・ライトだ。"
  },
  {
    "start": 6682736,
    "end": 6686728,
    "text": "その好例が、これまで見てきたガルの非直線性だと思う。"
  },
  {
    "start": 6686896,
    "end": 6688404,
    "text": "ここではNNGを使う。"
  },
  {
    "start": 6688864,
    "end": 6697128,
    "text": "さて、これは基本的に私がインとグルをバラバラにしたものだが、この公式を覚えているだろうか。"
  },
  {
    "start": 6697256,
    "end": 6700920,
    "text": "これはグールーで起きていることと同等のことだ。"
  },
  {
    "start": 6700992,
    "end": 6702524,
    "text": "アルゴリズム的には同じだ。"
  },
  {
    "start": 6703064,
    "end": 6711064,
    "text": "さて、デフォルトでは、Gitlをここで終了させる代わりにこれを使うとすると、torchコンパイルがなければどうなるでしょうか？"
  },
  {
    "start": 6711224,
    "end": 6715816,
    "text": "Pythonのインタープリターがここに来て、よし、入力がある。"
  },
  {
    "start": 6715960,
    "end": 6726684,
    "text": "さて、まずこの入力を3乗に上げてみよう。そうすると、入力を受けて3乗に上げるカーネルがディスパッチされ、そのカーネルが実行される。"
  },
  {
    "start": 6727184,
    "end": 6733506,
    "text": "このカーネルが実行されると、最終的にこの入力がGPUのメモリに保存される。"
  },
  {
    "start": 6733650,
    "end": 6737450,
    "text": "ここに、何が起きているのかのレイアウトの参考例がある。"
  },
  {
    "start": 6737482,
    "end": 6747698,
    "text": "そう、CPUがあり、これはどのコンピューターにも搭載されているもので、いくつかのコアがある。"
  },
  {
    "start": 6747746,
    "end": 6752626,
    "text": "これはすべてよく知られていることだが、今回GPUが追加され、GPUは少し異なるアーキテクチャになっている。"
  },
  {
    "start": 6752650,
    "end": 6758440,
    "text": "もちろん通信はできるし、CPUよりもはるかに多くのコアを搭載しているという点でも違う。"
  },
  {
    "start": 6758592,
    "end": 6761680,
    "text": "これらのコアはすべて、個々もずっとシンプルだ。"
  },
  {
    "start": 6761872,
    "end": 6763640,
    "text": "メモリーもあるんだろう？"
  },
  {
    "start": 6763712,
    "end": 6766204,
    "text": "この高帯域幅メモリ。"
  },
  {
    "start": 6766744,
    "end": 6772896,
    "text": "HPMが何の略なのか、今気づいたんだ。"
  },
  {
    "start": 6773040,
    "end": 6779208,
    "text": "これはメモリで、基本的にはコンピューターのラムに相当する。"
  },
  {
    "start": 6779376,
    "end": 6781964,
    "text": "何が起きているかというと、インプットが記憶の中に生きているのだ。"
  },
  {
    "start": 6782144,
    "end": 6795344,
    "text": "三乗入力をする場合、この入力はGPU、コア、そしてGPUの実際のチップ上のすべてのキャッシュとレジスタに送られなければならない。"
  },
  {
    "start": 6795964,
    "end": 6801344,
    "text": "を計算し、その結果をメモリに保存する。"
  },
  {
    "start": 6801924,
    "end": 6805900,
    "text": "この移動時間が多くの問題を引き起こすのだ。"
  },
  {
    "start": 6806052,
    "end": 6816880,
    "text": "このメモリ帯域幅を思い出してほしい。1秒間に約2テラバイトの通信が可能だ。"
  },
  {
    "start": 6817032,
    "end": 6820968,
    "text": "GPUはチップ上にあり、チップ内ではすべてが超高速だ。"
  },
  {
    "start": 6821096,
    "end": 6825048,
    "text": "メモリへの移動は非常に高価で、非常に長い時間がかかる。"
  },
  {
    "start": 6825216,
    "end": 6830200,
    "text": "つまり、入力をロードし、計算を行い、出力をロードバックする。"
  },
  {
    "start": 6830352,
    "end": 6832708,
    "text": "この往復にはかなりの時間がかかる。"
  },
  {
    "start": 6832896,
    "end": 6836676,
    "text": "その直後に、この定数を掛ける。"
  },
  {
    "start": 6836820,
    "end": 6842492,
    "text": "その後、別のカーネルをディスパッチし、その結果が戻ってくる。"
  },
  {
    "start": 6842668,
    "end": 6847464,
    "text": "すべての要素に定数が掛けられ、その結果がメモリに戻る。"
  },
  {
    "start": 6847884,
    "end": 6851308,
    "text": "そして、その結果を入力に戻す。"
  },
  {
    "start": 6851476,
    "end": 6857852,
    "text": "そのため、この全体が再びGPUに移動し、入力を追加して書き戻される。"
  },
  {
    "start": 6857988,
    "end": 6868462,
    "text": "テンソルコアやALUなどすべてがGPUのチップ上に格納されているからだ。"
  },
  {
    "start": 6868598,
    "end": 6878190,
    "text": "トーチ・コンパイルを使わないPytorchは、後でどのようなオペレーションを実行するか分からないので、これを最適化することができない。"
  },
  {
    "start": 6878302,
    "end": 6882606,
    "text": "3番までパワーを上げて、こうして、ああして、と指示するだけだ。"
  },
  {
    "start": 6882630,
    "end": 6884318,
    "text": "その順番でやるだけだ。"
  },
  {
    "start": 6884446,
    "end": 6890644,
    "text": "トーチ・コンパイルはあなたのコード全体を見て、ここに来て、待てよ、これらはすべてエレメント単位の操作だ、と気づくだろう。"
  },
  {
    "start": 6890804,
    "end": 6896204,
    "text": "GPUへの入力は1回だけだ。"
  },
  {
    "start": 6896364,
    "end": 6906932,
    "text": "そして、すべてのエレメントについて、GPU上にあるメモリー、あるいはその塊の中で、これらの操作をすべて行い、1回だけ書き戻す。"
  },
  {
    "start": 6907068,
    "end": 6908620,
    "text": "このような往復をするつもりはない。"
  },
  {
    "start": 6908692,
    "end": 6914226,
    "text": "これはカーネル・フュージョンと呼ばれるものの一例で、すべてが分割され、スピードアップされる主要な方法だ。"
  },
  {
    "start": 6914370,
    "end": 6923546,
    "text": "基本的に、後知恵の利点があり、何を計算するのかが正確に分かっていれば、メモリへのラウンドトリップを最適化することができ、メモリ帯域幅のコストを支払う必要はない。"
  },
  {
    "start": 6923690,
    "end": 6929694,
    "text": "これが根本的に、これらの操作のいくつかをより高速にするものであり、ここでのリードライトの意味するところである。"
  },
  {
    "start": 6930914,
    "end": 6933734,
    "text": "これは使わないので消させてください。"
  },
  {
    "start": 6934194,
    "end": 6947148,
    "text": "そうだ、トーチ・コンパイルを使うべきだ。コードは大幅に高速化し、1秒間に約12万5000トークンを処理できるようになったが、次に進むにはまだ長い道のりがある。"
  },
  {
    "start": 6947196,
    "end": 6956204,
    "text": "これは複雑なテーマだが、ここで何が起きているのかを高いレベルで理解する価値はある。"
  },
  {
    "start": 6956324,
    "end": 6961224,
    "text": "これについては、2時間くらいのビデオをまるまる1本使うこともできるだろうが、基本的にはそのプレビューに過ぎない。"
  },
  {
    "start": 6961924,
    "end": 6965184,
    "text": "このチップはGPUだ。"
  },
  {
    "start": 6965684,
    "end": 6968836,
    "text": "このチップでほとんどの計算が行われる。"
  },
  {
    "start": 6968980,
    "end": 6972436,
    "text": "このチップにはメモリも搭載されている。"
  },
  {
    "start": 6972580,
    "end": 6978780,
    "text": "メモリの大半は高帯域幅メモリHBMに搭載され、接続されている。"
  },
  {
    "start": 6978932,
    "end": 6982424,
    "text": "これらはつながっているが、基本的には2つの別々のチップだ。"
  },
  {
    "start": 6982844,
    "end": 6988584,
    "text": "これはGPUの漫画のような図を拡大したものだ。"
  },
  {
    "start": 6988924,
    "end": 6997068,
    "text": "このHBMは、皆さんにはとても小さいと思いますが、側面にはhbMと書かれています。"
  },
  {
    "start": 6997196,
    "end": 6999252,
    "text": "これがHBMへのリンクだ。"
  },
  {
    "start": 6999388,
    "end": 7001996,
    "text": "現在、HBMは再びオフチップになっている。"
  },
  {
    "start": 7002180,
    "end": 7007260,
    "text": "チップ上には、こうしたストリーミング・マルチプロセッサが多数存在する。"
  },
  {
    "start": 7007452,
    "end": 7008972,
    "text": "そのどれもがSMだ。"
  },
  {
    "start": 7009108,
    "end": 7010784,
    "text": "全部で120人いる。"
  },
  {
    "start": 7011084,
    "end": 7013700,
    "text": "ここで多くの計算が行われる。"
  },
  {
    "start": 7013852,
    "end": 7016908,
    "text": "これは1枚の写真を拡大したもの。"
  },
  {
    "start": 7017036,
    "end": 7018460,
    "text": "この4つの象限がある。"
  },
  {
    "start": 7018572,
    "end": 7020068,
    "text": "例えばテンソル・コアを参照。"
  },
  {
    "start": 7020116,
    "end": 7022330,
    "text": "マトリックス乗算の多くはここで起こる。"
  },
  {
    "start": 7022452,
    "end": 7029886,
    "text": "FP64、FB32、整数など、さまざまな計算を行うためのユニットが他にもある。"
  },
  {
    "start": 7030070,
    "end": 7034526,
    "text": "さて、ここで計算するためのロジックをすべて揃えた。"
  },
  {
    "start": 7034630,
    "end": 7038766,
    "text": "それに加えて、チップ上にはメモリが散りばめられている。"
  },
  {
    "start": 7038910,
    "end": 7043354,
    "text": "l 2つのキャッシュは、チップ上に存在するいくらかのメモリである。"
  },
  {
    "start": 7043694,
    "end": 7047230,
    "text": "SMS自体にはキャッシュが1つしかない。"
  },
  {
    "start": 7047342,
    "end": 7052654,
    "text": "この青いバーが1つで、レジスタもある。"
  },
  {
    "start": 7053874,
    "end": 7056114,
    "text": "だから、ここにはメモリーが保存されている。"
  },
  {
    "start": 7056234,
    "end": 7061054,
    "text": "しかし、このメモリーの格納方法は、HBMにおけるメモリーの格納方法とは大きく異なる。"
  },
  {
    "start": 7061394,
    "end": 7069934,
    "text": "これは、シリコンがどのように見えるかという点だけで、非常に異なる実装だ。"
  },
  {
    "start": 7071314,
    "end": 7077366,
    "text": "ここではトランジスタとコンデンサーを使うだろうが、ここではSRAMを使ったまったく異なる実装だ。"
  },
  {
    "start": 7077390,
    "end": 7086862,
    "text": "でも、要するに、チップの中にはメモリがあるんだ。"
  },
  {
    "start": 7086918,
    "end": 7087998,
    "text": "そこが重要なポイントだ。"
  },
  {
    "start": 7088086,
    "end": 7102672,
    "text": "これはちょっと変わったGPUの例で、例えばCPUのDRAMメモリの典型的な数字を示している。"
  },
  {
    "start": 7102758,
    "end": 7105564,
    "text": "特にGPUの場合、アクセスするには非常に高価になる。"
  },
  {
    "start": 7105604,
    "end": 7107516,
    "text": "ここではCPUを経由しなければならない。"
  },
  {
    "start": 7107700,
    "end": 7109500,
    "text": "次はHBMだ。"
  },
  {
    "start": 7109612,
    "end": 7116264,
    "text": "一般的なGPUには数十ギガバイトのHBMメモリが搭載されているが、前述の通り、アクセスするには非常に高価だ。"
  },
  {
    "start": 7116924,
    "end": 7126904,
    "text": "しかし、チップ全体に搭載されているメモリは数10メガバイトしかない。"
  },
  {
    "start": 7127204,
    "end": 7131740,
    "text": "メモリがチップ上で非常に高価なため、十分なスペースがないのだ。"
  },
  {
    "start": 7131892,
    "end": 7136436,
    "text": "そのため、その量は多くないが、相対的に見ればアクセスは光速だ。"
  },
  {
    "start": 7136620,
    "end": 7149492,
    "text": "基本的に、このようなカーネルがあるときはいつでも、ここで起こっていることをより正確に把握するために、デフォルトではグローバル・メモリ上にあるこれらの入力を受け取り、次に何らかの計算を実行する必要がある。"
  },
  {
    "start": 7149668,
    "end": 7155632,
    "text": "グローバル・メモリーからチップへのデータのストリーミングを開始する。"
  },
  {
    "start": 7155768,
    "end": 7160928,
    "text": "チップ上で計算を行い、それをストリームバックしてグローバル・メモリーに戻す。"
  },
  {
    "start": 7161096,
    "end": 7161760,
    "text": "そうだね。"
  },
  {
    "start": 7161912,
    "end": 7170444,
    "text": "だから、もしトーチ・コンパイルがなければ、チップを通してデータをストリーミングし、計算し、メモリに保存する。"
  },
  {
    "start": 7171184,
    "end": 7175112,
    "text": "トーチコンパイルされていれば、以前と同じようにメモリのストリーミングを開始する。"
  },
  {
    "start": 7175248,
    "end": 7181828,
    "text": "チップ上で処理しようとしているデータの塊がある。"
  },
  {
    "start": 7182016,
    "end": 7184044,
    "text": "その塊は今、チップ上にある。"
  },
  {
    "start": 7184164,
    "end": 7186732,
    "text": "チップ上にある間は、極めて高速に動作する。"
  },
  {
    "start": 7186828,
    "end": 7193412,
    "text": "カーネル・フュージョンがあれば、エレメント単位ですべてのオペレーションができる。"
  },
  {
    "start": 7193508,
    "end": 7197796,
    "text": "その後、グローバル・メモリに戻る。"
  },
  {
    "start": 7197940,
    "end": 7204916,
    "text": "オペレーター・フュージョンは基本的に、データの塊をチップ上に保持し、書き戻す前にその上で多くの計算を行うことを可能にする。"
  },
  {
    "start": 7205060,
    "end": 7207376,
    "text": "それは、莫大な節約になる。"
  },
  {
    "start": 7207500,
    "end": 7212564,
    "text": "だからトーチのコンパイルが速くなるんだ。"
  },
  {
    "start": 7213304,
    "end": 7218484,
    "text": "もう一度、メモリ階層とトーチ・コンパイルが何をしてくれるかを簡単に紹介しよう。"
  },
  {
    "start": 7218904,
    "end": 7223924,
    "text": "さて、トーチ・コンパイルは素晴らしいが、トーチ・コンパイルが見つけられない操作もある。"
  },
  {
    "start": 7224304,
    "end": 7228284,
    "text": "その驚くべき例がフラッシュ・アテンションである。"
  },
  {
    "start": 7228744,
    "end": 7240732,
    "text": "フラッシュ・アテンションは2022年にスタンフォード大学で発表された論文から生まれたもので、アテンションを実行し、それをより速く実行するための驚くべきアルゴリズムだ。"
  },
  {
    "start": 7240908,
    "end": 7251180,
    "text": "フラッシュ・アテンションがここに来て、この4行を取り出します。フラッシュ・アテンションはこの4行を本当に、本当に素早く実行します。"
  },
  {
    "start": 7251372,
    "end": 7252852,
    "text": "どうやるんだ？"
  },
  {
    "start": 7252948,
    "end": 7256664,
    "text": "まあ、フラッシュ・アテンションはカーネル・フュージョンの操作だ。"
  },
  {
    "start": 7257004,
    "end": 7267474,
    "text": "この図にあるように、ピトーチはドロップアウトを含む4つのオペレーションを行っている。"
  },
  {
    "start": 7267594,
    "end": 7275602,
    "text": "この4行のコードの代わりに、フラッシュ・アテンションの1つのカーネルに融合させるのだ。"
  },
  {
    "start": 7275778,
    "end": 7282134,
    "text": "これはカーネル・フュージョンのアルゴリズムだが、トーチ・コンパイルが見つけられないカーネル・フュージョンだ。"
  },
  {
    "start": 7282434,
    "end": 7290594,
    "text": "それを見つけられないのは、この場合、アテンションが実際にどのように実装されているかをアルゴリズム的に書き換える必要があるからだ。"
  },
  {
    "start": 7290894,
    "end": 7300814,
    "text": "フラッシュ・アテンションは、このアテンションよりもフロップの数が多いのだ。"
  },
  {
    "start": 7300974,
    "end": 7303634,
    "text": "フラッシュアテンションは実際、かなり速い。"
  },
  {
    "start": 7304374,
    "end": 7308182,
    "text": "実際、7.6倍速くなる可能性があるという。"
  },
  {
    "start": 7308358,
    "end": 7314398,
    "text": "それは、先ほど説明したように、メモリ階層を非常に意識しているからだ。"
  },
  {
    "start": 7314566,
    "end": 7327342,
    "text": "そのため、高帯域幅メモリーに何があるか、共有メモリーに何があるかということに非常に気を配り、高帯域幅メモリーへの読み書きが少なくなるように、計算のオーケストレーションに細心の注意を払っている。"
  },
  {
    "start": 7327438,
    "end": 7333470,
    "text": "だから、フロップ数を増やしても、コストがかかるのはHPMへのロードとストアだ。"
  },
  {
    "start": 7333622,
    "end": 7336846,
    "text": "だから特に、それが実現することはない。"
  },
  {
    "start": 7336990,
    "end": 7349266,
    "text": "このn×nのアテンション・マトリックス、つまりフラッシュ・アテンションは、このマトリックスがどの時点でも実体化されないように設計されており、HBMへの読み書きが行われることはない。"
  },
  {
    "start": 7349410,
    "end": 7350714,
    "text": "これは非常に大きな行列である。"
  },
  {
    "start": 7350754,
    "end": 7351002,
    "text": "そうだろう？"
  },
  {
    "start": 7351058,
    "end": 7367974,
    "text": "というのも、ここですべてのクエリーとキーが相互作用し、各ヘッド、各バッチエレメントについて、t×tの注目度行列を得ることになるからだ。"
  },
  {
    "start": 7369684,
    "end": 7373332,
    "text": "基本的に、これは大量のメモリであり、決して実現されることはない。"
  },
  {
    "start": 7373508,
    "end": 7383236,
    "text": "これを実現する方法は、基本的にこのアルゴリズムの基本的な書き換えが、以前に提案されたこのオンライン・ソフトマックスのトリックに依存していることである。"
  },
  {
    "start": 7383260,
    "end": 7384744,
    "text": "もう少ししたら論文をお見せします。"
  },
  {
    "start": 7385124,
    "end": 7398660,
    "text": "オンライン・ソフトマックスのトリックは、以前の論文から来たもので、正規化を行うためにソフトマックスへのすべての入力を実現することなく、ソフトマックスを段階的に評価する方法を示している。"
  },
  {
    "start": 7398852,
    "end": 7406984,
    "text": "mとlという中間変数があり、それを更新することで、オンライン方式でソフトマックスを評価することができる。"
  },
  {
    "start": 7408924,
    "end": 7412932,
    "text": "さて、フラッシュ・アテンションだが、実は最近、フラッシュ・アテンション2も発売された。"
  },
  {
    "start": 7412948,
    "end": 7418144,
    "text": "その論文はこちらにもアップしているが、フラッシュの注目度を計算する方法についてさらなる利益がある。"
  },
  {
    "start": 7418484,
    "end": 7424158,
    "text": "これが元になっている論文は、基本的にソフトマックスのオンライン・ノーマライザー計算である。"
  },
  {
    "start": 7424286,
    "end": 7429398,
    "text": "驚くべきことに、それはNvidiaから出てきたもので、2018年の本当に早い時期に出てきたものだ。"
  },
  {
    "start": 7429566,
    "end": 7432074,
    "text": "これはフラッシュ保持の4年前のことである。"
  },
  {
    "start": 7432614,
    "end": 7444246,
    "text": "本稿では、より少ないメモリアクセスで古典的なソフトマックスを計算する方法を提案し、このメモリアクセスの削減により、実際のハードウェア上でソフトマックスの性能が向上するはずだと仮定する。"
  },
  {
    "start": 7444430,
    "end": 7449048,
    "text": "この仮説は極めて正しい。"
  },
  {
    "start": 7449176,
    "end": 7456216,
    "text": "彼らがNvidiaの人間で、このことに気づいていたにもかかわらず、実際にフラッシュに注目されるまでに至らなかったということは、私にとって本当に魅力的なことだ。"
  },
  {
    "start": 7456360,
    "end": 7459472,
    "text": "それは4年後にスタンフォードからもたらされたものだった。"
  },
  {
    "start": 7459608,
    "end": 7468984,
    "text": "歴史的な経緯はよくわからないが、基本的にはこのソフトマックスへのオンラインアップデートを提案している。"
  },
  {
    "start": 7469104,
    "end": 7474030,
    "text": "ソフトマックスをストリーミング方式で計算するためだ。"
  },
  {
    "start": 7474152,
    "end": 7482122,
    "text": "そして、他のすべての演算とオンラインソフトマックス演算を融合させ、単一の融合カーネルフラッシュに注目させることができることに気づく。"
  },
  {
    "start": 7482258,
    "end": 7484374,
    "text": "それをこれから使うんだ。"
  },
  {
    "start": 7484714,
    "end": 7488922,
    "text": "つまり、記憶の階層を意識することの素晴らしい例だと思う。"
  },
  {
    "start": 7489018,
    "end": 7493450,
    "text": "フロップが重要なのではなく、メモリアクセスパターン全体が重要なのだ。"
  },
  {
    "start": 7493602,
    "end": 7495114,
    "text": "トーチのコンパイルは素晴らしい。"
  },
  {
    "start": 7495154,
    "end": 7499900,
    "text": "潜在的にトーチ・コンパイルが見つけられないような最適化がまだたくさんある。"
  },
  {
    "start": 7500042,
    "end": 7504376,
    "text": "もしかしたら、いつかはそうなるかもしれないが、今はまだ、それを求めるのは酷なことのように思える。"
  },
  {
    "start": 7504560,
    "end": 7505656,
    "text": "これからやることはこうだ。"
  },
  {
    "start": 7505680,
    "end": 7507776,
    "text": "フラッシュアテンションを使うつもりだ。"
  },
  {
    "start": 7507960,
    "end": 7516964,
    "text": "Pytorchでは基本的に、この4行をコメントアウトして1行に置き換える。"
  },
  {
    "start": 7517544,
    "end": 7523044,
    "text": "Pytorchでは、この複合演算をスケールド・プロダクト・アテンション（scaled product attention）と呼んでいる。"
  },
  {
    "start": 7523704,
    "end": 7530574,
    "text": "このような使い方をすると、Pytorchはフラッシュに注意を促す。"
  },
  {
    "start": 7530874,
    "end": 7537082,
    "text": "なぜトーチコンパイルが、この4行がまさにこの方法でフラッシュに注意を喚起するものだと気づかないのか、実は100％わからない。"
  },
  {
    "start": 7537178,
    "end": 7542362,
    "text": "そのためにもう一度やらなければならない。"
  },
  {
    "start": 7542538,
    "end": 7545482,
    "text": "さあ、着いた。"
  },
  {
    "start": 7545658,
    "end": 7552782,
    "text": "この複合アプリを使わなければならないので、トーチ・コンパイルが終わるまで少し待ってみよう。"
  },
  {
    "start": 7552898,
    "end": 7558438,
    "text": "それなら、60万5,661を達成したことを覚えておこう。"
  },
  {
    "start": 7558526,
    "end": 7559174,
    "text": "ここにあるよ。"
  },
  {
    "start": 7559214,
    "end": 7561034,
    "text": "それが予想される損失だ。"
  },
  {
    "start": 7561374,
    "end": 7565334,
    "text": "この変更の前に130ミリ秒を要した。"
  },
  {
    "start": 7565494,
    "end": 7578398,
    "text": "なぜなら、フラッシュ・リテンションはアルゴリズムの書き換えに過ぎず、カーネルが高速化されただけで、実際には計算には何の変化もないからだ。"
  },
  {
    "start": 7578446,
    "end": 7580364,
    "text": "まったく同じ最適化ができるはずだ。"
  },
  {
    "start": 7580744,
    "end": 7582152,
    "text": "だから、僕らはもっと速くなったんだ。"
  },
  {
    "start": 7582248,
    "end": 7588584,
    "text": "約95ミリ秒で6.058を達成した。"
  },
  {
    "start": 7588744,
    "end": 7593840,
    "text": "なるほど、浮動小数点のごまかし要素までは基本的に同じだ。"
  },
  {
    "start": 7594032,
    "end": 7601240,
    "text": "同じ計算だが、130回からおよそ90回96回と大幅に速くなった。"
  },
  {
    "start": 7601432,
    "end": 7604776,
    "text": "つまり、96÷130である。"
  },
  {
    "start": 7604840,
    "end": 7605486,
    "text": "イシュだ。"
  },
  {
    "start": 7605640,
    "end": 7609254,
    "text": "これはおそらく27％程度の改善だろう。"
  },
  {
    "start": 7610194,
    "end": 7612338,
    "text": "実に興味深い。"
  },
  {
    "start": 7612426,
    "end": 7613814,
    "text": "それはフラッシュアテンションである。"
  },
  {
    "start": 7614354,
    "end": 7622214,
    "text": "さて、そろそろ私のお気に入りの最適化のひとつに入るところだが、それは同時に最も間抜けで最も素晴らしい最適化でもある。"
  },
  {
    "start": 7622834,
    "end": 7625174,
    "text": "いつもちょっと驚かされるんだ。"
  },
  {
    "start": 7625754,
    "end": 7626130,
    "text": "とにかくだ。"
  },
  {
    "start": 7626162,
    "end": 7632540,
    "text": "だから、基本的には、数分前に言ったように、いい数字もあれば、醜い数字もある。"
  },
  {
    "start": 7632692,
    "end": 7635892,
    "text": "64は美しく、いい数字だ。"
  },
  {
    "start": 7635988,
    "end": 7637804,
    "text": "128はもっといい。"
  },
  {
    "start": 7637884,
    "end": 7639572,
    "text": "256は美しい。"
  },
  {
    "start": 7639708,
    "end": 7643068,
    "text": "これらの数字が美しいのは、その中に2のべき乗がたくさんあるからだ。"
  },
  {
    "start": 7643116,
    "end": 7644784,
    "text": "2で割ることは何度でもできる。"
  },
  {
    "start": 7645204,
    "end": 7652492,
    "text": "醜い数の例としては、13や17のようなもの、素数、偶数でない数などがある。"
  },
  {
    "start": 7652628,
    "end": 7661996,
    "text": "だから、ニューラルネットワークやCuDAを扱うコードでは、常にきれいな数字を使いたいものだ。"
  },
  {
    "start": 7662100,
    "end": 7665644,
    "text": "多くのカーネルは2のべき乗で書かれている。"
  },
  {
    "start": 7665724,
    "end": 7670212,
    "text": "16や64など、たくさんのサイズのブロックがある。"
  },
  {
    "start": 7670308,
    "end": 7672180,
    "text": "すべてがそういう言葉で書かれている。"
  },
  {
    "start": 7672332,
    "end": 7680132,
    "text": "入力が素敵な数字でできていない場合、あらゆる種類のロジックに対して、常に特別なケース処理が必要になる。"
  },
  {
    "start": 7680268,
    "end": 7681636,
    "text": "それがどんなものか見てみよう。"
  },
  {
    "start": 7681780,
    "end": 7685868,
    "text": "基本的には、コードをスキャンして醜い数字を探すのが、おおよそのヒューリスティックな方法だ。"
  },
  {
    "start": 7686036,
    "end": 7689224,
    "text": "3回はちょっと醜い。"
  },
  {
    "start": 7690284,
    "end": 7691316,
    "text": "100％の確信はない。"
  },
  {
    "start": 7691340,
    "end": 7695064,
    "text": "もしかしたら改善できるかもしれないが、これでは、醜いし、理想的ではない。"
  },
  {
    "start": 7697804,
    "end": 7699184,
    "text": "4回はいいね。"
  },
  {
    "start": 7699524,
    "end": 7701304,
    "text": "それはいいね。"
  },
  {
    "start": 7702484,
    "end": 7704148,
    "text": "1024はとてもいい。"
  },
  {
    "start": 7704196,
    "end": 7707384,
    "text": "212のパワーはちょっと怪しい。"
  },
  {
    "start": 7708164,
    "end": 7710722,
    "text": "2768の何乗もないのは素晴らしい。"
  },
  {
    "start": 7710868,
    "end": 7714154,
    "text": "50,257は本当に、本当に醜い数字だ。"
  },
  {
    "start": 7715894,
    "end": 7720478,
    "text": "まず第一に、奇妙だし、2のべき乗があまり入っていない。"
  },
  {
    "start": 7720606,
    "end": 7724638,
    "text": "これは非常に醜い数字であり、非常に疑わしい。"
  },
  {
    "start": 7724806,
    "end": 7727594,
    "text": "そして、スクロールダウンすると、これらの数字はすべて素敵なものだ。"
  },
  {
    "start": 7728014,
    "end": 7732630,
    "text": "となると、25番以外はほとんどいい数字だ。"
  },
  {
    "start": 7732782,
    "end": 7736378,
    "text": "つまり、このGPT-2 xLの構成では、ヘッド数は25である。"
  },
  {
    "start": 7736526,
    "end": 7737690,
    "text": "本当に醜い数字だ。"
  },
  {
    "start": 7737762,
    "end": 7738922,
    "text": "奇数だね。"
  },
  {
    "start": 7739058,
    "end": 7748694,
    "text": "実は最近、あるカーネルをこれだけ高速に走らせるために最適化しようとしているときに、特殊なケースの処理がたくさん必要になり、これが頭痛の種になった。"
  },
  {
    "start": 7748994,
    "end": 7750874,
    "text": "つまり、基本的にこの数字は"
  },
  {
    "start": 7751034,
    "end": 7754338,
    "text": "醜い数字もあるし、修正しやすい数字もある。"
  },
  {
    "start": 7754466,
    "end": 7760642,
    "text": "特に、ボキャブラリーのサイズが50,257というのは非常に醜い数字で、非常に疑わしい。"
  },
  {
    "start": 7760818,
    "end": 7761474,
    "text": "さて、そのとき。"
  },
  {
    "start": 7761514,
    "end": 7770742,
    "text": "このようなことを修正する場合、簡単な方法のひとつは、基本的に最も近い2のべき乗になるまで数字を増やすことだ。"
  },
  {
    "start": 7770878,
    "end": 7772262,
    "text": "もっといい数字がある。"
  },
  {
    "start": 7772318,
    "end": 7774254,
    "text": "50,304だ。"
  },
  {
    "start": 7774414,
    "end": 7775782,
    "text": "なぜだろう？"
  },
  {
    "start": 7775958,
    "end": 7783314,
    "text": "50,304は8で割ったり、16で割ったり、32や64で割ったりできるからだ。"
  },
  {
    "start": 7784054,
    "end": 7786078,
    "text": "128で割ることもできると思う。"
  },
  {
    "start": 7786166,
    "end": 7788754,
    "text": "うん、だからとてもいい数字だよ。"
  },
  {
    "start": 7789594,
    "end": 7792362,
    "text": "ここで行うのはGPT設定だ。"
  },
  {
    "start": 7792418,
    "end": 7795578,
    "text": "ボウキャップサイズを50,257に初期化しているのがわかるだろう。"
  },
  {
    "start": 7795746,
    "end": 7801334,
    "text": "その要素だけを3304にオーバーライドしてみよう。"
  },
  {
    "start": 7802514,
    "end": 7803374,
    "text": "オーケー。"
  },
  {
    "start": 7803794,
    "end": 7805530,
    "text": "それ以外は何も変わらない。"
  },
  {
    "start": 7805602,
    "end": 7807994,
    "text": "語彙を増やしているだけだ。"
  },
  {
    "start": 7808154,
    "end": 7814984,
    "text": "VCAPのサイズに2の累乗が入るように、偽のトークンを追加しているようなものだ。"
  },
  {
    "start": 7815154,
    "end": 7820572,
    "text": "ところで、私がここでやっているのは、ネットワークの計算量を増やすことだ。"
  },
  {
    "start": 7820668,
    "end": 7826372,
    "text": "もしフロップを数えるだけなら、僕らがやっているフロップの数を計算すれば、もっとフロップが増えることになる。"
  },
  {
    "start": 7826508,
    "end": 7830988,
    "text": "これで何も壊れないかどうかは、まだ考えなければならない。"
  },
  {
    "start": 7831156,
    "end": 7834224,
    "text": "これを実行したら、何が出てくるか見てみよう。"
  },
  {
    "start": 7834524,
    "end": 7840960,
    "text": "現在のところ、これは1ステップあたり96.5ミリ秒で実行されている。"
  },
  {
    "start": 7841112,
    "end": 7845164,
    "text": "僕はただ、どんな結果が出るか見てみようという感じなんだ。"
  },
  {
    "start": 7847384,
    "end": 7851704,
    "text": "コンパイルしている間に、このコードが実際に動くかどうか考えてみよう。"
  },
  {
    "start": 7851744,
    "end": 7852136,
    "text": "オーケー。"
  },
  {
    "start": 7852200,
    "end": 7857244,
    "text": "このようにボキャブラリーを増やした場合、実際にボキャブラリーがどこで使われるのかを見てみよう。"
  },
  {
    "start": 7858064,
    "end": 7862576,
    "text": "initまで振ってみると、もちろん埋め込みテーブルの中で使われていることがわかる。"
  },
  {
    "start": 7862640,
    "end": 7864424,
    "text": "トランスの底のほうまで。"
  },
  {
    "start": 7864544,
    "end": 7867846,
    "text": "これは、トランスフォーマーの一番上にある分類器層で使われる。"
  },
  {
    "start": 7867870,
    "end": 7869034,
    "text": "を2カ所に配置した。"
  },
  {
    "start": 7869694,
    "end": 7870686,
    "text": "見てみよう。"
  },
  {
    "start": 7870750,
    "end": 7872358,
    "text": "我々は93で走っている。"
  },
  {
    "start": 7872486,
    "end": 7875714,
    "text": "96.5秒ではなく93ミリ秒。"
  },
  {
    "start": 7876054,
    "end": 7883510,
    "text": "より多くの計算をすることで、およそ4％の改善が見られる。"
  },
  {
    "start": 7883702,
    "end": 7885990,
    "text": "その理由は、私たちが修正したからです。"
  },
  {
    "start": 7886102,
    "end": 7889114,
    "text": "私たちは醜い数字を素敵な数字に変えた。"
  },
  {
    "start": 7889574,
    "end": 7890110,
    "text": "そうしよう。"
  },
  {
    "start": 7890182,
    "end": 7893224,
    "text": "それについては、また少し説明するつもりだ。"
  },
  {
    "start": 7893344,
    "end": 7896624,
    "text": "とりあえず、これをやっても何も壊れないと自分を納得させよう。"
  },
  {
    "start": 7896704,
    "end": 7901136,
    "text": "そこでまず、トークンの埋め込みテーブルをw teにした。"
  },
  {
    "start": 7901160,
    "end": 7902280,
    "text": "我々はそれを大きくした。"
  },
  {
    "start": 7902392,
    "end": 7912584,
    "text": "GPTトークナイザーには50,256までのトークンしかないため、トークンが使われることはない。"
  },
  {
    "start": 7912744,
    "end": 7916640,
    "text": "そのため、追加した行にインデックスを付けることはない。"
  },
  {
    "start": 7916792,
    "end": 7922914,
    "text": "アクセスされることも使われることもないメモリを作ることで、スペースを少し無駄にしている。"
  },
  {
    "start": 7923294,
    "end": 7931110,
    "text": "というのも、このw個の重みは結局共有され、最後に分類器で使われることになるからだ。"
  },
  {
    "start": 7931302,
    "end": 7934582,
    "text": "その結果、クラシファイアはどうなった？"
  },
  {
    "start": 7934718,
    "end": 7943314,
    "text": "そして、もちろんトレーニングセットには存在しないトークンの確率を予測している。"
  },
  {
    "start": 7945024,
    "end": 7950216,
    "text": "したがって、ネットワークはこれらの確率がゼロになるように学習しなければならない。"
  },
  {
    "start": 7950400,
    "end": 7956124,
    "text": "従って、ネットワークが生成するロジットは、出力のこれらの次元を負の無限大に追い込まなければならない。"
  },
  {
    "start": 7956544,
    "end": 7963768,
    "text": "それは、すでにデータセットに入っている、いや、むしろデータセットに入っていない他のすべてのトークンと変わらない。"
  },
  {
    "start": 7963896,
    "end": 7969192,
    "text": "シェイクスピアはおそらく、50,257個のトークンのうち1000個しか使っていない。"
  },
  {
    "start": 7969288,
    "end": 7973044,
    "text": "ほとんどのトークンは、最適化によってすでに確率ゼロに追い込まれている。"
  },
  {
    "start": 7973184,
    "end": 7979784,
    "text": "同様の方法で、決して使われることのない、確率的にゼロに追い込まれなければならないトークンをいくつか導入したばかりだ。"
  },
  {
    "start": 7981284,
    "end": 7983724,
    "text": "機能的には、何も壊れていないが。"
  },
  {
    "start": 7983804,
    "end": 7993944,
    "text": "メモリは少し余計に使っているが、それ以外は私の知る限り無害な処理だ。"
  },
  {
    "start": 7994284,
    "end": 8001032,
    "text": "Cudaで述べたように、多くのカーネルがブロックタイルを使用しているため、より高速に動作している。"
  },
  {
    "start": 8001128,
    "end": 8004920,
    "text": "これらのブロックタイルは通常、2のべき乗の数字である。"
  },
  {
    "start": 8005032,
    "end": 8008672,
    "text": "計算は64のチャンクか32のチャンクのように行われる。"
  },
  {
    "start": 8008808,
    "end": 8021808,
    "text": "目的の計算がブロックタイルにうまく収まらない場合、最後の部分を実行するようなあらゆる種類の境界カーネルがある。"
  },
  {
    "start": 8021936,
    "end": 8035956,
    "text": "基本的に、多くのカーネルでは、入力を切り分けて、まず良い部分を処理し、次に、残った部分を処理する第二段階がある。"
  },
  {
    "start": 8036060,
    "end": 8037924,
    "text": "そのためのカーネルは非常に非効率的かもしれない。"
  },
  {
    "start": 8038044,
    "end": 8043692,
    "text": "そのため、基本的に余分なコンピュートをすべて回転させることになり、非効率極まりない。"
  },
  {
    "start": 8043788,
    "end": 8048412,
    "text": "入力にパッドを入れて、うまくフィットさせたほうがいい。"
  },
  {
    "start": 8048508,
    "end": 8051454,
    "text": "通常、経験則は実際に速く走ることになる。"
  },
  {
    "start": 8053074,
    "end": 8057666,
    "text": "これも4％改善した例である。"
  },
  {
    "start": 8057810,
    "end": 8061210,
    "text": "これはトーチコンパイルでも見つからなかったものだ。"
  },
  {
    "start": 8061362,
    "end": 8067706,
    "text": "いずれトーチ・コンパイルがこのような最適化を解明してくれることを期待したいが、今のところはこれだけだ。"
  },
  {
    "start": 8067850,
    "end": 8072298,
    "text": "PyTorchをナイトリーで使っていることも指摘しておかなければならない。"
  },
  {
    "start": 8072426,
    "end": 8084354,
    "text": "PyTorch 2.3.1またはそれ以前を使用している場合、この変更だけで、50,000→57→5304と、30％ほどの改善が見られます。"
  },
  {
    "start": 8084694,
    "end": 8095150,
    "text": "また、私の好きな例のひとつでもあるが、フードの下を理解し、それがどのように機能しているかを理解し、コードのパフォーマンスを上げるためにどのようなことをいじればいいかを知る必要がある。"
  },
  {
    "start": 8095342,
    "end": 8099086,
    "text": "さて、この時点でパフォーマンスは約11倍向上しているよね？"
  },
  {
    "start": 8099110,
    "end": 8104558,
    "text": "というのも、1ステップあたり約1000ミリ秒からスタートし、現在は93ミリ秒まで短縮しているからだ。"
  },
  {
    "start": 8104726,
    "end": 8106502,
    "text": "なかなかいいね。"
  },
  {
    "start": 8106598,
    "end": 8110274,
    "text": "GPUのリソースをよりうまく活用できるようになった。"
  },
  {
    "start": 8110574,
    "end": 8116966,
    "text": "ここからは、よりアルゴリズム的な変更と、実際の最適化そのものの改善についてお話しします。"
  },
  {
    "start": 8117150,
    "end": 8123710,
    "text": "GPT-2やGPT-3の論文で言及されているハイパーパラメーターに従いたい。"
  },
  {
    "start": 8123902,
    "end": 8128114,
    "text": "さて、悲しいかな、GPT-2は、実はあまり多くを語っていない。"
  },
  {
    "start": 8128614,
    "end": 8135222,
    "text": "モデルウェイトとコードを公開したのはとても良いことだが、論文自体は最適化の詳細について極めて曖昧だ。"
  },
  {
    "start": 8135398,
    "end": 8141046,
    "text": "彼らが公開したコード自体も、我々が見てきたコードも、これは推論コードに過ぎない。"
  },
  {
    "start": 8141110,
    "end": 8144014,
    "text": "ここにはトレーニングコードはなく、ハイパーパラメーターもほとんどない。"
  },
  {
    "start": 8144094,
    "end": 8145902,
    "text": "これもあまり多くを語ってはいない。"
  },
  {
    "start": 8146038,
    "end": 8148726,
    "text": "そのためには、GPT-3の論文に目を向ける必要がある。"
  },
  {
    "start": 8148910,
    "end": 8156650,
    "text": "GPT-3論文の付録には、さらに多くのハイパーパラメーターが掲載されている。"
  },
  {
    "start": 8156762,
    "end": 8164818,
    "text": "GPT-3の論文は一般的に、モデルトレーニングに必要な細部まで詳細に書かれている。"
  },
  {
    "start": 8164946,
    "end": 8167890,
    "text": "GPT-3モデルは発売されなかった。"
  },
  {
    "start": 8168002,
    "end": 8170634,
    "text": "GPT-2はウェイトはあるが、詳細は不明。"
  },
  {
    "start": 8170674,
    "end": 8173334,
    "text": "GPT-3ではディテールはたくさんあるが、ウェイトはない。"
  },
  {
    "start": 8174714,
    "end": 8178814,
    "text": "大雑把に言えば、GPT-2とGPT-3のアーキテクチャは非常によく似ている。"
  },
  {
    "start": 8179214,
    "end": 8182334,
    "text": "基本的にはほとんど変更はない。"
  },
  {
    "start": 8182374,
    "end": 8187878,
    "text": "コンテキストの長さが1024から2048に拡張され、それが大きな変更点のようなものだ。"
  },
  {
    "start": 8188046,
    "end": 8192718,
    "text": "トランスフォーマー周辺のハイパーパラメーターの一部が変更されたが、それ以外はほとんど同じモデルだ。"
  },
  {
    "start": 8192806,
    "end": 8199154,
    "text": "ただ、GPT-3はより大きなデータセットでより長く訓練され、より綿密な評価が行われている。"
  },
  {
    "start": 8199774,
    "end": 8207330,
    "text": "GPT-2の16億に対し、GPT-3は1750億。"
  },
  {
    "start": 8207482,
    "end": 8212026,
    "text": "というわけで、長くなったが、ハイパーパラメーターのいくつかをフォローするためにGPT-3ペーパーに行くことにしよう。"
  },
  {
    "start": 8212170,
    "end": 8219298,
    "text": "そこで、GPT-3のすべてのバージョンを訓練するために、ベータ1、ベータ2が0.9と0.95のアトムを使用する。"
  },
  {
    "start": 8219466,
    "end": 8231094,
    "text": "ここで、デフォルトが0.9と0.999であるbetasパラメータが、実際には0.9と0.95に設定されていることを確認しよう。"
  },
  {
    "start": 8232264,
    "end": 8237424,
    "text": "εパラメータは、デフォルトでマイナス8分の1であることがわかる。"
  },
  {
    "start": 8237504,
    "end": 8238912,
    "text": "これもマイナス8点だ。"
  },
  {
    "start": 8238968,
    "end": 8241524,
    "text": "はっきりさせるために入れておこう。"
  },
  {
    "start": 8242984,
    "end": 8248440,
    "text": "さて次は、勾配のグローバルノルムを1.0で切り取るという。"
  },
  {
    "start": 8248632,
    "end": 8257952,
    "text": "つまり、ロス・バックの直後に勾配を計算すれば、基本的にすべてのパラメータ・テンソルの勾配がわかるということだ。"
  },
  {
    "start": 8258088,
    "end": 8264164,
    "text": "人々が好むのは、基本的にクリップで留めて、ある種の最大規範を持つことだ。"
  },
  {
    "start": 8264744,
    "end": 8266964,
    "text": "Pytorchでは、これはかなり簡単にできる。"
  },
  {
    "start": 8267464,
    "end": 8271804,
    "text": "これは、グラデーションを計算した直後に挿入する1行のコードだ。"
  },
  {
    "start": 8272144,
    "end": 8278984,
    "text": "この効用関数がやっていることは、パラメーターのグローバルノルムを計算することだ。"
  },
  {
    "start": 8279064,
    "end": 8290527,
    "text": "すべてのパラメーターの勾配を二乗して、それをすべて足して、その平方根をとれば、それがパラメーター・ベクトルのノルムになる。"
  },
  {
    "start": 8290575,
    "end": 8295407,
    "text": "基本的には、その長さなんだ。"
  },
  {
    "start": 8295535,
    "end": 8300991,
    "text": "基本的には、長さが1.0以下であることを確認し、それをクリップする。"
  },
  {
    "start": 8301167,
    "end": 8306607,
    "text": "これを好んで使う理由は、最適化中に不運に見舞われることがあるからだ。"
  },
  {
    "start": 8306735,
    "end": 8309219,
    "text": "たぶん、データのバッチか何かが悪いんだろう。"
  },
  {
    "start": 8309351,
    "end": 8313188,
    "text": "バッチが非常に不運であれば、本当に高いロスが出るかもしれない。"
  },
  {
    "start": 8313316,
    "end": 8316036,
    "text": "本当に高い損失は、本当に高い勾配につながる可能性がある。"
  },
  {
    "start": 8316180,
    "end": 8321004,
    "text": "これは基本的に、モデルに衝撃を与え、最適化に衝撃を与える可能性がある。"
  },
  {
    "start": 8321164,
    "end": 8333324,
    "text": "勾配ノルムのクリッピングは、モデルが勾配の大きさと上限の点で、基本的に衝撃が大きくなりすぎるのを防ぐために使用される。"
  },
  {
    "start": 8333364,
    "end": 8338864,
    "text": "こうしてみると、深い問題の上にパッチを貼ったような、ちょっとやっつけ仕事のような解決策だ。"
  },
  {
    "start": 8339284,
    "end": 8341844,
    "text": "今でも頻繁にやっている。"
  },
  {
    "start": 8342004,
    "end": 8351076,
    "text": "さて、clip grad normはグラデーションのノルムを返しますが、これは有用な情報なので常に視覚化しておきたいと思います。"
  },
  {
    "start": 8351220,
    "end": 8356180,
    "text": "グラデーションのノルムを見ることができる。"
  },
  {
    "start": 8356252,
    "end": 8364242,
    "text": "クライミングで調子が悪く、トレーニング中に不安定になるような場合、ノルムが急上昇することがあるが、それは何らかの問題や不安定さがあることを意味する。"
  },
  {
    "start": 8364418,
    "end": 8374334,
    "text": "ここでのノルムはノルムとなり、0.4fとかにしよう。"
  },
  {
    "start": 8375514,
    "end": 8377174,
    "text": "これは単なる浮き輪だと思う。"
  },
  {
    "start": 8377594,
    "end": 8380294,
    "text": "だから印刷できるはずだ。"
  },
  {
    "start": 8381554,
    "end": 8384254,
    "text": "これがグローバルグラデーションクリッピングだ。"
  },
  {
    "start": 8385314,
    "end": 8388932,
    "text": "ここで、学習率スケジューラーの詳細に入る。"
  },
  {
    "start": 8389068,
    "end": 8393204,
    "text": "3eマイナス、4eのような固定された学習率を使うわけではない。"
  },
  {
    "start": 8393324,
    "end": 8397944,
    "text": "基本的には、コサイン減衰の学習レートスケジュールがある。"
  },
  {
    "start": 8399244,
    "end": 8405224,
    "text": "ウォームアップがあり、コサイン減衰があり、ある水平線上で10％になる。"
  },
  {
    "start": 8407324,
    "end": 8410476,
    "text": "だから、すぐに実装するつもりだ。"
  },
  {
    "start": 8410620,
    "end": 8412540,
    "text": "私はただ、ここに印刷された規範を見たいだけなんだ。"
  },
  {
    "start": 8412612,
    "end": 8413624,
    "text": "よし、行くぞ。"
  },
  {
    "start": 8414224,
    "end": 8418888,
    "text": "ここで何が起こったかというと、実は最初のうちはノルマが本当に高くて、30くらいなんだ。"
  },
  {
    "start": 8419056,
    "end": 8426724,
    "text": "トレーニングを続けると、1より下の値で安定するのがわかるだろう。"
  },
  {
    "start": 8427224,
    "end": 8429496,
    "text": "これはそれほどクレイジーではない。"
  },
  {
    "start": 8429520,
    "end": 8432496,
    "text": "最初の数ステージでノルムが高いのは珍しい。"
  },
  {
    "start": 8432680,
    "end": 8435008,
    "text": "基本的に、ここで起こっているのは、モデルが完全にランダムだということだ。"
  },
  {
    "start": 8435096,
    "end": 8438184,
    "text": "そのため、ネットワークのごく初期段階で、多くの学習が行われている。"
  },
  {
    "start": 8438304,
    "end": 8444064,
    "text": "その学習は、出力トークンのバイアスを学習するようなものだ。"
  },
  {
    "start": 8444184,
    "end": 8449404,
    "text": "というわけで、少し不安定な時間だが、ネットワークは通常、数回の反復で安定する。"
  },
  {
    "start": 8449864,
    "end": 8457684,
    "text": "通常なら、28から6、2、そして10と、少しファンキーに見えるだろうが。"
  },
  {
    "start": 8458624,
    "end": 8462564,
    "text": "完全に狂っているわけではないが、ちょっとファンキーな感じだ。"
  },
  {
    "start": 8463704,
    "end": 8466348,
    "text": "さて、それでは学習リスケジューラーに取りかかろう。"
  },
  {
    "start": 8466456,
    "end": 8474172,
    "text": "ここでGPT-3で使われている学習率スケジュールは、ウォームアップを伴うコサイン減衰学習スケジュールと呼ばれるものだ。"
  },
  {
    "start": 8474308,
    "end": 8488956,
    "text": "このように見えるのは、学習率が基本的にゼロ付近から始まり、ある程度の時間をかけて直線的に上昇し、そしてコサインのような形で下降し、ある種の最小学習率に至るということだ。"
  },
  {
    "start": 8488980,
    "end": 8489700,
    "text": "それはあなた次第だ。"
  },
  {
    "start": 8489772,
    "end": 8491904,
    "text": "ここで、最小学習率はゼロである。"
  },
  {
    "start": 8492224,
    "end": 8501480,
    "text": "この論文では、最初の2,600億トークンの間、コサイン減衰を使って学習率を10％まで下げると述べている。"
  },
  {
    "start": 8501672,
    "end": 8509084,
    "text": "最初の3億7,500万トークンにかけて、直線的なウォームアップがある。"
  },
  {
    "start": 8509664,
    "end": 8510816,
    "text": "これが学習率だ。"
  },
  {
    "start": 8510840,
    "end": 8512524,
    "text": "では、これを実装してみよう。"
  },
  {
    "start": 8512944,
    "end": 8515336,
    "text": "私はすでにここでそれを実行した。"
  },
  {
    "start": 8515440,
    "end": 8519020,
    "text": "この仕組みは、まず下にスクロールしてみよう。"
  },
  {
    "start": 8519132,
    "end": 8520756,
    "text": "トレーニングのループを少し変えた。"
  },
  {
    "start": 8520780,
    "end": 8523316,
    "text": "これはマックスステップの4Iだった。"
  },
  {
    "start": 8523380,
    "end": 8530676,
    "text": "ステップとは、forループの中の1つの最適化ステップという概念を持つように、今はステップに変えただけだ。"
  },
  {
    "start": 8530820,
    "end": 8537372,
    "text": "次に、get LRと呼ぶ新しい関数を使って、最適化のこのステップのLRを求める。"
  },
  {
    "start": 8537548,
    "end": 8539804,
    "text": "で、Pytorchで学習率を設定する。"
  },
  {
    "start": 8539884,
    "end": 8542004,
    "text": "これが学習率を設定する方法だと思う。"
  },
  {
    "start": 8542084,
    "end": 8550130,
    "text": "少し厄介なのは、基本的に、オプティマイザにはさまざまなパラメータ・グループが存在する可能性があり、それらを実際に反復しなければならないからだ。"
  },
  {
    "start": 8550162,
    "end": 8553254,
    "text": "現在、パラグループは1つしかないのに。"
  },
  {
    "start": 8553754,
    "end": 8558654,
    "text": "このforループのようなスタイルでLRを設定しなければならないというのが、今の私の印象だ。"
  },
  {
    "start": 8559034,
    "end": 8564574,
    "text": "このローカルLRで学習率を設定し、一番下に印刷する。"
  },
  {
    "start": 8565354,
    "end": 8567602,
    "text": "このループに加えた変更は以上だ。"
  },
  {
    "start": 8567698,
    "end": 8570308,
    "text": "そしてもちろん、LRは私のスケジューラーだ。"
  },
  {
    "start": 8570466,
    "end": 8575136,
    "text": "さて、Pytorchには実際に学習率スケジューラがあり、それを使うことができることを指摘しておく。"
  },
  {
    "start": 8575240,
    "end": 8578120,
    "text": "Pytorchにはコサイン学習率のスケジュールがあると思う。"
  },
  {
    "start": 8578232,
    "end": 8584200,
    "text": "正直なところ、5行程度のコードなので、このコードを使うのはあまり好きではないんだ。"
  },
  {
    "start": 8584312,
    "end": 8587608,
    "text": "このラインの内側で何が起きているのか、私は十分に理解している。"
  },
  {
    "start": 8587656,
    "end": 8593528,
    "text": "抽象的なものを使うのは好きではない。"
  },
  {
    "start": 8593576,
    "end": 8595564,
    "text": "だから、個人的なスタイルだ。"
  },
  {
    "start": 8595884,
    "end": 8599356,
    "text": "ここでの最大学習率は、仮に3Eマイナス4としよう。"
  },
  {
    "start": 8599460,
    "end": 8608420,
    "text": "GPT-3には、モデルサイズごとの最大学習率の表がある。"
  },
  {
    "start": 8608492,
    "end": 8620900,
    "text": "これは基本的に1212レイヤー768、GPT-3なので、GPT-3スモールはGPT2124 mのようなものだ。"
  },
  {
    "start": 8621092,
    "end": 8623924,
    "text": "ここでは学習率を6、6eマイナス4としていることがわかる。"
  },
  {
    "start": 8623964,
    "end": 8625344,
    "text": "私たちはもっと上に行ける"
  },
  {
    "start": 8625724,
    "end": 8633332,
    "text": "実際、それに倣って、最大学習率を6に設定することもできる。"
  },
  {
    "start": 8633388,
    "end": 8638304,
    "text": "最小学習率は論文の記述の10％。"
  },
  {
    "start": 8638724,
    "end": 8646504,
    "text": "ウォームアップするステップ数、そして最適化の最大ステップ数。"
  },
  {
    "start": 8647044,
    "end": 8648964,
    "text": "そうしたら、このコードを見直してもいい。"
  },
  {
    "start": 8649004,
    "end": 8652134,
    "text": "洞察力に欠けるし、面白くもない。"
  },
  {
    "start": 8652254,
    "end": 8657614,
    "text": "私はただ、反復回数に基づいて、どの学習率があるべきかを調節しているだけだ。"
  },
  {
    "start": 8657694,
    "end": 8659234,
    "text": "これはウォームアップの領域だ。"
  },
  {
    "start": 8661094,
    "end": 8663446,
    "text": "これは最適化後の領域である。"
  },
  {
    "start": 8663630,
    "end": 8665550,
    "text": "となると、ここはその中間の地域ということになる。"
  },
  {
    "start": 8665702,
    "end": 8668934,
    "text": "ここでコサイン学習率のスケジュールを計算する。"
  },
  {
    "start": 8669014,
    "end": 8671206,
    "text": "よかったら、詳しく説明してほしい。"
  },
  {
    "start": 8671390,
    "end": 8673794,
    "text": "これは基本的に、このカーブを実施するものだ。"
  },
  {
    "start": 8674574,
    "end": 8679484,
    "text": "すでに実行したが、こんな感じだ。"
  },
  {
    "start": 8681864,
    "end": 8687056,
    "text": "今走ると、非常に低い数値からスタートする。"
  },
  {
    "start": 8687160,
    "end": 8691824,
    "text": "学習率がゼロのまま更新しても意味がないからだ。"
  },
  {
    "start": 8691944,
    "end": 8693920,
    "text": "だからイット・プラス・ワンなんだ。"
  },
  {
    "start": 8693992,
    "end": 8699044,
    "text": "0回目の反復では、正確にはゼロではなく、非常に低い値を使用している。"
  },
  {
    "start": 8699384,
    "end": 8706694,
    "text": "そして、最大学習率までリニアにウォームアップする。この場合、私が走らせたときは3eマイナス4だったが、今は6eマイナス4になる。"
  },
  {
    "start": 8706854,
    "end": 8715550,
    "text": "その後、3eマイナス5まで減衰し始めるが、これは当時、元の学習率の10％であった。"
  },
  {
    "start": 8715742,
    "end": 8719194,
    "text": "今、私たちが正確に理解していないことがひとつある。"
  },
  {
    "start": 8721214,
    "end": 8722462,
    "text": "探してみるよ。"
  },
  {
    "start": 8722478,
    "end": 8740612,
    "text": "トレーニングのホライズンは3,000億トークンで、2,600億トークンで最初の学習率の10%まで下げ、260億トークン以降は10%でトレーニングしているとのことです。"
  },
  {
    "start": 8740788,
    "end": 8746484,
    "text": "基本的に、彼らの減衰時間は最大ステップ時間よりも短い。"
  },
  {
    "start": 8746604,
    "end": 8751624,
    "text": "致命的というわけではないが、まあ大丈夫だろう。"
  },
  {
    "start": 8752284,
    "end": 8758592,
    "text": "今の僕らの目的にはこれでいいんだ。"
  },
  {
    "start": 8758648,
    "end": 8761120,
    "text": "正直なところ、あまり大きな違いはないと思う。"
  },
  {
    "start": 8761272,
    "end": 8765472,
    "text": "どのような学習率スケジュールを使うかは、完全にあなた次第だということを指摘しておく。"
  },
  {
    "start": 8765528,
    "end": 8766924,
    "text": "いろいろなタイプがある。"
  },
  {
    "start": 8767864,
    "end": 8774804,
    "text": "コサイン学習率はGPT-2やGPT-3によって一般化されたが、他にも様々な学習率スケジュールが考案されている。"
  },
  {
    "start": 8775184,
    "end": 8781344,
    "text": "このようなネットワークのトレーニングには、どれが最も効果的なのか、活発な研究が行われている。"
  },
  {
    "start": 8781504,
    "end": 8786502,
    "text": "さて、次はバッチサイズを徐々に大きくしていくことについての論文だ。"
  },
  {
    "start": 8786638,
    "end": 8794234,
    "text": "バッチサイズには直線的な傾斜があり、非常に小さなバッチサイズから始めて、時間の経過とともに大きなバッチサイズへと傾斜していく。"
  },
  {
    "start": 8794614,
    "end": 8797550,
    "text": "私たちはこれをスキップし、一緒に仕事をするつもりはない。"
  },
  {
    "start": 8797662,
    "end": 8806238,
    "text": "なぜなら、最適化のステップごとに処理するトークンの数を変えることになるからだ。"
  },
  {
    "start": 8806406,
    "end": 8809106,
    "text": "私はその計算をとてもとてもとてもシンプルにしたい。"
  },
  {
    "start": 8809250,
    "end": 8813322,
    "text": "また、私の理解では、これは大きな改善という感じではない。"
  },
  {
    "start": 8813458,
    "end": 8817650,
    "text": "また、私の理解では、これはアルゴリズムによる最適化の改善とは違う。"
  },
  {
    "start": 8817682,
    "end": 8820250,
    "text": "システムとスピードの改善という意味合いが強い。"
  },
  {
    "start": 8820402,
    "end": 8829914,
    "text": "大雑把に言えば、最適化の初期段階では、やはりモデルは非常に非典型的な設定になっているからだ。"
  },
  {
    "start": 8830034,
    "end": 8836540,
    "text": "あなたが学んでいるのは、トレーニングセットで出てこないトークンを無視することだ。"
  },
  {
    "start": 8836572,
    "end": 8841524,
    "text": "非常に単純な偏見やそういうものを学ぶことが多い。"
  },
  {
    "start": 8841684,
    "end": 8849276,
    "text": "だから、あなたのネットワークにあるすべての例は、基本的に、これらのトークンを使え、そしてこれらのトークンを使うな、と言っているにすぎない。"
  },
  {
    "start": 8849380,
    "end": 8853140,
    "text": "つまり、すべての例からの勾配は、実際には極めて高い相関性を持っているのだ。"
  },
  {
    "start": 8853212,
    "end": 8861674,
    "text": "なぜなら、これらのトークンはすべて、これらのトークンは出現せず、これらのトークンは出現するということを伝えているだけだからだ。"
  },
  {
    "start": 8862014,
    "end": 8877534,
    "text": "バッチサイズを32kにすれば、トレーニングの初期段階と最適化の後期で、基本的にまったく同じ勾配を得ることができる。"
  },
  {
    "start": 8877614,
    "end": 8889504,
    "text": "単純なことを一通り学んだら、そこからが本番で、例によってグラデーションがより装飾的になり、ある意味で統計的な力を発揮するようになる。"
  },
  {
    "start": 8890604,
    "end": 8893584,
    "text": "ややこしくなるので省略する。"
  },
  {
    "start": 8894204,
    "end": 8899824,
    "text": "トレーニングの間、データは置換なしでサンプリングされる。"
  },
  {
    "start": 8900964,
    "end": 8903076,
    "text": "エポック境界に達するまで。"
  },
  {
    "start": 8903180,
    "end": 8913020,
    "text": "無置換というのは、ある固定されたプールからサンプリングするのではなく、シーケンスを採取して、それを使って訓練し、さらにプールのシーケンスを返すようなものだ。"
  },
  {
    "start": 8913132,
    "end": 8914532,
    "text": "彼らはプールを疲弊させている。"
  },
  {
    "start": 8914588,
    "end": 8919212,
    "text": "シーケンスを描くと、次のトレーニングのエポックまで消えてしまう。"
  },
  {
    "start": 8919388,
    "end": 8926772,
    "text": "データ・ローダーはデータのチャンクを繰り返し処理するので、その代わりはない。"
  },
  {
    "start": 8926868,
    "end": 8930636,
    "text": "次のエポックまで、再び抽選の対象となることはない。"
  },
  {
    "start": 8930740,
    "end": 8932744,
    "text": "我々は基本的にすでにそうしている。"
  },
  {
    "start": 8935404,
    "end": 8939464,
    "text": "すべてのモデルで重み減衰0.1を使用し、少量の正則化を行う。"
  },
  {
    "start": 8940004,
    "end": 8941948,
    "text": "ウェイト減衰を実装しよう。"
  },
  {
    "start": 8942076,
    "end": 8944468,
    "text": "ここにあるように、私はすでに変更を加えている。"
  },
  {
    "start": 8944596,
    "end": 8956340,
    "text": "特に、ここでオプティマイザーを作成する代わりに、モデル内部で新しいconfigureoptimizers関数を作成し、代わりにハイパーパラメーターを渡している。"
  },
  {
    "start": 8956532,
    "end": 8960664,
    "text": "オプティマイザー・オブジェクトを返すことになっているconfigure optimizersを見てみよう。"
  },
  {
    "start": 8965524,
    "end": 8968420,
    "text": "複雑そうに見えて、実は簡単なんだ。"
  },
  {
    "start": 8968492,
    "end": 8973236,
    "text": "ただ、慎重にやっているだけだし、いくつか設定項目があるんだ。"
  },
  {
    "start": 8973380,
    "end": 8989064,
    "text": "この行に関して最も重要なのは、ここにウェイト減衰のパラメーターがあることだ。これをoptim groupsというものに渡しているのだが、最終的にはatom wのオプティマイザーに行くことになる。"
  },
  {
    "start": 8989964,
    "end": 8994146,
    "text": "アダムWでデフォルトで使用されているウェイト減衰は0.01だ。"
  },
  {
    "start": 8994220,
    "end": 8999154,
    "text": "GPT-3の論文で使われているものより10倍も低い。"
  },
  {
    "start": 9001294,
    "end": 9005182,
    "text": "ウェイトは、基本的にはオプティマイザー・グループを通してアトムWに入ることになる。"
  },
  {
    "start": 9005278,
    "end": 9008054,
    "text": "さて、この機能で他に何が起こっているのか？"
  },
  {
    "start": 9008174,
    "end": 9016502,
    "text": "ここで重要なのは、パラメーターを減衰させるべきものとそうでないものに分けていることだ。"
  },
  {
    "start": 9016638,
    "end": 9024828,
    "text": "特に、ディケイ・バイアスやその他の一次元テンソルを待たないのが一般的である。"
  },
  {
    "start": 9024996,
    "end": 9033132,
    "text": "1次元テンソルはノード崩壊パラメータにあり、これらはレイヤーのノルムスケールやバイアスのようなものでもある。"
  },
  {
    "start": 9033228,
    "end": 9035244,
    "text": "それを減量するのはあまり意味がない。"
  },
  {
    "start": 9035364,
    "end": 9043824,
    "text": "ほとんどの場合、行列の乗算に参加する重みを減衰させたいし、埋め込みを減衰させる可能性もある。"
  },
  {
    "start": 9044244,
    "end": 9061234,
    "text": "ウェイトを減衰させることになぜ意味があるのかは、以前のビデオで説明しました。ウェイトをすべて引き下げると、最適化により多くのウェイトを使わざるを得なくなり、個々のウェイトが大きくなりすぎるのを防ぐことができるからです。"
  },
  {
    "start": 9063054,
    "end": 9070994,
    "text": "ネットワークは、より多くのチャンネルに仕事を振り分けることを余儀なくされる。"
  },
  {
    "start": 9072814,
    "end": 9075174,
    "text": "だから、そのように分けているんだ。"
  },
  {
    "start": 9075214,
    "end": 9080544,
    "text": "ここではエンベッディングとmatmulの参加ウェイトだけを減衰させている。"
  },
  {
    "start": 9080964,
    "end": 9086340,
    "text": "減衰させるパラメータの数を表示しているのだが、ほとんどのパラメータが減衰するわけではない。"
  },
  {
    "start": 9086532,
    "end": 9108008,
    "text": "以前のatom wにはこのオプションはありませんでしたが、Pytorchの後のバージョンではこのオプションが導入されました。"
  },
  {
    "start": 9108176,
    "end": 9116336,
    "text": "というのも、以前のバージョンではイコールが融合されていないものがあったからだ。"
  },
  {
    "start": 9116520,
    "end": 9121644,
    "text": "これがアトムWで、以前は存在せず、後から追加されたものである。"
  },
  {
    "start": 9121984,
    "end": 9124640,
    "text": "何が起こっているのか、ここにいくつか資料がある。"
  },
  {
    "start": 9124832,
    "end": 9132034,
    "text": "というのも、ヒューズは比較的新しいもので、十分な時間を与えたいからだ。"
  },
  {
    "start": 9132194,
    "end": 9133666,
    "text": "デフォルトではフューズドを使用しない。"
  },
  {
    "start": 9133730,
    "end": 9138122,
    "text": "fusedは、それが利用可能で、Cudaで実行されている場合、より高速です。"
  },
  {
    "start": 9138298,
    "end": 9147122,
    "text": "これは、すべてのパラメータ・テンソルをforループで反復して更新する代わりに行うものだ。"
  },
  {
    "start": 9147218,
    "end": 9160124,
    "text": "つまり、これらのカーネルを1つのカーネルに融合させれば、多くのオーバーヘッドがなくなり、1回ですべてのパラメーターを更新するカーネルを呼び出せるということだ。"
  },
  {
    "start": 9160584,
    "end": 9167164,
    "text": "つまり、基本的には、すべてのテンソルを反復する代わりに、atomwの更新のためのカーネルフュージョンなのだ。"
  },
  {
    "start": 9168024,
    "end": 9171464,
    "text": "これは、私が気に入っているコンフィグ・オプティマイザーの機能だ。"
  },
  {
    "start": 9171624,
    "end": 9179638,
    "text": "しかし、ここからいくつかのプリントを見ることができる。"
  },
  {
    "start": 9179776,
    "end": 9181614,
    "text": "どんなものか見てみよう。"
  },
  {
    "start": 9182554,
    "end": 9189578,
    "text": "崩壊テンソルの数は50で、ほとんどのパラメータを占めていることがわかる。"
  },
  {
    "start": 9189666,
    "end": 9192594,
    "text": "これらは主にレイヤー・ノルム・パラメーターのバイアスである。"
  },
  {
    "start": 9192714,
    "end": 9197174,
    "text": "10万本しかないから、ほとんどは腐敗している。"
  },
  {
    "start": 9197474,
    "end": 9201978,
    "text": "その場合、アトムwの融合実装を使うことになり、より高速になる。"
  },
  {
    "start": 9202146,
    "end": 9205378,
    "text": "もし使えるのであれば、それを使うことをお勧めする。"
  },
  {
    "start": 9205466,
    "end": 9207562,
    "text": "なぜデフォルトにしないのかは、実は100％わからない。"
  },
  {
    "start": 9207578,
    "end": 9209734,
    "text": "かなり穏やかで無害に思える。"
  },
  {
    "start": 9210394,
    "end": 9212554,
    "text": "また、フューズド実装を使用しているためでもある。"
  },
  {
    "start": 9212594,
    "end": 9215454,
    "text": "だから、私たちは落ちてしまったのだと思う。"
  },
  {
    "start": 9216914,
    "end": 9225410,
    "text": "以前は1ステップあたり93ミリ秒かかっていた実行時間が、融合アトムwオプティマイザを使うことで1ステップあたり90ミリ秒に短縮されていることに注目してほしい。"
  },
  {
    "start": 9225602,
    "end": 9241194,
    "text": "1回のコミットで、融合アトムを導入し、時間を改善し、ウェイトディケイを追加したり変更したりしているが、ウェイトディケイしているのは2次元のパラメータ、埋め込みと線形に参加する行列だけだ。"
  },
  {
    "start": 9241654,
    "end": 9249326,
    "text": "これを外して、このラインはこれで終わり。"
  },
  {
    "start": 9249510,
    "end": 9251558,
    "text": "ここで話を続ける前に、もうひとつ簡単なメモを。"
  },
  {
    "start": 9251646,
    "end": 9264914,
    "text": "重み減衰、学習率、バッチサイズ、アトムパラメーター、ベータ1、ベータ2、イプシロンなどの関係は、最適化の文献では非常に複雑な数学的関係であることを指摘しておきたい。"
  },
  {
    "start": 9265074,
    "end": 9271170,
    "text": "このビデオの大部分は、OpenAIが使った設定をコピーペーストしているだけです。"
  },
  {
    "start": 9271242,
    "end": 9273954,
    "text": "これは複雑なテーマで、かなり奥が深い。"
  },
  {
    "start": 9274074,
    "end": 9283594,
    "text": "このビデオでは、パラメータをコピーしたいだけなんだ。"
  },
  {
    "start": 9284614,
    "end": 9292758,
    "text": "さて、次に進みたいのは、この段落のことだ。ところで、データ・ローダーを改良したときに話を戻そう。"
  },
  {
    "start": 9292886,
    "end": 9311120,
    "text": "モデルによって、トランスネットワークのサイズを決定するトランスのハイパーパラメーターが異なることにお気づきだろう。"
  },
  {
    "start": 9311302,
    "end": 9312804,
    "text": "学習速度も違う。"
  },
  {
    "start": 9312844,
    "end": 9316984,
    "text": "大きなネットワークほど学習率がやや低くなるパターンが見られる。"
  },
  {
    "start": 9317444,
    "end": 9325784,
    "text": "小さなネットワークでは小さなバッチサイズが使われ、大きなネットワークでは大きなバッチサイズが使われる。"
  },
  {
    "start": 9326164,
    "end": 9337554,
    "text": "というのも、もし私がここに来て、このBをセットしようとしたら、私のバイクはどこにある？"
  },
  {
    "start": 9341164,
    "end": 9346476,
    "text": "Bイコール、どこにデータを呼び出せばいいのか？"
  },
  {
    "start": 9346620,
    "end": 9347812,
    "text": "よし、bは16に等しい。"
  },
  {
    "start": 9347948,
    "end": 9349224,
    "text": "セットしようとすると"
  },
  {
    "start": 9351044,
    "end": 9356500,
    "text": "0.5百万ではないことに注意しなければならない。これはトークン数のバッチサイズだからだ。"
  },
  {
    "start": 9356652,
    "end": 9359900,
    "text": "行のすべてが1024トークンだ。"
  },
  {
    "start": 9360012,
    "end": 9363830,
    "text": "0.5、e 600万、1024を割る。"
  },
  {
    "start": 9363972,
    "end": 9368114,
    "text": "これには488のバッチサイズが必要だ。"
  },
  {
    "start": 9368234,
    "end": 9376826,
    "text": "問題は、ここに来てこれを488に設定することができないことだ。"
  },
  {
    "start": 9377010,
    "end": 9387546,
    "text": "というのも、繰り返しになるが、バッチサイズは他の最適化ハイパーパラメーターや学習率などと相関関係があるからだ。"
  },
  {
    "start": 9387650,
    "end": 9390682,
    "text": "すべてのハイパーパラメーターを忠実に表現したい。"
  },
  {
    "start": 9390778,
    "end": 9395454,
    "text": "従って、バッチサイズはおよそ0.5百万とする必要がある。"
  },
  {
    "start": 9395754,
    "end": 9400106,
    "text": "問題は、小さなGpuしかない場合、どうやって0."
  },
  {
    "start": 9400250,
    "end": 9403254,
    "text": "そのためには、グラデーション累積と呼ばれるものを使う必要がある。"
  },
  {
    "start": 9403794,
    "end": 9405482,
    "text": "次はその話をしよう。"
  },
  {
    "start": 9405578,
    "end": 9410914,
    "text": "これにより、設定した任意のバッチサイズをシリアルでシミュレートすることができる。"
  },
  {
    "start": 9411074,
    "end": 9413978,
    "text": "だから、バッチサイズは0."
  },
  {
    "start": 9414066,
    "end": 9415786,
    "text": "もっと長く走るしかない。"
  },
  {
    "start": 9415890,
    "end": 9423872,
    "text": "バッチサイズ0.5百万をシミュレートするためには、複数のシーケンスを処理し、基本的にそれらのすべての勾配を加算しなければならない。"
  },
  {
    "start": 9424008,
    "end": 9425368,
    "text": "次はその話をしよう。"
  },
  {
    "start": 9425496,
    "end": 9429312,
    "text": "さて、ここでコードを追加して実装を始めた。"
  },
  {
    "start": 9429488,
    "end": 9434112,
    "text": "基本的に私がしたことは、まず、希望するバッチサイズの合計を設定した。"
  },
  {
    "start": 9434248,
    "end": 9436368,
    "text": "これはまさに0.5ミリオンである。"
  },
  {
    "start": 9436496,
    "end": 9442480,
    "text": "円フレックス19が2つあれば524 288になるからだ。"
  },
  {
    "start": 9442592,
    "end": 9444764,
    "text": "およそ0.5百万人で、いい数字だ。"
  },
  {
    "start": 9445284,
    "end": 9449064,
    "text": "現在、マイクロバッチサイズは16である。"
  },
  {
    "start": 9449364,
    "end": 9456172,
    "text": "これは、トランスフォーマーに入るCのbとtが前方後方にあることに変わりはない。"
  },
  {
    "start": 9456308,
    "end": 9458044,
    "text": "アップデートをするつもりはない。"
  },
  {
    "start": 9458124,
    "end": 9459784,
    "text": "これから何度も前進後退をするつもりだ。"
  },
  {
    "start": 9460364,
    "end": 9464396,
    "text": "そして、これらのグラデーションはすべて、パラメータ・グラデーションにプラス・イコールする。"
  },
  {
    "start": 9464460,
    "end": 9465740,
    "text": "全部足し算になるんだ。"
  },
  {
    "start": 9465892,
    "end": 9469992,
    "text": "フォワード・バックワード・グラード・アカムステップを何回やるか。"
  },
  {
    "start": 9470148,
    "end": 9473524,
    "text": "そして、それがすべて蓄積されたら、1回のアップデートを行うつもりだ。"
  },
  {
    "start": 9474184,
    "end": 9482244,
    "text": "特に、マイクロバッチサイズは、1回の処理で何トークン、何行を前方後方に処理するかを制御しているところだ。"
  },
  {
    "start": 9483064,
    "end": 9487616,
    "text": "ここで我々は16回124をやっている。"
  },
  {
    "start": 9487720,
    "end": 9493680,
    "text": "前方後方に16、3、8、4個のトークンを使っている。"
  },
  {
    "start": 9493872,
    "end": 9497356,
    "text": "私たちは19人に対し2人を起用することになっている。"
  },
  {
    "start": 9497540,
    "end": 9499076,
    "text": "おっと、僕は何をやっているんだ？"
  },
  {
    "start": 9499260,
    "end": 9502188,
    "text": "合計19人に2人。"
  },
  {
    "start": 9502316,
    "end": 9504424,
    "text": "グラタコーンは32歳になる。"
  },
  {
    "start": 9507124,
    "end": 9510020,
    "text": "従って、ここでのグラデーションは32となる。"
  },
  {
    "start": 9510172,
    "end": 9512944,
    "text": "我々は32の前進後退をしなければならない。"
  },
  {
    "start": 9513644,
    "end": 9515012,
    "text": "そして1回の更新。"
  },
  {
    "start": 9515148,
    "end": 9519236,
    "text": "これで、1回の前進後退に約100ミリ秒かかることがわかった。"
  },
  {
    "start": 9519340,
    "end": 9524222,
    "text": "そのうちの32本をこなすと、1歩がおよそ3秒になる。"
  },
  {
    "start": 9524398,
    "end": 9525994,
    "text": "ナプキンの計算だよ。"
  },
  {
    "start": 9527094,
    "end": 9528510,
    "text": "それがグラディカム・ステップだ。"
  },
  {
    "start": 9528542,
    "end": 9530086,
    "text": "今、私たちはそれを実際に実行しなければならない。"
  },
  {
    "start": 9530150,
    "end": 9545182,
    "text": "というのも、この部分とこの部分、前方と後方、この32回を繰り返さなければならないからだ。"
  },
  {
    "start": 9545318,
    "end": 9547426,
    "text": "では、それをどう実装するか見てみよう。"
  },
  {
    "start": 9547590,
    "end": 9548882,
    "text": "こっちへ行こう"
  },
  {
    "start": 9549018,
    "end": 9551554,
    "text": "実際、毎回新しいバッチをロードしなければならない。"
  },
  {
    "start": 9551594,
    "end": 9553210,
    "text": "こっちへ移そう。"
  },
  {
    "start": 9553362,
    "end": 9555162,
    "text": "さて、ここでインナーループがある。"
  },
  {
    "start": 9555298,
    "end": 9562614,
    "text": "gradicumステップの微小ステップでは、このようにする。"
  },
  {
    "start": 9563354,
    "end": 9566106,
    "text": "後方へのロスは常にグラディエーションをもたらすことを忘れてはならない。"
  },
  {
    "start": 9566170,
    "end": 9570274,
    "text": "私たちは最後の後方でインサイドをやっていて、グラデーションには常にプラス・イコールがある。"
  },
  {
    "start": 9570434,
    "end": 9575084,
    "text": "後方グラデーションのすべてのロットで、グラデーション・テスターを追加する。"
  },
  {
    "start": 9576904,
    "end": 9585004,
    "text": "そして、すべてのグラデーションをそこに持っていき、正規化する。"
  },
  {
    "start": 9586184,
    "end": 9587760,
    "text": "私たちはとても近い。"
  },
  {
    "start": 9587912,
    "end": 9591792,
    "text": "実は、ここには微妙で深い問題がある。"
  },
  {
    "start": 9591848,
    "end": 9593240,
    "text": "これは実は正しくない。"
  },
  {
    "start": 9593352,
    "end": 9599428,
    "text": "なぜこれがまだ十分でないのか、考えてみてほしい。"
  },
  {
    "start": 9599596,
    "end": 9606184,
    "text": "さて、Jupyterノートブックを復活させたので、簡単なおもちゃのような設定で、何が起きているのかを注意深く考えることができる。"
  },
  {
    "start": 9606644,
    "end": 9612744,
    "text": "個の数値からなる16個のベクトルを受け取り、1個の数値を返す非常に単純なニューラルネットを作ってみよう。"
  },
  {
    "start": 9613524,
    "end": 9619744,
    "text": "では、ここで私はいくつかのランダムな例、xといくつかのターゲットyを作成します。"
  },
  {
    "start": 9620124,
    "end": 9625424,
    "text": "ということは、ここでは平均二乗損を使って損を計算していることになる。"
  },
  {
    "start": 9626024,
    "end": 9634484,
    "text": "基本的に、これは4つの個別の例であり、これら4つの例の平均二乗損失で単純回帰しているだけである。"
  },
  {
    "start": 9635224,
    "end": 9641124,
    "text": "損失を計算し、それを逆算して勾配を見ると、これが達成された勾配である。"
  },
  {
    "start": 9642224,
    "end": 9643976,
    "text": "さて、ここでの敗戦の目的である。"
  },
  {
    "start": 9644160,
    "end": 9649616,
    "text": "MSCロスでは、損失関数のデフォルトはリダクションが平均であることに注意。"
  },
  {
    "start": 9649760,
    "end": 9657542,
    "text": "ここでは4つの例の平均損失額を計算している。"
  },
  {
    "start": 9657678,
    "end": 9660406,
    "text": "これがまさに敗戦の目的である。"
  },
  {
    "start": 9660550,
    "end": 9664950,
    "text": "これは4つの独立した例があるので、4つの平均値である。"
  },
  {
    "start": 9665102,
    "end": 9672994,
    "text": "そして、4つの例とその平均二乗誤差、二乗誤差、そしてこれが平均二乗誤差となる。"
  },
  {
    "start": 9673654,
    "end": 9679950,
    "text": "そのため、二乗誤差を計算し、それを正規化して平均値とする。"
  },
  {
    "start": 9679982,
    "end": 9680152,
    "text": "だから"
  },
  {
    "start": 9680198,
    "end": 9681716,
    "text": "ここに4つの例がある。"
  },
  {
    "start": 9681900,
    "end": 9694940,
    "text": "このグラデーション累積バージョンでは、グラデーションを4段階にして、グラデーションをリセットする。"
  },
  {
    "start": 9695092,
    "end": 9702348,
    "text": "4つのステップを踏んだが、今はその代わりにすべての例を個別に評価し、何度も後方へ失点している。"
  },
  {
    "start": 9702476,
    "end": 9705108,
    "text": "そして、そこから得られたグラデーションを見る。"
  },
  {
    "start": 9705276,
    "end": 9712064,
    "text": "基本的に今、私たちは関数を前進させ、まったく同じ損失を計算し、それを後退させ、それを4回繰り返す。"
  },
  {
    "start": 9712184,
    "end": 9716964,
    "text": "グラデーションを見ると、グラデーションが一致していないことに気づくだろう。"
  },
  {
    "start": 9717944,
    "end": 9725688,
    "text": "ここでは、4つのバッチを1回行ったが、ここではバッチサイズ1の勾配累積ステップを4回行った。"
  },
  {
    "start": 9725856,
    "end": 9727724,
    "text": "グラデーションは同じではない。"
  },
  {
    "start": 9728184,
    "end": 9733726,
    "text": "基本的に両者が同じでない理由は、まさにこの平均二乗誤差が失われるからである。"
  },
  {
    "start": 9733790,
    "end": 9736198,
    "text": "この敗戦の1クォーターが失われた。"
  },
  {
    "start": 9736246,
    "end": 9748406,
    "text": "なぜなら、ここで起こることは、ループのひとつひとつの損失目標が、平均二乗誤差に過ぎないからだ。"
  },
  {
    "start": 9748510,
    "end": 9752838,
    "text": "これは0回目の反復での損失であり、最初の3分の1も同様である。"
  },
  {
    "start": 9753006,
    "end": 9756834,
    "text": "そして、ロスを逆算すると、グラディエントが蓄積されることになる。"
  },
  {
    "start": 9757214,
    "end": 9764514,
    "text": "何が起こるかというと、グラディエントでの蓄積は、基本的に損失での合計と同じだということだ。"
  },
  {
    "start": 9765934,
    "end": 9772934,
    "text": "私たちの損失は、1/4という外側の要素を抜きにして、実際にここにある。"
  },
  {
    "start": 9773094,
    "end": 9776974,
    "text": "ノーマライザーがないため、グラデーションがずれている。"
  },
  {
    "start": 9777094,
    "end": 9783994,
    "text": "だから、これを解決する方法、あるいはそのうちのひとつは、基本的にはここに来て、損失イコール損失、4で割ると言うことができる。"
  },
  {
    "start": 9785794,
    "end": 9793014,
    "text": "今何が起こっているかというと、我々は損失を拡大し、これらのすべての場所の前に4分の1を導入している。"
  },
  {
    "start": 9795874,
    "end": 9798414,
    "text": "個々の損失はすべて4分の1ずつになる。"
  },
  {
    "start": 9798794,
    "end": 9802922,
    "text": "そして、後退するときには、これらのすべてが合計で累積される。"
  },
  {
    "start": 9803058,
    "end": 9806298,
    "text": "今、これらの部品のひとつひとつに1/4がある。"
  },
  {
    "start": 9806426,
    "end": 9808814,
    "text": "これで我々の損失は同等になる。"
  },
  {
    "start": 9809554,
    "end": 9814924,
    "text": "これを実行すると、グラデーションが同じになっているのがわかる。"
  },
  {
    "start": 9815304,
    "end": 9835536,
    "text": "簡単に言うと、この単純な例で、ステップを踏んでいくと、基本的にこれが正しくない理由は、ここでのMSCの損失と同じように、モデルで計算している損失も平均値の減少を使っているからだということがわかる。"
  },
  {
    "start": 9835720,
    "end": 9836672,
    "text": "損失はどこに？"
  },
  {
    "start": 9836768,
    "end": 9838164,
    "text": "Fドットクロスエントロピー。"
  },
  {
    "start": 9839184,
    "end": 9843136,
    "text": "デフォルトでは、クロスエントロピーの減少もここにある。"
  },
  {
    "start": 9843240,
    "end": 9850204,
    "text": "なぜ表示されないのかわからないが、これは平均値で、すべてのb by t要素での平均損失だろう？"
  },
  {
    "start": 9851864,
    "end": 9853888,
    "text": "そこには平均的な減少がある。"
  },
  {
    "start": 9853976,
    "end": 9857440,
    "text": "このグラデーションの積み重ねだけでは、その部分が欠けてしまう。"
  },
  {
    "start": 9857592,
    "end": 9862468,
    "text": "従って、これを修正する方法は、単純にグラデーションの累積ステップ数を補正することである。"
  },
  {
    "start": 9862576,
    "end": 9864812,
    "text": "同じように、この損失を分割することができる。"
  },
  {
    "start": 9864988,
    "end": 9872564,
    "text": "特にここでは、プラス・イコール・ロス÷グラディエント・アキュムレーション・ステップ数で行っている。"
  },
  {
    "start": 9872724,
    "end": 9888712,
    "text": "副操縦士が修正したようなものだが、同じように、損失をスケールダウンしているのだ。"
  },
  {
    "start": 9888848,
    "end": 9895336,
    "text": "したがって、グラディカムステップで割ったロスを合計すると、追加のノーマライザーを回収していることになる。"
  },
  {
    "start": 9895520,
    "end": 9904084,
    "text": "勾配が同じになるので、この2つは元の最適化と同じになる。"
  },
  {
    "start": 9904704,
    "end": 9909032,
    "text": "もう少し手直しが必要だったので、ここで最適化を開始した。"
  },
  {
    "start": 9909128,
    "end": 9916700,
    "text": "特に、きれいに印刷したいので、まず最初に、損失に対するアキュムレーターのようなものを作る必要がある。"
  },
  {
    "start": 9916772,
    "end": 9921624,
    "text": "最終的なマイクロステップで最終的な損失だけを印刷することになるからだ。"
  },
  {
    "start": 9921924,
    "end": 9927876,
    "text": "その代わり、クムのロスをゼロで初期化し、それにロスを積み重ねる。"
  },
  {
    "start": 9928020,
    "end": 9937052,
    "text": "detachを使っているので、テンソルをグラフから切り離し、値を追跡しているだけだ。"
  },
  {
    "start": 9937148,
    "end": 9939954,
    "text": "私はリーフノードを追加する際に、このようなリーフノードを作っている。"
  },
  {
    "start": 9940334,
    "end": 9943742,
    "text": "これはクムの損失であり、ここでは損失の代わりにそれを印刷している。"
  },
  {
    "start": 9943878,
    "end": 9954114,
    "text": "それに加えて、処理されるトークン内の勾配のステップを考慮しなければならなかった。"
  },
  {
    "start": 9955054,
    "end": 9958022,
    "text": "というわけで、手短に言えば、最適化がここにある。"
  },
  {
    "start": 9958198,
    "end": 9960246,
    "text": "合理的に見えるだろう？"
  },
  {
    "start": 9960270,
    "end": 9961830,
    "text": "いいところからスタートしている。"
  },
  {
    "start": 9961982,
    "end": 9967346,
    "text": "段階的なステップを32と計算したが、ここでは約3秒だ。"
  },
  {
    "start": 9967370,
    "end": 9967934,
    "text": "そうだね。"
  },
  {
    "start": 9970674,
    "end": 9974042,
    "text": "これはかなり良さそうだ。"
  },
  {
    "start": 9974098,
    "end": 9979994,
    "text": "さて、あなたの最適化とここでの実装が正しいかどうかを確認したい場合は、サイドで作業してください。"
  },
  {
    "start": 9980154,
    "end": 9988258,
    "text": "さて、総パッチ・サイズと勾配累積ステップがあるので、bの設定は純粋にパフォーマンス最適化のようなものだ。"
  },
  {
    "start": 9988386,
    "end": 9993626,
    "text": "もし大きなGPUを持っているなら、これを32に増やせばもう少し速くなるだろう。"
  },
  {
    "start": 9993770,
    "end": 9996674,
    "text": "GPUが非常に小さい場合は、8個か4個で試すことができる。"
  },
  {
    "start": 9996834,
    "end": 10009534,
    "text": "というのも、勾配累積が作動し、必要に応じてすべてをシリアルに処理することができるからだ。"
  },
  {
    "start": 10010114,
    "end": 10012810,
    "text": "グラデーションの蓄積はこれで終わりだろう。"
  },
  {
    "start": 10012922,
    "end": 10015614,
    "text": "よし、今こそ重火器を持ち出す時だ。"
  },
  {
    "start": 10016074,
    "end": 10025532,
    "text": "これまでトレーニングには1つのGPUしか使っていないことにお気づきだろうが、実際には8つのGPUにお金を払っている。"
  },
  {
    "start": 10025668,
    "end": 10037304,
    "text": "特に、彼らは同時にトークンを使ってコラボレーションし、最適化する。"
  },
  {
    "start": 10037644,
    "end": 10041116,
    "text": "そのために、Pytorchの分散データ・パラレルを使うつもりだ。"
  },
  {
    "start": 10041220,
    "end": 10044332,
    "text": "また、レガシー・データのパラレルもあるが、これは使わないことをお勧めする。"
  },
  {
    "start": 10044428,
    "end": 10047884,
    "text": "それはレガシーな分配データのようなものだ。"
  },
  {
    "start": 10047924,
    "end": 10049620,
    "text": "パラレルは非常にシンプルな方法で機能する。"
  },
  {
    "start": 10049772,
    "end": 10057544,
    "text": "GPUが8つあるので、8つのプロセスを立ち上げ、各プロセスにGPUを割り当てる。"
  },
  {
    "start": 10057924,
    "end": 10063196,
    "text": "各プロセスのトレーニング・ループと、これまで取り組んできたことは、ほとんど同じに見えるだろう。"
  },
  {
    "start": 10063340,
    "end": 10067860,
    "text": "各GPUは、それに関する限り、我々がこれまで作ってきたものをそのまま使っているだけだ。"
  },
  {
    "start": 10068012,
    "end": 10073684,
    "text": "今、密かに8人いて、全員がデータの微妙に異なる部分を処理しようとしている。"
  },
  {
    "start": 10074264,
    "end": 10083084,
    "text": "さらにもうひとつ、グラデーションを計算する部分を追加し、そのグラデーションの平均を計算する。"
  },
  {
    "start": 10084184,
    "end": 10089232,
    "text": "そうして、彼らはここで計算負荷を共同作業することになる。"
  },
  {
    "start": 10089408,
    "end": 10097900,
    "text": "この8つすべてを使うには、もうPytorch train GPT-2 PYだけでスクリプトを起動するのはやめよう。"
  },
  {
    "start": 10098052,
    "end": 10101420,
    "text": "PytorchのTorch runという特別なコマンドを使って実行する。"
  },
  {
    "start": 10101452,
    "end": 10102584,
    "text": "もう少ししたらわかるだろう。"
  },
  {
    "start": 10103004,
    "end": 10110744,
    "text": "トーチが実行されると、パイソン・スクリプトは8つのトーチを並列に実行する。"
  },
  {
    "start": 10111164,
    "end": 10120956,
    "text": "各プロセスが、基本的にどのプロセスであるかを調べることができる環境変数を作成するのだ。"
  },
  {
    "start": 10121140,
    "end": 10126804,
    "text": "例えば、tortronはランク、ローカルランク、ワールドサイズの環境変数を設定する。"
  },
  {
    "start": 10126964,
    "end": 10130996,
    "text": "だから、これはDDPが作動しているかどうかを検知する悪い方法だ。"
  },
  {
    "start": 10131060,
    "end": 10142784,
    "text": "トーチランを使うのであれば、DDPが動作しているのであれば、CUDAが使用可能であることを確認しなければならない。"
  },
  {
    "start": 10144524,
    "end": 10146916,
    "text": "これがセットアップのコードだ。"
  },
  {
    "start": 10147020,
    "end": 10151296,
    "text": "重要なのは、ワールドサイズがあるということだ。"
  },
  {
    "start": 10151360,
    "end": 10161600,
    "text": "各プロセスは基本的に、まったく同じコードをまったく同じ時間に実行する。"
  },
  {
    "start": 10161752,
    "end": 10167448,
    "text": "これらのプロセスの唯一の違いは、DTPランクが異なることだ。"
  },
  {
    "start": 10167616,
    "end": 10175470,
    "text": "GPUゼロのDDPランクはゼロ、GPU1のDDPランクは1、など。"
  },
  {
    "start": 10175622,
    "end": 10178078,
    "text": "そうでなければ、全員がまったく同じスクリプトを実行していることになる。"
  },
  {
    "start": 10178206,
    "end": 10185614,
    "text": "ただ、DDPrankは少し異なる整数になり、それが例えば同じデータで実行されないように調整する方法なんだ。"
  },
  {
    "start": 10185694,
    "end": 10189034,
    "text": "私たちは、データの異なる部分に対して実行させたいのです。"
  },
  {
    "start": 10189654,
    "end": 10194118,
    "text": "さて、ローカルランクというのは、マルチノードの設定でのみ使われるものだ。"
  },
  {
    "start": 10194206,
    "end": 10196478,
    "text": "HPusを搭載しているノードは1つしかない。"
  },
  {
    "start": 10196646,
    "end": 10201234,
    "text": "つまり、ローカルランクは1つのノードのGPUのランクです。"
  },
  {
    "start": 10201694,
    "end": 10204474,
    "text": "ゼロから7まで。"
  },
  {
    "start": 10205094,
    "end": 10207422,
    "text": "私たちの場合、ほとんどがシングルボックスで動いている。"
  },
  {
    "start": 10207518,
    "end": 10210294,
    "text": "私たちが気にしているのは、順位と世界の大きさだ。"
  },
  {
    "start": 10210414,
    "end": 10218874,
    "text": "これは8であり、このスクリプトの特定のインスタンス化が実行されるGPUによって変わる。"
  },
  {
    "start": 10219734,
    "end": 10233842,
    "text": "ここで、ローカルランクに従って、デバイスをCuda colonに設定していることを確認します。colonは、複数のGPUがある場合に、どのGPUを使用するかを示します。"
  },
  {
    "start": 10234018,
    "end": 10240330,
    "text": "このプロセスのローカルランクに応じて、適切なGPUを使用することになる。"
  },
  {
    "start": 10240402,
    "end": 10243254,
    "text": "どのGPUがどのプロセスで使われているかが衝突することはない。"
  },
  {
    "start": 10244274,
    "end": 10249530,
    "text": "これはDDprancのequals、equals zeroである。"
  },
  {
    "start": 10249682,
    "end": 10253732,
    "text": "マスター・プロセスは任意のプロセス番号ゼロである。"
  },
  {
    "start": 10253858,
    "end": 10257016,
    "text": "印刷、ロギング、チェックポイントなど、多くのことをやってくれる。"
  },
  {
    "start": 10257120,
    "end": 10261184,
    "text": "それ以外のプロセスは、ほとんど補助的な計算プロセスとして考えられている。"
  },
  {
    "start": 10261344,
    "end": 10264672,
    "text": "そのため、マスター・プロセス・ゼロはさらに仕事を増やすことになる。"
  },
  {
    "start": 10264768,
    "end": 10268164,
    "text": "他のすべてのプロセスは、ほとんど前進後退をするだけだ。"
  },
  {
    "start": 10268904,
    "end": 10273888,
    "text": "DDPを使用しておらず、これらの変数が設定されていない場合は、シングルGPUトレーニングに戻ります。"
  },
  {
    "start": 10274016,
    "end": 10280316,
    "text": "つまり、ランクはゼロ、ワールドサイズは1、そして我々はマスタープロセスである。"
  },
  {
    "start": 10280480,
    "end": 10285104,
    "text": "私たちはデバイスを自動検出しようとしますが、これは通常の世界です。"
  },
  {
    "start": 10286204,
    "end": 10288904,
    "text": "ここまではDDPを初期化しただけだ。"
  },
  {
    "start": 10289564,
    "end": 10296316,
    "text": "トーチランの場合、8つのコピーが並行して走ることになる。"
  },
  {
    "start": 10296460,
    "end": 10298304,
    "text": "それぞれランクが違う。"
  },
  {
    "start": 10298604,
    "end": 10303264,
    "text": "この後、すべてが正しく行われるようにしなければならない。"
  },
  {
    "start": 10303684,
    "end": 10311544,
    "text": "複数のプロセスを実行するときに厄介なのは、常に8つのプロセスが並行して動いていることを想像しなければならないことだ。"
  },
  {
    "start": 10311664,
    "end": 10319376,
    "text": "コードを読みながら、8つの、つまり8つのパイソンインタープリターがこれらのコード行を走っていることを想像しなければならない。"
  },
  {
    "start": 10319480,
    "end": 10322904,
    "text": "両者の違いは、DDPランクが違うだけだ。"
  },
  {
    "start": 10323064,
    "end": 10331832,
    "text": "全員がここにやってきて、全員がまったく同じ種を選び、大雑把に言えば、他のコピーが走っていることにまったく気づかずに、すべての計算をする。"
  },
  {
    "start": 10331888,
    "end": 10335402,
    "text": "そう、だからみんなまったく同じ計算をするんだ。"
  },
  {
    "start": 10335458,
    "end": 10342934,
    "text": "そのため、ワールドサイズやランクを考慮して計算を調整しなければならない。"
  },
  {
    "start": 10343434,
    "end": 10349370,
    "text": "特に、マイクロバッチやシーケンスの長さは、すべてGPUごとのものですよね？"
  },
  {
    "start": 10349562,
    "end": 10354026,
    "text": "今、並行していくつものプロセスが動いている。"
  },
  {
    "start": 10354210,
    "end": 10371334,
    "text": "というのも、各プロセスはb×t回の処理を行い、その数はこれだけあるからだ。"
  },
  {
    "start": 10372434,
    "end": 10385022,
    "text": "16×124×8GPUで131だから、バッチサイズにうまく収まるようにしたい。"
  },
  {
    "start": 10385218,
    "end": 10385782,
    "text": "いいかい？"
  },
  {
    "start": 10385838,
    "end": 10393294,
    "text": "ということは、524288ということは、現在のセッティングではグラダクームが4つになるということですよね？"
  },
  {
    "start": 10393454,
    "end": 10397614,
    "text": "各GPUで処理されるのは16倍の124回になる。"
  },
  {
    "start": 10397694,
    "end": 10398774,
    "text": "それから8つのGPUがある。"
  },
  {
    "start": 10398814,
    "end": 10406274,
    "text": "8つのGPUで1回の前進後退で131,000トークンを行う予定だ。"
  },
  {
    "start": 10407134,
    "end": 10412374,
    "text": "グラデーションの累積ステップをうまく導き出せるように、これがうまくフィットするようにしたい。"
  },
  {
    "start": 10412994,
    "end": 10417026,
    "text": "ああ、ここでコメントを調整しよう。"
  },
  {
    "start": 10417170,
    "end": 10420134,
    "text": "倍のDDP世界サイズ。"
  },
  {
    "start": 10420514,
    "end": 10424746,
    "text": "では、各GPUはこれを計算する。"
  },
  {
    "start": 10424890,
    "end": 10426970,
    "text": "ここからが問題なんだ。"
  },
  {
    "start": 10427002,
    "end": 10432490,
    "text": "各工程がプリントをし、全員がプリントをする。"
  },
  {
    "start": 10432562,
    "end": 10435954,
    "text": "このプリントを8部用意するつもりだ。"
  },
  {
    "start": 10436114,
    "end": 10439442,
    "text": "これに対処する方法のひとつが、まさにこのマスター・プロセス変数である。"
  },
  {
    "start": 10439458,
    "end": 10443358,
    "text": "もし、マスター・プロセスなら、これを守れ。"
  },
  {
    "start": 10443526,
    "end": 10449446,
    "text": "そうしないと、すべてのプロセスがまったく同じ変数を計算してしまうからだ。"
  },
  {
    "start": 10449470,
    "end": 10455006,
    "text": "データ・ローダーに入る前に、これを8回もプリントする必要はない。"
  },
  {
    "start": 10455110,
    "end": 10457014,
    "text": "リファクタリングが必要なのは明らかだ。"
  },
  {
    "start": 10457174,
    "end": 10464190,
    "text": "多分、この時点でプリントをして、スピンだけして退場したほうがいいと思う。"
  },
  {
    "start": 10464222,
    "end": 10484894,
    "text": "syslogとSys exitをインポートして印刷し、iMgpu、ddprank、IMGP rankをインポートして印刷する。"
  },
  {
    "start": 10486914,
    "end": 10491126,
    "text": "では、これを実際に動かしてみて、どう動くか見てみよう。"
  },
  {
    "start": 10491250,
    "end": 10493582,
    "text": "どんな感じか見てみたいから、ちょっと走ってみてよ。"
  },
  {
    "start": 10493718,
    "end": 10497342,
    "text": "通常、PythonはこのようにGPT-2 PIを起動する。"
  },
  {
    "start": 10497478,
    "end": 10500630,
    "text": "さて、トーチランで走るとこんな感じになる。"
  },
  {
    "start": 10500782,
    "end": 10508686,
    "text": "torchはスタンドアロンで動作し、プロセス数は、例えば、GPUが8つあるので8つで、GPT-2 pyを訓練します。"
  },
  {
    "start": 10508870,
    "end": 10511622,
    "text": "コマンドはこのようになる。"
  },
  {
    "start": 10511758,
    "end": 10514034,
    "text": "トーチランは8本だ。"
  },
  {
    "start": 10514734,
    "end": 10516054,
    "text": "どうなるか見てみよう"
  },
  {
    "start": 10516174,
    "end": 10519606,
    "text": "最初は少し忙しくなる。"
  },
  {
    "start": 10519630,
    "end": 10520646,
    "text": "ここにはいろいろなことがある。"
  },
  {
    "start": 10520670,
    "end": 10526518,
    "text": "まず第一に、配布されたいくつかの警告がある。"
  },
  {
    "start": 10526566,
    "end": 10534798,
    "text": "これは、コードがセットアップされ、プロセスがオンラインになり、プロセスが立ち上がるまでの間、予備的な収集の失敗を見ているようなものだと思う。"
  },
  {
    "start": 10534886,
    "end": 10540414,
    "text": "100％の確信があるわけではないが、それから実際のプリントに入る。"
  },
  {
    "start": 10540574,
    "end": 10549976,
    "text": "すべてのプロセスがダウンし、最初のプリントは偶然にもプロセス5からもたらされた。"
  },
  {
    "start": 10550120,
    "end": 10551216,
    "text": "と印刷された。"
  },
  {
    "start": 10551280,
    "end": 10561084,
    "text": "プロセス5は基本的に、GPU5で最初に \"unprocess \"と表示され、次にマスタープロセスからこれらのスプリントが生まれる。"
  },
  {
    "start": 10562304,
    "end": 10565080,
    "text": "プロセス5がなぜか先に終わった。"
  },
  {
    "start": 10565112,
    "end": 10568324,
    "text": "ただ、オペレーティング・システムがどのようにプロセスを実行するようにスケジューリングしているかによる。"
  },
  {
    "start": 10568744,
    "end": 10577364,
    "text": "その後、GPUゼロが終了し、GPU3と2が終了し、おそらくプロセス5か何かが終了した。"
  },
  {
    "start": 10578024,
    "end": 10586176,
    "text": "DDPは、マルチGPUの設定を適切に破棄しなかったので、それをとても嫌っている。"
  },
  {
    "start": 10586320,
    "end": 10589924,
    "text": "そのため、デストラクトする前にプロセスグループが破壊されることはない。"
  },
  {
    "start": 10590664,
    "end": 10592120,
    "text": "それは本当に気に入らない。"
  },
  {
    "start": 10592192,
    "end": 10601034,
    "text": "実際のアプリケーションでは、DDPを適切にクリーンアップするために、destroy process groupを呼び出したい。"
  },
  {
    "start": 10601114,
    "end": 10604914,
    "text": "その後、GPUの残りの部分が終了し、終了となる。"
  },
  {
    "start": 10605074,
    "end": 10610202,
    "text": "基本的に、これらのプロセスがいつ実行されるかはまったく任意であり、保証はできないが、並行して実行されている。"
  },
  {
    "start": 10610338,
    "end": 10611854,
    "text": "印刷はしてほしくない。"
  },
  {
    "start": 10612714,
    "end": 10616494,
    "text": "次はこれを消そう。"
  },
  {
    "start": 10617914,
    "end": 10629560,
    "text": "次に、データ・ローダー・ライトを作成する際に、マルチプロセス設定を認識させる必要があることを確認したい。"
  },
  {
    "start": 10629672,
    "end": 10635472,
    "text": "もちろん、各プロセスがデータセットの異なる部分を処理するように、各プロセスがそれぞれのデータの塊を取得するようにしたい。"
  },
  {
    "start": 10635648,
    "end": 10637264,
    "text": "それを調整しよう。"
  },
  {
    "start": 10637424,
    "end": 10645724,
    "text": "特にシンプルで素朴な方法としては、データローダーにランクとサイズを確実に渡すことだ。"
  },
  {
    "start": 10646544,
    "end": 10650984,
    "text": "そしてここに来て、私たちはランクとプロセスを手に入れ、それを保存していることがわかる。"
  },
  {
    "start": 10651364,
    "end": 10659076,
    "text": "現在位置がゼロになることはないだろう。"
  },
  {
    "start": 10659260,
    "end": 10666264,
    "text": "この方法のひとつは、基本的に自己bに塩tをかけ、それにプロセスランクをかける。"
  },
  {
    "start": 10667444,
    "end": 10673252,
    "text": "プロセス・ランク0はゼロから始まるが、プロセス・ランク1はb時間tから始まる。"
  },
  {
    "start": 10673388,
    "end": 10677406,
    "text": "工程ランク2は、2回b回t回などで開始される。"
  },
  {
    "start": 10677590,
    "end": 10679034,
    "text": "これが初期化である。"
  },
  {
    "start": 10679734,
    "end": 10691590,
    "text": "でも、前進するときは、b×t倍ではなく、b×t×プロセス数だけ前進するんだ。"
  },
  {
    "start": 10691742,
    "end": 10698934,
    "text": "基本的に、我々が消費するトークンの総数は、b×t×プロセス数である。"
  },
  {
    "start": 10699094,
    "end": 10705964,
    "text": "彼らは皆、別の階級に行き、そのポジションは全体の塊によって前進しなければならない。"
  },
  {
    "start": 10707184,
    "end": 10714536,
    "text": "とすると、ここではb回目のt回目のソフト処理数プラス1がトークン数を超えることになる。"
  },
  {
    "start": 10714640,
    "end": 10715664,
    "text": "それからループするんだ。"
  },
  {
    "start": 10715784,
    "end": 10719044,
    "text": "ループするときは、もちろんまったく同じようにループしたい。"
  },
  {
    "start": 10719344,
    "end": 10721484,
    "text": "リセットバックのようなものだ。"
  },
  {
    "start": 10722344,
    "end": 10728940,
    "text": "これは、非常にシンプルな分散データ・ローダー・ライトのための最もシンプルな変更だ。"
  },
  {
    "start": 10729132,
    "end": 10736476,
    "text": "プロセスランクが0でプロセスが1の場合、全体が以前と同じになることに気づくだろう。"
  },
  {
    "start": 10736620,
    "end": 10741264,
    "text": "これで複数のプロセスを実行させることができ、問題なく動作するはずだ。"
  },
  {
    "start": 10743284,
    "end": 10744784,
    "text": "データローダーをやろう。"
  },
  {
    "start": 10745164,
    "end": 10751704,
    "text": "さて、次はデータ・ローダーの初期化が終わったら、ここに来てGPTモデルを作成する。"
  },
  {
    "start": 10752464,
    "end": 10755360,
    "text": "8つのプロセスで8つのGPTモデルを作成します。"
  },
  {
    "start": 10755472,
    "end": 10759920,
    "text": "というのも、ここではシードが固定されているため、どれも同じ同じモデルを作るからだ。"
  },
  {
    "start": 10760112,
    "end": 10764616,
    "text": "全員が自分のランクのデバイスに移動させ、モデルをコンパイルする。"
  },
  {
    "start": 10764760,
    "end": 10769352,
    "text": "モデルが同一であるため、8つの同一のコンパイルが並行して行われる。"
  },
  {
    "start": 10769448,
    "end": 10770524,
    "text": "それでいいんだ。"
  },
  {
    "start": 10770944,
    "end": 10774408,
    "text": "これはステップごとのことだから、何も変わらない。"
  },
  {
    "start": 10774496,
    "end": 10783004,
    "text": "というのも、現在行っているすべての変更は、ステップ内の変更のようなものだからだ。"
  },
  {
    "start": 10783584,
    "end": 10788848,
    "text": "ここで重要なのは、モデルを構築するときに、実はちょっとした作業が必要だということだ。"
  },
  {
    "start": 10789016,
    "end": 10790264,
    "text": "ロジットの取得は非推奨。"
  },
  {
    "start": 10790304,
    "end": 10791684,
    "text": "モデルを作成する。"
  },
  {
    "start": 10793024,
    "end": 10798564,
    "text": "実際にモデルを分散データ並列コンテナにラップする必要がある。"
  },
  {
    "start": 10799264,
    "end": 10804122,
    "text": "これは、モデルをDDPコンテナにラップする方法である。"
  },
  {
    "start": 10804288,
    "end": 10805998,
    "text": "これがDDPのドキュメントだ。"
  },
  {
    "start": 10806046,
    "end": 10807790,
    "text": "かなり広範囲に及ぶ。"
  },
  {
    "start": 10807942,
    "end": 10814630,
    "text": "複数のプロセスが絡むと、すべてが10倍複雑になるので、注意すべき点はたくさんある。"
  },
  {
    "start": 10814782,
    "end": 10818582,
    "text": "大雑把に言えば、このデバイスのIDは渡されなければならないと思う。"
  },
  {
    "start": 10818678,
    "end": 10824126,
    "text": "さて、残念なことに、どのようなデバイスIDなのか、そのドキュメントが極めて不明確である。"
  },
  {
    "start": 10824270,
    "end": 10834880,
    "text": "実際にここに来てみると、デバイスIDが何であるかについてのこのコメントは、おおよそ意味不明だが、DDPのローカルランクであることは間違いないだろう。"
  },
  {
    "start": 10834952,
    "end": 10837764,
    "text": "DDPランクではなく、ローカルランクだ。"
  },
  {
    "start": 10838224,
    "end": 10840448,
    "text": "これがここで通過するものだ。"
  },
  {
    "start": 10840576,
    "end": 10841800,
    "text": "これはモデルを包み込む。"
  },
  {
    "start": 10841912,
    "end": 10846608,
    "text": "特に、DDPがフォワードパスでやってくれることは、実はまったく同じ動作なのだ。"
  },
  {
    "start": 10846696,
    "end": 10864574,
    "text": "私の理解では、フォワードパスでは何も変更すべきではありませんが、バックワードパスでは、あなたがバックワードパスを実行しているように、最も単純な設定では、バックワードパスが個々の独立したGPU上でオーバーすると、各独立したGPUがすべてのパラメータのグラディエントを持っています。"
  },
  {
    "start": 10864914,
    "end": 10877014,
    "text": "DDPがやってくれることは、後進が通過すると、Allreduceと呼ばれるものを呼び出し、基本的にすべてのランクのグラデーションの平均をとることだ。"
  },
  {
    "start": 10877634,
    "end": 10881266,
    "text": "そうすれば、すべてのランクにその平均値が入金される。"
  },
  {
    "start": 10881370,
    "end": 10884918,
    "text": "すべてのランクが平均で終わる。"
  },
  {
    "start": 10885106,
    "end": 10886718,
    "text": "だから、基本的にはそれがコミュニケーションなんだ。"
  },
  {
    "start": 10886806,
    "end": 10889110,
    "text": "グラデーションを同期させ、平均化するだけだ。"
  },
  {
    "start": 10889222,
    "end": 10890814,
    "text": "それがDDPだ。"
  },
  {
    "start": 10890934,
    "end": 10905022,
    "text": "というのも、トランスフォーマーのレイヤーをバックワード・パスで通過する際に、バックワード・パスが行われている間にグラデーションの通信をディスパッチすることができるからだ。"
  },
  {
    "start": 10905118,
    "end": 10911278,
    "text": "勾配の伝達と同期、そしてバックワードパスが重なっている。"
  },
  {
    "start": 10911406,
    "end": 10916194,
    "text": "そのほうが効率的だし、そのほうがいい。"
  },
  {
    "start": 10916494,
    "end": 10918354,
    "text": "それがDDPだ。"
  },
  {
    "start": 10918974,
    "end": 10922086,
    "text": "前進は変わらず、後進もほとんど変わらない。"
  },
  {
    "start": 10922150,
    "end": 10925634,
    "text": "この平均値を上乗せしている。"
  },
  {
    "start": 10925974,
    "end": 10928966,
    "text": "よし、では最適化に行こう。"
  },
  {
    "start": 10929150,
    "end": 10930598,
    "text": "ここでは何も変わらない。"
  },
  {
    "start": 10930766,
    "end": 10936794,
    "text": "ここでの最適化、つまりインナーループで、DDPにおけるこれらの勾配の同期について考えてみよう。"
  },
  {
    "start": 10937234,
    "end": 10945134,
    "text": "基本的に、デフォルトでは、先ほど言ったように、ここで後方へロスをすると、後方パスが実行され、グラデーションが同期されます。"
  },
  {
    "start": 10946794,
    "end": 10963810,
    "text": "ここでの問題は、グラデーションの累積ステップのループのため、実際には、後方でロスが発生するたびに同期を取りたくないということです。なぜなら、私たちはグラデーションを堆積させるだけで、それを連続的に行っているからです。"
  },
  {
    "start": 10963842,
    "end": 10965360,
    "text": "それは非常にもったいないことだ。"
  },
  {
    "start": 10965522,
    "end": 10974660,
    "text": "基本的には、それらを足し算して、一番最後のステップで、マイクロステップ、マイクロステップがグラダカムステップマイナスになるときだけだ。"
  },
  {
    "start": 10974812,
    "end": 10981260,
    "text": "この最後のステップで初めて、グラデーションを平均化するためのオーバードースを行う。"
  },
  {
    "start": 10981412,
    "end": 10989972,
    "text": "ちなみに、公式認可の方法は、この同期コンテキストマネージャーを使わないことだ。"
  },
  {
    "start": 10990148,
    "end": 10996022,
    "text": "Pytorchによると、これはDDPプロセス間の勾配同期を無効にするためのコンテキスト・マネージャーだという。"
  },
  {
    "start": 10996198,
    "end": 10999034,
    "text": "その中で、グラデーションが蓄積されていく。"
  },
  {
    "start": 10999934,
    "end": 11003670,
    "text": "基本的に、シンクがなければコミュニケーションは成立しない。"
  },
  {
    "start": 11003782,
    "end": 11014262,
    "text": "彼らは、DDPノシンクを行い、グラディエント・アキュムレーションでグラッドを積み重ね、別の入力でDDPをやり直せと言っているのだ。"
  },
  {
    "start": 11014398,
    "end": 11016622,
    "text": "ただ、これは本当に好きではない。"
  },
  {
    "start": 11016758,
    "end": 11018378,
    "text": "ただ、本当に好きじゃないんだ。"
  },
  {
    "start": 11018486,
    "end": 11021650,
    "text": "ここにコードをコピーペーストして、コンテキスト・マネージャーを使わなければならないという事実。"
  },
  {
    "start": 11021682,
    "end": 11023234,
    "text": "これは本当に醜い。"
  },
  {
    "start": 11023354,
    "end": 11030746,
    "text": "このソースコードを見ると、入力時にこの変数をトグルするだけであることがわかる。"
  },
  {
    "start": 11030890,
    "end": 11037706,
    "text": "これには後方グラッドの同期が必要で、この切り替えや変更が行われている。"
  },
  {
    "start": 11037810,
    "end": 11046238,
    "text": "これは、基本的にステップを踏むと、グラデーションが同期されるかどうかを決定するためにトグルされる変数である。"
  },
  {
    "start": 11046366,
    "end": 11049474,
    "text": "実はそれを直接使うのが好きなんだ。"
  },
  {
    "start": 11049894,
    "end": 11056350,
    "text": "その代わり、私が好きなのは次のようなことだ。"
  },
  {
    "start": 11056462,
    "end": 11063554,
    "text": "DDPを使うのであれば、基本的には同期を取るだけだ。"
  },
  {
    "start": 11063854,
    "end": 11069570,
    "text": "この変数が真になるのは、最終反復のときだけだ。"
  },
  {
    "start": 11069702,
    "end": 11073858,
    "text": "マイクロステップ内の他のすべての反復では、偽でありたい。"
  },
  {
    "start": 11073986,
    "end": 11076106,
    "text": "このように切り替えるだけだ。"
  },
  {
    "start": 11076250,
    "end": 11081134,
    "text": "後方グラフ同期が必要なのは、マイクロステップが最後のステップのときだけである。"
  },
  {
    "start": 11081994,
    "end": 11093402,
    "text": "DDPを変更すれば、この変数が消えてしまうかもしれないからだ。"
  },
  {
    "start": 11093498,
    "end": 11099730,
    "text": "今のところ、これでうまくいくと思っているし、コンテキスト・マネージャーの使用やコードの重複を避けることができる。"
  },
  {
    "start": 11099882,
    "end": 11105706,
    "text": "変数をトグルしているだけなのに、ロストバックするとほとんどのステップが同期されず、最後のステップが同期されるんだ。"
  },
  {
    "start": 11105810,
    "end": 11121174,
    "text": "だから、これが終わって外に出れば、すべての順位が突然、すべての順位に保存されていたすべてのグラデーションの平均値を持つようになる。"
  },
  {
    "start": 11121494,
    "end": 11130126,
    "text": "今、私たちはそれが私たちが望んでいることなのか、またこれで十分なのか、そしてそれが損失とどう連動するのかを考えなければならない。"
  },
  {
    "start": 11130230,
    "end": 11131518,
    "text": "ロス・アクムとは何か？"
  },
  {
    "start": 11131606,
    "end": 11133382,
    "text": "今、それを考えよう。"
  },
  {
    "start": 11133518,
    "end": 11141126,
    "text": "私が言いたいのは、グラディエーションを平均化したことは素晴らしいが、ロスのアクムはまだ影響を受けていないということだ。"
  },
  {
    "start": 11141230,
    "end": 11144518,
    "text": "これはDDPコンテナの外側だ。"
  },
  {
    "start": 11144606,
    "end": 11146234,
    "text": "平均化されていない"
  },
  {
    "start": 11146734,
    "end": 11156774,
    "text": "ここでクームのロスをプリントする場合、おそらくマスタープロセスのランクゼロにプリントすることになる。"
  },
  {
    "start": 11157114,
    "end": 11164162,
    "text": "その代わりに、全プロセスの損失とその平均を表示させたい。"
  },
  {
    "start": 11164218,
    "end": 11166106,
    "text": "平均損失額も知りたい。"
  },
  {
    "start": 11166250,
    "end": 11171534,
    "text": "単純にこの後、これは私が過去に使ったコードだ。"
  },
  {
    "start": 11172874,
    "end": 11174974,
    "text": "ロスfの代わりにロスが欲しい。"
  },
  {
    "start": 11176594,
    "end": 11182754,
    "text": "またTDPであれば、distはピトーチに分散される。"
  },
  {
    "start": 11182914,
    "end": 11184094,
    "text": "私はそれを輸入する。"
  },
  {
    "start": 11184434,
    "end": 11185734,
    "text": "何を輸入すればいいのですか？"
  },
  {
    "start": 11187594,
    "end": 11188534,
    "text": "なんてことだ。"
  },
  {
    "start": 11189394,
    "end": 11191866,
    "text": "このファイルは手に負えなくなってきたか？"
  },
  {
    "start": 11192050,
    "end": 11195250,
    "text": "もしそうなら、torchをdist."
  },
  {
    "start": 11195362,
    "end": 11198734,
    "text": "ディスト・ドット・オール・リデュース"
  },
  {
    "start": 11199354,
    "end": 11201856,
    "text": "ロシコムで平均値を出している。"
  },
  {
    "start": 11202010,
    "end": 11205108,
    "text": "したがって、このKoomテンソルの損失はすべてのランクに存在する。"
  },
  {
    "start": 11205236,
    "end": 11211916,
    "text": "アベレージのリダクションをすべて呼び出すと、それらの数値の平均が作成され、その平均がすべてのランクに加算される。"
  },
  {
    "start": 11212060,
    "end": 11218844,
    "text": "この呼び出しの後のすべてのランクは、クムの損失が平均化される。"
  },
  {
    "start": 11218964,
    "end": 11223740,
    "text": "だから、このマスター・プロセスで印刷する場合、クームの損失は他のすべてのランクでも同じである。"
  },
  {
    "start": 11223892,
    "end": 11229874,
    "text": "ここでマスター・プロセスが、おっと、こんな風に印刷したい。"
  },
  {
    "start": 11230294,
    "end": 11230710,
    "text": "オーケー。"
  },
  {
    "start": 11230742,
    "end": 11234846,
    "text": "最後に、トークンをさらに処理しないように注意しなければならない。"
  },
  {
    "start": 11234950,
    "end": 11247394,
    "text": "倍のDDPワールド・サイズ、これが上記で処理したトークンの数であり、それ以外はすべて問題ないはずだ。"
  },
  {
    "start": 11247774,
    "end": 11260044,
    "text": "もうひとつ気をつけなければならないのは、先ほども言ったように、ニッケルを知っていてよかったと思えるように、プロセスグループを破壊しておくことだ。"
  },
  {
    "start": 11261024,
    "end": 11262624,
    "text": "それでいいはずだ。"
  },
  {
    "start": 11262664,
    "end": 11263848,
    "text": "試乗してみよう。"
  },
  {
    "start": 11263936,
    "end": 11267704,
    "text": "さて、スクリプトを立ち上げ、間もなくここに印刷されるはずだ。"
  },
  {
    "start": 11267864,
    "end": 11270352,
    "text": "今は8つのGPUを同時に使ってトレーニングしている。"
  },
  {
    "start": 11270448,
    "end": 11275284,
    "text": "ステップの勾配の累積は32ではなく、8を割って4になっている。"
  },
  {
    "start": 11275984,
    "end": 11280742,
    "text": "そうでなければ、最適化はこのようになる。"
  },
  {
    "start": 11280878,
    "end": 11282398,
    "text": "わあ、すごいスピードだ。"
  },
  {
    "start": 11282446,
    "end": 11288518,
    "text": "現在、毎秒150万トークンを処理している。"
  },
  {
    "start": 11288646,
    "end": 11290702,
    "text": "すごい数字だ。"
  },
  {
    "start": 11290798,
    "end": 11296366,
    "text": "シェイクスピアのデータセットはとても小さいので、おそらく何度もエポックを繰り返しているのだろう。"
  },
  {
    "start": 11296550,
    "end": 11298314,
    "text": "だいたいこんな感じだ。"
  },
  {
    "start": 11299614,
    "end": 11307354,
    "text": "ところで、オプティマイザーを構成するモデルだったのだが、今はDDPモデルになっているので、これは機能しない。"
  },
  {
    "start": 11307474,
    "end": 11315018,
    "text": "その代わり、オプティマイザを構成する生モデルにしなければならない。生モデルは私がここで作成するものだ。"
  },
  {
    "start": 11315146,
    "end": 11332008,
    "text": "DDPの場合、モデル・モジュールと呼ばれるもので、GPT-2の生のnnモジュールが格納され、呼び出したいconfigure optimizers関数が含まれている。"
  },
  {
    "start": 11332146,
    "end": 11349064,
    "text": "1つ気になるのは、シングルGPUで動作させた場合と、シングルGPUで32グラディカムで動作させた場合を比較すると、数値が正確には一致しないことだ。"
  },
  {
    "start": 11349924,
    "end": 11352748,
    "text": "なぜそうなるかというと、ちょっとつまらない理由なんだ。"
  },
  {
    "start": 11352876,
    "end": 11358324,
    "text": "その理由は、データ・ローダーでは基本的に、少し違った方法でバッチを反復しているだけだからだ。"
  },
  {
    "start": 11358444,
    "end": 11361076,
    "text": "というのも、今はページ全体のデータを探しているからだ。"
  },
  {
    "start": 11361220,
    "end": 11367796,
    "text": "もしそのチャンクがトークンの数を超えていれば、ループするだけだ。"
  },
  {
    "start": 11367940,
    "end": 11375196,
    "text": "そのため、シングルGPUとGPUプロセスでは、リセット方法が若干異なります。"
  },
  {
    "start": 11375300,
    "end": 11379068,
    "text": "だから、ロットが微妙に違うので、数字も微妙に違ってくる。"
  },
  {
    "start": 11379236,
    "end": 11386080,
    "text": "これでいいと自分を納得させるひとつの方法は、バッチの総サイズをもっと小さくし、BとTを小さくすることだ。"
  },
  {
    "start": 11386232,
    "end": 11391936,
    "text": "だから、124×8の4倍を使ったと思う。"
  },
  {
    "start": 11392040,
    "end": 11395024,
    "text": "パッチのサイズは32768とした。"
  },
  {
    "start": 11395184,
    "end": 11400456,
    "text": "そこで、シングルGPUで8つのグラデーション累積ステップを実行するようにした。"
  },
  {
    "start": 11400520,
    "end": 11407072,
    "text": "その後、マルチGPUで、データローダーの境界の影響を減らして、数字が一致するのがわかるだろう。"
  },
  {
    "start": 11407168,
    "end": 11410418,
    "text": "要するに、僕たちは今、本当に、本当に速いスピードで走っているんだ。"
  },
  {
    "start": 11410576,
    "end": 11420182,
    "text": "最適化はGPT-2と3つのハイパーパラメータでほぼ一貫している。"
  },
  {
    "start": 11420238,
    "end": 11421854,
    "text": "では、次に移ろう。"
  },
  {
    "start": 11421894,
    "end": 11429994,
    "text": "では、GPT-2とGPT-3でどのようなデータセットが使われたかを見てみよう。"
  },
  {
    "start": 11430574,
    "end": 11433734,
    "text": "オープンウェブテキストと呼ばれる再現の試みがある。"
  },
  {
    "start": 11433894,
    "end": 11447442,
    "text": "基本的に、大雑把に言えば、彼らがこの論文で言っているのは、Redditからすべてのアウトバウンドリンクをスクレイピングし、少なくとも3つのカルマで、それが彼らの出発点のようなもので、すべてのウェブページ、すべてのウェブページとその中のすべてのテキストを収集したということだ。"
  },
  {
    "start": 11447578,
    "end": 11451930,
    "text": "というわけで、これは4500万のリンクで、最終的には40GBのテキストになった。"
  },
  {
    "start": 11452082,
    "end": 11457250,
    "text": "だから、GPT-2がデータセットについて言っているのは、だいたいこういうことなんだ。"
  },
  {
    "start": 11457282,
    "end": 11459426,
    "text": "それは基本的にRedditからのアウトバウンドリンクだ。"
  },
  {
    "start": 11459610,
    "end": 11469014,
    "text": "GPT-3ではトレーニングデータセットのセクションがあり、そこで一般的なクロールについて話し始める。"
  },
  {
    "start": 11469554,
    "end": 11478010,
    "text": "実際、GPT-2でも一般的なクロールについて話していたと思うが、基本的に非常にノイジーなので、それ自体ではあまり質の高いデータセットとは言えない。"
  },
  {
    "start": 11478042,
    "end": 11482426,
    "text": "これはインターネットの完全にランダムな部分集合であり、あなたが考えているよりずっと悪い。"
  },
  {
    "start": 11482610,
    "end": 11494724,
    "text": "しかし、そのほとんどは広告スパムのようなもので、数字や在庫をランダムに並べ、呪いをかけている。"
  },
  {
    "start": 11495104,
    "end": 11502704,
    "text": "そのため、人々は自分で作成し、注意深く扱った混合データを使ってトレーニングするのが好きなのだ。"
  },
  {
    "start": 11502824,
    "end": 11506472,
    "text": "これらのデータ混合物の大部分は、通常、コミックロールとなる。"
  },
  {
    "start": 11506528,
    "end": 11509336,
    "text": "例えば、トークンの50％がコミックロールになる。"
  },
  {
    "start": 11509480,
    "end": 11512896,
    "text": "そしてGPT-3では、ウェブテキストを使用している。"
  },
  {
    "start": 11512960,
    "end": 11517832,
    "text": "レッドディットはアウトバウンドだが、例えば書籍やウィキペディアも追加している。"
  },
  {
    "start": 11517928,
    "end": 11520312,
    "text": "他にもいろいろなことを追加することができる。"
  },
  {
    "start": 11520488,
    "end": 11523512,
    "text": "さて、このGPT-3のデータセットも公開されることはなかった。"
  },
  {
    "start": 11523648,
    "end": 11540640,
    "text": "例えば、赤いパジャマのデータセット、もっと具体的に言えば、赤いパジャマデータセットのスリムなパジャマサブセットである。"
  },
  {
    "start": 11540792,
    "end": 11549624,
    "text": "私が知っている限りでは、これはより一般的なクロールだが、処理方法は異なる。"
  },
  {
    "start": 11549784,
    "end": 11553600,
    "text": "そしてGitHub Books archiveやWikipedia Stack Exchangeがある。"
  },
  {
    "start": 11553712,
    "end": 11556564,
    "text": "このようなデータセットは、これらのデータ混合に入るだろう。"
  },
  {
    "start": 11556904,
    "end": 11562048,
    "text": "具体的には、最近出たもので私が気に入っているのは、Findwebデータセットというものだ。"
  },
  {
    "start": 11562216,
    "end": 11570828,
    "text": "これは基本的に、本当に質の高い一般的なクロールデータを収集し、フィルタリングする試みである。"
  },
  {
    "start": 11570996,
    "end": 11582540,
    "text": "さらに最近、ハギング・フェイスは、1.3兆の教育コンテンツと5.4兆の高教育コンテンツを含む、この素晴らしいウェブ・サブセットを発表した。"
  },
  {
    "start": 11582732,
    "end": 11588388,
    "text": "基本的に、彼らは一般的なクロールを非常に質の高い教育的サブセットにフィルタリングしようとしている。"
  },
  {
    "start": 11588556,
    "end": 11590852,
    "text": "これが私たちが使うものだ。"
  },
  {
    "start": 11590988,
    "end": 11599128,
    "text": "ファイン・ウェブには長いウェブページがあり、データをどのように処理するかについて詳しく説明されている。"
  },
  {
    "start": 11599216,
    "end": 11604704,
    "text": "データの混合物などに興味があり、これらのスケールでデータがどのように処理されるかに興味があるなら、ぜひお勧めしたい。"
  },
  {
    "start": 11604784,
    "end": 11606284,
    "text": "このページを見てくれ。"
  },
  {
    "start": 11606704,
    "end": 11613964,
    "text": "具体的には、ファイン・ウェブ（インターネット上の教育コンテンツ）を扱うことになると思う。"
  },
  {
    "start": 11615184,
    "end": 11622364,
    "text": "彼らの指標にある教育的内容のトレーニングは、実に、実にうまく機能することを示している。"
  },
  {
    "start": 11623144,
    "end": 11636088,
    "text": "何兆ものトークンをトレーニングするつもりはないので、このサンプル、100億トークンのサブサンプルを使います。"
  },
  {
    "start": 11636216,
    "end": 11644528,
    "text": "というのも、経験的に、私のこれまでの数回の実験では、GPT-2の性能に本当に近づけるにはこれで十分だし、作業も簡単だからだ。"
  },
  {
    "start": 11644576,
    "end": 11647472,
    "text": "では、サンプルの10本でやってみよう。"
  },
  {
    "start": 11647608,
    "end": 11653840,
    "text": "我々の目標は、それをダウンロードし、処理し、データ・ローダーがそれを処理できるようにすることだ。"
  },
  {
    "start": 11653912,
    "end": 11655440,
    "text": "さあ、始めよう。"
  },
  {
    "start": 11655552,
    "end": 11663808,
    "text": "さて、ここでもうひとつのファイルを紹介したが、これは基本的にハグする顔のデータセットから細かいウェブをダウンロードするものだ。"
  },
  {
    "start": 11663936,
    "end": 11673714,
    "text": "すべてのデータを事前に処理してトークン化し、データの破片をローカルディスクのフォルダに保存する。"
  },
  {
    "start": 11674134,
    "end": 11683414,
    "text": "これを実行している間に、データセット・ビューアーを見て、この中に何があるのかを知ることができます。"
  },
  {
    "start": 11683534,
    "end": 11684430,
    "text": "ちょっと面白いね。"
  },
  {
    "start": 11684462,
    "end": 11697238,
    "text": "つまり、基本的にはかなりうまくいっているように見えるんだ。フランスの原子力エネルギーの話や、メキシコのアメリカの話、カササギJの話などね。"
  },
  {
    "start": 11697286,
    "end": 11700400,
    "text": "実際、彼らのフィルターはかなり機能しているようだ。"
  },
  {
    "start": 11700592,
    "end": 11706272,
    "text": "ちなみに、ここでのフィルターはllama 370 bを使って自動的に適用されたものだと思う。"
  },
  {
    "start": 11706408,
    "end": 11712844,
    "text": "つまり、基本的にLLMは、どのコンテンツが教育的であるかを判断し、そのフィルターを通すことになる。"
  },
  {
    "start": 11713264,
    "end": 11714296,
    "text": "それはかなりクールだ。"
  },
  {
    "start": 11714400,
    "end": 11721512,
    "text": "さて、脚本そのものについては、LLMほど興味深くなく、LLM中心でもないので、脚本全文を紹介するつもりはない。"
  },
  {
    "start": 11721648,
    "end": 11727480,
    "text": "これを実行すると、基本的には、まずデータセットをロードする。"
  },
  {
    "start": 11727512,
    "end": 11731244,
    "text": "これを実行するには、pipでdatasetsをインストールする必要がある。"
  },
  {
    "start": 11733344,
    "end": 11739320,
    "text": "データセットをダウンロードし、このデータセット内のすべてのドキュメントをトークン化する。"
  },
  {
    "start": 11739432,
    "end": 11750360,
    "text": "さて、文書をトークン化するとき、ひとつの文書をトークン化するには、まずテキスト・トークンの末尾からトークンを開始することに気づくだろう。"
  },
  {
    "start": 11750512,
    "end": 11753230,
    "text": "これはGPT-2トークナイザーの特別なトークンです。"
  },
  {
    "start": 11753312,
    "end": 11757250,
    "text": "50,256は文末のID。"
  },
  {
    "start": 11757402,
    "end": 11760594,
    "text": "これは、本文の終わりと呼ばれていても、文書を始めるものである。"
  },
  {
    "start": 11760714,
    "end": 11763922,
    "text": "これは文書を開始する最初のトークンである。"
  },
  {
    "start": 11764098,
    "end": 11767602,
    "text": "次に、その文書のすべてのトークンで拡張する。"
  },
  {
    "start": 11767778,
    "end": 11770186,
    "text": "それからnumpy配列を作る。"
  },
  {
    "start": 11770370,
    "end": 11773094,
    "text": "私たちは、すべてのトークンの間にあることを確認します。"
  },
  {
    "start": 11773674,
    "end": 11777194,
    "text": "よし、デバッグしてみよう。"
  },
  {
    "start": 11777354,
    "end": 11778850,
    "text": "オーケー、それについては申し訳ない。"
  },
  {
    "start": 11778962,
    "end": 11781674,
    "text": "Pythonでfloatの除算を使ったのが原因だった。"
  },
  {
    "start": 11781714,
    "end": 11785124,
    "text": "これは整数除算でなければならない。"
  },
  {
    "start": 11787104,
    "end": 11787496,
    "text": "オーケー。"
  },
  {
    "start": 11787520,
    "end": 11790624,
    "text": "基本的に、ここでのトークン化は比較的簡単である。"
  },
  {
    "start": 11790744,
    "end": 11793484,
    "text": "MP Un 16 のトークンを返す。"
  },
  {
    "start": 11793864,
    "end": 11800536,
    "text": "16から1を引いた2が65,000なので、スペースを少し節約するためにウン16を使っている。"
  },
  {
    "start": 11800640,
    "end": 11803884,
    "text": "GPT-2の最大トークンIDはそれをはるかに下回る。"
  },
  {
    "start": 11804344,
    "end": 11807984,
    "text": "そしてここにはマルチプロセッシングのコードがたくさんある。"
  },
  {
    "start": 11808024,
    "end": 11809606,
    "text": "私はそれを通り抜けるつもりはない。"
  },
  {
    "start": 11809630,
    "end": 11814958,
    "text": "データセットをロードし、トークン化し、すべてをシャードに保存する。"
  },
  {
    "start": 11815086,
    "end": 11817194,
    "text": "シャードはnumpyファイルである。"
  },
  {
    "start": 11817814,
    "end": 11823834,
    "text": "これはトーチ・テンソルにとてもよく似ている。"
  },
  {
    "start": 11824334,
    "end": 11828566,
    "text": "最初のシャード、ゼロゼロは検証シャードである。"
  },
  {
    "start": 11828630,
    "end": 11831422,
    "text": "他のシャードはすべてトレーニング用のシャードだ。"
  },
  {
    "start": 11831558,
    "end": 11834646,
    "text": "さっきも言ったように、どれも1億トークンが入っている。"
  },
  {
    "start": 11834670,
    "end": 11835474,
    "text": "その通りだ。"
  },
  {
    "start": 11836474,
    "end": 11843506,
    "text": "というのも、1つの巨大なファイルがあれば、ファイルをシャード化する作業が簡単になるからだ。"
  },
  {
    "start": 11843530,
    "end": 11845794,
    "text": "ディスク上で作業するのが難しいこともある。"
  },
  {
    "start": 11845914,
    "end": 11849934,
    "text": "だから、シャーディングしたほうが、その観点からはすっきりする。"
  },
  {
    "start": 11850754,
    "end": 11853018,
    "text": "それで、このまま走らせることにしよう。"
  },
  {
    "start": 11853066,
    "end": 11857642,
    "text": "おそらく30分ほどかかるだろう。"
  },
  {
    "start": 11857698,
    "end": 11860138,
    "text": "その後、このデータで実際にトレーニングをするために戻ってくる。"
  },
  {
    "start": 11860266,
    "end": 11863020,
    "text": "今回は、実際に合法的な事前トレーニングを行うつもりだ。"
  },
  {
    "start": 11863052,
    "end": 11864820,
    "text": "これは良いデータだ。"
  },
  {
    "start": 11864972,
    "end": 11866964,
    "text": "我々は1秒間にたくさんのトークンを行っている。"
  },
  {
    "start": 11867124,
    "end": 11868252,
    "text": "我々は8つのGPUを持っている。"
  },
  {
    "start": 11868308,
    "end": 11869260,
    "text": "コードはできている。"
  },
  {
    "start": 11869372,
    "end": 11871900,
    "text": "というわけで、実際に本格的なトレーニング・ランを行うことになった。"
  },
  {
    "start": 11871972,
    "end": 11873864,
    "text": "少し戻ろう。"
  },
  {
    "start": 11874164,
    "end": 11875524,
    "text": "よし、戻ってきた。"
  },
  {
    "start": 11875684,
    "end": 11881544,
    "text": "lseでfine webを実行すると、100個のシャードがあることがわかる。"
  },
  {
    "start": 11882484,
    "end": 11885532,
    "text": "各シャードは1億トークンだから、それは理にかなっている。"
  },
  {
    "start": 11885588,
    "end": 11888664,
    "text": "そのうちの100枚は合計で100億トークンだ。"
  },
  {
    "start": 11889064,
    "end": 11897072,
    "text": "さて、メインファイルに移って、データ・ローダーの調整を行った。"
  },
  {
    "start": 11897168,
    "end": 11899864,
    "text": "細かいウェブの破片を使いたい。"
  },
  {
    "start": 11900024,
    "end": 11904200,
    "text": "ここに、基本的にシャードをロードするためのコードがある。"
  },
  {
    "start": 11904392,
    "end": 11914196,
    "text": "Un 16 numpyファイルをロードし、トーチ・ロング・テンソルに変換する。"
  },
  {
    "start": 11914360,
    "end": 11916744,
    "text": "では、ここではすべてのシャードを列挙しているだけだ。"
  },
  {
    "start": 11917124,
    "end": 11927236,
    "text": "また、データ・ローダー・ライトにスプリットを追加し、スプリット・トレインだけでなく、スプリット・バル、ゼロ・スプリット、そしてシャードをロードできるようにした。"
  },
  {
    "start": 11927300,
    "end": 11932324,
    "text": "となると、ここでは現在のポジションだけでなく、現在のシャードもある。"
  },
  {
    "start": 11932444,
    "end": 11934460,
    "text": "我々はシャード内にポジションを持っている。"
  },
  {
    "start": 11934572,
    "end": 11944484,
    "text": "そして、1つのシャードのトークンがなくなったら、まずシャードを前進させ、必要ならループさせ、それからトークンを取得して位置を再調整する。"
  },
  {
    "start": 11944784,
    "end": 11948884,
    "text": "このデータ・ローダーは、すべてのシャードを反復処理する。"
  },
  {
    "start": 11949464,
    "end": 11950552,
    "text": "それを変えたんだ。"
  },
  {
    "start": 11950608,
    "end": 11957884,
    "text": "データを処理している間にもうひとつやったことは、トレイン・ローダーがスプリット・トレインになったことだ。"
  },
  {
    "start": 11958384,
    "end": 11961232,
    "text": "この下で、いくつかの数字をセットしたんだ。"
  },
  {
    "start": 11961288,
    "end": 11977284,
    "text": "私たちは1ステップにつき2～19のトークンを使用しており、およそ100億トークンを使用したいと考えています。"
  },
  {
    "start": 11977404,
    "end": 11983732,
    "text": "仮に100億トークンを使ったとすると、それを2で割って19にすると、19,073ステップになる。"
  },
  {
    "start": 11983788,
    "end": 11985024,
    "text": "そこから来ているんだ。"
  },
  {
    "start": 11985364,
    "end": 11991344,
    "text": "GPT-3の論文によれば、3億7500万トークンで学習レートをウォームアップしたという。"
  },
  {
    "start": 11991744,
    "end": 12000368,
    "text": "ここに来て、375の6つのトークンを2で割ると、19は715歩になる。"
  },
  {
    "start": 12000456,
    "end": 12003024,
    "text": "だからウォームアップのステップは715に設定されている。"
  },
  {
    "start": 12003144,
    "end": 12008084,
    "text": "これはGPT-3が使ったウォームアップのスケジュールと完全に一致する。"
  },
  {
    "start": 12008424,
    "end": 12011632,
    "text": "ところで、715はとてもマイルドだと思う。"
  },
  {
    "start": 12011728,
    "end": 12013608,
    "text": "これはかなりアグレッシブにできるだろう。"
  },
  {
    "start": 12013656,
    "end": 12015764,
    "text": "おそらく100でも十分だと思う。"
  },
  {
    "start": 12016964,
    "end": 12017692,
    "text": "大丈夫だよ。"
  },
  {
    "start": 12017708,
    "end": 12026744,
    "text": "GPT-3の正確なハイパーパラメーターがわかるように、今は置いておこう。"
  },
  {
    "start": 12027244,
    "end": 12027932,
    "text": "私たちは走ることができる。"
  },
  {
    "start": 12027988,
    "end": 12032424,
    "text": "これでスクリプトを起動できる。"
  },
  {
    "start": 12033324,
    "end": 12033828,
    "text": "実際にそうだ。"
  },
  {
    "start": 12033876,
    "end": 12035184,
    "text": "すみません、もうひとつやらせてください。"
  },
  {
    "start": 12038844,
    "end": 12039864,
    "text": "すみません。"
  },
  {
    "start": 12041364,
    "end": 12045368,
    "text": "私のGpuでは、実際にはもっと多くのバッチサイズに対応できるし、太ることもできると思う。"
  },
  {
    "start": 12045456,
    "end": 12049912,
    "text": "私のGPUにはマイクロバッチサイズとして64個搭載できる。"
  },
  {
    "start": 12049968,
    "end": 12051524,
    "text": "やってみよう"
  },
  {
    "start": 12055224,
    "end": 12059696,
    "text": "記憶違いかもしれないが、1GPUあたり124の64倍ということだ。"
  },
  {
    "start": 12059760,
    "end": 12061208,
    "text": "となると、8つのGPUがあることになる。"
  },
  {
    "start": 12061336,
    "end": 12072254,
    "text": "ということは、もしこれがフィットすれば、勾配累積をする必要さえないことになる。なぜなら、これは単に全バッチサイズに乗算されるだけだから、勾配累積はない。"
  },
  {
    "start": 12072954,
    "end": 12075214,
    "text": "それがフィットすれば、かなり速く走れるだろう。"
  },
  {
    "start": 12086754,
    "end": 12088094,
    "text": "行こう、行こう。"
  },
  {
    "start": 12088954,
    "end": 12092294,
    "text": "つまり、もしこれがうまくいくなら、これは基本的に本格的なトレーニング前のランニングということになる。"
  },
  {
    "start": 12092874,
    "end": 12097686,
    "text": "まだログは取っていないし、検証スプリットの評価もしていない。"
  },
  {
    "start": 12097790,
    "end": 12098374,
    "text": "それは違う。"
  },
  {
    "start": 12098414,
    "end": 12100870,
    "text": "私たちはまだ \"T \"の字も \"I \"の字も書いていない。"
  },
  {
    "start": 12101022,
    "end": 12106366,
    "text": "このまましばらく走らせれば、かなりいいモデルができそうだ。"
  },
  {
    "start": 12106470,
    "end": 12111246,
    "text": "GPT2124mと同等か、それ以上かもしれない。"
  },
  {
    "start": 12111350,
    "end": 12115390,
    "text": "順調のようだね。"
  },
  {
    "start": 12115462,
    "end": 12118514,
    "text": "我々は毎秒150万トークンを処理している。"
  },
  {
    "start": 12120694,
    "end": 12122634,
    "text": "ここにあるものはすべて良さそうだ。"
  },
  {
    "start": 12123354,
    "end": 12129134,
    "text": "1回の反復で330ミリ秒を費やしている。"
  },
  {
    "start": 12130314,
    "end": 12131482,
    "text": "どこで印刷するんだ？"
  },
  {
    "start": 12131538,
    "end": 12132722,
    "text": "1973."
  },
  {
    "start": 12132858,
    "end": 12134442,
    "text": "だから、19.0.7だ。"
  },
  {
    "start": 12134498,
    "end": 12140474,
    "text": "0.33の3倍はこの秒数、この分数だ。"
  },
  {
    "start": 12140634,
    "end": 12143774,
    "text": "これは1.7時間作動する。"
  },
  {
    "start": 12145434,
    "end": 12151818,
    "text": "このように1時間半も走れば、グラデーションの累積を使う必要もない。"
  },
  {
    "start": 12151866,
    "end": 12154106,
    "text": "あなたのGPUにはそんな余裕はないかもしれない。"
  },
  {
    "start": 12154210,
    "end": 12157458,
    "text": "その場合は、収まるまでバッチサイズを小さくしていけばいい。"
  },
  {
    "start": 12157586,
    "end": 12159254,
    "text": "いい数字に抑える。"
  },
  {
    "start": 12161074,
    "end": 12162218,
    "text": "それはかなりエキサイティングだ。"
  },
  {
    "start": 12162346,
    "end": 12165658,
    "text": "現在、学習率を温めているところなので、まだ非常に低いことがわかるだろう。"
  },
  {
    "start": 12165706,
    "end": 12166642,
    "text": "ひとつはマイナス4。"
  },
  {
    "start": 12166738,
    "end": 12173614,
    "text": "次の数ステップで、6eマイナス4まで上昇する。"
  },
  {
    "start": 12174494,
    "end": 12175294,
    "text": "とてもクールだ。"
  },
  {
    "start": 12175414,
    "end": 12187806,
    "text": "今、私がしたいことは、TとIを交差させ、検証を分割して評価することだ。そして、どのように検証を実行し、どのようにロギングを行い、どのように損失を可視化するか、そしてすべての良いことを考え出そう。"
  },
  {
    "start": 12187910,
    "end": 12190934,
    "text": "では、実際に走る前にその話をしよう。"
  },
  {
    "start": 12191054,
    "end": 12194670,
    "text": "では、バリデーション・スプリットで評価するようにコードを調整しました。"
  },
  {
    "start": 12194782,
    "end": 12202404,
    "text": "つまり、split equals Valを渡してValローダーを作成すれば、基本的に検証シャード専用のデータローダーが作成される。"
  },
  {
    "start": 12204344,
    "end": 12211528,
    "text": "もうひとつは、データ・ローダーに新しい関数resetを導入したことで、これはinitと呼ばれ、基本的にデータ・ローダーをリセットする。"
  },
  {
    "start": 12211656,
    "end": 12218324,
    "text": "というのも、メイン・トレーニング・ループに来たときにとても役に立つからだ。"
  },
  {
    "start": 12218824,
    "end": 12225144,
    "text": "基本的に、0回目の反復を含む100回目の反復ごとに、モデルを評価モードにする。"
  },
  {
    "start": 12225264,
    "end": 12230296,
    "text": "バル・ローダーをリセットして、グラデーションは関係なくなった。"
  },
  {
    "start": 12230440,
    "end": 12239604,
    "text": "基本的には20ステップにわたって勾配を累積し、それをすべて平均して検証損失をプリントアウトする。"
  },
  {
    "start": 12240064,
    "end": 12245608,
    "text": "つまり、基本的にはトレーニングループとまったく同じロジックだ。"
  },
  {
    "start": 12245736,
    "end": 12248304,
    "text": "それは推論に過ぎない。"
  },
  {
    "start": 12248344,
    "end": 12250408,
    "text": "私たちは損失を計測し、それを合計しているだけだ。"
  },
  {
    "start": 12250496,
    "end": 12254184,
    "text": "それ以外はすべて当てはまるし、これまで見てきたのとまったく同じだ。"
  },
  {
    "start": 12254344,
    "end": 12260444,
    "text": "つまり、最初の反復も含めて、100回目の反復ごとに検証結果を表示することになる。"
  },
  {
    "start": 12261504,
    "end": 12262584,
    "text": "それはいいね。"
  },
  {
    "start": 12262744,
    "end": 12266728,
    "text": "そうすれば、どの程度オーバーフィットしているのか、ある程度、少しはわかるだろう。"
  },
  {
    "start": 12266896,
    "end": 12273312,
    "text": "とはいえ、我々はほぼ無限大のデータを持っているので、列車とバルのロスはほぼ同じだと考えている。"
  },
  {
    "start": 12273488,
    "end": 12284482,
    "text": "GPT2124mをOpenAIがリリースしているので、それを使って初期化することができます。"
  },
  {
    "start": 12284578,
    "end": 12289914,
    "text": "つまり、このモデルが124mの距離でどの程度一般化できるかの目安になる。"
  },
  {
    "start": 12289994,
    "end": 12290858,
    "text": "それは違う。"
  },
  {
    "start": 12291026,
    "end": 12294042,
    "text": "ウェブ認証が分割されているのを見つけて申し訳ない。"
  },
  {
    "start": 12294178,
    "end": 12301410,
    "text": "とはいえ、GPT-2はまったく異なるデータ分布でトレーニングされているため、GPT-2との超フェアな比較にはならないが、それでも興味深いデータポイントのようなものだ。"
  },
  {
    "start": 12301602,
    "end": 12310908,
    "text": "いずれにせよ、このようなトレーニングでは、オーバーフィッティングになっていないかどうかを確認するために、必ず検証を分割する必要がある。"
  },
  {
    "start": 12311036,
    "end": 12315824,
    "text": "これは特に、トレーニングデータのエポック数を増やした場合に懸念される。"
  },
  {
    "start": 12316644,
    "end": 12319172,
    "text": "例えば、今はシングル・エポックだけをやっている。"
  },
  {
    "start": 12319228,
    "end": 12332730,
    "text": "もし、10エポックとかでトレーニングするようになったら、十分な大きさのモデルを持っている場合、データを記憶しすぎているかもしれないので、本当に注意する必要がある。"
  },
  {
    "start": 12332852,
    "end": 12333102,
    "text": "オーケー。"
  },
  {
    "start": 12333118,
    "end": 12339374,
    "text": "それに加えて、スクリプトの一番下に、昔からのサンプリング用のコードが残っていたのを覚えているだろうか。"
  },
  {
    "start": 12339494,
    "end": 12343550,
    "text": "そのコードを削除して、ここに移したんだ。"
  },
  {
    "start": 12343622,
    "end": 12355822,
    "text": "たまにはサンプル検証を行い、たまにはサンプルを生成し、それを100ステップごとに行い、すべてのステップでトレーニングを行う。"
  },
  {
    "start": 12355958,
    "end": 12359906,
    "text": "そうやって、僕は今、構造を持っていて、これを1000回繰り返しているんだ。"
  },
  {
    "start": 12360050,
    "end": 12362294,
    "text": "反復1000のサンプルはこちら。"
  },
  {
    "start": 12365834,
    "end": 12366186,
    "text": "こんにちは。"
  },
  {
    "start": 12366210,
    "end": 12368774,
    "text": "私は言語モデルで、これ以上クリエイティブになることはできない。"
  },
  {
    "start": 12369914,
    "end": 12374934,
    "text": "私は言語モデルであり、ここで学んでいる言語ファイルはコンピューターの始まりである。"
  },
  {
    "start": 12376034,
    "end": 12385800,
    "text": "まだ文字化けが残っていますが、反復1000回目で、最大学習率にギリギリ到達したところです。"
  },
  {
    "start": 12385922,
    "end": 12387744,
    "text": "これはまだ勉強中だ。"
  },
  {
    "start": 12388044,
    "end": 12391624,
    "text": "あと1,000個ほどでサンプルが上がってくるところだ。"
  },
  {
    "start": 12392924,
    "end": 12393784,
    "text": "オーケー。"
  },
  {
    "start": 12396204,
    "end": 12396852,
    "text": "オーケー。"
  },
  {
    "start": 12396948,
    "end": 12400144,
    "text": "これはね、モデルはまだ若い赤ちゃんなんだ。"
  },
  {
    "start": 12400564,
    "end": 12408060,
    "text": "さて、基本的にここに書いたサンプリング・コードはすべて、以前から知っているものばかりだ。"
  },
  {
    "start": 12408172,
    "end": 12422822,
    "text": "Pytorchでジェネレーターオブジェクトを作成し、乱数のサンプリングを直接コントロールできるようにしました。"
  },
  {
    "start": 12422918,
    "end": 12425502,
    "text": "トレーニングのループから完全に外れるようにしたい。"
  },
  {
    "start": 12425678,
    "end": 12433230,
    "text": "だから、特別なサンプリングRNGを使い、すべてのランクが異なるシードを持つようにシードするんだ。"
  },
  {
    "start": 12433342,
    "end": 12439688,
    "text": "ここで乱数を多項式で消費し、サンプリングを行う。"
  },
  {
    "start": 12439816,
    "end": 12442192,
    "text": "そこでジェネレーター・オブジェクトを渡すようにしている。"
  },
  {
    "start": 12442328,
    "end": 12443804,
    "text": "それ以外は同じだ。"
  },
  {
    "start": 12444264,
    "end": 12448128,
    "text": "もうひとつは、走りが少し遅くなっていることにお気づきだろう。"
  },
  {
    "start": 12448256,
    "end": 12452640,
    "text": "というのも、これをサンプルにするためには、トーチコンパイルを無効にしなければならなかったからだ。"
  },
  {
    "start": 12452792,
    "end": 12455048,
    "text": "だから、少し遅く走っている。"
  },
  {
    "start": 12455176,
    "end": 12457144,
    "text": "なぜか、トーチのコンパイルなしで動作する。"
  },
  {
    "start": 12457184,
    "end": 12462680,
    "text": "トーチでモデルをコンパイルすると、Pytorchから本当に恐ろしいエラーが出ます。"
  },
  {
    "start": 12462792,
    "end": 12467110,
    "text": "おそらく、このコードがリリースされたりする頃には、直っているかもしれない。"
  },
  {
    "start": 12467222,
    "end": 12474154,
    "text": "とりあえずfalseで終了して、トルクコンパイルを復活させます。"
  },
  {
    "start": 12475054,
    "end": 12477126,
    "text": "後で修正しようと思う。"
  },
  {
    "start": 12477310,
    "end": 12480502,
    "text": "ところで、私はこのコードをすべて公開する予定だ。"
  },
  {
    "start": 12480598,
    "end": 12484950,
    "text": "実際、何かを追加するたびにgitコミットをするよう、細心の注意を払ってきた。"
  },
  {
    "start": 12485062,
    "end": 12492506,
    "text": "というわけで、完全にゼロからスタートしたレポ全体を、現在に至るまで、そしてこの後にも公開するつもりだ。"
  },
  {
    "start": 12492610,
    "end": 12495734,
    "text": "そのため、gitのコミット履歴にはすべてが正確に記録されているはずだ。"
  },
  {
    "start": 12496834,
    "end": 12498714,
    "text": "それはいいことだと思う。"
  },
  {
    "start": 12498794,
    "end": 12503706,
    "text": "あなたがGitHubにアクセスする頃には、このバグが取り除かれ、動作するようになっていればいいのですが。"
  },
  {
    "start": 12503850,
    "end": 12508866,
    "text": "さて、ここで最適化を実行し、ステップを踏んでいる。"
  },
  {
    "start": 12508890,
    "end": 12511274,
    "text": "今、トレーニングの30％を終えたところだ。"
  },
  {
    "start": 12511314,
    "end": 12519444,
    "text": "このトレーニングの間に、検証サイクルを補足するために使用する評価をひとつ紹介したい。"
  },
  {
    "start": 12519824,
    "end": 12520392,
    "text": "こんにちは。"
  },
  {
    "start": 12520408,
    "end": 12523432,
    "text": "スワッグは2019年の本紙から。"
  },
  {
    "start": 12523488,
    "end": 12525320,
    "text": "もう5年前の評価だ。"
  },
  {
    "start": 12525512,
    "end": 12530040,
    "text": "ヘラスワグの仕組みは、基本的に文章完成のデータセットがある。"
  },
  {
    "start": 12530192,
    "end": 12533544,
    "text": "どの質問も多肢選択式だ。"
  },
  {
    "start": 12533584,
    "end": 12536064,
    "text": "私たちは基本的に文脈を共有している。"
  },
  {
    "start": 12536144,
    "end": 12538896,
    "text": "女性がバケツと犬を持って外にいるようにね。"
  },
  {
    "start": 12539040,
    "end": 12541456,
    "text": "犬は風呂を避けようと走り回っている。"
  },
  {
    "start": 12541600,
    "end": 12556426,
    "text": "A、石鹸でバケツを洗い流し、犬の頭をドライヤーで乾かす、B、ホースで石鹸がつかないようにする、C、犬を濡らすとまた逃げる、D、犬と一緒にバスタブに入る。"
  },
  {
    "start": 12556610,
    "end": 12574724,
    "text": "つまり、基本的には、これらの複数の選択肢は、そのうちの1つが文の自然な続きであり、他のものはそうでないように構成されているということである。"
  },
  {
    "start": 12574764,
    "end": 12575812,
    "text": "それは意味がない。"
  },
  {
    "start": 12575948,
    "end": 12581516,
    "text": "そのため、あまり訓練されていないモデルは、これらを見分けることができない。"
  },
  {
    "start": 12581620,
    "end": 12590844,
    "text": "多くの世界に関する知識を持っていて、世界について多くを語ることができるモデルは、このような完成形を作り出すことができるだろう。"
  },
  {
    "start": 12591004,
    "end": 12596160,
    "text": "これらの文章はActivitynetとWikihowからの出典である。"
  },
  {
    "start": 12596352,
    "end": 12605288,
    "text": "論文の下のほうに、Wikihowのドメインの種類のクールなチャートのようなものがある。"
  },
  {
    "start": 12605336,
    "end": 12621514,
    "text": "電子機器や家庭菜園にコンピュータからたくさんの文章があり、最も可能性の高い完成形とその完成形の正体を見つけるために、世界について知る必要がある種類のことを幅広くカバーしている。"
  },
  {
    "start": 12622014,
    "end": 12634126,
    "text": "もうひとつ、テラ・スワッグについて興味深いのは、誤った選択肢を意図的に、敵対的なソースで構成していることだ。"
  },
  {
    "start": 12634270,
    "end": 12639022,
    "text": "それらは単なるランダムな文章ではなく、実際には言語モデルによって生成された文章なのだ。"
  },
  {
    "start": 12639158,
    "end": 12644950,
    "text": "言語モデルは基本的に難しいと感じるが、人間は簡単だと感じるように生成される。"
  },
  {
    "start": 12645102,
    "end": 12648810,
    "text": "だから、このセットでは95％の精度があるという。"
  },
  {
    "start": 12648922,
    "end": 12652706,
    "text": "当時、最先端の言語モデルは48％しかなかった。"
  },
  {
    "start": 12652850,
    "end": 12655254,
    "text": "だから、当時はこれが良いベンチマークだった。"
  },
  {
    "start": 12655594,
    "end": 12659654,
    "text": "この論文の詳細を読むことができる。"
  },
  {
    "start": 12660474,
    "end": 12670074,
    "text": "しかし、指摘しておきたいのは、これは5年前のことで、それ以来、ハロー・スワッグに何が起こったかというと、それは完全にただ解決されたということだ。"
  },
  {
    "start": 12670194,
    "end": 12673114,
    "text": "というわけで、言語モデルは96％になった。"
  },
  {
    "start": 12673194,
    "end": 12679326,
    "text": "基本的に、最後の4％というのは、おそらくデータセットに誤りがあるか、設問が本当に、本当に難しいのだろう。"
  },
  {
    "start": 12679470,
    "end": 12682614,
    "text": "だから、基本的にこのデータセットは、言語モデルに関しては、ある意味、破綻している。"
  },
  {
    "start": 12682694,
    "end": 12685714,
    "text": "当時、最高の言語モデルは50％程度だった。"
  },
  {
    "start": 12686054,
    "end": 12689326,
    "text": "これが事態の進展だ。"
  },
  {
    "start": 12689510,
    "end": 12698718,
    "text": "ちなみにGPT-2では使われていないが、GPT-3では地獄のスワッグ評価がある。"
  },
  {
    "start": 12698846,
    "end": 12701274,
    "text": "多くの人がヘラ・スワガーを使っている。"
  },
  {
    "start": 12702244,
    "end": 12714784,
    "text": "GPT-3の結果がここに引用されているので、GPT-3がヘラ・スワッグの評価において、さまざまなモデルのチェックポイントで何パーセントの精度を達成したかがわかる。"
  },
  {
    "start": 12715084,
    "end": 12723116,
    "text": "ハロー・スワッグが好まれている理由は、スムースな評価であり、早期シグナルを提供する評価だからだ。"
  },
  {
    "start": 12723300,
    "end": 12734376,
    "text": "アーリーシグナルとは、小さな言語モデルでもランダムな確率で25％からスタートするが、徐々に向上し、25、26、27......と表示されるようになることを意味する。"
  },
  {
    "start": 12734560,
    "end": 12746920,
    "text": "そのため、スムーズで、初期シグナルがあり、長い間存在している。"
  },
  {
    "start": 12746952,
    "end": 12749884,
    "text": "だから、みんなこの評価が好きなんだ。"
  },
  {
    "start": 12750704,
    "end": 12754714,
    "text": "さて、この評価方法だが、次のようになる。"
  },
  {
    "start": 12755534,
    "end": 12760590,
    "text": "先ほど言ったように、私たちはコンテクストを共有しており、これは多肢選択課題のようなものだ。"
  },
  {
    "start": 12760702,
    "end": 12772286,
    "text": "モデルに多肢選択式の問題を与えて、a、b、c、dのいずれかを選択させる代わりに、それを行うことはできない。"
  },
  {
    "start": 12772390,
    "end": 12777766,
    "text": "彼らは多肢選択式の選択肢のひとつにラベルを関連付けるという概念を理解していない。"
  },
  {
    "start": 12777950,
    "end": 12779090,
    "text": "彼らはそれを理解していない。"
  },
  {
    "start": 12779182,
    "end": 12781322,
    "text": "私たちはそれをネイティブの形で彼らに与えなければならない。"
  },
  {
    "start": 12781418,
    "end": 12784334,
    "text": "ネイティブ・フォームはトークン補完である。"
  },
  {
    "start": 12784714,
    "end": 12785634,
    "text": "これが我々の仕事だ。"
  },
  {
    "start": 12785674,
    "end": 12792026,
    "text": "4つの行とt個のトークン（tが何であろうと）からなるバッチを作る。"
  },
  {
    "start": 12792210,
    "end": 12796314,
    "text": "そして、共有されたコンテキスト、それは基本的に4つの選択肢のコンテキストである。"
  },
  {
    "start": 12796434,
    "end": 12800490,
    "text": "そのトークンはすべての行で共有される。"
  },
  {
    "start": 12800642,
    "end": 12802154,
    "text": "となると、4つの選択肢がある。"
  },
  {
    "start": 12802274,
    "end": 12806010,
    "text": "私たちはそれを並べて、正しい選択肢を1つだけ選ぶようなものです。"
  },
  {
    "start": 12806042,
    "end": 12808432,
    "text": "この場合、ラベル3、オプション3。"
  },
  {
    "start": 12808608,
    "end": 12813444,
    "text": "したがって、これが正しい選択肢であり、選択肢1、2、4は正しくない。"
  },
  {
    "start": 12813904,
    "end": 12817120,
    "text": "さて、これらのオプションは長さが異なるかもしれない。"
  },
  {
    "start": 12817232,
    "end": 12828684,
    "text": "そして、この中のいくつかはパディングされた寸法となり、使用されない寸法となる。"
  },
  {
    "start": 12828984,
    "end": 12836438,
    "text": "つまり、トークンが必要で、正しいラベルが必要で、どのトークンがアクティブかを示すマスクが必要なのだ。"
  },
  {
    "start": 12836566,
    "end": 12840114,
    "text": "マスクは、これらのパディングされた領域に対してゼロとなる。"
  },
  {
    "start": 12840734,
    "end": 12843358,
    "text": "そうやってバッチを作るんだ。"
  },
  {
    "start": 12843526,
    "end": 12847814,
    "text": "言語モデルがa,b,c,dのいずれかを予測するようにする。"
  },
  {
    "start": 12847974,
    "end": 12860694,
    "text": "この仕組みは基本的に、トークンとその確率を見て、トークンの平均確率が最も低いか高い選択肢を選ぶというものだ。"
  },
  {
    "start": 12861074,
    "end": 12867614,
    "text": "なぜなら、言語モデルによれば、それが最も可能性の高い完了だからである。"
  },
  {
    "start": 12867914,
    "end": 12878374,
    "text": "ここで確率を見て、選択肢の平均を出し、最も確率の高いものを選ぶだけだ。"
  },
  {
    "start": 12878834,
    "end": 12881374,
    "text": "これがヘラースワッグのやり方だ。"
  },
  {
    "start": 12882834,
    "end": 12887594,
    "text": "これはGPT-3のやり方でもあると思う。"
  },
  {
    "start": 12889214,
    "end": 12891646,
    "text": "私が知る限り、GPT-3はこうしていた。"
  },
  {
    "start": 12891790,
    "end": 12896878,
    "text": "ハロー、スワッグ」と表示される可能性がある他のイヴェールでは、このように表示されない場合もあるので注意が必要だ。"
  },
  {
    "start": 12897006,
    "end": 12903662,
    "text": "多肢選択式で、1回だけ文脈を与え、それから4つの完成度を与えるようなこともある。"
  },
  {
    "start": 12903798,
    "end": 12908454,
    "text": "そのため、モデルは可能な限り最良の選択肢を選ぶ前に、4つの選択肢をすべて見ることができる。"
  },
  {
    "start": 12908574,
    "end": 12914574,
    "text": "モデルにとっては、選択肢を選ぶときに他の選択肢を見ることができるのだから。"
  },
  {
    "start": 12915434,
    "end": 12917874,
    "text": "残念ながら、私たちのようなサイズのモデルにはそれができない。"
  },
  {
    "start": 12917914,
    "end": 12921074,
    "text": "それができるのは大きなサイズのモデルだけだ。"
  },
  {
    "start": 12921234,
    "end": 12926186,
    "text": "だから、私たちのモデルは、他の選択肢を見ることができないというハンディキャップを背負っている。"
  },
  {
    "start": 12926250,
    "end": 12933650,
    "text": "彼らは一度に1つの選択肢しか見ることができず、確率を割り当てるだけで、この指標では正しい選択肢が勝たなければならない。"
  },
  {
    "start": 12933842,
    "end": 12938862,
    "text": "よし、ではこれを簡単に実装してスクリプトに組み込んでみよう。"
  },
  {
    "start": 12939038,
    "end": 12944774,
    "text": "さて、ここで紹介するのは、hello Swag Pyという新しいファイルだ。"
  },
  {
    "start": 12944934,
    "end": 12951014,
    "text": "ディープコード、ディープコードとはちょっと違うので、全部を説明するつもりはない。"
  },
  {
    "start": 12951054,
    "end": 12960110,
    "text": "GitHubからhello swagをダウンロードして、すべてのサンプルをレンダリングしているんだけど、全部で10,000のサンプルがあるんだ。"
  },
  {
    "start": 12960262,
    "end": 12963134,
    "text": "私はそれらをこのフォーム形式にレンダリングしている。"
  },
  {
    "start": 12964114,
    "end": 12982818,
    "text": "このレンダリング例関数の最後で、トークンを返しているのがわかるだろう。トークンの4×t配列のトークン、どの部分がオプションかを示すマスク、それ以外はすべてゼロだ。"
  },
  {
    "start": 12982986,
    "end": 12985774,
    "text": "これが正しいラベルです。"
  },
  {
    "start": 12986154,
    "end": 12989100,
    "text": "これで、例を反復してレンダリングすることができる。"
  },
  {
    "start": 12989202,
    "end": 12994864,
    "text": "GPT-2をハギングフェースからロードできるevaluate関数がここにある。"
  },
  {
    "start": 12994984,
    "end": 13006044,
    "text": "ここではevalが実行され、基本的には私が説明したように計算され、最も低い確率または最も高い確率を持つオプションを予測する。"
  },
  {
    "start": 13006424,
    "end": 13010884,
    "text": "そのためには、基本的にクロスエントロピーの損失を評価すればいい。"
  },
  {
    "start": 13011224,
    "end": 13015192,
    "text": "基本的には、シーケンスの次のトークンを予測することの損失を評価している。"
  },
  {
    "start": 13015328,
    "end": 13019604,
    "text": "であれば、平均損失が最も少ない行に注目する。"
  },
  {
    "start": 13020104,
    "end": 13024992,
    "text": "という選択肢を予想として選ぶ。"
  },
  {
    "start": 13025088,
    "end": 13028024,
    "text": "その後、統計やプリントなどを行う。"
  },
  {
    "start": 13028184,
    "end": 13029880,
    "text": "それがテレスワグの評価方法だ。"
  },
  {
    "start": 13029912,
    "end": 13039324,
    "text": "このスクリプトを実行すると、ハロー・スワッグが29.55%になることがわかります。"
  },
  {
    "start": 13040884,
    "end": 13042556,
    "text": "それがここでのパフォーマンスだ。"
  },
  {
    "start": 13042620,
    "end": 13046844,
    "text": "ランダムな確率は25％であることを思い出してほしい。"
  },
  {
    "start": 13047004,
    "end": 13054012,
    "text": "GPT-2 XLは最大で、GPT-2はおよそ49％まで上がる。"
  },
  {
    "start": 13054148,
    "end": 13058980,
    "text": "今日の技術水準が95％以上であることを考えれば、これはかなり低い値である。"
  },
  {
    "start": 13059132,
    "end": 13061424,
    "text": "今となっては間違いなく古いモデルだ。"
  },
  {
    "start": 13061804,
    "end": 13068058,
    "text": "そしてもうひとつ、Eleutherハーネスと呼ばれるものがある。これは言語モデルの検証を実行するための非常に一般的なインフラだ。"
  },
  {
    "start": 13068196,
    "end": 13073194,
    "text": "この2つの数字が微妙に違っている。"
  },
  {
    "start": 13073614,
    "end": 13083862,
    "text": "もしかしたら、彼らは完答だけでなく多肢選択式も実際にやっていて、それが食い違いの原因になっている可能性もあるが、それについては100％の確信はない。"
  },
  {
    "start": 13083918,
    "end": 13084886,
    "text": "見てみないとね。"
  },
  {
    "start": 13084990,
    "end": 13093874,
    "text": "今のところ、我々のスクリプトは29.55と報告している。だから、もしGPDをゼロから124mまで自分たちでトレーニングするのであれば、この数字を叩き出したい。"
  },
  {
    "start": 13096184,
    "end": 13103936,
    "text": "では、このevalを実際にメインのトレーニング・スクリプトに組み込んでいくことにしよう。"
  },
  {
    "start": 13104080,
    "end": 13120252,
    "text": "というのも、ヘラスワッグを定期的に評価することで、ヘラスワッグの経年変化を追跡し、いつ29.55の領域を超えるかどうかを見極めたいからだ。"
  },
  {
    "start": 13120428,
    "end": 13123772,
    "text": "GPTをそのPIに訓練するためのいくつかの変更点を説明しよう。"
  },
  {
    "start": 13123948,
    "end": 13130344,
    "text": "ここで最初にやったことは、コンパイルをオプションで使えるようにしたことだ。"
  },
  {
    "start": 13130884,
    "end": 13139380,
    "text": "コンパイルの問題点は、残念ながらコードは速くなるが、評価コードやサンプリングコードが壊れてしまうことだ。"
  },
  {
    "start": 13139452,
    "end": 13141788,
    "text": "なぜなのかわからないが、とても不気味なメッセージが表示される。"
  },
  {
    "start": 13141916,
    "end": 13147436,
    "text": "コードベースをGitHubにアップする頃には、修正できているといいんだけど。"
  },
  {
    "start": 13147500,
    "end": 13151664,
    "text": "今のところ、トーチ・コンパイルなしで動かしている。"
  },
  {
    "start": 13152124,
    "end": 13154144,
    "text": "トーチ・コンパイルなしで実行している。"
  },
  {
    "start": 13154684,
    "end": 13164340,
    "text": "また、ログ・ディレクトリ・ログを作成し、列車法、検証ロス、ヘラースワッグの精度を記録するログTXTを置くことができるようにした。"
  },
  {
    "start": 13164492,
    "end": 13172024,
    "text": "とてもシンプルなテキストファイルを書き込むために開いて、最初は空っぽにしてから追記する。"
  },
  {
    "start": 13173494,
    "end": 13178354,
    "text": "私は、最後のステップがいつあるかを教えてくれる簡単な変数を作った。"
  },
  {
    "start": 13178854,
    "end": 13187434,
    "text": "そして、基本的にこのループの中で定期的に、250回目の反復ごとに、あるいは最後のステップで、検証損失を評価する。"
  },
  {
    "start": 13188054,
    "end": 13199180,
    "text": "250回反復するごとに、hello swagを評価することになる。ただし、コンパイラを使っていない場合に限る。"
  },
  {
    "start": 13199372,
    "end": 13206132,
    "text": "ハロー・スワッグを評価するこのコードについては、また後ほど説明する。"
  },
  {
    "start": 13206188,
    "end": 13207956,
    "text": "また、モデルからサンプルを採取する。"
  },
  {
    "start": 13208060,
    "end": 13212676,
    "text": "というわけで、これはビデオを始めたころの古いコードだとわかるはずだ。"
  },
  {
    "start": 13212820,
    "end": 13214624,
    "text": "我々はモデルからサンプリングしているだけだ。"
  },
  {
    "start": 13215284,
    "end": 13224946,
    "text": "そして最後に、ハロー・スワッグを検証、サンプル、評価した後に、トレーニングのステップを踏む。"
  },
  {
    "start": 13225100,
    "end": 13230870,
    "text": "だから、これはトレーニングのひとつのステップであり、これが何をするものかはすべて熟知しているはずだ。"
  },
  {
    "start": 13231062,
    "end": 13234954,
    "text": "ここで最後に、トレーニングのロスが出たら、それをファイルに書き込む。"
  },
  {
    "start": 13235454,
    "end": 13239830,
    "text": "唯一変わったのは、このヘラス・ワギー・ヴァルのセクションだけだ。"
  },
  {
    "start": 13240022,
    "end": 13244798,
    "text": "この仕組みは、すべてのGPUをhellaswagで協力させようとしているんだ。"
  },
  {
    "start": 13244966,
    "end": 13253174,
    "text": "つまり、すべての例を反復し、各プロセスは自分に割り当てられた例だけを選ぶ。"
  },
  {
    "start": 13253294,
    "end": 13258794,
    "text": "Iをワールドサイズで修正し、ランクと同じにしなければならない。"
  },
  {
    "start": 13259374,
    "end": 13269194,
    "text": "次に、例をレンダリングしてGPUに載せ、ロジットを取得し、基本的に最も損失の少ないオプションを予測するのに役立つヘルパー関数を作成します。"
  },
  {
    "start": 13269694,
    "end": 13274234,
    "text": "これがここに来て、予想が当たっていれば、カウントを続ける。"
  },
  {
    "start": 13274654,
    "end": 13279856,
    "text": "もし複数のプロセスが共同作業をしているのであれば、それぞれの統計情報を同期させる必要がある。"
  },
  {
    "start": 13280000,
    "end": 13289124,
    "text": "そこで、統計量をテンソルにまとめ、これをドット・アレダイソンと呼び、合計する方法がある。"
  },
  {
    "start": 13289824,
    "end": 13295524,
    "text": "ということは、ここではテンソルからアンラップして、int値だけにするのだ。"
  },
  {
    "start": 13295864,
    "end": 13299804,
    "text": "この場合、マスター・プロセスはhellasフラグの精度を表示し、ログに記録する。"
  },
  {
    "start": 13300704,
    "end": 13305276,
    "text": "そういうことだ。"
  },
  {
    "start": 13305420,
    "end": 13307148,
    "text": "今、僕が走っているのはそれなんだ。"
  },
  {
    "start": 13307236,
    "end": 13315028,
    "text": "この最適化を見ると、私たちはちょうど世代交代をしたところであり、これは約20,000のうちの10,000のステップだ。"
  },
  {
    "start": 13315076,
    "end": 13315260,
    "text": "そうだろう？"
  },
  {
    "start": 13315292,
    "end": 13317268,
    "text": "もう半分終わった。"
  },
  {
    "start": 13317436,
    "end": 13320372,
    "text": "現段階では、このようなサンプルを入手している。"
  },
  {
    "start": 13320428,
    "end": 13321504,
    "text": "見てみよう。"
  },
  {
    "start": 13322244,
    "end": 13326624,
    "text": "こんにちは。私は言語モデルを使って、ある種の出力を生成したいと思っています。"
  },
  {
    "start": 13327164,
    "end": 13330144,
    "text": "こんにちは、私は言語モデルで、多くの企業で開発者をしています。"
  },
  {
    "start": 13331124,
    "end": 13332584,
    "text": "ラマ語モデル。"
  },
  {
    "start": 13334164,
    "end": 13336064,
    "text": "何か面白いものが見つかるかな。"
  },
  {
    "start": 13349004,
    "end": 13354036,
    "text": "自分で調べればわかることだが、確かに予測はランダムではなくなってきている。"
  },
  {
    "start": 13354220,
    "end": 13363324,
    "text": "このモデルはもう少し自覚的で、言語モデルであることにもう少し特化した言葉を使っているようだ。"
  },
  {
    "start": 13364144,
    "end": 13367568,
    "text": "こんにちは、私は言語モデルで、言語がコミュニケーションにどのように使われるかが好きです。"
  },
  {
    "start": 13367696,
    "end": 13371864,
    "text": "私は語学モデルで、英語とドイツ語を話す予定です。"
  },
  {
    "start": 13372024,
    "end": 13372968,
    "text": "そうか、わからない。"
  },
  {
    "start": 13373056,
    "end": 13377760,
    "text": "この最適化が終わるまで待って、どんなサンプルができるか見てみよう。"
  },
  {
    "start": 13377872,
    "end": 13388254,
    "text": "列車バルブとハロー・スウェイの精度も見て、GPT-2に関してどうなっているか確認するつもりだ。"
  },
  {
    "start": 13388634,
    "end": 13399178,
    "text": "そこで、この右側のJupyterノートブックに焦点を当て、基本的に列車のvalとhela、そしてhelaのスコアを視覚化できる新しいセルを作成した。"
  },
  {
    "start": 13399306,
    "end": 13403970,
    "text": "基本的には、書いているログファイルを解析するようなものだ。"
  },
  {
    "start": 13404162,
    "end": 13407810,
    "text": "この多くは、退屈なmatplotlibのコードと同じだ。"
  },
  {
    "start": 13407922,
    "end": 13410934,
    "text": "基本的に、これが私たちの最適化の姿だ。"
  },
  {
    "start": 13411454,
    "end": 13420766,
    "text": "19,073ステップを実行し、これはおよそ100億トークンに相当する。"
  },
  {
    "start": 13420830,
    "end": 13422126,
    "text": "おっと、大変だ。"
  },
  {
    "start": 13422270,
    "end": 13425714,
    "text": "これは、サンプル10bのファインウェブの1エポックである。"
  },
  {
    "start": 13426294,
    "end": 13431062,
    "text": "左が損失、青がトレーニングの損失だ。"
  },
  {
    "start": 13431158,
    "end": 13445064,
    "text": "オレンジ色で示したのが検証の損失で、赤色の水平線は、IGPTの2124mモデルがこの細かいウェブの検証セットで評価されたときのチェックポイントです。"
  },
  {
    "start": 13445604,
    "end": 13448132,
    "text": "それを上回っていることがわかるだろう。"
  },
  {
    "start": 13448188,
    "end": 13452484,
    "text": "オレンジが赤を下回っているので、このデータセットの検証セットを上回っていることになる。"
  },
  {
    "start": 13452644,
    "end": 13456684,
    "text": "さっきも言ったように、データの分布はGPTがトレーニングしたものとはまったく違う。"
  },
  {
    "start": 13456764,
    "end": 13463180,
    "text": "これは正確には公平な比較ではないが、クロスチェックの参考にはなるだろう。"
  },
  {
    "start": 13463332,
    "end": 13469444,
    "text": "今、私たちが理想とするのは、保留され、比較可能で、ある程度標準的なものだ。"
  },
  {
    "start": 13470104,
    "end": 13473208,
    "text": "だから、僕らにとっては、それがヘリスワグだったりする。"
  },
  {
    "start": 13473296,
    "end": 13478728,
    "text": "ヘレスワッグの25％からの進歩が赤で示されている。"
  },
  {
    "start": 13478776,
    "end": 13483624,
    "text": "赤がOpenAI GPT 2124 mモデル。"
  },
  {
    "start": 13483784,
    "end": 13485760,
    "text": "このヘレスワグはここで実現した。"
  },
  {
    "start": 13485912,
    "end": 13493454,
    "text": "3,000億トークンで学習させたGPT-3モデル124mは緑を達成。"
  },
  {
    "start": 13493794,
    "end": 13495442,
    "text": "それはこっちだ。"
  },
  {
    "start": 13495578,
    "end": 13504094,
    "text": "GPTの2124mモデルを基本的に上回っていることがわかるだろう。"
  },
  {
    "start": 13504474,
    "end": 13512654,
    "text": "興味深いことに、GPT-2が1000億トークンで訓練されたのに対して、我々は100億トークンの訓練だけでこれを実現できた。"
  },
  {
    "start": 13512954,
    "end": 13517696,
    "text": "なぜか、トレーニング用のトークンはかなり少なくて済んだ。"
  },
  {
    "start": 13517850,
    "end": 13525708,
    "text": "テンプレートとトレーニングだけで、この精度に匹敵する、あるいはそれを上回ることができる理由には、多くの可能性がある。"
  },
  {
    "start": 13525836,
    "end": 13532820,
    "text": "第一に、OpenaigPt 2がより広範なデータ分布でトレーニングされた可能性がある。"
  },
  {
    "start": 13532932,
    "end": 13540464,
    "text": "特に、fineweb.eduはすべて英語で、多言語ではないし、コードにそれほど多くの数学があるわけでもない。"
  },
  {
    "start": 13541324,
    "end": 13547978,
    "text": "だから、数学とコードと多言語は、オリジナルのGPT-2モデルから容量を盗んでいた可能性がある。"
  },
  {
    "start": 13548106,
    "end": 13554058,
    "text": "基本的に、これがうまくいかない理由の一部かもしれない。"
  },
  {
    "start": 13554186,
    "end": 13555614,
    "text": "他にも理由はたくさんある。"
  },
  {
    "start": 13555954,
    "end": 13559890,
    "text": "例えば、ヘレスワグの評価はかなり古く、5年かそこらだ。"
  },
  {
    "start": 13560002,
    "end": 13568066,
    "text": "何らかの形で、あるいは同じように、ヘラスワグの側面がファインウェブのトレーニングセットに組み込まれている可能性はある。"
  },
  {
    "start": 13568170,
    "end": 13573936,
    "text": "確かなことはわからないが、もしそうだとすれば、我々は基本的に検証曲線の代わりにトレーニング曲線を見ていることになる。"
  },
  {
    "start": 13574120,
    "end": 13582216,
    "text": "要するに、これは完璧な評価ではないし、いくつかの注意点もある。"
  },
  {
    "start": 13582360,
    "end": 13592640,
    "text": "このようなデータセットを作ろうとする場合、ごく一般的なテストセットがトレーニングセットに含まれないようにするのだろう。"
  },
  {
    "start": 13592752,
    "end": 13597168,
    "text": "例えば、ハグフェイスがファインウェブを作ったとき、彼らはヘラースワッグを評価として使った。"
  },
  {
    "start": 13597256,
    "end": 13604124,
    "text": "私は、彼らが重複を排除し、トレーニングセットに地獄のようなスワッグがないことを確認することを望むが、私たちは確かめることができない。"
  },
  {
    "start": 13604984,
    "end": 13608232,
    "text": "もうひとつ、この損失曲線について簡単に触れておきたい。"
  },
  {
    "start": 13608288,
    "end": 13610888,
    "text": "これは本当に奇妙に見える。"
  },
  {
    "start": 13611016,
    "end": 13620124,
    "text": "これが何なのか100％わかっているわけではないが、100億個の細かいウェブのサンプルが適切にシャッフルされていなかったからではないかと思う。"
  },
  {
    "start": 13621064,
    "end": 13628024,
    "text": "このデータにはまだ完全には理解していない問題があり、奇妙な周期性がある。"
  },
  {
    "start": 13628364,
    "end": 13642500,
    "text": "というのも、私たちは非常に怠惰な方法で、すべてのトークンを直列化し、並べ替えやランダム・サンプリングは一切行わずに、ただゼロから繰り返しているだけだからだ。"
  },
  {
    "start": 13642692,
    "end": 13655244,
    "text": "これは理想的とは言えないが、このレポにたどり着くまでに、これらのことがいくつか修正され、このビルド・ナノグトのレポがリリースされることを願っている。"
  },
  {
    "start": 13655404,
    "end": 13658228,
    "text": "今は少し醜く、予備的なものに見える。"
  },
  {
    "start": 13658396,
    "end": 13660588,
    "text": "君がここに来る頃には、もっといい天気になっているといいね。"
  },
  {
    "start": 13660716,
    "end": 13667076,
    "text": "ここでは正誤表を示し、ビデオの後に起こったいくつかのことについて話すつもりだ。"
  },
  {
    "start": 13667220,
    "end": 13670504,
    "text": "小さな問題は解決していると期待している。"
  },
  {
    "start": 13671164,
    "end": 13681844,
    "text": "今のところ、基本的には、これは我々のトレーニングが完全に間違っているわけではなく、トークン予算のわずか10倍で精度を上回ることができることを示している。"
  },
  {
    "start": 13683584,
    "end": 13687104,
    "text": "データセットが改善された可能性もある。"
  },
  {
    "start": 13687224,
    "end": 13690800,
    "text": "オリジナルのGPT-2データセットはウェブテキストだった。"
  },
  {
    "start": 13690912,
    "end": 13694328,
    "text": "データセットに多くの注意と配慮が払われていない可能性がある。"
  },
  {
    "start": 13694416,
    "end": 13703400,
    "text": "これはLLMのごく初期のことで、今は重複の排除、フィルタリング、質の高いフィルタリングなど、優れた実践についてより厳しくチェックされるようになっている。"
  },
  {
    "start": 13703472,
    "end": 13707056,
    "text": "私たちがトレーニングしているデータセットが、トークンごとの質が高いだけかもしれない。"
  },
  {
    "start": 13707160,
    "end": 13709416,
    "text": "それも後押ししているかもしれない。"
  },
  {
    "start": 13709600,
    "end": 13713764,
    "text": "考えなければならない注意点はいくつもあるが、今のところはこれで満足している。"
  },
  {
    "start": 13714664,
    "end": 13723696,
    "text": "そう、次に興味があったのは、見ての通り、今は朝だから、オーバーナイトがあって、基本的にどこまで結果を押し上げられるか見たかったんだ。"
  },
  {
    "start": 13723840,
    "end": 13731292,
    "text": "オーバーナイトランをするために、私は基本的に1回のエポックに約2時間かかる代わりに、4回のエポックを行った。"
  },
  {
    "start": 13731388,
    "end": 13733628,
    "text": "私が寝ている間に8時間かかる。"
  },
  {
    "start": 13733756,
    "end": 13738076,
    "text": "そのため、4エポック、つまりおよそ400億トークンのトレーニングを行った。"
  },
  {
    "start": 13738260,
    "end": 13740744,
    "text": "どこまで行けるか試していたんだ。"
  },
  {
    "start": 13741404,
    "end": 13742644,
    "text": "これが唯一の変更点だった。"
  },
  {
    "start": 13742684,
    "end": 13750024,
    "text": "スクリプトを再実行し、40bのログファイルをポイントして読むと、カーブはこのようになる。"
  },
  {
    "start": 13751244,
    "end": 13763194,
    "text": "さて、これを説明しますと、まず第一に、異なるエポックを通じて周期性の問題があること、そしてfineweb.eduのデータセットに奇妙な点があることです。"
  },
  {
    "start": 13763614,
    "end": 13776318,
    "text": "そうでなければ、ヘラ・スワッグは実際に大きく上昇し、GPT3124mの精度を達成しそうになったが、まだまだだった。"
  },
  {
    "start": 13776366,
    "end": 13779514,
    "text": "もう少し長く眠れなかったのが残念だ。"
  },
  {
    "start": 13780474,
    "end": 13786146,
    "text": "もしこれがファイト・エポック・ランだったら、ここまで来られたかもしれない。"
  },
  {
    "start": 13786330,
    "end": 13793814,
    "text": "ここでひとつ指摘しておきたいのは、マルチ・エポック走行をする場合、実はデータ・ローダーにはあまり注意を払っていないということだ。"
  },
  {
    "start": 13795314,
    "end": 13801618,
    "text": "このデータローダーは、まったく同じフォーマットで、まったく同じ順序でデータを処理する。"
  },
  {
    "start": 13801706,
    "end": 13816234,
    "text": "実際にデータをランダムに順列化し、新しいエポックごとにすべてのシャードで文書を順列化し、シャードを順列化する可能性さえある。"
  },
  {
    "start": 13816974,
    "end": 13819542,
    "text": "そうすれば、前評判の悪さを減らすのに大いに役立つだろう。"
  },
  {
    "start": 13819638,
    "end": 13833438,
    "text": "最適化のためにも、同じフォーマットで表示されないほうがいいし、文書が互いにどのように続いていくかにランダム性が出てくる。"
  },
  {
    "start": 13833486,
    "end": 13836134,
    "text": "次にテキスト終了トークン、そして次のドキュメント。"
  },
  {
    "start": 13836254,
    "end": 13853034,
    "text": "なぜなら、ドキュメントの順番は重要ではないはずで、基本的に依存関係を壊したくないはずだからだ。"
  },
  {
    "start": 13853334,
    "end": 13855526,
    "text": "だから、私たちのデータレターは現在、そのようなことはしていない。"
  },
  {
    "start": 13855550,
    "end": 13858274,
    "text": "それは、あなたが考えることができる一つの改善点だ。"
  },
  {
    "start": 13860254,
    "end": 13864808,
    "text": "もうひとつ指摘しておきたいのは、わずか400億トークンでGPT-3の精度にほぼ匹敵しているということだ。"
  },
  {
    "start": 13864926,
    "end": 13867828,
    "text": "GPT-3は3,000億トークンで訓練された。"
  },
  {
    "start": 13867916,
    "end": 13873824,
    "text": "ここでも、学習効率に関して約10倍の改善が見られる。"
  },
  {
    "start": 13874284,
    "end": 13882424,
    "text": "もうひとつは、前回すでに述べたことのほかに、何に起因するのか正確にはわからない。"
  },
  {
    "start": 13882924,
    "end": 13887620,
    "text": "もうひとつ簡単に触れておきたいのは、ここでの最大LRだ。"
  },
  {
    "start": 13887732,
    "end": 13895930,
    "text": "以前の関連レポジトリで、すでに何人かがこの方法で少し遊んでいるのを見た。"
  },
  {
    "start": 13896082,
    "end": 13898738,
    "text": "最大学習率はもっと高くなる可能性がある。"
  },
  {
    "start": 13898866,
    "end": 13907186,
    "text": "私たちが受け継いでいるGPT-3のハイパーパラメータは、どういうわけか実際には非常に保守的で、学習レートを上げても大丈夫なのです。"
  },
  {
    "start": 13907370,
    "end": 13912890,
    "text": "これらのハイパーパラメーターの多くは調整可能なので、自由に弄ってほしい。"
  },
  {
    "start": 13912962,
    "end": 13916314,
    "text": "おそらく正確に設定されていないのだろう。"
  },
  {
    "start": 13916474,
    "end": 13921154,
    "text": "基本的には、これで逃げ切れる可能性はある。"
  },
  {
    "start": 13922374,
    "end": 13929686,
    "text": "GPT-3に忠実でありたいのであれば、次のような違いも必要だろう。"
  },
  {
    "start": 13929830,
    "end": 13936230,
    "text": "GPT-3のシーケンス長は2倍で、1024ではなく2048だ。"
  },
  {
    "start": 13936342,
    "end": 13949864,
    "text": "そして、まったく同じ数のトークン、つまり1反復または1ステップあたり50万個のトークンが必要なら、これを32個に減らして、それでも50万個になるようにする。"
  },
  {
    "start": 13950524,
    "end": 13962564,
    "text": "そうすれば、GPT-3と同じ配列長になり、私が知る限り、基本的に他のモデルもほぼ同じになる。"
  },
  {
    "start": 13962644,
    "end": 13965704,
    "text": "GPT-2とGPT-3は非常によく似たモデルだからだ。"
  },
  {
    "start": 13966244,
    "end": 13970592,
    "text": "ここで、一晩かけてトレーニングされたモデルのサンプルを見てみよう。"
  },
  {
    "start": 13970728,
    "end": 13974480,
    "text": "これが最適化だ。"
  },
  {
    "start": 13974632,
    "end": 13984804,
    "text": "このように、私たちは76,290までステップを踏み、33.24のヘラスバッグを手に入れた。"
  },
  {
    "start": 13985264,
    "end": 13988240,
    "text": "これらはモデルからのサンプルの一部である。"
  },
  {
    "start": 13988432,
    "end": 13993928,
    "text": "これを読み、ビデオを一時停止すれば、より首尾一貫していることがわかるだろう。"
  },
  {
    "start": 13994096,
    "end": 13999612,
    "text": "だから、彼らは実際に言語モデルという事実に取り組んでいるんだ。"
  },
  {
    "start": 13999748,
    "end": 14006144,
    "text": "こんにちは、私は言語モデルです。"
  },
  {
    "start": 14008564,
    "end": 14010824,
    "text": "私は言語モデルであって、プログラミング言語ではない。"
  },
  {
    "start": 14012564,
    "end": 14014196,
    "text": "私はコミュニケーションの取り方を知っている。"
  },
  {
    "start": 14014380,
    "end": 14015664,
    "text": "私はPythonを使っている。"
  },
  {
    "start": 14018764,
    "end": 14028604,
    "text": "分からないが、これを一時停止して見て、100億だけトレーニングしたモデルと比較すれば、これらのモデルがより首尾一貫していることが分かるだろう。"
  },
  {
    "start": 14029344,
    "end": 14033120,
    "text": "ところで、もうひとつ追加したのが、このコードの塊だ。"
  },
  {
    "start": 14033272,
    "end": 14046504,
    "text": "基本的に、検証損失を評価した直後、私たちがマスター・プロセスであれば、5000ステップごとに検証損失をログに記録することに加えて、チェックポイントを保存することになる。"
  },
  {
    "start": 14046664,
    "end": 14052762,
    "text": "チェックポイントは、モデルを保存しておいて後で何らかの形で使えるという点でいい。"
  },
  {
    "start": 14052938,
    "end": 14064250,
    "text": "もし最適化を再開したいのであれば、モデルを保存するだけでなく、オプティマイザーの状態DiCTも保存しなければならない。"
  },
  {
    "start": 14064402,
    "end": 14066922,
    "text": "MとVがある"
  },
  {
    "start": 14067098,
    "end": 14070290,
    "text": "オプティマイザーも適切に再開する必要がある。"
  },
  {
    "start": 14070362,
    "end": 14074082,
    "text": "RNGシードや乱数ジェネレーターなどには注意しなければならない。"
  },
  {
    "start": 14074178,
    "end": 14080686,
    "text": "最適化を再開するためには、トレーニングプロセスの状態をよく考えなければならない。"
  },
  {
    "start": 14080830,
    "end": 14083334,
    "text": "モデルを保存するだけなら、こうする。"
  },
  {
    "start": 14083454,
    "end": 14089830,
    "text": "ひとつは、このモデルをもっと慎重に評価したいからだ。"
  },
  {
    "start": 14089982,
    "end": 14094102,
    "text": "ここではヘラスワッグの評価を手探りでやっているようなものだ。"
  },
  {
    "start": 14094198,
    "end": 14101654,
    "text": "例えば、ルーサーの評価硬度のように。"
  },
  {
    "start": 14101814,
    "end": 14104824,
    "text": "評価硬度硬度。"
  },
  {
    "start": 14105564,
    "end": 14109252,
    "text": "これは言語モデルを評価する方法でもある。"
  },
  {
    "start": 14109388,
    "end": 14128780,
    "text": "そのため、基本的に別のインフラを使用して、さまざまな評価でモデルをより徹底的に評価し、例えば数学のコードや異なる言語などを含む他の多くのタスクで、冒頭のIGPT 2モデルと比較したいと思う可能性があります。"
  },
  {
    "start": 14128932,
    "end": 14131104,
    "text": "これもうれしい機能だ。"
  },
  {
    "start": 14133014,
    "end": 14139166,
    "text": "それからもうひとつ言っておきたいのは、ここで構築したものはすべて、トレーニングの前段階に過ぎないということだ。"
  },
  {
    "start": 14139310,
    "end": 14143262,
    "text": "ここでのGPTは夢の書類だ。"
  },
  {
    "start": 14143318,
    "end": 14144702,
    "text": "次のトークンを予測するだけだ。"
  },
  {
    "start": 14144798,
    "end": 14147114,
    "text": "チャットのように話しかけることはできない。"
  },
  {
    "start": 14148654,
    "end": 14153366,
    "text": "モデルと話したければ、チャット形式に微調整しなければならない。"
  },
  {
    "start": 14153510,
    "end": 14157906,
    "text": "監督による微調整やSFDを見るのであれば、実際にはそれほど複雑なことではない。"
  },
  {
    "start": 14158070,
    "end": 14162890,
    "text": "つまり、データセットをより会話的なデータセットに入れ替えるということだ。"
  },
  {
    "start": 14163002,
    "end": 14167218,
    "text": "ユーザーアシスタント、ユーザーアシスタントのような構造があり、私たちはそれを微調整するだけです。"
  },
  {
    "start": 14167266,
    "end": 14172826,
    "text": "次に、基本的にユーザー・トークンを入力し、アシスタント・トークンをサンプリングする。"
  },
  {
    "start": 14172930,
    "end": 14177694,
    "text": "それ以上の深い意味はないが、基本的にはデータセットを入れ替えてトレーニングを続ける。"
  },
  {
    "start": 14178354,
    "end": 14181170,
    "text": "とりあえず、プレトレーニングはここまでにしておこう。"
  },
  {
    "start": 14181362,
    "end": 14189464,
    "text": "もうひとつ簡単にお見せしておきたいのは、もちろん、今日私たちが作り上げたものは、先ほどのこのリポジトリであるNanogptに向けて構築されたものだということです。"
  },
  {
    "start": 14190084,
    "end": 14197784,
    "text": "また、実はもうひとつnano GPTの実装があり、それはLLM Cという私が取り組んでいる最近のプロジェクトに隠れている。"
  },
  {
    "start": 14198244,
    "end": 14209456,
    "text": "LLM CはGPT-2またはGPT-3トレーニングの純粋なCUDA実装であり、CUDAを直接使用し、CUDAとして記述されています。"
  },
  {
    "start": 14209620,
    "end": 14214624,
    "text": "さて、ここでのnano GPTは、pytorchにおけるC実装の参照コードとして機能する。"
  },
  {
    "start": 14214704,
    "end": 14219400,
    "text": "この2つを正確に一致させようとしているが、C CUDAの方が速いことを期待している。"
  },
  {
    "start": 14219512,
    "end": 14224728,
    "text": "もちろん、現在は直接最適化された実装であるため、そうなっているようだ。"
  },
  {
    "start": 14224896,
    "end": 14229472,
    "text": "LLM CのGPT-2 pyは基本的にnano GPTです。"
  },
  {
    "start": 14229648,
    "end": 14239744,
    "text": "このファイルをスクロールすると、この講義で作り上げたものとよく似たものがたくさん見つかるだろう。"
  },
  {
    "start": 14239904,
    "end": 14245724,
    "text": "次にtraingpt 2 cuを見ると、これはCのCUDA実装だ。"
  },
  {
    "start": 14246024,
    "end": 14252936,
    "text": "MPi、ニッケル、GPU、Cuda、C Cがたくさんあり、それらに精通していなければならない。"
  },
  {
    "start": 14253080,
    "end": 14261530,
    "text": "これが出来上がれば、実際に2つを並べて走らせることができ、まったく同じ結果が得られることになる。"
  },
  {
    "start": 14261672,
    "end": 14263806,
    "text": "LMCは実際に速く走る。"
  },
  {
    "start": 14263870,
    "end": 14264910,
    "text": "見てみよう。"
  },
  {
    "start": 14265062,
    "end": 14269686,
    "text": "左側にはPytorch Nano GPTのようなものがある。"
  },
  {
    "start": 14269790,
    "end": 14271726,
    "text": "右はLLMCのコール。"
  },
  {
    "start": 14271870,
    "end": 14274074,
    "text": "ここで2つを発表する。"
  },
  {
    "start": 14275094,
    "end": 14277118,
    "text": "どちらもシングルGPUで動作する予定だ。"
  },
  {
    "start": 14277206,
    "end": 14283302,
    "text": "ここではLM CをGPU 1に置いているが、これはデフォルトでGPU 0をつかむ。"
  },
  {
    "start": 14283478,
    "end": 14292694,
    "text": "すると、LLM Cがコンパイルされ、スペースが確保され、ステップしていることがわかる。"
  },
  {
    "start": 14293434,
    "end": 14305098,
    "text": "基本的に、LLN C NVCC secudaコンパイルよりtorchコンパイルの方がここでは少し遅いので、その間Pytorchはまだコンパイルしている。"
  },
  {
    "start": 14305266,
    "end": 14310226,
    "text": "というわけで、このプログラムはすでに走り始めていて、まだトーチのコンパイルを待っているところだ。"
  },
  {
    "start": 14310370,
    "end": 14314918,
    "text": "もちろん、これはGPT-2とGPT-3に特化した実装だ。"
  },
  {
    "start": 14315066,
    "end": 14318958,
    "text": "Pytorchは非常に一般的なニューラルネットワークのフレームワークなので、正確には比較できない。"
  },
  {
    "start": 14319046,
    "end": 14323654,
    "text": "GPT-2と3を鍛えるだけなら、LM Cはとても速い。"
  },
  {
    "start": 14323814,
    "end": 14329794,
    "text": "スペースも取らないし、スタートも速いし、1ステップあたりのスピードも速い。"
  },
  {
    "start": 14330814,
    "end": 14332990,
    "text": "というわけで、PyTorchはここで足踏みを始めた。"
  },
  {
    "start": 14333142,
    "end": 14340914,
    "text": "ご覧の通り、ここでは毎秒約223,000トークン、ここでは毎秒約185,000トークンで動いている。"
  },
  {
    "start": 14342854,
    "end": 14344334,
    "text": "かなり遅い。"
  },
  {
    "start": 14344494,
    "end": 14350206,
    "text": "ピトーチの実装からすべての果汁を絞り出したという自信はない。"
  },
  {
    "start": 14350350,
    "end": 14359554,
    "text": "ここで重要なのは、ステップを整列させれば、この2つの間に印刷される損失とノルムが同一であることがわかるということだ。"
  },
  {
    "start": 14360134,
    "end": 14364614,
    "text": "左はピトーチ、右はこのccodonの実装である。"
  },
  {
    "start": 14364694,
    "end": 14367874,
    "text": "同じだけど、こっちの方が速く走る。"
  },
  {
    "start": 14368254,
    "end": 14377542,
    "text": "LMCも簡単にお見せしたかったのですが、これは並列実装です。"
  },
  {
    "start": 14377638,
    "end": 14379398,
    "text": "ちょっと面白いね。"
  },
  {
    "start": 14379526,
    "end": 14385134,
    "text": "さて、そろそろビデオをまとめなければならない。"
  },
  {
    "start": 14385294,
    "end": 14388662,
    "text": "私たちは多くの土地をカバーし、すべてをゼロから作り上げた。"
  },
  {
    "start": 14388798,
    "end": 14395004,
    "text": "簡単にまとめると、我々はGPTTとGPT-3の論文を見ていた。"
  },
  {
    "start": 14395904,
    "end": 14400584,
    "text": "私たちは、あなたがこのようなトレーニングランをどのように設定するのか、そしてそれに関わるすべての考慮事項を調べていた。"
  },
  {
    "start": 14400704,
    "end": 14402400,
    "text": "すべてをゼロから書き上げた。"
  },
  {
    "start": 14402552,
    "end": 14414164,
    "text": "その結果、2時間のトレーニング走行または一晩の走行で、GPT-2とGPT-3の1億2400万パラメータ・チェックポイントを非常に大きく一致させることができることがわかった。"
  },
  {
    "start": 14414904,
    "end": 14424994,
    "text": "原理的には、私たちが書いたコードは、計算リソースがあればさらに大きなモデルを訓練することができる。"
  },
  {
    "start": 14425934,
    "end": 14428390,
    "text": "残された課題はいくつかある。"
  },
  {
    "start": 14428542,
    "end": 14436254,
    "text": "ここでのロスは、細かいウェブデータのサンプリングに関係していると思われるが、なぜトーチコンパイルをオンにできないのだろうか？"
  },
  {
    "start": 14436414,
    "end": 14438990,
    "text": "現在、世代交代とハロー・スワッグ。"
  },
  {
    "start": 14439102,
    "end": 14441150,
    "text": "データローダーでどうしたんだ？"
  },
  {
    "start": 14441182,
    "end": 14444790,
    "text": "エポック境界線に到達した時点で、データを順列化すべきだろう。"
  },
  {
    "start": 14444902,
    "end": 14455474,
    "text": "このような問題は他にもいくつかあり、このビデオと一緒に公開する予定のnano GPTのビルド・レポジトリで、時間をかけていくつかドキュメント化する予定だ。"
  },
  {
    "start": 14455894,
    "end": 14475844,
    "text": "もし何か質問があったり、僕らがカバーしたことについて話したいことがあれば、discussionタブに行って、僕らがここで話せるようにするか、あなたが貢献したいことに応じてissuesやpull requestに行ってください。"
  },
  {
    "start": 14477984,
    "end": 14485064,
    "text": "それ以外は、今のところ、かなり満足している。このビデオを楽しんでもらえたらうれしい。"
  }
]