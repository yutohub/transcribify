[
  {
    "start": 13690,
    "end": 14510,
    "text": "皆さん、こんにちは。"
  },
  {
    "start": 14660,
    "end": 16298,
    "text": "基調講演を楽しんでいただけたなら幸いだ。"
  },
  {
    "start": 16314,
    "end": 16942,
    "text": "そうだった。"
  },
  {
    "start": 16996,
    "end": 20442,
    "text": "皆さん、OpenAIの最初の開発者会議での時間を楽しんでいることと思います。"
  },
  {
    "start": 20586,
    "end": 28982,
    "text": "この分科会では、あなたが最も関心のある問題を解決する際に、LLMのパフォーマンスを最大化するために使えるさまざまなテクニックについてお話しします。"
  },
  {
    "start": 29116,
    "end": 31314,
    "text": "私の名前はジョン・アラードです。"
  },
  {
    "start": 31362,
    "end": 37954,
    "text": "私はOpenAIのファインチューニングプロダクトチームでエンジニアリングリードを務めていますが、OpenAIのファインチューニングにとって、本当にエキサイティングな数ヶ月でした。"
  },
  {
    "start": 38002,
    "end": 43530,
    "text": "8月に3.5ターボの微調整を開始し、開発者コミュニティからの反響に圧倒されました。"
  },
  {
    "start": 43680,
    "end": 45526,
    "text": "私たちはその後に、いくつかの重要な機能を追加した。"
  },
  {
    "start": 45558,
    "end": 53178,
    "text": "ファンクションの呼び出しデータに対する微調整もあれば、既存の微調整済みモデルを使い、その上で微調整を続けるという継続的な微調整もある。"
  },
  {
    "start": 53264,
    "end": 56400,
    "text": "私たちは、プラットフォーム内で微調整を行うための完全なUIまで立ち上げた。"
  },
  {
    "start": 57410,
    "end": 61726,
    "text": "この数カ月間、我々は業界のあらゆる分野の開発者たちと緊密に仕事をすることができた。"
  },
  {
    "start": 61748,
    "end": 69790,
    "text": "インディーズの開発者、新興企業の開発者、そして地球上で最も大きな企業の開発者。"
  },
  {
    "start": 69870,
    "end": 71842,
    "text": "これらの問題を解決するために、彼らはどのようにLLMを利用しようとしているのだろうか？"
  },
  {
    "start": 71896,
    "end": 75762,
    "text": "具体的には、LLMの微調整でどのようにこれらの問題を解決しようとしているのか。"
  },
  {
    "start": 75896,
    "end": 78290,
    "text": "今日は、これらの洞察のいくつかを皆さんと分かち合いたい。"
  },
  {
    "start": 78360,
    "end": 81400,
    "text": "ということで、まずは同僚のコリンに話を譲ろう。"
  },
  {
    "start": 82170,
    "end": 83206,
    "text": "ありがとう、ジョン。"
  },
  {
    "start": 83388,
    "end": 85398,
    "text": "みなさん、コリン、はじめまして。"
  },
  {
    "start": 85484,
    "end": 91914,
    "text": "私はヨーロッパでソリューション・プラクティスの責任者を務めており、基本的には戦略的な顧客と協力して、彼らの最も複雑な問題を解決しようとしている。"
  },
  {
    "start": 92032,
    "end": 100774,
    "text": "この1年間、LLMを確実にプロダクションに送り込もうとする誰もが、最適化に最も力を注いできた。"
  },
  {
    "start": 100902,
    "end": 102782,
    "text": "なぜそこにこだわるのか？"
  },
  {
    "start": 102916,
    "end": 105600,
    "text": "まあ、LLMSを最適化するのは難しい。"
  },
  {
    "start": 106610,
    "end": 117994,
    "text": "あらゆるフレームワークがあり、あらゆるコンテンツが公開され、あらゆる種類の指標やさまざまなツールが提供されているにもかかわらず、それはいまだに最大の焦点のひとつであり、最適化するためのワンストップショップは存在しない。"
  },
  {
    "start": 118042,
    "end": 121186,
    "text": "どのような問題を抱え、どのようにアプローチするかによる。"
  },
  {
    "start": 121208,
    "end": 130582,
    "text": "今日お見せしたいのは、何が問題なのか、どうアプローチすればいいのかを考えるためのフレームワークと、それを解決するためのツールのようなものです。"
  },
  {
    "start": 130716,
    "end": 135414,
    "text": "なぜ難しいのか、その理由から始めよう。"
  },
  {
    "start": 135532,
    "end": 139990,
    "text": "何が問題なのかを正確に知るために、シグナルとノイズを分離するのは難しい。"
  },
  {
    "start": 140060,
    "end": 141270,
    "text": "これが最初の場所だ。"
  },
  {
    "start": 141420,
    "end": 146038,
    "text": "もうひとつは、LLMではパフォーマンスが実に抽象的で測定が難しいということだ。"
  },
  {
    "start": 146134,
    "end": 149098,
    "text": "どの程度の問題があるのかを知るのは本当に難しい。"
  },
  {
    "start": 149184,
    "end": 157610,
    "text": "何が問題なのか、どれだけの問題を抱えているのかが分かっても、特定した問題を解決するためにどのようなアプローチを使うのかを知ることも難しい。"
  },
  {
    "start": 157770,
    "end": 159950,
    "text": "それが今日の焦点だ。"
  },
  {
    "start": 160020,
    "end": 162794,
    "text": "今日の話は、パフォーマンスを最大化するためのものだ。"
  },
  {
    "start": 162922,
    "end": 174450,
    "text": "私たちがここで皆さんに持って帰っていただきたいのは、さまざまなオプションがどのようなものであるかのメンタルモデルであり、どのような場合にどちらを使うべきかの理解であり、そして皆さん自身がこの最適化の旅を続ける自信なのです。"
  },
  {
    "start": 174790,
    "end": 179550,
    "text": "つまり、LLMのパフォーマンスを最適化することは、必ずしも直線的ではないのだ。"
  },
  {
    "start": 179630,
    "end": 186994,
    "text": "多くの人がこのようなチャートを提示し、プロンプト・エンジニアリングから始まり、検索機能拡張世代に移り、微調整に移る。"
  },
  {
    "start": 187042,
    "end": 195594,
    "text": "これはLLMを最適化するアプローチに似ているが、検索、拡張生成、微調整はそれぞれ異なる問題を解決するため、これは問題がある。"
  },
  {
    "start": 195712,
    "end": 200922,
    "text": "扱う問題のカテゴリーによって、一方が必要な場合もあれば、もう一方が必要な場合もある。"
  },
  {
    "start": 200976,
    "end": 202890,
    "text": "私たちはもっとこう考えている。"
  },
  {
    "start": 203040,
    "end": 205630,
    "text": "最適化には2つの軸がある。"
  },
  {
    "start": 205700,
    "end": 207370,
    "text": "そのひとつがコンテキストの最適化だ。"
  },
  {
    "start": 207450,
    "end": 211150,
    "text": "あなたの問題を解決するために、モデルは何を知る必要があるのか？"
  },
  {
    "start": 211300,
    "end": 213562,
    "text": "もうひとつはLM最適化だ。"
  },
  {
    "start": 213626,
    "end": 214910,
    "text": "モデルはどのように行動する必要があるのか？"
  },
  {
    "start": 214980,
    "end": 216914,
    "text": "そのために必要な方法とは？"
  },
  {
    "start": 216952,
    "end": 221250,
    "text": "あるいは、実際に問題を解決するために必要な行動とは何か？"
  },
  {
    "start": 221400,
    "end": 225502,
    "text": "典型的な流れは、左下のプロンプト・エンジニアリングから始まる。"
  },
  {
    "start": 225566,
    "end": 228946,
    "text": "プロンプト・エンジニアリングでは、その両方ができる。"
  },
  {
    "start": 228968,
    "end": 231030,
    "text": "プロンプト・エンジニアリングは常に最良のスタート地点である。"
  },
  {
    "start": 231100,
    "end": 232774,
    "text": "すぐにテストして学ぶことができる。"
  },
  {
    "start": 232892,
    "end": 245146,
    "text": "まず最初にすべきことは、プロンプトから始めて、評価までたどり着き、アウトプットを一貫して評価する方法を考え、そこから、これはコンテキストの問題なのか、それともモデルをどのように行動させる必要があるのかという種類の問題なのかを決めることだ。"
  },
  {
    "start": 245248,
    "end": 252006,
    "text": "より多くの文脈や関連性の高い文脈が必要な場合は、検索、拡張世代、ラグに進む。"
  },
  {
    "start": 252118,
    "end": 256250,
    "text": "より一貫した指導が必要なら、微調整に入る。"
  },
  {
    "start": 256330,
    "end": 257694,
    "text": "この2つが重なっている。"
  },
  {
    "start": 257732,
    "end": 258506,
    "text": "添加物だ。"
  },
  {
    "start": 258618,
    "end": 260670,
    "text": "その両方が必要な場合もある。"
  },
  {
    "start": 260740,
    "end": 265920,
    "text": "1つか2つだけ使った例と、全部使って問題を解決した例を紹介する。"
  },
  {
    "start": 266230,
    "end": 270594,
    "text": "つまり、典型的な最適化の旅は、しばしば次のようになる。"
  },
  {
    "start": 270712,
    "end": 276830,
    "text": "左下からスタートし、プロンプトが表示され、評価を作成し、ベースラインを把握する。"
  },
  {
    "start": 276920,
    "end": 281106,
    "text": "そして、典型的なシンプルな次のステップとして、いくつかのショット例を加える。"
  },
  {
    "start": 281138,
    "end": 286854,
    "text": "モデルにどのように動作させたいか、入力と出力のペアの例をいくつか与える。"
  },
  {
    "start": 286972,
    "end": 291430,
    "text": "この時点で言っておくが、実際、この数発の例でパフォーマンスはかなり向上している。"
  },
  {
    "start": 291500,
    "end": 295674,
    "text": "そのプロセスを工業化するために、知識ベースのようなものにつなげよう。"
  },
  {
    "start": 295792,
    "end": 299526,
    "text": "そこで通常、ある種の検索機能拡張世代が追加される。"
  },
  {
    "start": 299718,
    "end": 307134,
    "text": "さて、コンテキストはできたが、毎回私たちが望むようなフォーマットやスタイルで出力されるわけではないとしよう。"
  },
  {
    "start": 307252,
    "end": 309566,
    "text": "そうすれば、モデルを微調整できるかもしれない。"
  },
  {
    "start": 309748,
    "end": 314974,
    "text": "そして次のステップは、もしかしたら検索が思うようにいかないかもしれないということだ。"
  },
  {
    "start": 315012,
    "end": 317682,
    "text": "もしかしたら、そのコンテンツはモデルが必要としているものにもっと関連しているかもしれない。"
  },
  {
    "start": 317736,
    "end": 321314,
    "text": "その後、再び検索増強世代を最適化する。"
  },
  {
    "start": 321432,
    "end": 330834,
    "text": "検索補強世代を再び最適化したところで、更新した検索補強世代で紹介した新しい例を使って、モデルを再び微調整したい。"
  },
  {
    "start": 330962,
    "end": 335638,
    "text": "ここで、私たちが目にする典型的な最適化の流れを少し紹介しよう。"
  },
  {
    "start": 335724,
    "end": 343370,
    "text": "できるだけ単純な言葉でまとめるなら、何かを試して、評価し、そしてまた何かを試す。"
  },
  {
    "start": 343520,
    "end": 346122,
    "text": "というようなものだ。"
  },
  {
    "start": 346176,
    "end": 348122,
    "text": "では、それぞれの象限に飛び込んでみよう。"
  },
  {
    "start": 348176,
    "end": 350982,
    "text": "まずは左下のプロンプト・エンジニアリングから。"
  },
  {
    "start": 351046,
    "end": 354022,
    "text": "それから、検索増強世代の微調整に移る。"
  },
  {
    "start": 354086,
    "end": 359946,
    "text": "そして、私自身とジョンが挑戦した実践的なチャレンジで、これが実際にどのように機能するかをお見せします。"
  },
  {
    "start": 360138,
    "end": 362426,
    "text": "だから、迅速なエンジニアリング。"
  },
  {
    "start": 362538,
    "end": 370770,
    "text": "さて、聴衆の皆さんのほとんどは、このことについてとてもとてもよくご存じだと思いますので、かなりのスピードで飛ばしていきますが、いつも最初に、皆さんにこの原則を知っていただくのが一番です。"
  },
  {
    "start": 370840,
    "end": 375138,
    "text": "つまり、プロンプト・エンジニアリング、ここにいくつかの戦略がある。"
  },
  {
    "start": 375224,
    "end": 380214,
    "text": "これは私たちのドキュメントにあるベストプラクティスから来ているものだが、念のため復習しておこう。"
  },
  {
    "start": 380252,
    "end": 386422,
    "text": "まず、明確な指示を書くということだが、これはどういうことか例を示そう。"
  },
  {
    "start": 386556,
    "end": 405614,
    "text": "第二に、複雑なタスクをより単純なサブタスクに分割すること。もしモデルが、解決するために与えているサブユニットやサブタスクの種類ごとに、予測や一連の予測を行おうとしているようなものだと想像できるのであれば、その問題を分解して実行する可能性が高くなるように、できるだけ具体的な指示を与えるべきである。"
  },
  {
    "start": 405732,
    "end": 408062,
    "text": "同様に、GPTに考える時間を与えること。"
  },
  {
    "start": 408196,
    "end": 411838,
    "text": "そのためによく使われるフレームワークの例を挙げよう。"
  },
  {
    "start": 411924,
    "end": 416090,
    "text": "最後に、これはすでに述べたことだが、組織的な変化をテストすることだ。"
  },
  {
    "start": 416170,
    "end": 428158,
    "text": "あることを変え、また別のことを変え、また別のことを変え......と、評価マトリックスの上をあちこち飛び回り、正しい方向に進んでいるようには感じられないのだ。"
  },
  {
    "start": 428254,
    "end": 436102,
    "text": "そのためには、しっかりとした評価基準や、一般的にはLLMオペのようなものが必要である。"
  },
  {
    "start": 436236,
    "end": 444790,
    "text": "その後、最も一般的な次のステップは、参照テキストに拡張したり、外部ツールにアクセスできるようにすることである。"
  },
  {
    "start": 444870,
    "end": 448358,
    "text": "まず最初に、これらが実際にどのようなものかをまとめておこう。"
  },
  {
    "start": 448454,
    "end": 451654,
    "text": "まず第一に、迅速なエンジニアリングのための2、3の直感。"
  },
  {
    "start": 451702,
    "end": 459070,
    "text": "プロンプト・エンジニアリングは何度か言ったが、もう一度言う。"
  },
  {
    "start": 459220,
    "end": 468450,
    "text": "テスト、早期学習、そして評価と組み合わせることで、さらなる最適化を設定するためのベースラインを提供することができる。"
  },
  {
    "start": 468600,
    "end": 471090,
    "text": "いくつか良くないことがある。"
  },
  {
    "start": 471160,
    "end": 475662,
    "text": "新情報を導入することで、かなりの情報をプロンプトに詰め込むことができる。"
  },
  {
    "start": 475726,
    "end": 479358,
    "text": "GPT4ターボでは、プロンプトに大量の情報を詰め込むことができる。"
  },
  {
    "start": 479454,
    "end": 487218,
    "text": "とはいえ、プロンプト・エンジニアリングを使ってそれを実現するのは超スケーラブルな方法ではない。"
  },
  {
    "start": 487314,
    "end": 495034,
    "text": "また、複雑なスタイルやメソッドを確実に再現することも、実際にモデルに見せられる例の数という点では、やはりコンテキスト・ウィンドウによって制限される。"
  },
  {
    "start": 495152,
    "end": 500890,
    "text": "スタート地点としては最適だが、タスクの複雑さによっては、そこまでたどり着けないかもしれない。"
  },
  {
    "start": 500960,
    "end": 504646,
    "text": "最後に、プロンプト・エンジニアリングでよくある問題だが、トークンの使用を最小限に抑えることだ。"
  },
  {
    "start": 504678,
    "end": 515002,
    "text": "問題にぶつかり続け、実際にその問題に対処するためにプロンプトにどんどんファセットを追加し続け、結局、トークンをどんどん使うことになり、その結果、レイテンシーやあらゆる種類のコストがかかる。"
  },
  {
    "start": 515076,
    "end": 519620,
    "text": "繰り返しになるが、迅速なエンジニアリングではなく、その特別な問題に対処する優れた方法でもない。"
  },
  {
    "start": 520230,
    "end": 524766,
    "text": "やってはいけないこと、やるべきことをプロンプトとともに簡単にまとめる。"
  },
  {
    "start": 524798,
    "end": 533782,
    "text": "曖昧な指示とかなりランダムな出力で、かなりひどいプロンプトがここにある。"
  },
  {
    "start": 533836,
    "end": 538214,
    "text": "何が提示され、何が任務かを正確に伝える明確な指示。"
  },
  {
    "start": 538332,
    "end": 539302,
    "text": "考える時間を与える。"
  },
  {
    "start": 539356,
    "end": 541206,
    "text": "これは特に良い例ではない。"
  },
  {
    "start": 541228,
    "end": 545946,
    "text": "ステップ・バイ・ステップで課題に取り組むように言っているんだ。"
  },
  {
    "start": 545968,
    "end": 553722,
    "text": "リアクト・フレームワークのように、推論のステップを記述させることで、基本的に答えを導き出す手助けをするようなものだ。"
  },
  {
    "start": 553856,
    "end": 556350,
    "text": "リアクト・フレームワークは、そのアプローチ方法のひとつに過ぎない。"
  },
  {
    "start": 556420,
    "end": 572190,
    "text": "GPTに考える時間を与えることは、非常に複雑な論理的推論が必要な場合に対処するもう一つの素晴らしい方法です。なぜなら、GPTは結局のところ、Xトークン予測器であり、プロンプトの強さに応じて、その答えに近づくために必要なトークンを表示するからです。"
  },
  {
    "start": 572270,
    "end": 576862,
    "text": "最後は、複雑な仕事を単純な仕事に分解することだ。"
  },
  {
    "start": 576926,
    "end": 583302,
    "text": "この場合、私は各ステップをほとんど予測のように考え、この場合はできるだけ明確に並べると述べた。"
  },
  {
    "start": 583436,
    "end": 587730,
    "text": "右側には、かなりきれいにフォーマットされたJSON出力が見える。"
  },
  {
    "start": 587890,
    "end": 592010,
    "text": "基本的なことだが、次に進む前にもう一度おさらいしておこう。"
  },
  {
    "start": 592160,
    "end": 593802,
    "text": "よくある次のステップ。"
  },
  {
    "start": 593856,
    "end": 602682,
    "text": "つまり、プロンプト・エンジニアリングでは、基本的に自分がどう行動したいかをモデルに伝えようとしているわけだが、実際にどのトークンがモデルに最も影響を与えているかを知るのは非常に難しいことが多い。"
  },
  {
    "start": 602736,
    "end": 607870,
    "text": "そのためには、\"伝える \"のではなく、\"見せる \"ことから始めるのがいい。"
  },
  {
    "start": 607940,
    "end": 614782,
    "text": "入力と出力のペアを与え、あなたが望むような動作を実際に見せるのです。"
  },
  {
    "start": 614916,
    "end": 621214,
    "text": "これは、次のステップへとうまく導いてくれる。"
  },
  {
    "start": 621262,
    "end": 626082,
    "text": "実践的なセクションで見ていくことになるが、そのおかげで私たちは実践的な仕事に取り組むことができる。"
  },
  {
    "start": 626136,
    "end": 634598,
    "text": "彼らはそれを産業化したいのだ。そして、ユーザーの質問に基づくとか、この特定の問題の文脈が何であろうと、その数ショットの例が文脈に基づいたものであることを望んでいる。"
  },
  {
    "start": 634684,
    "end": 641190,
    "text": "そこでフソットを獲得し、リトリーブ、オーグメンテッド・ジェネレーション、ラグに移るのが一般的だ。"
  },
  {
    "start": 641350,
    "end": 653274,
    "text": "というわけで、ラグに入る前に、基本的にどこに行くべきかを考えるための簡単なメンタル・モデルをお見せしましょう。"
  },
  {
    "start": 653392,
    "end": 655162,
    "text": "私たちはまず、迅速なエンジニアリングから始めた。"
  },
  {
    "start": 655226,
    "end": 662494,
    "text": "我々は評価し、ギャップを特定した。そして今、必要なのは検索機能拡張世代なのか、それとも微調整なのかを決めようとしている。"
  },
  {
    "start": 662612,
    "end": 667042,
    "text": "短期記憶と長期記憶の問題のように考えると、役に立つことがある。"
  },
  {
    "start": 667096,
    "end": 671490,
    "text": "試験の準備をするようなものだ。"
  },
  {
    "start": 671640,
    "end": 675714,
    "text": "あなたのプロンプトは、彼らが試験を完了するために必要な指示を与えているのです。"
  },
  {
    "start": 675832,
    "end": 682902,
    "text": "微調整とは、質問に答えるために必要な方法論や実際のフレームワークを学ぶために、事前に行うすべての勉強のようなものだ。"
  },
  {
    "start": 683036,
    "end": 687270,
    "text": "リトリーバル・オーギュメンテッド・ジェネレーションは、彼らが実際に試験に臨むときにオープンブックを与えるようなものだ。"
  },
  {
    "start": 687340,
    "end": 695914,
    "text": "そして、検索拡張世代とは、本を開き、適切なページに行き、実際に必要なコンテンツを引き出すことができることを意味する。"
  },
  {
    "start": 696032,
    "end": 699082,
    "text": "だから、この2つのことはまったく違うことを解決しているのだ。"
  },
  {
    "start": 699136,
    "end": 702858,
    "text": "方法論や内容がなければ、ある問題を解決することは不可能になる。"
  },
  {
    "start": 702944,
    "end": 705790,
    "text": "この場合、短期記憶に問題があると仮定している。"
  },
  {
    "start": 705860,
    "end": 709774,
    "text": "質問に答えるために必要な正しい文脈をモデルに与えたい。"
  },
  {
    "start": 709972,
    "end": 716126,
    "text": "つまり、検索拡張世代（Rag）とは、モデルにドメイン固有のコンテンツへのアクセスを与えることなのだ。"
  },
  {
    "start": 716308,
    "end": 719086,
    "text": "では、ラグとは何かを簡単に振り返ってみよう。"
  },
  {
    "start": 719108,
    "end": 723070,
    "text": "この会場にいるほとんどの人が知っていることだと思うが、みんなのためにもう一度おさらいしておこう。"
  },
  {
    "start": 723140,
    "end": 731830,
    "text": "通常、ナレッジ・ベースやコンテンツが必要な領域から始め、それを使って質問に答えることになる。"
  },
  {
    "start": 731900,
    "end": 737314,
    "text": "この場合、ごく一般的なフローを使うことになる。つまり、ドキュメントをいくつか用意して、それを埋め込んで、どこかに貼り付ける。"
  },
  {
    "start": 737362,
    "end": 743642,
    "text": "繰り返しになるが、おそらく世の中の人たちは、自分なりの検索サービスや利用する文書の情報源を持っているだろうし、それはそれで結構なことだ。"
  },
  {
    "start": 743696,
    "end": 747850,
    "text": "この例では、いくつかの文書があり、それらを埋め込み、知識ベースを作ると仮定する。"
  },
  {
    "start": 748000,
    "end": 753478,
    "text": "そして、ユーザーがやってきて、例えば、カナダの人口は？"
  },
  {
    "start": 753574,
    "end": 759022,
    "text": "LLMにそれを直接渡す代わりに、ある種の検索を使ってナレッジベースにそれを送るんだ。"
  },
  {
    "start": 759076,
    "end": 761438,
    "text": "類似検索をするとしよう。"
  },
  {
    "start": 761524,
    "end": 762878,
    "text": "私たちはいくつかのコンテンツを後退させるつもりだ。"
  },
  {
    "start": 762964,
    "end": 768734,
    "text": "カナダの人口を示すコンテンツを用意し、それをプロンプトと組み合わせます。"
  },
  {
    "start": 768782,
    "end": 776722,
    "text": "LLMに、これが問題で、これが内容で、この質問にこの内容で答えてくれ、と言うんだ。"
  },
  {
    "start": 776856,
    "end": 779086,
    "text": "というわけで、ラグを簡単に振り返ってみよう。"
  },
  {
    "start": 779198,
    "end": 786402,
    "text": "そこで、プロンプト・エンジニアリングのときと同じように、ラグを使うべきときとそうでないときについて、私たちが培ってきた直感を少しお話ししたいと思います。"
  },
  {
    "start": 786546,
    "end": 792034,
    "text": "RaGが適しているのは、やはり新しい情報をモデルに導入し、その知識を更新することだ。"
  },
  {
    "start": 792082,
    "end": 794934,
    "text": "これは今できる数少ない方法のひとつだ。"
  },
  {
    "start": 794972,
    "end": 797046,
    "text": "実は、お客さんがやってくる最大の問題のひとつなんだ。"
  },
  {
    "start": 797068,
    "end": 798674,
    "text": "10万もの書類があるんだ。"
  },
  {
    "start": 798722,
    "end": 801034,
    "text": "モデルには、この書類で知ってほしいんだ。"
  },
  {
    "start": 801162,
    "end": 807598,
    "text": "残念なことに、今のところ、10万ものドキュメントを一度に処理し、そのすべての知識をモデルに与えるようなスケーラブルな方法はない。"
  },
  {
    "start": 807684,
    "end": 815822,
    "text": "つまり、解決してほしい特定の問題に基づいて、文脈に沿った知識を与えるということだ。"
  },
  {
    "start": 815966,
    "end": 824622,
    "text": "同様に、コンテンツを制御することによって幻覚を減らすことは、検索拡張世代を使用する非常に一般的なユースケースの一つである。"
  },
  {
    "start": 824686,
    "end": 827598,
    "text": "それが微調整とどううまく組み合わされるかは、後で少し見てみよう。"
  },
  {
    "start": 827694,
    "end": 835698,
    "text": "典型的なユースケースは、モデルにコンテンツを与え、そのコンテンツを質問に答えるためにのみ使用し、知識を使用しないように指示を与えることだ。"
  },
  {
    "start": 835794,
    "end": 843994,
    "text": "それなら、知識を特定の知識ベースに限定し、幻覚を減らそうとする典型的な方法のようなものだ。"
  },
  {
    "start": 844032,
    "end": 847750,
    "text": "そこでも言及したが、幅広い領域への理解を埋め込むことだ。"
  },
  {
    "start": 847830,
    "end": 855742,
    "text": "現在のところ、検索機能拡張世代では、法律や医学が何であるかを教えることはできない。"
  },
  {
    "start": 855796,
    "end": 860286,
    "text": "残念なことに、それは検索拡張世代がさせてくれることのひとつではない。"
  },
  {
    "start": 860388,
    "end": 864538,
    "text": "同様に、新しい言語、形式、スタイルを学ぶためにモデルを教えること。"
  },
  {
    "start": 864714,
    "end": 873806,
    "text": "これはおそらく微調整の領域で、方法論や問題解決へのアプローチ方法を教え、トークンの使用量を減らそうとしているのだろう。"
  },
  {
    "start": 873838,
    "end": 876654,
    "text": "実際、ラグではもっともっと多くのトークンを追加することになる。"
  },
  {
    "start": 876702,
    "end": 878750,
    "text": "インプット・アウトプットの例をどんどん増やしていくんだ。"
  },
  {
    "start": 878830,
    "end": 889798,
    "text": "というのも、彼らはまず、自分が納得できるレベルまで精度を上げようとしているからだ。"
  },
  {
    "start": 889884,
    "end": 891962,
    "text": "それについては、ジョンが後でたくさん話してくれるだろう。"
  },
  {
    "start": 892016,
    "end": 896698,
    "text": "ここでラグは、本当に最適化しようとしているだけで、必要なだけの文脈を与えようとしている。"
  },
  {
    "start": 896704,
    "end": 902154,
    "text": "質問に答えて、なぜここでサクセスストーリーを共有しないのですか？"
  },
  {
    "start": 902192,
    "end": 907758,
    "text": "というのも、迅速なエンジニアリングとラグを使えば、こうしたことは簡単なことのように聞こえるが、実際にはとても難しいことなのだ。"
  },
  {
    "start": 907844,
    "end": 912110,
    "text": "これを実際に実現するためには、何度も繰り返し、多くのテストと学習が必要だ。"
  },
  {
    "start": 912180,
    "end": 923122,
    "text": "この例では、ある顧客が2つの異なる知識ベースとLLMを持つラグパイプラインを持っており、その仕事はユーザーの質問を受け、どの知識ベースを使用するかを決定し、クエリを実行し、それを使用して質問に答えることでした。"
  },
  {
    "start": 923256,
    "end": 931622,
    "text": "そして、エンベッディングがどれほど優れたものになるか、どれほど簡単になるか、みんな本当に興奮していたんだ。"
  },
  {
    "start": 931676,
    "end": 935702,
    "text": "ベースラインの精度は45％だったので、それほど良くはない。"
  },
  {
    "start": 935836,
    "end": 943894,
    "text": "その時、私たちが試したことは本当にたくさんあって、その横にダニや十字のような小さな印をつけたのは、私たちがいくつのことを試し、いくつのことが実際に生産に移ったかを示すためだった。"
  },
  {
    "start": 943942,
    "end": 946470,
    "text": "ダニのあるものは、実際に生産に持ち込んだものだ。"
  },
  {
    "start": 946550,
    "end": 949734,
    "text": "十字のものは、私たちが試して捨てたものだ。"
  },
  {
    "start": 949782,
    "end": 962014,
    "text": "質問で類似性検索をする代わりに、偽の答えを生成して、それで類似性検索をする。"
  },
  {
    "start": 962052,
    "end": 964094,
    "text": "今回はうまくいかなかった。"
  },
  {
    "start": 964292,
    "end": 966314,
    "text": "埋め込みの微調整も試みた。"
  },
  {
    "start": 966362,
    "end": 972258,
    "text": "実際に、モデルが正しい答えを導き出せるように、トレーニングセットに基づいて埋め込み空間を変更するようなものだ。"
  },
  {
    "start": 972344,
    "end": 981586,
    "text": "しかし、あまりに高価で遅かったので、機能的でないという理由で廃棄せざるを得なかった。"
  },
  {
    "start": 981698,
    "end": 984146,
    "text": "最後にやったのは、チャンキングとエンベッドだ。"
  },
  {
    "start": 984178,
    "end": 991138,
    "text": "さまざまな大きさの情報のかたまりを試してみたり、さまざまなコンテンツを埋め込んでみたりして、モデルが最も関連性の高いものを識別できるようにした。"
  },
  {
    "start": 991234,
    "end": 997722,
    "text": "でも、まだお客さんの前に出せるようなものにはほど遠い。"
  },
  {
    "start": 997856,
    "end": 1001674,
    "text": "65%に到達するまでに20回は繰り返しただろう。"
  },
  {
    "start": 1001712,
    "end": 1005918,
    "text": "今の段階では、このままプラグを抜くのか、という感じだった。"
  },
  {
    "start": 1006004,
    "end": 1008090,
    "text": "そして、再ランキングを試みた。"
  },
  {
    "start": 1008170,
    "end": 1016626,
    "text": "クロスエンコーダを適用して結果をランク付けし直すか、あるいは、ああ、これは研究だから、最新の文書がいいかもしれない、といったようなルールベースのものを使う。"
  },
  {
    "start": 1016728,
    "end": 1021006,
    "text": "実際、そのおかげで成績は大きく伸びたし、クラス分けもできた。"
  },
  {
    "start": 1021118,
    "end": 1033362,
    "text": "そして、どちらのドメインに分類されたかに応じて、プロンプトに追加のメタデータを与えて、どのコンテンツが最も関連性が高いかを判断できるようにする。"
  },
  {
    "start": 1033426,
    "end": 1035186,
    "text": "この場合も、かなり良いバンプだ。"
  },
  {
    "start": 1035218,
    "end": 1036022,
    "text": "つまり85％だ。"
  },
  {
    "start": 1036076,
    "end": 1039298,
    "text": "今は、生産にこぎ着けられるかどうかの瀬戸際にいるように見える。"
  },
  {
    "start": 1039394,
    "end": 1042434,
    "text": "そして最後に試みたのが、さらなるプロンプト・エンジニアリングだった。"
  },
  {
    "start": 1042482,
    "end": 1046234,
    "text": "スタート地点に戻り、プロンプトをより良いものにしようとした。"
  },
  {
    "start": 1046352,
    "end": 1051174,
    "text": "そして、私たちが間違えている問題のカテゴリーを調べ、それからツールを導入した。"
  },
  {
    "start": 1051222,
    "end": 1056698,
    "text": "例えば、構造化データに関する質問で、ドキュメントから数字を抜き出す必要があることに気づいた。"
  },
  {
    "start": 1056794,
    "end": 1066298,
    "text": "変数を入れてクエリーを実行し、実際に構造化されたデータの答えを返す。"
  },
  {
    "start": 1066394,
    "end": 1077554,
    "text": "最後に、クエリの拡張というのがあって、誰かが3つの質問を1つにして、それをクエリのリストにパースして、それらをすべて並列に実行して結果を出し、1つの結果に合成するんだ。"
  },
  {
    "start": 1077672,
    "end": 1081762,
    "text": "これらのことを組み合わせることで、98％の精度を達成することができた。"
  },
  {
    "start": 1081826,
    "end": 1084562,
    "text": "このプロセスで、微調整を使ったことは一度もない。"
  },
  {
    "start": 1084626,
    "end": 1090322,
    "text": "というのも、冒頭で述べたように、本番に臨むには微調整が必要な場合が多いという前提があるからだ。"
  },
  {
    "start": 1090386,
    "end": 1094342,
    "text": "実際、このケースでは、私たちが対処している問題はすべて文脈に左右されるようなものだった。"
  },
  {
    "start": 1094406,
    "end": 1099130,
    "text": "私たちが適切なコンテクストを与えていないか、コンテクストブロックのどれが正しいのかがわかっていないかのどちらかだった。"
  },
  {
    "start": 1099200,
    "end": 1103626,
    "text": "だからこそ、私たちがここで解決しようとしている問題が何なのかを知ることが非常に重要なのだ。"
  },
  {
    "start": 1103648,
    "end": 1107326,
    "text": "というのも、もしファイン・チューニングに行っていたら、お金も時間も無駄になっていただろうからだ。"
  },
  {
    "start": 1107428,
    "end": 1110606,
    "text": "だからこそ、これは私たちが満足できるサクセスストーリーなのだ。"
  },
  {
    "start": 1110708,
    "end": 1114738,
    "text": "私は少し違うことを言いたかったのだと思う。"
  },
  {
    "start": 1114904,
    "end": 1115378,
    "text": "クールだ。"
  },
  {
    "start": 1115464,
    "end": 1116260,
    "text": "ありがとう。"
  },
  {
    "start": 1120070,
    "end": 1120930,
    "text": "甘い。"
  },
  {
    "start": 1121510,
    "end": 1128482,
    "text": "というのも、ボロ布のような素晴らしいコンテンツがあることもあるからだ。"
  },
  {
    "start": 1128536,
    "end": 1132194,
    "text": "モールはそれを利用して質問に答えるが、それが大失敗に終わることもある。"
  },
  {
    "start": 1132242,
    "end": 1134438,
    "text": "別の顧客の例を挙げよう。"
  },
  {
    "start": 1134524,
    "end": 1141074,
    "text": "ある顧客は、検索拡張世代を使うことで幻覚を減らそうとしていた。"
  },
  {
    "start": 1141122,
    "end": 1144106,
    "text": "彼らはモデルに、あなたのコンテンツだけを使うように言った。"
  },
  {
    "start": 1144208,
    "end": 1147590,
    "text": "そして、幻覚であることをチェックし、フラグを立てる人間のラベラーがいた。"
  },
  {
    "start": 1147670,
    "end": 1153230,
    "text": "そのうちの1つで、お客さんに面白い人がいて、『盛り上がるにはどんな曲がいい？"
  },
  {
    "start": 1153300,
    "end": 1156106,
    "text": "モデルは『DON'T STOP BELIEVING BY JOURNEY』で戻ってきた。"
  },
  {
    "start": 1156218,
    "end": 1159706,
    "text": "これは間違いなく幻覚だ。"
  },
  {
    "start": 1159818,
    "end": 1162714,
    "text": "幸いなことに、それは幻覚ではなかった。"
  },
  {
    "start": 1162762,
    "end": 1164174,
    "text": "実は彼らのコンテンツだった。"
  },
  {
    "start": 1164292,
    "end": 1171166,
    "text": "誰かが、財務分析を活性化させるために最適な曲を手に入れるにはどうしたらいいか、という内容の文書を書いていた。"
  },
  {
    "start": 1171278,
    "end": 1174226,
    "text": "信じることを止めるな、というような答えだった。"
  },
  {
    "start": 1174328,
    "end": 1178990,
    "text": "このような、まあ、ある種の面白さは、ラグのような例でもある。"
  },
  {
    "start": 1179070,
    "end": 1186374,
    "text": "もしモデルにコンテンツだけを使うように指示し、検索がうまくいかなかった場合、モデルが正解を導き出す可能性は0％になる。"
  },
  {
    "start": 1186492,
    "end": 1193082,
    "text": "なぜ私がこのようなことを言うかというと、ボロ布を評価するときに、実際には、うまくいかない可能性のあるまったく別のアクセスが加わってしまうからだ。"
  },
  {
    "start": 1193136,
    "end": 1199338,
    "text": "ミスを犯す可能性のあるLLMと、解決された問題ではない検索があるようなものだ。"
  },
  {
    "start": 1199424,
    "end": 1207738,
    "text": "だからこそ、オープンソースコミュニティが考え出した評価フレームワークのいくつかを紹介したかったし、特にエクスプローディング・グラデーションを紹介したかったのだ。"
  },
  {
    "start": 1207834,
    "end": 1213120,
    "text": "彼らはラガというフレームワークを開発した。"
  },
  {
    "start": 1216050,
    "end": 1225010,
    "text": "基本的に、さまざまな評価指標を分解し、GitHubにアップロードしてそのまま使うこともできるし、ニーズに合わせて調整することもできる。"
  },
  {
    "start": 1225080,
    "end": 1227902,
    "text": "基本的には、4つの測定基準がある。"
  },
  {
    "start": 1227966,
    "end": 1235318,
    "text": "そのうちの2つは、LLMがその質問にどれだけうまく答えているかを測定するもので、2つは、その内容がその質問にどれだけ実際に関連していたかを測定するものである。"
  },
  {
    "start": 1235484,
    "end": 1238946,
    "text": "LLM側から始めると、まず忠実さだ。"
  },
  {
    "start": 1239058,
    "end": 1245402,
    "text": "それは、答えを事実に分解し、その事実のひとつひとつを内容と照合する。"
  },
  {
    "start": 1245536,
    "end": 1249930,
    "text": "事実を照合できなければ、それは幻覚であり、数字を返す。"
  },
  {
    "start": 1250000,
    "end": 1253782,
    "text": "その数字がある閾値以上であれば、幻覚を見つけたということでブロックする。"
  },
  {
    "start": 1253846,
    "end": 1257774,
    "text": "基本的に、これは非常に有用な指標のひとつだ。"
  },
  {
    "start": 1257812,
    "end": 1259386,
    "text": "もうひとつは、回答の関連性だ。"
  },
  {
    "start": 1259498,
    "end": 1268786,
    "text": "多くの場合、モデルは大量のコンテンツを取得し、そのコンテンツをうまく利用した回答を作成するが、実際にはユーザーが最初に質問した内容とは何の関係もない。"
  },
  {
    "start": 1268888,
    "end": 1272002,
    "text": "というのが、この指標が実際に測っているものだ。"
  },
  {
    "start": 1272056,
    "end": 1276590,
    "text": "しかし、関連性は非常に低い。"
  },
  {
    "start": 1276670,
    "end": 1286070,
    "text": "つまり、モデルが実際に、エンジニアにプロンプトを出すか、モデルが質問にもっと注意を払い、実際にそうでない場合はコンテンツを使用しないことを決定するように、ここで何かをする必要があるのでしょう。"
  },
  {
    "start": 1286220,
    "end": 1290358,
    "text": "それはLLM側と同じですが、もう一方は、内容がどれだけ適切か、ということです。"
  },
  {
    "start": 1290444,
    "end": 1299506,
    "text": "というのも、先に述べたように、ラグの典型的な例は、コンテキスト・ウィンドウにどんどんコンテキストを入れていくようなものだからだ。"
  },
  {
    "start": 1299538,
    "end": 1302638,
    "text": "50個のチャンクを与えれば、正しい答えが返ってくる。"
  },
  {
    "start": 1302724,
    "end": 1315246,
    "text": "ロスト・イン・ザ・ミドル（Lost in the middle）」と呼ばれる論文もあります。コンテンツを与えれば与えるほど、モデルは幻覚を見始めたり、途中のコンテンツを忘れ始めたりします。"
  },
  {
    "start": 1315348,
    "end": 1318546,
    "text": "実際にあなたが欲しいのは、最も正確なコンテンツのようなものだ。"
  },
  {
    "start": 1318648,
    "end": 1323362,
    "text": "そこでこの指標は、検索されたコンテンツのノイズに対するシグナル比を評価する。"
  },
  {
    "start": 1323416,
    "end": 1331286,
    "text": "これは、すべてのコンテンツブロックを答えと比較し、そのコンテンツがその答えで使用されたかどうかを判断します。"
  },
  {
    "start": 1331388,
    "end": 1339010,
    "text": "精度は非常に高いが、コンテキストの精度は5％程度だろう。"
  },
  {
    "start": 1339090,
    "end": 1343110,
    "text": "実際に、もっと少ない内容で正しい答えを導き出すことができるのだろうか？"
  },
  {
    "start": 1343190,
    "end": 1355034,
    "text": "これは、プロダクションやプロダクションに近づいてから、直感的にもっともっととコンテクストを増やしていくような観点から考え始めることが、人々にとって本当に役に立つ分野のひとつだと思う。"
  },
  {
    "start": 1355082,
    "end": 1358510,
    "text": "実際、この指標は非常に堅実な計算方法を与えてくれる。"
  },
  {
    "start": 1359170,
    "end": 1361838,
    "text": "文脈を増やすことは、実際に役に立っているのだろうか？"
  },
  {
    "start": 1362004,
    "end": 1363914,
    "text": "最後はコンテクスト・リコールである。"
  },
  {
    "start": 1363962,
    "end": 1366522,
    "text": "必要な関連情報をすべて検索できるか？"
  },
  {
    "start": 1366586,
    "end": 1371858,
    "text": "基本的に、その質問に答えるために必要な関連情報が、実際にコンテンツの中にあるのか？"
  },
  {
    "start": 1371944,
    "end": 1373522,
    "text": "これは逆の問題だ。"
  },
  {
    "start": 1373576,
    "end": 1374690,
    "text": "検索はできるのか？"
  },
  {
    "start": 1374760,
    "end": 1379874,
    "text": "一番上に押し出され、実際にコンテキスト・ウィンドウに表示されているものは、実際に質問に答えているのだろうか？"
  },
  {
    "start": 1379992,
    "end": 1393562,
    "text": "この数値が非常に低い場合、検索を最適化する必要があること、再ランキングを追加する必要があること、埋め込みを微調整する必要があること、より関連性の高いコンテンツを実際に表示させるために別の埋め込みを試してみる必要があることがわかります。"
  },
  {
    "start": 1393696,
    "end": 1401318,
    "text": "それは、私たちが迅速なエンジニアリングとボロ布からできる限りの性能を引き出そうとしているようなものだからだ。"
  },
  {
    "start": 1401414,
    "end": 1404942,
    "text": "また、あなたが答えようとしている問題が違うこともある。"
  },
  {
    "start": 1404996,
    "end": 1409150,
    "text": "時には、あなたが実行させようとしているタスクに問題があることもある。"
  },
  {
    "start": 1409220,
    "end": 1412846,
    "text": "そこで横道にそれて、実際に微調整を試みることになる。"
  },
  {
    "start": 1412878,
    "end": 1416130,
    "text": "ここから先はジョンに引き継ぐ。"
  },
  {
    "start": 1423430,
    "end": 1425170,
    "text": "微調整について話そう。"
  },
  {
    "start": 1425590,
    "end": 1429606,
    "text": "ここまで、私たちはプロンプトのテクニックに注目してきました。"
  },
  {
    "start": 1429628,
    "end": 1438290,
    "text": "そこで、タスクに対するLLMのパフォーマンスを最適化するために、サンプル時にLLMのコンテキスト・ウィンドウを詰める賢い方法を見つけ出すのだ。"
  },
  {
    "start": 1438370,
    "end": 1442070,
    "text": "ファインチューニングは、プロンプティングとはまったく別のテクニックだ。"
  },
  {
    "start": 1442650,
    "end": 1444538,
    "text": "まずは定義からですね？"
  },
  {
    "start": 1444544,
    "end": 1456958,
    "text": "ファインチューニングとは、特に大規模な言語モデルの文脈では、既存の学習済みモデルを用いて、そのモデルが学習された元のデータセットよりも小さく、多くの場合よりドメインに特化した新しいデータセットで学習プロセスを継続することである。"
  },
  {
    "start": 1457124,
    "end": 1465290,
    "text": "つまり、ファインチューニングとは、基本的にベースとなるモデルを微調整し、最終的にまったく異なるモデルに仕上げるという、まさに変革のプロセスなのだ。"
  },
  {
    "start": 1465450,
    "end": 1470290,
    "text": "ファイン・チューニングという名前は、このプロセスを見事に表現しているよね。"
  },
  {
    "start": 1470360,
    "end": 1474418,
    "text": "私たちはまず、膨大で多様なデータセットで訓練されたモデルから始める。"
  },
  {
    "start": 1474504,
    "end": 1480678,
    "text": "これらのモデルは、3.5ターボやGPT4のように、世界に関する非常に一般的な知識をたくさん持っている。"
  },
  {
    "start": 1480764,
    "end": 1490120,
    "text": "私たちは、こうした非常に一般的なモデルのひとつを取り上げ、それを特殊化し、本質的にその能力を磨いて、私たちが関心を寄せるタスクにより適したものにする。"
  },
  {
    "start": 1491690,
    "end": 1493786,
    "text": "そもそも、なぜ微調整をするのか？"
  },
  {
    "start": 1493808,
    "end": 1497830,
    "text": "私は、微調整の主な、そして関連する2つの利点を強調したい。"
  },
  {
    "start": 1497990,
    "end": 1504570,
    "text": "つまり、ファインチューニングをすることで、ファインチューニングなしでは不可能なレベルのパフォーマンスを達成できることが多いということだ。"
  },
  {
    "start": 1504650,
    "end": 1513806,
    "text": "ここで少し直感を植え付けると、プロンプトのテクニックを使う場合、モデルに見せることができるデータ量に関しては、モデルのコンテキストサイズによって制限されますよね？"
  },
  {
    "start": 1513828,
    "end": 1515786,
    "text": "低い方だと、数千トークンといったところだろう。"
  },
  {
    "start": 1515818,
    "end": 1520106,
    "text": "ハイエンドでは、ターボ、GPT4ターボを使用している場合、128,000トークンといったところでしょうか。"
  },
  {
    "start": 1520218,
    "end": 1524682,
    "text": "でも、微調整中にモデルに見せられるデータの量に比べれば、こんなことは何でもない。"
  },
  {
    "start": 1524746,
    "end": 1535830,
    "text": "何百万、何億というトークンのデータを微調整するのは些細なことなので、最大のLLMのコンテキスト・ウィンドウに詰め込むことができるよりも、微調整しながらモデルに多くの例を見せることができる。"
  },
  {
    "start": 1536890,
    "end": 1542598,
    "text": "2つ目の利点は、微調整されたモデルは、対応する基本モデルよりも対話が効率的であることが多いことだ。"
  },
  {
    "start": 1542614,
    "end": 1545866,
    "text": "この効率の良さには2通りある。"
  },
  {
    "start": 1545968,
    "end": 1556174,
    "text": "最初に言っておくと、微調整されたモデルと対話する場合、そのモデルで望ましいパフォーマンスレベルに到達するために、それほど複雑なプロンプトのテクニックを使う必要はないことが多い。"
  },
  {
    "start": 1556212,
    "end": 1562970,
    "text": "多くの場合、それほど複雑な指示を出す必要はなく、明示的なスキーマを提供する必要もなく、文脈に沿った例を提供する必要もない。"
  },
  {
    "start": 1563050,
    "end": 1569758,
    "text": "これが意味するのは、リクエストごとに送信するプロンプトトークンの数が減るということだ。"
  },
  {
    "start": 1569774,
    "end": 1569906,
    "text": "そうだろう？"
  },
  {
    "start": 1569928,
    "end": 1574210,
    "text": "微調整されたモデルとのやりとりの方が、レイテンシーもコスト効率も良いのだ。"
  },
  {
    "start": 1574950,
    "end": 1584498,
    "text": "次に、ファインチューニングの一般的なユースケースは、基本的にGBD 4のような非常に大きなモデルから3.5ターボのような小さなモデルへの知識の蒸留である。"
  },
  {
    "start": 1584594,
    "end": 1590246,
    "text": "そのため、コストやレイテンシーの観点からは、大きなモデルよりも小さなモデルとやり取りする方が常に効率的なのだ。"
  },
  {
    "start": 1590428,
    "end": 1592726,
    "text": "例を見てみよう。"
  },
  {
    "start": 1592748,
    "end": 1596614,
    "text": "これはLLMSで解決したい一般的なタスクの例だ。"
  },
  {
    "start": 1596662,
    "end": 1604618,
    "text": "ここでやっていることは、基本的に不動産物件に関する自然言語の記述を取り出し、その物件に関する構造化された情報を抽出しようとしているのだ。"
  },
  {
    "start": 1604794,
    "end": 1613418,
    "text": "だから、微調整なしでこの問題を解決しようとするなら、基本的にはプロンプト技術のツールボックスを開き、複雑な命令を書くことになる。"
  },
  {
    "start": 1613514,
    "end": 1617026,
    "text": "モデルに出力させたい明示的なスキーマをここで提供する。"
  },
  {
    "start": 1617048,
    "end": 1619278,
    "text": "これはPythonのパイダンティック・モデルのように定義されている。"
  },
  {
    "start": 1619374,
    "end": 1623586,
    "text": "私たちはモデルに対して、いくつかの文脈上の例を提供することもできるだろう。"
  },
  {
    "start": 1623688,
    "end": 1628606,
    "text": "そこで、モデルに新しい不動産物件を自然言語で与えると、出力が得られる。"
  },
  {
    "start": 1628638,
    "end": 1629622,
    "text": "出力はかなりいい。"
  },
  {
    "start": 1629676,
    "end": 1633446,
    "text": "実はここに間違いがあって、それはかなり些細な間違いなんだ。"
  },
  {
    "start": 1633468,
    "end": 1638214,
    "text": "希望する日付を抽出する代わりに、現在の日付をテンプレート化したのだ。"
  },
  {
    "start": 1638332,
    "end": 1640246,
    "text": "これを修正するのは簡単だろう？"
  },
  {
    "start": 1640268,
    "end": 1645898,
    "text": "新しいルールを追加し、本質的に新しい文脈の例を追加すれば、おそらくこの問題を解決できるだろう。"
  },
  {
    "start": 1645984,
    "end": 1648490,
    "text": "微調整を加えながら、どのようにアプローチするか見てみよう。"
  },
  {
    "start": 1649230,
    "end": 1653034,
    "text": "微調整をするために、比較的単純なデータセットから始めるんだ。"
  },
  {
    "start": 1653072,
    "end": 1654346,
    "text": "ここに例がある。"
  },
  {
    "start": 1654378,
    "end": 1656954,
    "text": "これらの例がシンプルであることに気づいてほしい。"
  },
  {
    "start": 1657002,
    "end": 1662266,
    "text": "複雑な指示もなければ、正式なスキーマもない。"
  },
  {
    "start": 1662298,
    "end": 1667370,
    "text": "私たちが与えているのは、不動産物件に関する自然言語による説明と、次に希望する構造化された出力だけだ。"
  },
  {
    "start": 1667450,
    "end": 1670018,
    "text": "このデータセットを使ってモデルを微調整する。"
  },
  {
    "start": 1670184,
    "end": 1675570,
    "text": "そして、この微調整されたモデルを新しい不動産物件に適用すると、本質的に問題が解決されることがわかる。"
  },
  {
    "start": 1675640,
    "end": 1676466,
    "text": "この場合はそうだね。"
  },
  {
    "start": 1676488,
    "end": 1681154,
    "text": "これは単純な例に過ぎないが、この場合、このモデルはパフォーマンスと効率の両方を兼ね備えている。"
  },
  {
    "start": 1681202,
    "end": 1686786,
    "text": "サンプリング時には、複雑な指示を与える必要はなく、文脈の中での学習も、明示的なスキーマも必要ない。"
  },
  {
    "start": 1686818,
    "end": 1690150,
    "text": "このモデルは、プロンプトのテクニックだけでやっていたときよりもうまくいっている。"
  },
  {
    "start": 1691930,
    "end": 1694730,
    "text": "微調整はかなり複雑なプロセスになる。"
  },
  {
    "start": 1694800,
    "end": 1701260,
    "text": "そのため、微調整があなたのユースケースに有効な場合とそうでない場合について、適切な期待値を設定することが重要だ。"
  },
  {
    "start": 1701710,
    "end": 1706310,
    "text": "微調整は、ベースモデルにすでに存在する知識を強調するのに実に適している。"
  },
  {
    "start": 1706400,
    "end": 1708938,
    "text": "例えば、テキストをSQLに変換するタスクのようなものだ。"
  },
  {
    "start": 1708954,
    "end": 1720578,
    "text": "3.5ターボやGPT 4のような非常に強力な汎用ベースモデルがあり、SQL構文、SQLのさまざまな方言、データベースの仕組みなど、SQLについて理解すべきことをすべて理解している。"
  },
  {
    "start": 1720664,
    "end": 1729606,
    "text": "特定のSQL方言を本質的に強調するようにモデルを微調整したり、特にエラーになりやすいエッジケースに入らないようにモデルを強制したりしたいかもしれませんね？"
  },
  {
    "start": 1729628,
    "end": 1733958,
    "text": "基本的にベースモデルに存在する知識を取り入れ、そのサブセットを強調しているのだ。"
  },
  {
    "start": 1734124,
    "end": 1739746,
    "text": "ファインチューニングは、モデルの出力の構造や音色を変更したりカスタマイズしたりするのにも最適です。"
  },
  {
    "start": 1739778,
    "end": 1745578,
    "text": "つまり、ファインチューニングの初期のキラーユースケースの1つは、モデルに有効なJSONを強制的に出力させることだったんだね？"
  },
  {
    "start": 1745584,
    "end": 1751318,
    "text": "プログラムでモデルとやりとりする場合、有効なJSONを取得することは、プログラムで処理する上で非常に簡単だからです。"
  },
  {
    "start": 1751334,
    "end": 1755206,
    "text": "もしそれが無効なJSONであれば、多くのエラー・ケースが発生することになる。"
  },
  {
    "start": 1755318,
    "end": 1757386,
    "text": "最後に、モデルに複雑な指示を教える。"
  },
  {
    "start": 1757418,
    "end": 1759166,
    "text": "まあ、これは先に述べたような理由からだろう？"
  },
  {
    "start": 1759188,
    "end": 1766000,
    "text": "微調整の過程で、モデルのコンテキスト・ウィンドウに詰め込むことができないほど多くの例を、モデルに見せることができるんだ。"
  },
  {
    "start": 1766770,
    "end": 1772478,
    "text": "その一方で、微調整はモデルに新しい知識を追加するのに適していない。"
  },
  {
    "start": 1772564,
    "end": 1782258,
    "text": "LLMに存在する知識は、この非常に大規模な事前トレーニングの実行中にLLMにインプットされたものであり、この限られた微調整の実行中に新しい知識をLLMにインプットすることは基本的にできない。"
  },
  {
    "start": 1782274,
    "end": 1788134,
    "text": "新しい知識をモデルに取り入れようとするなら、今コリンが言ったような理由から、ラグに注目したい。"
  },
  {
    "start": 1788332,
    "end": 1792070,
    "text": "次に、微調整は新しいユースケースを素早く反復するのには向いていない。"
  },
  {
    "start": 1792140,
    "end": 1794374,
    "text": "微調整なら、比較的ゆっくりとしたフィードバックループになる。"
  },
  {
    "start": 1794422,
    "end": 1798598,
    "text": "データセットの作成や、微調整のためのその他の要素には多くの投資が必要だ。"
  },
  {
    "start": 1798614,
    "end": 1800540,
    "text": "最初からそうするな"
  },
  {
    "start": 1802030,
    "end": 1807146,
    "text": "私は本質的に微調整の成功例を見てみたかったのだが、これはCanvaの友人たちによるものだ。"
  },
  {
    "start": 1807178,
    "end": 1819834,
    "text": "ここでのタスクは本質的に、ユーザーが望むデザイン・モックを自然言語で記述してLLMに渡し、LLMが構造化されたデザイン・ガイドラインのセットのように出力することだった。"
  },
  {
    "start": 1819962,
    "end": 1824602,
    "text": "そして、その構造化されたデザイン・ガイドラインを使って実物大のモックを作成し、それをユーザーに見せることができる。"
  },
  {
    "start": 1824666,
    "end": 1824918,
    "text": "そうだろう？"
  },
  {
    "start": 1824964,
    "end": 1828290,
    "text": "これは基本的に、アイデアを出して実物大のモックを手に入れるための簡単な方法だ。"
  },
  {
    "start": 1828370,
    "end": 1834070,
    "text": "ここでユーザーは、「赤いグラデーションが欲しい」「プロフィール写真が欲しい」「インスタグラムの投稿のようなスタイルがいい」などと言う。"
  },
  {
    "start": 1834140,
    "end": 1836886,
    "text": "それはLLMに通じるもので、ここでは非常に構造化されたものを出力することになっている。"
  },
  {
    "start": 1836908,
    "end": 1841426,
    "text": "タイトルがあり、既知のキーワードからいくつかのキーワードを使ったスタイルがある。"
  },
  {
    "start": 1841538,
    "end": 1848650,
    "text": "ヒーロー画像の説明と、画像検索エンジンに与える実際の検索があり、フルサイズのモック用の画像を生成する。"
  },
  {
    "start": 1849070,
    "end": 1855006,
    "text": "キャンバがやったことは、基本的にベースモデルに3.5ターボを搭載し、次にGBT 4を搭載したことだ。"
  },
  {
    "start": 1855108,
    "end": 1858026,
    "text": "彼らは本質的に、このタスクでのパフォーマンスを見たかったのだ。"
  },
  {
    "start": 1858058,
    "end": 1859182,
    "text": "パフォーマンスはあまり良くなかった。"
  },
  {
    "start": 1859236,
    "end": 1862602,
    "text": "そのため、基本的には専門家である人間の評価者によって評価された。"
  },
  {
    "start": 1862666,
    "end": 1870820,
    "text": "彼らが発見したのは、これらのモデルが賢明なアウトプットを出力することはできても、設計の観点から見ると、アウトプットは実際には無関係であるということだった。"
  },
  {
    "start": 1871190,
    "end": 1873262,
    "text": "その後、微調整に入った。"
  },
  {
    "start": 1873326,
    "end": 1876158,
    "text": "そのため、3.5ターボにファインチューンを施した。"
  },
  {
    "start": 1876174,
    "end": 1877646,
    "text": "その結果には本当に驚いている。"
  },
  {
    "start": 1877678,
    "end": 1877826,
    "text": "そうだろう？"
  },
  {
    "start": 1877848,
    "end": 1883638,
    "text": "ベースの3.5ターボモデルに勝っただけでなく、GBT 4を大幅に上回った。"
  },
  {
    "start": 1883724,
    "end": 1898538,
    "text": "つまり、3.5ターボとGPT4は、賢明だが無関係なデザインモックを出力することが多いのに対して、微調整された3.5ターボは、canva内の専門家評価者による評価では、むしろ優れたデザインモックを出力することが多かったということですね？"
  },
  {
    "start": 1898624,
    "end": 1903054,
    "text": "なぜこのユースケースがうまくいったのかを考えるなら、まず、新しい知識は必要ないだろう？"
  },
  {
    "start": 1903092,
    "end": 1912714,
    "text": "この問題を解決するのに必要な知識はすべてベースモデルの中に存在していたが、モデルは非常に特殊な、非常に具体的な構造のアウトプットを出力する必要があった。"
  },
  {
    "start": 1912842,
    "end": 1918306,
    "text": "そして、非常に質の高いトレーニングデータを使用し、そのデータと比較するための優れたベースラインがあった。"
  },
  {
    "start": 1918328,
    "end": 1920174,
    "text": "実質的には3.5ターボを評価している。"
  },
  {
    "start": 1920222,
    "end": 1921538,
    "text": "彼らはGPTを4つ評価した。"
  },
  {
    "start": 1921624,
    "end": 1923646,
    "text": "自分たちが成功しているところとそうでないところを理解していた。"
  },
  {
    "start": 1923678,
    "end": 1928958,
    "text": "だから彼らは、微調整がこのタスクのためにアプローチする、あるいはこのタスクのために使う良いテクニックのようになることを知っていた。"
  },
  {
    "start": 1929054,
    "end": 1932274,
    "text": "ここで、微調整の注意点についてお話ししたい。"
  },
  {
    "start": 1932322,
    "end": 1939846,
    "text": "私がとても気に入っている素晴らしいブログ記事の著者がいるんだけど、彼らはAIアシスタントをライティングアシスタントにする実験をしていたんだ。"
  },
  {
    "start": 1939868,
    "end": 1941346,
    "text": "そこで彼らはチャットGBTを試みた。"
  },
  {
    "start": 1941378,
    "end": 1943078,
    "text": "彼らはAPIからいくつかのベースモデルを試した。"
  },
  {
    "start": 1943174,
    "end": 1947226,
    "text": "彼らは感動していたが、これらのモデルが自分たちのトーンを正しく捉えていないことに失望していた。"
  },
  {
    "start": 1947248,
    "end": 1952886,
    "text": "彼らは、ブログ記事やソーシャルメディアへの投稿、Eメールの草稿を書くときに使う、とても独特な口調を持っている。"
  },
  {
    "start": 1952918,
    "end": 1955210,
    "text": "ベースモデルはこのトーンを表現できていなかった。"
  },
  {
    "start": 1955290,
    "end": 1961774,
    "text": "彼らは素晴らしいアイデアを持っていて、2年分のスラックメッセージ、合計14万通をダウンロードしようと言ったんだ。"
  },
  {
    "start": 1961892,
    "end": 1966794,
    "text": "彼らは、これらのスラックメッセージを微調整に対応できるデータ形式にフォーマットするスクリプトを書いた。"
  },
  {
    "start": 1966842,
    "end": 1970062,
    "text": "その後、3.5ターボをファインチューニングした。"
  },
  {
    "start": 1970206,
    "end": 1976382,
    "text": "データを収集し、集計し、微調整が可能なフォーマットに変換する。"
  },
  {
    "start": 1976446,
    "end": 1977346,
    "text": "モデルを微調整する。"
  },
  {
    "start": 1977448,
    "end": 1981446,
    "text": "最終的にこのプロセスを経て、微調整されたモデルを手に入れ、そのモデルに何かを依頼するわけだ。"
  },
  {
    "start": 1981468,
    "end": 1984690,
    "text": "プロンプト・エンジニアリングについて500語のブログ記事を書いてくれませんか？"
  },
  {
    "start": 1984770,
    "end": 1989900,
    "text": "このモデル、このパーソナライズされたライティング・アシスタントは、もちろん、午前中にやります、と答える。"
  },
  {
    "start": 1994510,
    "end": 1995260,
    "text": "それだ。"
  },
  {
    "start": 1997470,
    "end": 2002310,
    "text": "少し驚いたし、ショックを受けたのは確かだ。"
  },
  {
    "start": 2002470,
    "end": 2006400,
    "text": "彼はフォローアップして、あなたが書いた方がいい、今すぐ書いてくれ、頼むよ、という感じだ。"
  },
  {
    "start": 2008770,
    "end": 2011760,
    "text": "モデルはオーケーと言って、あとは何もしないんだろ？"
  },
  {
    "start": 2015010,
    "end": 2019582,
    "text": "私たちファインチューニング・チームは、これで本当に興奮した。"
  },
  {
    "start": 2019646,
    "end": 2024274,
    "text": "一歩引いて考えれば、微調整は本当にうまくいった。"
  },
  {
    "start": 2024312,
    "end": 2028798,
    "text": "本質的に著者が求めていたのは、自分の文体を再現できるモデルだったのだ。"
  },
  {
    "start": 2028894,
    "end": 2032946,
    "text": "つまり、彼らが手に入れたのは、彼らの文体を再現できるモデルであり、しかし、彼らのたるんだ文体だったのだ。"
  },
  {
    "start": 2032978,
    "end": 2037746,
    "text": "スラックでどのようにコミュニケーションをとっているか考えてみると、とても簡潔で、意識の流れのようなスタイルだ。"
  },
  {
    "start": 2037778,
    "end": 2041442,
    "text": "句読点や文法的な正しさを度々見落としている。"
  },
  {
    "start": 2041506,
    "end": 2043978,
    "text": "彼らが手にしたのは、それを再現したモデルだった。"
  },
  {
    "start": 2044064,
    "end": 2044458,
    "text": "そうだろう？"
  },
  {
    "start": 2044544,
    "end": 2058846,
    "text": "自分のトーンを再現するためにモデルを微調整することは、実際には微調整のための比較的良いユースケースであるが、ここでのエラーは、彼らが提供しているデータ、モデルが、彼らがそのモデルに求めている最終的な行動を本当に再現しているかどうかを十分に考えなかったことである。"
  },
  {
    "start": 2058948,
    "end": 2066586,
    "text": "100通のスラックメッセージを受け取り、200通のスラックメッセージを受け取り、モデルを微調整し、実験し、正しい方向に進んでいるかどうかを確認する。"
  },
  {
    "start": 2066618,
    "end": 2066766,
    "text": "そうだね。"
  },
  {
    "start": 2066788,
    "end": 2069642,
    "text": "私がモデルに再現させたい音色に近づいているだろうか？"
  },
  {
    "start": 2069706,
    "end": 2078694,
    "text": "そうでないことはすぐにわかっただろうし、Eメールやブログ記事、ソーシャルメディアへの投稿で微調整しただろう。"
  },
  {
    "start": 2078892,
    "end": 2080882,
    "text": "いくつかの例を見て、直感を養った。"
  },
  {
    "start": 2080946,
    "end": 2083846,
    "text": "モデルの微調整はどのように行うのですか？"
  },
  {
    "start": 2083948,
    "end": 2087014,
    "text": "他のMLの問題と同じように、最初のステップはデータセットを手に入れることだ。"
  },
  {
    "start": 2087052,
    "end": 2090038,
    "text": "ほとんどのML問題と同様、実はこれが最も難しい部分なのだ。"
  },
  {
    "start": 2090124,
    "end": 2103914,
    "text": "データセットを正しく入手する方法には、オープンソースのデータセットをダウンロードする方法、プライベートマーケットでデータを購入する方法、人間のラベラーにお金を払って基本的にデータを収集し、ラベルを付けてもらう方法、あるいは、そのモデルの利用規約が特定のユースケースをサポートしていれば、より大きなモデルから抽出する方法などがある。"
  },
  {
    "start": 2104032,
    "end": 2107360,
    "text": "何らかの方法で、微調整を行うためのデータセットを用意しなければならない。"
  },
  {
    "start": 2107730,
    "end": 2110270,
    "text": "次に、実際にトレーニングプロセスを開始することになる。"
  },
  {
    "start": 2110340,
    "end": 2113694,
    "text": "だから、これはトレーニングのやり方によって大きく変わってくる。"
  },
  {
    "start": 2113812,
    "end": 2119386,
    "text": "OpenAI fine Tuning APIのようなターンキー・ソリューションを使えば、これは比較的簡単です。"
  },
  {
    "start": 2119428,
    "end": 2124094,
    "text": "オープンソースのモデルを微調整するのであれば、GPUを自前で用意したり、フレームワークを使ったりすればいい。"
  },
  {
    "start": 2124142,
    "end": 2131910,
    "text": "少し複雑ですが、トレーニング中にチューニング可能なハイパーパラメーターを理解することが重要です。"
  },
  {
    "start": 2131980,
    "end": 2132694,
    "text": "そうだろう？"
  },
  {
    "start": 2132892,
    "end": 2134306,
    "text": "オーバーフィットになりやすい？"
  },
  {
    "start": 2134338,
    "end": 2135634,
    "text": "フィットしていない可能性が高い？"
  },
  {
    "start": 2135682,
    "end": 2139190,
    "text": "破滅的な忘れ方をするほど微調整するつもりなのか？"
  },
  {
    "start": 2139850,
    "end": 2145306,
    "text": "利用可能なハイパーパラメータと、それが微調整されたモデルに与える影響を理解することが重要なのだ。"
  },
  {
    "start": 2145488,
    "end": 2149642,
    "text": "次に指摘したいのは、損失関数を理解することが本当に重要だということだ。"
  },
  {
    "start": 2149696,
    "end": 2155770,
    "text": "LLMを微調整するとき、損失関数を見るとき、それは次のトークン予測の代用になります。"
  },
  {
    "start": 2155930,
    "end": 2164270,
    "text": "これはLLMを微調整しているときには素晴らしいことだが、次のトークン予測は、気になる下流タスクのパフォーマンスとあまり相関がないことが多い。"
  },
  {
    "start": 2164340,
    "end": 2169730,
    "text": "コード生成について考えるなら、ひとつの問題を解決するためにコードを書く方法はさまざまな種類がある。"
  },
  {
    "start": 2169800,
    "end": 2182166,
    "text": "つまり、次のトークン予測や完全なトークン照合を行うだけであれば、モデルの損失や損失関数の変化は、下流のタスクにおけるパフォーマンスの変化と相関しない可能性があるということですね？"
  },
  {
    "start": 2182188,
    "end": 2186646,
    "text": "次にモデルを評価することが重要なんだ。"
  },
  {
    "start": 2186668,
    "end": 2189062,
    "text": "だから、モデルを評価する方法はいくつかある。"
  },
  {
    "start": 2189116,
    "end": 2194038,
    "text": "基本的には、アウトプットを専門家の人間に見てもらい、ある尺度で実際にランク付けしてもらうことができる。"
  },
  {
    "start": 2194214,
    "end": 2200906,
    "text": "もう1つのテクニックは、基本的に異なるモデルを用いて出力を生成し、それらを互いにランク付けすることだ。"
  },
  {
    "start": 2200928,
    "end": 2209086,
    "text": "絶対的なランキングではなく、チェスで得られるELOスコアのようなものを使えば、より強力なモデルにアウトプットをランク付けしてもらうようなこともできる。"
  },
  {
    "start": 2209108,
    "end": 2215310,
    "text": "これは、GPT4を使って微調整されたオープンソースモデルやGPT3.5ターボの出力をランク付けする際によく見られることだ。"
  },
  {
    "start": 2216210,
    "end": 2220002,
    "text": "最終的には、それを実際に配備し、推論時にそこからサンプリングすることになる。"
  },
  {
    "start": 2220056,
    "end": 2224626,
    "text": "つまり、最後の3つのポイントは、フィードバックループとデータフィードバックループのようなものを形成することができるんだね？"
  },
  {
    "start": 2224648,
    "end": 2235526,
    "text": "モデルを訓練し、評価し、本番環境に導入し、本番環境でモデルからサンプルを収集し、それを使って新しいデータセットを構築し、データセットをサンプリングし、少しキュレーションし、そのデータセットでさらに微調整する。"
  },
  {
    "start": 2235548,
    "end": 2237686,
    "text": "ここでフライホイールのようなものが動く。"
  },
  {
    "start": 2237868,
    "end": 2243846,
    "text": "ここまで、いくつかのベストプラクティスについてお話ししてきましたが、微調整に関して私たちが推奨するベストプラクティスのいくつかを正式にまとめたいと思います。"
  },
  {
    "start": 2243878,
    "end": 2244218,
    "text": "そうだろう？"
  },
  {
    "start": 2244304,
    "end": 2247946,
    "text": "まずは、迅速なエンジニアリングと数ショットの学習から始めることだね。"
  },
  {
    "start": 2248048,
    "end": 2250726,
    "text": "これらは非常にシンプルで、投資額の少ないテクニックのようなものだ。"
  },
  {
    "start": 2250758,
    "end": 2254986,
    "text": "LLMがどのように運営され、あなたの問題に対してどのように取り組むのか、直感を与えてくれるだろう。"
  },
  {
    "start": 2255088,
    "end": 2257120,
    "text": "それは素晴らしいスタート地点だ。"
  },
  {
    "start": 2257890,
    "end": 2267294,
    "text": "次に、微調整に移る前に、ベースラインを確立することが本当に重要だということです。これは、canvaのサクセスストーリーにつながりますが、彼らは3.5ターボで実験し、GPT 4で実験しました。"
  },
  {
    "start": 2267332,
    "end": 2272062,
    "text": "彼らは、自分たちが解決しようとしている問題の裏も表も本当によく理解していた。"
  },
  {
    "start": 2272126,
    "end": 2273886,
    "text": "彼らはそれらのモデルの失敗例を理解していた。"
  },
  {
    "start": 2273918,
    "end": 2279110,
    "text": "彼らはモデルがうまくいっている場所を理解していたので、微調整で何をターゲットにしたいかを正確に理解していた。"
  },
  {
    "start": 2279530,
    "end": 2282520,
    "text": "最後に、微調整に関しては、小さく始めることだ。"
  },
  {
    "start": 2283530,
    "end": 2287650,
    "text": "14万件ものスラックメッセージをダウンロードせず、一発でやってしまおう。"
  },
  {
    "start": 2287730,
    "end": 2294030,
    "text": "小規模で質の高いデータセットを開発し、微調整を行い、モデルを評価し、正しい方向に進んでいるかどうかを確認する。"
  },
  {
    "start": 2294130,
    "end": 2303166,
    "text": "モデルを微調整し、その出力を見て、どの領域で苦戦しているかを確認し、新しいデータでその領域を特にターゲットにするんだ。"
  },
  {
    "start": 2303188,
    "end": 2310778,
    "text": "LLMと微調整に関しては、データの質がデータの量に勝ることが本当に重要だ。"
  },
  {
    "start": 2310874,
    "end": 2314394,
    "text": "訓練プロセスのデータ量の部分は、事前訓練で行われた。"
  },
  {
    "start": 2314442,
    "end": 2314654,
    "text": "そうだね。"
  },
  {
    "start": 2314692,
    "end": 2321426,
    "text": "今、あなたは本当に質の高い例を少なくして、微調整やラグについてだけ話したいようだね？"
  },
  {
    "start": 2321448,
    "end": 2325470,
    "text": "特定のユースケースでこれらを組み合わせたい場合は、両方の長所を生かすことができる。"
  },
  {
    "start": 2325550,
    "end": 2332594,
    "text": "この仕組みは、複雑な命令を理解できるようにモデルを微調整することで、複雑な命令を提供する必要がなくなるというものだ。"
  },
  {
    "start": 2332722,
    "end": 2335910,
    "text": "サンプル時のショット例は少ないですよね？"
  },
  {
    "start": 2335980,
    "end": 2338838,
    "text": "このモデルは基本的に、非常に効率的に使用することができる。"
  },
  {
    "start": 2339004,
    "end": 2343802,
    "text": "これが意味するのは、サンプル時に提供する必要のあるプロンプトトークンを実質的に最小限に抑えられるということだ。"
  },
  {
    "start": 2343856,
    "end": 2344122,
    "text": "オーケー。"
  },
  {
    "start": 2344176,
    "end": 2348378,
    "text": "もはや複雑なプロンプト・エンジニアリングは必要なく、微調整されたモデルに組み込まれているからだ。"
  },
  {
    "start": 2348464,
    "end": 2351398,
    "text": "これは、検索されたコンテキストの容量が増えることを意味する。"
  },
  {
    "start": 2351574,
    "end": 2355254,
    "text": "そうすれば、ラグを使って文脈に関連した知識を注入することができる。"
  },
  {
    "start": 2355302,
    "end": 2357806,
    "text": "この点で、利用可能なコンテクストは基本的に最大化されている。"
  },
  {
    "start": 2357828,
    "end": 2369490,
    "text": "もちろん、コンテキストの中には、あなたが解決しようとしている実際の問題との間に偽の相関関係があるかもしれないので、コンテキストの過飽和にならないように注意しなければならない。"
  },
  {
    "start": 2370070,
    "end": 2373602,
    "text": "とはいえ、ここまで理論的な話をしてきた。"
  },
  {
    "start": 2373656,
    "end": 2375266,
    "text": "これから、この理論の応用について話す。"
  },
  {
    "start": 2375298,
    "end": 2377720,
    "text": "では、コリンに話を戻します。"
  },
  {
    "start": 2378570,
    "end": 2385266,
    "text": "ありがとう、ジョニー。"
  },
  {
    "start": 2385378,
    "end": 2385750,
    "text": "クールだ。"
  },
  {
    "start": 2385820,
    "end": 2387890,
    "text": "ああ、ではこの理論をすべてひっくるめて考えてみよう。"
  },
  {
    "start": 2387970,
    "end": 2393574,
    "text": "私たちが取り組むことにした問題は、スパイダー1.0ベンチマークだった。"
  },
  {
    "start": 2393622,
    "end": 2401898,
    "text": "自然言語による質問とデータベーススキーマが与えられたとき、その質問に答える構文的に正しいSQLクエリを作成できるか？"
  },
  {
    "start": 2402064,
    "end": 2404366,
    "text": "例を挙げると次のようになる。"
  },
  {
    "start": 2404468,
    "end": 2410254,
    "text": "このデータベース・スキーマと一番下の質問から、右のSQLクエリを作成できますか？"
  },
  {
    "start": 2410372,
    "end": 2413742,
    "text": "古典的な問題で、いろいろな試みがある。"
  },
  {
    "start": 2413796,
    "end": 2416874,
    "text": "私たちがしたことは、皆さんにお伝えしたようなアドバイスに従うことでした。"
  },
  {
    "start": 2416922,
    "end": 2424306,
    "text": "私たちはまず、プロンプト・エンジニアリングとラグから始めた。私たちが使っているさまざまな方法のいくつかを紹介すれば、私たちが試したことの詳細に触れることができるだろう。"
  },
  {
    "start": 2424488,
    "end": 2426994,
    "text": "まずは可能な限りシンプルな雑巾がけから始めた。"
  },
  {
    "start": 2427042,
    "end": 2429042,
    "text": "私たちは簡単なリトリーブを始めた。"
  },
  {
    "start": 2429106,
    "end": 2436422,
    "text": "コサイン類似度と同じように、質問を使って、基本的には似たような質問に答えたSQLクエリを見つける。"
  },
  {
    "start": 2436476,
    "end": 2438278,
    "text": "という質問で類似検索をする。"
  },
  {
    "start": 2438444,
    "end": 2440546,
    "text": "また、埋め込みのフォーマットを変えてみました。"
  },
  {
    "start": 2440578,
    "end": 2444198,
    "text": "私たちは、孤立した2、3の例でプロンプト・エンジニアリングをたくさん試した。"
  },
  {
    "start": 2444294,
    "end": 2447594,
    "text": "結果は、すぐにおわかりのように、あまり良いものではなかった。"
  },
  {
    "start": 2447712,
    "end": 2455290,
    "text": "私たちがしたことは、この問題について考え、ある質問が異なるデータベーススキーマを持つ場合、まったく異なる答えになる可能性があるということです。"
  },
  {
    "start": 2455370,
    "end": 2459326,
    "text": "質問で類似検索をすることは、この問題ではあまり意味がない。"
  },
  {
    "start": 2459428,
    "end": 2466302,
    "text": "その質問に対する仮定の答えを検索に使うことで、この問題に対するより良い結果が得られるかもしれない。"
  },
  {
    "start": 2466356,
    "end": 2468606,
    "text": "私たちが行ったのは、仮想の文書埋め込みを使うことでした。"
  },
  {
    "start": 2468638,
    "end": 2472434,
    "text": "私たちは仮想のSQLクエリを生成し、それを類似検索に使用した。"
  },
  {
    "start": 2472472,
    "end": 2474546,
    "text": "その結果、パフォーマンスが大幅に向上した。"
  },
  {
    "start": 2474568,
    "end": 2479266,
    "text": "この問題では、単純なフィルタリングだけでなく、文脈検索も試した。"
  },
  {
    "start": 2479298,
    "end": 2488518,
    "text": "私たちは、質問の難易度をランク付けして、基本的に同じ難易度の例だけをボロ雑巾のように持ち帰ることにしたんだ。"
  },
  {
    "start": 2488604,
    "end": 2490834,
    "text": "その結果、少し改善された。"
  },
  {
    "start": 2490962,
    "end": 2493654,
    "text": "その後、さらに高度なテクニックをいくつか試した。"
  },
  {
    "start": 2493702,
    "end": 2496806,
    "text": "思考の連鎖のようなものを試してみるのもいいだろう。"
  },
  {
    "start": 2496838,
    "end": 2502522,
    "text": "カラムを特定し、テーブルを特定し、最後にクエリを構築するようにするといいかもしれない。"
  },
  {
    "start": 2502576,
    "end": 2505950,
    "text": "私たちが決めたのは、実はとてもシンプルなことだった。"
  },
  {
    "start": 2506020,
    "end": 2508302,
    "text": "私たちは自己整合性チェックを行った。"
  },
  {
    "start": 2508356,
    "end": 2516000,
    "text": "実際にクエリーを構築し、クエリーを実行させ、失敗した場合はエラーメッセージを表示させ、少し注釈を加えてから再試行させた。"
  },
  {
    "start": 2516450,
    "end": 2520930,
    "text": "でも、GBDを修理させるようなものだ。"
  },
  {
    "start": 2521000,
    "end": 2527700,
    "text": "レイテンシーが大きな問題ではなく、コストも気にならないようなユースケースであれば、かなりうまくいくだろう。"
  },
  {
    "start": 2528070,
    "end": 2531718,
    "text": "結果はこんな感じだった。"
  },
  {
    "start": 2531804,
    "end": 2533734,
    "text": "この話をするためにここに来たようなものだ。"
  },
  {
    "start": 2533772,
    "end": 2537074,
    "text": "右端は、迅速なエンジニアリングでたどり着いた場所だ。"
  },
  {
    "start": 2537122,
    "end": 2538102,
    "text": "あまり良くない。"
  },
  {
    "start": 2538156,
    "end": 2545686,
    "text": "最初は69％だったが、シュート例をいくつか追加して2点ほど改善した。"
  },
  {
    "start": 2545718,
    "end": 2548634,
    "text": "そのボロ布が、実はここでさらなる改善をもたらす可能性があることを教えてくれた。"
  },
  {
    "start": 2548672,
    "end": 2552626,
    "text": "という質問で試してみたところ、3％のパフォーマンス向上が見られた。"
  },
  {
    "start": 2552758,
    "end": 2554510,
    "text": "そして、その答えを使って。"
  },
  {
    "start": 2554580,
    "end": 2558654,
    "text": "これはかなりクールだ。"
  },
  {
    "start": 2558692,
    "end": 2566414,
    "text": "そのため、実際に入力された質問ではなく、仮定の質問を検索に使用するだけで、当初よりも大幅にパフォーマンスを向上させることができた。"
  },
  {
    "start": 2566452,
    "end": 2572734,
    "text": "となると、私たちがしたことは例数を増やしただけで、このアプローチでは4点差まで詰め寄ることができた。"
  },
  {
    "start": 2572862,
    "end": 2583074,
    "text": "プロンプト・エンジニアリングから始まり、Regに移行し、これらの非常に基本的なアプローチからどれだけのパフォーマンスを引き出すことができるかを示している。"
  },
  {
    "start": 2583202,
    "end": 2588834,
    "text": "その時点で、微調整に切り替えて、それ以上できるかどうかを確認することにした。"
  },
  {
    "start": 2588882,
    "end": 2590940,
    "text": "そこでジョンにバトンタッチする。"
  },
  {
    "start": 2592830,
    "end": 2593146,
    "text": "そうだね。"
  },
  {
    "start": 2593168,
    "end": 2598694,
    "text": "微調整のために、私たちはAI規模での微調整のために、私たちの優先的なパートナーに引き渡した。"
  },
  {
    "start": 2598742,
    "end": 2601930,
    "text": "そこで彼らは、私たちが推奨しているように、まずベースラインを確立することから始めた。"
  },
  {
    "start": 2602000,
    "end": 2604926,
    "text": "そうですね、前のスライドで見た69％と同じベースラインですね。"
  },
  {
    "start": 2605028,
    "end": 2607722,
    "text": "これは単純なプロンプト・エンジニアリングのテクニックによるものだ。"
  },
  {
    "start": 2607866,
    "end": 2615262,
    "text": "そして、単純なプロンプト・エンジニアリング・テクニックでGBT4を微調整した。"
  },
  {
    "start": 2615316,
    "end": 2615486,
    "text": "そうだろう？"
  },
  {
    "start": 2615508,
    "end": 2620510,
    "text": "つまり、非常にシンプルで、単純で、微調整されたモデルで、少し迅速なエンジニアリングを施しただけで、82％近くまで上昇したのだ。"
  },
  {
    "start": 2620580,
    "end": 2620814,
    "text": "そうだろう？"
  },
  {
    "start": 2620852,
    "end": 2623454,
    "text": "我々は今、最先端技術のすぐ近くにいる。"
  },
  {
    "start": 2623572,
    "end": 2638262,
    "text": "そして、このモデルを使ってラグを使い、高度なラグ技術ではなく、質問に基づいてコンテキストウィンドウにいくつかの例を動的に挿入したところ、83.5％の結果を得た。"
  },
  {
    "start": 2638316,
    "end": 2644162,
    "text": "ここで強調したいのは、開発セットのスパイダーリーダーボードを見ると、使用されているテクニックが非常に複雑だということだ。"
  },
  {
    "start": 2644226,
    "end": 2644406,
    "text": "そうだね。"
  },
  {
    "start": 2644428,
    "end": 2651374,
    "text": "データの前処理や後処理が多く、実際にモデルを評価するためのスクリプトにエッジケースをハードコーディングすることも多い。"
  },
  {
    "start": 2651492,
    "end": 2652846,
    "text": "ここでは、実際にそれを使う必要はなかった。"
  },
  {
    "start": 2652868,
    "end": 2653006,
    "text": "そうだね。"
  },
  {
    "start": 2653028,
    "end": 2660682,
    "text": "単純なファインチューニング、単純なプロンプトエンジニアリング、ベストプラクティスに従うだけで、このよく知られたベンチマークでは、本当に最先端のレベルに達することができた。"
  },
  {
    "start": 2660746,
    "end": 2664750,
    "text": "だから、ファインチューニングとラグを組み合わせたときのパワーを示しているようなものだ。"
  },
  {
    "start": 2665490,
    "end": 2672098,
    "text": "つまり、要約すると、問題に取り組んでいるときにLLMのパフォーマンスを向上させたいと思ったら、まずは迅速なエンジニアリング・テクニックから始めるということだね？"
  },
  {
    "start": 2672104,
    "end": 2673054,
    "text": "これらは非常に低投資だ。"
  },
  {
    "start": 2673102,
    "end": 2679086,
    "text": "LLMを使えば、素早く反復することができるし、解決しようとしている問題にアプローチするための有効な手法としてLLMを検証することもできる。"
  },
  {
    "start": 2679198,
    "end": 2684598,
    "text": "パフォーマンスが頭打ちになるまでプロンプトを繰り返し、その後、エラーの種類を分析する必要がある。"
  },
  {
    "start": 2684684,
    "end": 2690326,
    "text": "モデルに新しい知識やより多くのコンテクストを導入する必要がある場合は、ボロ道を行けばいい。"
  },
  {
    "start": 2690508,
    "end": 2701370,
    "text": "モデルが指示に従わない場合、厳密または斬新な出力構造に従う必要がある場合、あるいは一般的に、より効率的な方法でモデルと対話する必要がある場合は、微調整を試す時です。"
  },
  {
    "start": 2701450,
    "end": 2705246,
    "text": "このプロセスが直線的でないように、私たちが強調したいのはそのことだ。"
  },
  {
    "start": 2705268,
    "end": 2713870,
    "text": "あなたが本当に満足できるポイントに到達するまでには、49回の反復が必要かもしれないし、あなたの旅のように、これらのテクニックの間を行ったり来たりすることになるだろう。"
  },
  {
    "start": 2714290,
    "end": 2717470,
    "text": "とはいえ、このトークを楽しんでいただけたなら幸いである。"
  },
  {
    "start": 2717620,
    "end": 2719966,
    "text": "コリンと私は今日一日ここにいる。"
  },
  {
    "start": 2719988,
    "end": 2730850,
    "text": "質問があれば、どうぞ。"
  }
]