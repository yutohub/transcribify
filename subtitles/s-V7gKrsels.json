[
  {
    "start": 330,
    "end": 3514,
    "text": "さて、何回かに分けて説明しよう。"
  },
  {
    "start": 3642,
    "end": 10234,
    "text": "最初のパスでは、活性化関数の「何が」「どこで」「どのように」のハイレベルな概要を説明する。"
  },
  {
    "start": 10362,
    "end": 11502,
    "text": "2回目のパスは？"
  },
  {
    "start": 11556,
    "end": 15470,
    "text": "そのギャップを埋め、いくつかの例を紹介しよう。"
  },
  {
    "start": 16130,
    "end": 19760,
    "text": "エキサイティングな展開が待っている。"
  },
  {
    "start": 21810,
    "end": 22990,
    "text": "ケース1。"
  },
  {
    "start": 23140,
    "end": 27810,
    "text": "このニューラルネットワークは隠れ層が1つで、派手さはない。"
  },
  {
    "start": 27970,
    "end": 33410,
    "text": "入力を取り出し、重みを掛け合わせ、バイアス項を加え、次の層に渡す。"
  },
  {
    "start": 33570,
    "end": 38540,
    "text": "このニューラルネットワークは、線で分離可能なデータを分類する方法を学習することができる。"
  },
  {
    "start": 41070,
    "end": 41820,
    "text": "いいね。"
  },
  {
    "start": 42190,
    "end": 45530,
    "text": "データがもう少し複雑だったら？"
  },
  {
    "start": 48030,
    "end": 49260,
    "text": "私はこれを手に入れた。"
  },
  {
    "start": 49890,
    "end": 51038,
    "text": "私はこれを手に入れた。"
  },
  {
    "start": 51204,
    "end": 51920,
    "text": "そうだね。"
  },
  {
    "start": 52610,
    "end": 53840,
    "text": "待って、違う。"
  },
  {
    "start": 54850,
    "end": 56880,
    "text": "なぜ入らないのか？"
  },
  {
    "start": 58530,
    "end": 60382,
    "text": "もうそんなに簡単ではないだろう？"
  },
  {
    "start": 60516,
    "end": 63890,
    "text": "私たちのネットワークは単純すぎて、データのパターンを捉えることができない。"
  },
  {
    "start": 64040,
    "end": 71060,
    "text": "データをただ次のレイヤーに通すのではなく、非線形関数に通してみよう。"
  },
  {
    "start": 74430,
    "end": 75500,
    "text": "そうそう。"
  },
  {
    "start": 78530,
    "end": 82970,
    "text": "ネットワークはこのデータを2次曲線で分類することができた。"
  },
  {
    "start": 83130,
    "end": 88110,
    "text": "このデータを通す関数を活性化関数と呼ぶ。"
  },
  {
    "start": 88260,
    "end": 103334,
    "text": "この関数は、シグモイド、アークテン、リール、リーキーリール、パラメトリックリール、スイフト、エル、マックスアウトなど、数え上げればきりがない。"
  },
  {
    "start": 103532,
    "end": 105800,
    "text": "これらの機能のどれでも、ここでは機能しただろう。"
  },
  {
    "start": 106890,
    "end": 110710,
    "text": "もしデータがもっと複雑だったら？"
  },
  {
    "start": 111210,
    "end": 114358,
    "text": "データが複雑になれば、レイヤーを増やすことになる。"
  },
  {
    "start": 114454,
    "end": 119180,
    "text": "シグモイド関数を使うとしよう。"
  },
  {
    "start": 120350,
    "end": 121100,
    "text": "いいかい？"
  },
  {
    "start": 122030,
    "end": 123130,
    "text": "もっといい。"
  },
  {
    "start": 124430,
    "end": 125740,
    "text": "少し良くなった。"
  },
  {
    "start": 127650,
    "end": 130320,
    "text": "待って、どうして？"
  },
  {
    "start": 134770,
    "end": 138880,
    "text": "ネットワークはより多くの例を見ているにもかかわらず、何も学んでいない。"
  },
  {
    "start": 139190,
    "end": 141860,
    "text": "これが消失勾配問題である。"
  },
  {
    "start": 142310,
    "end": 144946,
    "text": "シグモイド関数のせいだ。"
  },
  {
    "start": 145128,
    "end": 146580,
    "text": "情報を搾り取る。"
  },
  {
    "start": 147350,
    "end": 153890,
    "text": "バックプロパゲーションのステップでは、勾配はどんどん小さくなり、最終的には消滅する。"
  },
  {
    "start": 154050,
    "end": 156280,
    "text": "グラデーションがないということは、学習がないということだ。"
  },
  {
    "start": 156650,
    "end": 163030,
    "text": "これを解決するには、Reluのように情報を圧迫しない活性化関数を使うことだ。"
  },
  {
    "start": 163470,
    "end": 166380,
    "text": "すべてのニューロンでreluを使い、何が起こるか見てみよう。"
  },
  {
    "start": 169630,
    "end": 170380,
    "text": "オーケー。"
  },
  {
    "start": 172430,
    "end": 173450,
    "text": "もっといい。"
  },
  {
    "start": 174670,
    "end": 175830,
    "text": "ああ、そうだね。"
  },
  {
    "start": 176000,
    "end": 177310,
    "text": "街に出る。"
  },
  {
    "start": 179090,
    "end": 180190,
    "text": "嗚呼。"
  },
  {
    "start": 181410,
    "end": 187486,
    "text": "Reluは実際にうまく機能している。"
  },
  {
    "start": 187668,
    "end": 192980,
    "text": "ランダムな場合、あるいはいくつかのランダムな場合に、このような結果になるかもしれない。"
  },
  {
    "start": 194550,
    "end": 197540,
    "text": "そう、そうだ。"
  },
  {
    "start": 198950,
    "end": 199700,
    "text": "オーケー。"
  },
  {
    "start": 201370,
    "end": 203560,
    "text": "なぜ私はまだこの問題を抱えているのか？"
  },
  {
    "start": 205930,
    "end": 211190,
    "text": "どうやら、以前と同じような \"学べない \"問題にぶつかっているようだ。"
  },
  {
    "start": 211340,
    "end": 213222,
    "text": "これが瀕死のレルーの問題である。"
  },
  {
    "start": 213356,
    "end": 221414,
    "text": "シグモイドが入力を圧迫するという点では、消失勾配問題と似ている。"
  },
  {
    "start": 221542,
    "end": 226620,
    "text": "この問題は、リルーがゼロより小さい入力を完全にブロックしていることに起因している。"
  },
  {
    "start": 227090,
    "end": 230378,
    "text": "解決策としては、アクティベーションを導入することだ。"
  },
  {
    "start": 230474,
    "end": 240050,
    "text": "ネガティブなケースでも、エルーとリキ・レルーがうまく対処してくれたので、その日は救われた。"
  },
  {
    "start": 243560,
    "end": 246912,
    "text": "出力ニューロンの活性化については？"
  },
  {
    "start": 247056,
    "end": 249280,
    "text": "まあ、それは解決する問題によるね。"
  },
  {
    "start": 249440,
    "end": 252336,
    "text": "分類では、ソフトマックス出力を使う。"
  },
  {
    "start": 252448,
    "end": 261530,
    "text": "この層のニューロン数は分類のクラス数であり、値は特定のクラスに属する確率を表す。"
  },
  {
    "start": 262700,
    "end": 267204,
    "text": "さて、回帰問題の場合、実数の出力が必要である。"
  },
  {
    "start": 267332,
    "end": 270044,
    "text": "私たちは活性化関数をまったく使わない傾向がある。"
  },
  {
    "start": 270162,
    "end": 275496,
    "text": "ネットワークが1つの実数を出力する場合、出力ニューロンは1つしか使わない。"
  },
  {
    "start": 275608,
    "end": 280316,
    "text": "もし2つの実数を吐き出すなら、活性化されていない2つの出力ニューロンを使う。"
  },
  {
    "start": 280508,
    "end": 282930,
    "text": "これがどこに向かっているのか、もうおわかりだと思う。"
  },
  {
    "start": 284660,
    "end": 288860,
    "text": "さて、これが活性化関数の最初のパスである。"
  },
  {
    "start": 289020,
    "end": 294976,
    "text": "我々は4つのケースを検討し、出力層のソフトマックス活性化を見てみた。"
  },
  {
    "start": 295088,
    "end": 304484,
    "text": "さて、これらのケースをもう一度確認し、ケース1、つまり1つのヒン層と活性化なしのシンプルなニューラルネットワークに戻って、さらに詳細を追加してみよう。"
  },
  {
    "start": 304612,
    "end": 307640,
    "text": "出力はソフトマックス。"
  },
  {
    "start": 308060,
    "end": 310600,
    "text": "これがラインセパレーターにつながる。"
  },
  {
    "start": 311100,
    "end": 312232,
    "text": "どうしてそうなるのか？"
  },
  {
    "start": 312286,
    "end": 322860,
    "text": "ここで、wとbは重みとバイアスであり、hは隠れ層の出力である。"
  },
  {
    "start": 323600,
    "end": 328028,
    "text": "hはwの1、xはbの1であることがわかる。"
  },
  {
    "start": 328194,
    "end": 331996,
    "text": "したがって、これを出力方程式に代入することができる。"
  },
  {
    "start": 332188,
    "end": 340480,
    "text": "そして括弧を展開すると、ほらほら、出力は入力の一次方程式になる。"
  },
  {
    "start": 340920,
    "end": 344580,
    "text": "この出力はソフトマックスの活性化に渡される。"
  },
  {
    "start": 345160,
    "end": 358730,
    "text": "ニューロンが2つあるので、2つのニューロンを1と2と呼ぶとすると、最初のニューロンの出力活性化は、1へのeを1へのeと2へのeの和で割ったものと書くことができる。"
  },
  {
    "start": 359340,
    "end": 363770,
    "text": "分子と分母の両方をeの1乗で割ることができる。"
  },
  {
    "start": 365420,
    "end": 368380,
    "text": "そして、出力方程式の値を代入することができる。"
  },
  {
    "start": 369600,
    "end": 384690,
    "text": "そして項を展開してみると、u 1からu 2を引いたものはuゼロと書くことができ、これも定数であることがわかる。"
  },
  {
    "start": 385620,
    "end": 390210,
    "text": "結局、シグモイド関数に帰結する。"
  },
  {
    "start": 390680,
    "end": 394390,
    "text": "これらの値を使って境界線をプロットすると、それは直線になる。"
  },
  {
    "start": 396280,
    "end": 399296,
    "text": "基本的にはロジスティック回帰に集約される。"
  },
  {
    "start": 399488,
    "end": 401908,
    "text": "このアニメーションはそれをよく捉えている。"
  },
  {
    "start": 402074,
    "end": 409050,
    "text": "ロジスティック回帰に関するもっとサイケデリックなビジュアルについては、ビデオもあるのでそちらをご覧いただきたい。"
  },
  {
    "start": 411340,
    "end": 415620,
    "text": "さて、ケース2は、線で区切れないデータがある場合だ。"
  },
  {
    "start": 415790,
    "end": 419260,
    "text": "以前やったのは、シグモイド活性化を加えることだった。"
  },
  {
    "start": 420720,
    "end": 425740,
    "text": "出力はより複雑になり、より適切な決定境界が得られるようになった。"
  },
  {
    "start": 426640,
    "end": 430064,
    "text": "クールなケース3、さらに複雑なデータ"
  },
  {
    "start": 430182,
    "end": 432720,
    "text": "そのためにレイヤーの数を増やす。"
  },
  {
    "start": 433060,
    "end": 438704,
    "text": "シグモイドのようにデータを圧迫する関数では、勾配の消失という問題が生じる。"
  },
  {
    "start": 438902,
    "end": 440240,
    "text": "なぜこのようなことが起こるのか？"
  },
  {
    "start": 440390,
    "end": 442052,
    "text": "シグモイド関数を見てみよう。"
  },
  {
    "start": 442186,
    "end": 447572,
    "text": "プラスまたはマイナスの値が大きい点では、勾配は本当にゼロに近づく。"
  },
  {
    "start": 447706,
    "end": 448676,
    "text": "これは良くない。"
  },
  {
    "start": 448778,
    "end": 462420,
    "text": "ニューラルネットワークでは、これらの勾配はバックプロパゲートされるので、このニューロンを見て、このニューロンは後の層にあり、このニューロンの勾配がゼロに近いとすると、その前の層のニューロンはさらに低い勾配を持つことになる。"
  },
  {
    "start": 462580,
    "end": 467748,
    "text": "この低い値は、最終的にゼロになるまで後方に伝播していく。"
  },
  {
    "start": 467934,
    "end": 473230,
    "text": "勾配がゼロになると、そのニューロンは役に立たなくなり、学習もできなくなる。"
  },
  {
    "start": 473840,
    "end": 478430,
    "text": "これが消失勾配の問題であり、なぜこれが問題なのかはおわかりいただけると思う。"
  },
  {
    "start": 478960,
    "end": 482048,
    "text": "これを解決するには、根本的な原因を理解する必要がある。"
  },
  {
    "start": 482214,
    "end": 485600,
    "text": "それはシグモイド関数の絞り込みの性質である。"
  },
  {
    "start": 485750,
    "end": 492468,
    "text": "つまり、シグモイドは実数を受け取り、それをある小さな固定範囲0と1の間に押し込めるということだ。"
  },
  {
    "start": 492634,
    "end": 503590,
    "text": "関数のこのような絞り込みの性質は、小さな勾配を生み出す。arctan関数も同じで、実数を負の1と正の1の間で絞り込む。"
  },
  {
    "start": 504360,
    "end": 511290,
    "text": "これを解決するには、reluのように値を絞らない関数を使うことだ。"
  },
  {
    "start": 511980,
    "end": 518270,
    "text": "ケース4では、瀕死のレルーの問題にぶつかりそうな状況が見られた。"
  },
  {
    "start": 519360,
    "end": 520908,
    "text": "なぜそうなるのか？"
  },
  {
    "start": 521074,
    "end": 523052,
    "text": "では、reluの機能を見てみよう。"
  },
  {
    "start": 523106,
    "end": 523564,
    "text": "今すぐだ。"
  },
  {
    "start": 523682,
    "end": 527320,
    "text": "ポジティブな入力に対しては、フィルターなしで情報を通過させる。"
  },
  {
    "start": 527400,
    "end": 529932,
    "text": "マイナス入力の場合は完全にブロックされる。"
  },
  {
    "start": 529996,
    "end": 531490,
    "text": "それが問題なのだ。"
  },
  {
    "start": 531940,
    "end": 536524,
    "text": "トレーニング中、バイアスが非常にネガティブになるときがあるかもしれない。"
  },
  {
    "start": 536652,
    "end": 540540,
    "text": "wxプラスbはほとんどのニューロンにとってマイナスである。"
  },
  {
    "start": 540700,
    "end": 547860,
    "text": "ほとんどのニューロンは前進ステップ中にオフになり、したがってほとんどのニューロンは後退ステップ中にもオフになる。"
  },
  {
    "start": 548010,
    "end": 556036,
    "text": "さらに悪いのは、神経入力がバイアスに打ち勝つほどポジティブでなければ、これらのニューロンは死んだままになってしまうことだ。"
  },
  {
    "start": 556148,
    "end": 558440,
    "text": "死んだニューロンは学習しない。"
  },
  {
    "start": 558590,
    "end": 565640,
    "text": "この問題を解決するために、リーキー・リリュー（leaky relu）またはエル・アクティブ（elu activation）を使って、負の入力で学習が起こるようにする。"
  },
  {
    "start": 568620,
    "end": 571108,
    "text": "またしても、この日は救われた。"
  },
  {
    "start": 571204,
    "end": 574500,
    "text": "このビデオで、活性化関数について理解を深めていただけたなら幸いです。"
  },
  {
    "start": 574580,
    "end": 578512,
    "text": "私の他のコンテンツもチェックしてください。"
  },
  {
    "start": 578646,
    "end": 579036,
    "text": "さようなら。"
  }
]