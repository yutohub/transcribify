[
  {
    "start": 280,
    "end": 4782,
    "text": "これは以前のRust対Goベンチマークの改良版である。"
  },
  {
    "start": 4886,
    "end": 11766,
    "text": "錆びたコードを改善するためのヒントをたくさんもらった。"
  },
  {
    "start": 11838,
    "end": 21022,
    "text": "また、ファイバーを使うのは公平な比較にならないとの苦情も多かったので、このベンチマークでは標準的なgoライブラリに変更した。"
  },
  {
    "start": 21086,
    "end": 31430,
    "text": "最初のベンチマークを見ていないなら、フレームワークそのもの、特にRust hecticsとGolang標準ライブラリの比較から始める。"
  },
  {
    "start": 31510,
    "end": 37266,
    "text": "ハードコードされたデバイスをJSON形式でクライアントに返すエンドポイントがあります。"
  },
  {
    "start": 37378,
    "end": 44882,
    "text": "私は、CPU使用量、メモリー使用量、クライアント側からの各リクエストを処理する待ち時間を測定している。"
  },
  {
    "start": 44986,
    "end": 63538,
    "text": "さらに、それぞれのアプリケーションが処理できるリクエスト数、可用性、CPUスロットリングを分析する。"
  },
  {
    "start": 63634,
    "end": 66610,
    "text": "さて、2つ目のテストはもう少し面白い。"
  },
  {
    "start": 66730,
    "end": 81338,
    "text": "私は、アプリケーションがローカルファイルシステムからファイルを読み込み、そのファイルを3つのバケットにアップロードし、そのファイルに関するメタデータをリレーショナルデータベースに保存するという、一般的なユースケースを実装した。"
  },
  {
    "start": 81474,
    "end": 85830,
    "text": "私の場合はpostgresなので、それらのグラフも取り上げます。"
  },
  {
    "start": 87960,
    "end": 93736,
    "text": "まず第一に、ファイバーは高速HTTPの上に構築されている。"
  },
  {
    "start": 93888,
    "end": 99680,
    "text": "しかし、FastHtPは特定の高性能エッジケース向けに設計されている。"
  },
  {
    "start": 99800,
    "end": 111584,
    "text": "1秒間に数千の小規模から中規模のリクエストを処理する必要があり、一貫してミリ秒以下の応答時間を必要とする場合は、このライブラリの使用を検討するとよいだろう。"
  },
  {
    "start": 111712,
    "end": 118780,
    "text": "とはいえ、彼ら自身はほとんどの場合、デフォルトで標準的なgoライブラリを使うことを推奨している。"
  },
  {
    "start": 118900,
    "end": 124636,
    "text": "高速HTTPよりも標準的なNATパッケージを使うことには、いくつかの利点がある。"
  },
  {
    "start": 124748,
    "end": 133772,
    "text": "例えば、HTTP 2をサポートし、より安定しており、より多くのHTTPコーナーケースを処理できるなどの利点がある。"
  },
  {
    "start": 133876,
    "end": 140332,
    "text": "もしあなたが個人的なプロジェクトに取り組んでいるのであれば、ファイバーと高速HTTPを使ってもまったく問題ない。"
  },
  {
    "start": 140476,
    "end": 149600,
    "text": "しかし、企業顧客向けのソフトウェアを開発する場合は、このフレームワークを選択する前に、すべてのリスクを慎重に検討するようにしてください。"
  },
  {
    "start": 152000,
    "end": 163736,
    "text": "多くの方々のおかげで、サビのコードを改善するためのヒントをたくさん得ることができました。まず、カーゴファイルにセクションを追加することができます。"
  },
  {
    "start": 163808,
    "end": 169664,
    "text": "まず、LTOをtrueに設定する。これはlink time optimizationsの略で、リンク時間の最適化を意味する。"
  },
  {
    "start": 169752,
    "end": 179602,
    "text": "リンク時間が長くなる代償として、プログラム全体を解析してより最適化されたコードを生成することができる。"
  },
  {
    "start": 179706,
    "end": 182746,
    "text": "次に、codegenの単位を1に設定する。"
  },
  {
    "start": 182858,
    "end": 189950,
    "text": "値が大きいほど、コードのコンパイルにかかる時間は短くなるが、パフォーマンスが低下する可能性がある。"
  },
  {
    "start": 190370,
    "end": 194362,
    "text": "有効なオプションは0以上の整数である。"
  },
  {
    "start": 194466,
    "end": 199618,
    "text": "最後に、panicをabortに設定すると、プロセスが即座に終了する。"
  },
  {
    "start": 199714,
    "end": 208498,
    "text": "パニックが発生した場合は、これらのページですべての最適化オプションについて読むことができますリンクは、ビデオの説明で提供されます。"
  },
  {
    "start": 208594,
    "end": 217186,
    "text": "では、コードの改良点を見てみよう。最初のテストでは、私のgoコードは文字列型のUuidを使っていた。"
  },
  {
    "start": 217298,
    "end": 225066,
    "text": "しかし、オリジナルのロスコでは、JSONへの変換が遅くなる可能性のあるuuid型を使っていた。"
  },
  {
    "start": 225178,
    "end": 231298,
    "text": "このことに気づいたデビッドに感謝する。"
  },
  {
    "start": 231394,
    "end": 234706,
    "text": "また、diwashishから別のPRを受け取った。"
  },
  {
    "start": 234818,
    "end": 240118,
    "text": "私のオリジナルのロス・コードを全面的に書き直したため、あなたの名前を読み間違えていたらお詫びします。"
  },
  {
    "start": 240174,
    "end": 245014,
    "text": "これは大きな違いであり、コードをよりすっきりと整理することができた。"
  },
  {
    "start": 245102,
    "end": 255254,
    "text": "コードには多くのリファクタリングとアップデートがあり、オリジナル版と改良版の両方をprsと一緒に説明文にリンクしておく。"
  },
  {
    "start": 255342,
    "end": 268410,
    "text": "例えば、デバイス構造体のuuidに文字列を使用する代わりに、書き込みタイプでより最適化されたコピーを使用できるようになった。"
  },
  {
    "start": 268490,
    "end": 272250,
    "text": "何かお気づきの点がありましたら、お知らせください。"
  },
  {
    "start": 272330,
    "end": 278970,
    "text": "それか、もっといいのは、説明付きのprを作成することだ。"
  },
  {
    "start": 279050,
    "end": 283150,
    "text": "コードを改善するためのヒントをくださった皆さんに感謝します。"
  },
  {
    "start": 285370,
    "end": 288546,
    "text": "さて、最初のテストを実行しよう。"
  },
  {
    "start": 288698,
    "end": 293338,
    "text": "まず、両方のアプリケーションをアイドル状態で15分間動かしてみる。"
  },
  {
    "start": 293474,
    "end": 300302,
    "text": "Golangは最初からずっと多くのメモリを使い、CPUも少し多いことがわかります。"
  },
  {
    "start": 300486,
    "end": 304366,
    "text": "よし、テストジョブもKubernetesにデプロイしてみよう。"
  },
  {
    "start": 304438,
    "end": 308214,
    "text": "これらのテストはeksクラスタを使ってAWSで実行している。"
  },
  {
    "start": 308342,
    "end": 316230,
    "text": "お気づきかもしれないが、私はラージECの2つのインスタンスタイプ、例えば4つのエクストララージや8つのエクストララージを使っている。"
  },
  {
    "start": 316350,
    "end": 323650,
    "text": "これは、アプリケーションがそれらのマシンのすべてのCPUやメモリにアクセスできるという意味ではない。"
  },
  {
    "start": 323730,
    "end": 327578,
    "text": "Kubernetesでは、リクエストとリミットの両方を定義する。"
  },
  {
    "start": 327674,
    "end": 342994,
    "text": "例えば、CPUやメモリの使用率をパーセントで測定する場合、仮想マシン全体ではなく、定義された制限値を使用し、その制限値に対してアプリケーションがどれだけリソースを使用しているかを計算します。"
  },
  {
    "start": 343082,
    "end": 350726,
    "text": "さて、テスト・サビを開始すると、アプリケーションのレイテンシが今回のゴーよりも低くなっているのがわかるだろう。"
  },
  {
    "start": 350818,
    "end": 358654,
    "text": "これは部分的にはコードの改良と、ファイバーの代わりに標準ライブラリを使っていることによるものだ。"
  },
  {
    "start": 358742,
    "end": 368142,
    "text": "前回のベンチマークでは、ファイバーと標準ライブラリーとジンを比較したので、そのベンチマークもチェックするといいだろう。"
  },
  {
    "start": 368246,
    "end": 380774,
    "text": "また、各アプリケーションが1秒間に受け取るリクエスト数を計算するために、ヒストグラムカウントを使用し、Prometheusのレート関数を適用した1秒あたりのリクエスト数のグラフもあります。"
  },
  {
    "start": 380902,
    "end": 389198,
    "text": "レイテンシーと1秒あたりのリクエスト数は、アプリケーション自体ではなく、クライアント側からの測定であることに留意してください。"
  },
  {
    "start": 389294,
    "end": 397062,
    "text": "これにより、実際のエンドユーザーがあなたのAPIとインタラクトしたときにどのような体験をするかを、より正確に把握することができる。"
  },
  {
    "start": 397166,
    "end": 414394,
    "text": "アプリケーションをKubernetesにデプロイし、Nginx ingressのようなイングレスコントローラーを使用する場合、すべてのアプリケーションを個別に計測する代わりに、各アプリケーションのNginXイングレスコントローラーからこれらのメトリクスを直接収集することができます。"
  },
  {
    "start": 414562,
    "end": 417242,
    "text": "その方法を説明したビデオがある。"
  },
  {
    "start": 417346,
    "end": 419394,
    "text": "では、可用性について話そう。"
  },
  {
    "start": 419562,
    "end": 428426,
    "text": "これは、成功したリクエスト数を総リクエスト数で割って100を掛けたものとして計算される。"
  },
  {
    "start": 428578,
    "end": 434546,
    "text": "また、稼働率が時間とともにどのように変化するかを測定するために、レート関数を適用する。"
  },
  {
    "start": 434658,
    "end": 444048,
    "text": "たとえば、クライアントがステータスコード400や500のリクエストを受け取るようになると、可用性は低下します。"
  },
  {
    "start": 444144,
    "end": 454664,
    "text": "平均リクエスト時間が1ミリ秒以下であるこのベンチマークでは、クライアントのタイムアウトを100ミリ秒に定義した。"
  },
  {
    "start": 454752,
    "end": 461752,
    "text": "これはほとんどのアプリケーションでは一般的ではないが、今回はハードコードされた値をクライアントに返している。"
  },
  {
    "start": 461856,
    "end": 468990,
    "text": "もし100ミリ秒以上かかったら、アプリケーションが劣化し始めたことを意味する。"
  },
  {
    "start": 469120,
    "end": 479506,
    "text": "また、リクエストに100ミリ秒以上かかる場合、クライアントは408ステータスコードを受け取り、これも可用性に影響する。"
  },
  {
    "start": 479658,
    "end": 481482,
    "text": "最後に、スロットリングだ。"
  },
  {
    "start": 481586,
    "end": 493114,
    "text": "これは、ポッド内のコンテナのCPU使用率が100に達すると、Kubernetesがスロットルを開始し、アプリケーションに影響を与え、応答時間が長くなる場合に発生する。"
  },
  {
    "start": 493202,
    "end": 502478,
    "text": "NginXイングレス・コントローラーのようなCPU集約型のアプリケーションを実行している場合、CPUスロットルとレイテンシーを注意深く監視する必要がある。"
  },
  {
    "start": 502574,
    "end": 509582,
    "text": "テスト全体では2時間半ほどかかったが、このデモでは数分に短縮した。"
  },
  {
    "start": 509726,
    "end": 517870,
    "text": "次に、各アプリケーションへのリクエスト数を徐々に増やしていき、それぞれの動作の違いを見てみましょう。"
  },
  {
    "start": 517950,
    "end": 525030,
    "text": "GolangアプリケーションはRubstアプリケーションよりはるかに多くのCPUを使い、レイテンシも高い。"
  },
  {
    "start": 525150,
    "end": 533386,
    "text": "毎秒約14,000リクエストで、Golangアプリケーションは100cpuのcpu使用量に達するので、劣化し始めます。"
  },
  {
    "start": 533458,
    "end": 539090,
    "text": "Kubernetesはスロットリングを開始し、アベイラビリティグラフでレイテンシが増加する。"
  },
  {
    "start": 539170,
    "end": 543322,
    "text": "また、いくつかのリクエストがタイムアウトし始めることもわかる。"
  },
  {
    "start": 543426,
    "end": 551434,
    "text": "この構成とポッドの制限では、Golangアプリケーションは1秒あたり14,000リクエストまでしか処理できません。"
  },
  {
    "start": 551522,
    "end": 558162,
    "text": "では、サビアプリケーションの限界点を見つけるために、さらに数分間テストを実行してみよう。"
  },
  {
    "start": 558266,
    "end": 564112,
    "text": "毎秒25,000リクエストになると、錆びたアプリケーションも故障し始めた。"
  },
  {
    "start": 564216,
    "end": 570320,
    "text": "kubernetesでは、障害は必ずしもCPUやメモリ使用量が100に達することを意味しない。"
  },
  {
    "start": 570400,
    "end": 579440,
    "text": "各アプリケーションはヘルスチェックを行わなければならないが、この場合、多くのリクエストがタイムアウトし始めるため、さびついたアプリケーションは失敗する。"
  },
  {
    "start": 579520,
    "end": 590974,
    "text": "さて、クライアントのタイムアウトを100ミリ秒から例えば100秒に増やしても、サビがより多くのリクエストを処理することにはならない。"
  },
  {
    "start": 591062,
    "end": 598574,
    "text": "いや、それは準備完了を意味するだけで、propsはタイムアウトを開始し、kubernetesはサービスプールからポートを削除する。"
  },
  {
    "start": 598662,
    "end": 607110,
    "text": "これは、サービスの可用性を維持するために、遅いアプリケーションや不健康なアプリケーションを削除するKubernetesの優れたメカニズムだ。"
  },
  {
    "start": 607270,
    "end": 614502,
    "text": "別のビデオでは、Kubernetesの健全性チェックの様々なタイプ（liveness readinessなど）について説明している。"
  },
  {
    "start": 614606,
    "end": 629040,
    "text": "サビ管アプリケーションをオートスケーリングしたいのであれば、水平ポッドのオートスケーラーのCPU使用率に依存しないようにするか、少なくとも閾値を低く、40％か50％に設定する。"
  },
  {
    "start": 629160,
    "end": 635520,
    "text": "あるいは、レイテンシーやリクエスト/秒のようなカスタムメトリクスを使用してスケーリングすることもできる。"
  },
  {
    "start": 635680,
    "end": 641640,
    "text": "PrometheusのメトリクスをHPAと統合する方法を説明したビデオをご覧ください。"
  },
  {
    "start": 641720,
    "end": 645900,
    "text": "では、各グラフをテスト期間中ずっと開いてみよう。"
  },
  {
    "start": 655350,
    "end": 658050,
    "text": "まず、1秒あたりのリクエスト数グラフ。"
  },
  {
    "start": 672520,
    "end": 674360,
    "text": "次にレイテンシーだ。"
  },
  {
    "start": 674440,
    "end": 680552,
    "text": "タイムアウトにもかかわらず、なぜレイテンシが100ミリ秒を超えるのか不思議に思うかもしれない。"
  },
  {
    "start": 680656,
    "end": 696168,
    "text": "ヒストグラムを使ってリクエストを計測しているのですが、関数全体が101ミリ秒でもかかるので、200ミリ秒のような高いバケットがインクリメントされるのです。"
  },
  {
    "start": 696264,
    "end": 707480,
    "text": "ヒストグラムがどのように機能するかについては、リクエストの待ち時間を測定するためにサマリーとヒストグラムのどちらを使うかを含めて、前回のRust vs goベンチマークで説明した。"
  },
  {
    "start": 711060,
    "end": 713012,
    "text": "次はCPU使用率だ。"
  },
  {
    "start": 713116,
    "end": 722684,
    "text": "この場合、Golangアプリケーションは100 cpuの使用率に達するが、Rustアプリケーションは約60%で劣化し始める。"
  },
  {
    "start": 722812,
    "end": 728644,
    "text": "つまり、リクエストの処理が非常に遅くなり、クライアントのタイムアウトが発生する。"
  },
  {
    "start": 728732,
    "end": 736610,
    "text": "例えば、典型的なリクエストにかかる時間は1ミリ秒未満だが、CPU使用率は約60％である。"
  },
  {
    "start": 736730,
    "end": 745034,
    "text": "これらのリクエストは1秒、2秒、あるいは10秒かかることもあり、アプリケーションを圧倒し、利用できなくなることもある。"
  },
  {
    "start": 745162,
    "end": 752350,
    "text": "しかし、Kubernetesのレディネス・チェックとクライアントのタイムアウトのおかげで、一部のリクエストはまだ処理できる。"
  },
  {
    "start": 754730,
    "end": 756714,
    "text": "次にメモリ使用量である。"
  },
  {
    "start": 756802,
    "end": 759978,
    "text": "錆びたアプリケーションのメモリが急増していることに気づくだろう。"
  },
  {
    "start": 760074,
    "end": 771210,
    "text": "これは、アプリケーションがリクエストの処理に時間がかかるようになり、リクエストがどんどん溜まってメモリ使用量が増えるときに起こる。"
  },
  {
    "start": 773150,
    "end": 775206,
    "text": "そして、アベイラビリティ・グラフができる。"
  },
  {
    "start": 775318,
    "end": 784890,
    "text": "このケースで失敗したリクエストのほとんどは、タイムアウトを意味する408ステータスコードを返し、可用性を低下させた。"
  },
  {
    "start": 787310,
    "end": 789726,
    "text": "最後に、CPUスロットリングだ。"
  },
  {
    "start": 789838,
    "end": 794028,
    "text": "この2つのアプリケーションの動作が異なることにお気づきだろう。"
  },
  {
    "start": 794134,
    "end": 805380,
    "text": "Golangアプリケーションは利用可能な全てのリソースを使用して全てのリクエストを処理しますが、Rustアプリケーションは約60%のCPU使用率で遅くなります。"
  },
  {
    "start": 807920,
    "end": 810912,
    "text": "では、2つ目のテストを実行してみよう。"
  },
  {
    "start": 811016,
    "end": 821760,
    "text": "このテストでは、3つのバケットにアップロードされたローカルファイルシステムからファイルを読み込み、そのファイルに関するメタデータをpostgresデータベースに書き込む。"
  },
  {
    "start": 821880,
    "end": 830632,
    "text": "これは最適化されたサビコードであることに留意してほしいが、それでも1秒間に数回しかリクエストを処理しないときに気づいた。"
  },
  {
    "start": 830776,
    "end": 839248,
    "text": "Golangアプリケーションは、s threeバケットにファイルをアップロードする時間が半分になり、CPU使用率も低くなった。"
  },
  {
    "start": 839344,
    "end": 845480,
    "text": "一方、データベースの待ち時間は、2つのアプリケーション間でほぼ同じままである。"
  },
  {
    "start": 845640,
    "end": 853804,
    "text": "これは、goリンク用のAWS SDKが、Rust用のものよりもはるかに最適化されているからだと思います。"
  },
  {
    "start": 853892,
    "end": 862988,
    "text": "AWSは最近、錆のラムダ・ランタイムを複製したので、錆のクライアント・ライブラリのメンテナンスにそれほど時間を割いていない。"
  },
  {
    "start": 863164,
    "end": 871228,
    "text": "ラムダ関数を作るのにRustを使うことはできるが、ジェネリック・ランタイムを使い、最終的なバイナリをそこにコピーする必要がある。"
  },
  {
    "start": 871324,
    "end": 879950,
    "text": "どちらのアプリケーションも毎秒30リクエストは問題なく処理できるが、それを超えると待ち時間が増え始める。"
  },
  {
    "start": 880140,
    "end": 888490,
    "text": "各アプリケーションが受け取るリクエストが多ければ多いほど、サビとゴリンクの性能差は小さくなる。"
  },
  {
    "start": 888570,
    "end": 894730,
    "text": "この構成では、両方のアプリケーションは1秒間に最大50リクエストを処理できる。"
  },
  {
    "start": 894890,
    "end": 907698,
    "text": "この結果を引き起こすボトルネックがあるかもしれないので、もし調査したいのであれば、遠慮なく見て、さらに最適化できるようであれば、prを作成し、新しいビデオをリリースしてください。"
  },
  {
    "start": 907874,
    "end": 913650,
    "text": "これらのテストの実行にはMenioを使用したが、ローカルにデプロイしてテストすることも簡単にできる。"
  },
  {
    "start": 913770,
    "end": 918550,
    "text": "では、各グラフをテスト期間中ずっと開いてみよう。"
  },
  {
    "start": 923570,
    "end": 992872,
    "text": "Requests per second client latency cpu usage memory usage availability database latency s three latency これは、s three バケットにファイルをアップロードするのにかかる時間を測定します。"
  },
  {
    "start": 992976,
    "end": 1004152,
    "text": "私は各アプリケーション内の関数呼び出しを測定し、各アプリケーションがpostgresデータベースと確立した接続数の詳細をソースコードで見つけることができます。"
  },
  {
    "start": 1004296,
    "end": 1012128,
    "text": "私はPrometheus postgres exporterを使ってデータベースのメトリクスを公開し、Prometheus自体でスクレイピングしています。"
  },
  {
    "start": 1012304,
    "end": 1017996,
    "text": "もし、グラフを作成し、その他の指標を視覚化すべきだとお考えでしたら、お知らせください。"
  },
  {
    "start": 1018068,
    "end": 1019780,
    "text": "テストは以上だ。"
  },
  {
    "start": 1019900,
    "end": 1022764,
    "text": "次に何をテストしてほしいか教えてほしい。"
  },
  {
    "start": 1022892,
    "end": 1027148,
    "text": "異なるプログラミング言語やオープンソースプロジェクトとか？"
  },
  {
    "start": 1027284,
    "end": 1032420,
    "text": "このプレイリストには他にもたくさんのベンチマークがある。"
  },
  {
    "start": 1032580,
    "end": 1035340,
    "text": "ご視聴ありがとうございました。また次のビデオでお会いしましょう。"
  }
]