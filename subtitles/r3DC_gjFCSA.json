[
  {
    "start": 600,
    "end": 2014,
    "text": "ようこそ、ようこそ、ようこそ、ようこそ。"
  },
  {
    "start": 2054,
    "end": 2954,
    "text": "どうぞお入りください。"
  },
  {
    "start": 3694,
    "end": 6030,
    "text": "まず最初に、私が飛び込む前に、私のイメージをどう思いますか？"
  },
  {
    "start": 6102,
    "end": 7714,
    "text": "気味が悪い？"
  },
  {
    "start": 8214,
    "end": 14166,
    "text": "指を3本立てているラマをお願いしたんだ。"
  },
  {
    "start": 14350,
    "end": 16646,
    "text": "あれは実は、6回目のプロンプトの後だったと思う。"
  },
  {
    "start": 16710,
    "end": 21622,
    "text": "ジェニー・アイは面白いし、不気味だし、時々ゾッとする。"
  },
  {
    "start": 21638,
    "end": 25514,
    "text": "とにかく、ロブが言ったように、僕はジョー・スパイザック、メタと一緒だ。"
  },
  {
    "start": 26774,
    "end": 31918,
    "text": "溶岩3について話をしに来たんだが、その前に、ここでジェナイについて話しているこの男は誰なんだ？"
  },
  {
    "start": 32086,
    "end": 35470,
    "text": "私はもう10年以上AIの世界にいると思う。"
  },
  {
    "start": 35622,
    "end": 37142,
    "text": "私はオープンソースに多くの時間を費やした。"
  },
  {
    "start": 37198,
    "end": 42074,
    "text": "私は神様のためにPytorchの開発に時間を費やしてきた。"
  },
  {
    "start": 42974,
    "end": 50874,
    "text": "それを土台にして、メタのチームを作り上げ、カフェ時代に戻ってずっと仕事をしてきた。"
  },
  {
    "start": 51534,
    "end": 52316,
    "text": "オニキス"
  },
  {
    "start": 52430,
    "end": 60744,
    "text": "私はグーグルやアマゾンにいたので、AI、特にオープンサイエンスやOpenAIにはかなり長い間関わってきました。"
  },
  {
    "start": 60784,
    "end": 62888,
    "text": "アドバイスやエンジェル投資もたくさんしている。"
  },
  {
    "start": 62936,
    "end": 65920,
    "text": "そこに掲載されている企業の中には、おそらく皆さんが知っている企業もあるでしょう。"
  },
  {
    "start": 65952,
    "end": 68480,
    "text": "私が投資するのは、創業者ととても仲の良い会社だけだ。"
  },
  {
    "start": 68552,
    "end": 71164,
    "text": "私が使っているカミソリはこれ1本だ。"
  },
  {
    "start": 72224,
    "end": 73968,
    "text": "今日は本当にエキサイティングな話があるんだ。"
  },
  {
    "start": 74016,
    "end": 78072,
    "text": "これは超新情報で、超ホットなオフショアだ。"
  },
  {
    "start": 78128,
    "end": 84414,
    "text": "このスライドは、今朝Uberでここに来たときにたくさん作ったんだ。"
  },
  {
    "start": 84954,
    "end": 88618,
    "text": "llama、この時点で誰もがllamaのことを耳にしたことがあると思う。"
  },
  {
    "start": 88666,
    "end": 90698,
    "text": "大丈夫だよ。"
  },
  {
    "start": 90866,
    "end": 91774,
    "text": "ありがとう。"
  },
  {
    "start": 93474,
    "end": 94786,
    "text": "間違いなく僕の赤ちゃんだ。"
  },
  {
    "start": 94850,
    "end": 97334,
    "text": "私たちにはこのような素晴らしいチームがいる。"
  },
  {
    "start": 97754,
    "end": 101618,
    "text": "私はその小さな一部であり、このチームの一員であることをとても誇りに思っている。"
  },
  {
    "start": 101666,
    "end": 105826,
    "text": "ラマに入る前に少し歴史を振り返っておこう。"
  },
  {
    "start": 105890,
    "end": 110290,
    "text": "私たちは実際にメタ、実際には2023年2月頃にan.orgを始めた。"
  },
  {
    "start": 110322,
    "end": 113818,
    "text": "5年、10年という感じだ。"
  },
  {
    "start": 113906,
    "end": 115114,
    "text": "まるで古代史のようだ。"
  },
  {
    "start": 115154,
    "end": 119122,
    "text": "我々は2023年2月にバックをスタートさせた。"
  },
  {
    "start": 119298,
    "end": 122434,
    "text": "基本的には、メタの垣根を越えて多くのチームを集めている。"
  },
  {
    "start": 122474,
    "end": 128402,
    "text": "私はAIのプラットフォームや、公正なフェイスブックのAI研究、基礎的なAI研究に時間を費やしてきた。"
  },
  {
    "start": 128458,
    "end": 147648,
    "text": "今、私たちはメタな存在であり、基本的に、システムmlからモデリング担当者、データ担当者、そして領域研究を行うただひたすら優秀な研究者まで、全社的にAIで最も賢く聡明な人たちを集めている。"
  },
  {
    "start": 147776,
    "end": 155824,
    "text": "もし、あなたが私たちの作品のいくつかをご存知であれば、例えばEmuのような、実は私たちの想像上のフラッシュです。"
  },
  {
    "start": 155904,
    "end": 157884,
    "text": "今朝、これを作った。"
  },
  {
    "start": 159544,
    "end": 161040,
    "text": "もちろんラマを飼わなければならない。"
  },
  {
    "start": 161112,
    "end": 165394,
    "text": "雨の中、小さな赤い納屋の前をスキップするかわいい白いラマ。"
  },
  {
    "start": 165734,
    "end": 168654,
    "text": "ちなみに、これはプロンプトで生成されたもので、完全に無料でできる。"
  },
  {
    "start": 168694,
    "end": 176926,
    "text": "メタAIに行き、imagineをクリックし、生成してアニメートをクリックすれば、すべてやってくれる。"
  },
  {
    "start": 176990,
    "end": 177914,
    "text": "かなりクールだよ。"
  },
  {
    "start": 178774,
    "end": 180166,
    "text": "ラマについて少し。"
  },
  {
    "start": 180270,
    "end": 184398,
    "text": "では、これまでの軌跡を振り返ってみよう。"
  },
  {
    "start": 184566,
    "end": 187554,
    "text": "まるで100万年前のことのようだ。"
  },
  {
    "start": 188534,
    "end": 191072,
    "text": "我々は7月にラマ2をリリースした。"
  },
  {
    "start": 191238,
    "end": 200404,
    "text": "これらは商業的に入手可能で、商業的に使用可能なモデルであり、基本的に100人ほどのパートナー、100人以上のパートナーがいた。"
  },
  {
    "start": 200564,
    "end": 206344,
    "text": "コード生成のために微調整されたコード固有のモデルである。"
  },
  {
    "start": 206764,
    "end": 210384,
    "text": "コードについて話したり、Pythonに使ったり。"
  },
  {
    "start": 210804,
    "end": 213204,
    "text": "養子縁組なんてとんでもない。"
  },
  {
    "start": 213364,
    "end": 220150,
    "text": "私たちは、ハグする顔の上に私たちのモデルの1億7000万ダウンロードを超えるような今まで見てきた。"
  },
  {
    "start": 220302,
    "end": 222278,
    "text": "つまり、5万近い派生モデルがある。"
  },
  {
    "start": 222326,
    "end": 226318,
    "text": "人々はさまざまな用途に合わせて微調整し、再アップロードし、リーダーボードを叩く。"
  },
  {
    "start": 226366,
    "end": 229126,
    "text": "GitHubには12,000以上のプロジェクトがあるんだ。"
  },
  {
    "start": 229230,
    "end": 233754,
    "text": "文字通り、ラマにちなんだ名前のスタートアップがある。"
  },
  {
    "start": 234254,
    "end": 236942,
    "text": "12月には『Purple Llama』というものをリリースした。"
  },
  {
    "start": 237038,
    "end": 238478,
    "text": "パープラマをご存じですか？"
  },
  {
    "start": 238646,
    "end": 239246,
    "text": "名前が少ない？"
  },
  {
    "start": 239270,
    "end": 242406,
    "text": "さて、私はその手のショーに失望したので、それに飛び込むつもりだ。"
  },
  {
    "start": 242510,
    "end": 245206,
    "text": "パープル・ラマの話はまた今度。"
  },
  {
    "start": 245390,
    "end": 249846,
    "text": "オープンな信頼と安全のための包括的なプロジェクトであるPurplamaをリリースした。"
  },
  {
    "start": 249990,
    "end": 253046,
    "text": "ジェネイの時代において、信頼と安全がいかに重要であるか、私たちは知っている。"
  },
  {
    "start": 253070,
    "end": 254674,
    "text": "それについてはもっとたくさん話すつもりだ。"
  },
  {
    "start": 255054,
    "end": 263222,
    "text": "私たちは基本的に、プロンプトの入力をどのようにフィルタリングするかというインプット・アウトプットのセーフガードや、モデルが実際に生成するものなどをリリースした。"
  },
  {
    "start": 263358,
    "end": 268582,
    "text": "私たちはまた、ハギング・フェイスに関する初のオープン・サイバーセキュリティ評価ベンチマークを導入した。"
  },
  {
    "start": 268638,
    "end": 271914,
    "text": "それについては、また言いたいことがある。"
  },
  {
    "start": 272534,
    "end": 276274,
    "text": "クラウド・プロバイダーはこれらのツールを導入している。"
  },
  {
    "start": 276784,
    "end": 286808,
    "text": "人々は自分のモデルをハグフェイスに載せて、サイバーセキュリティの世代という点で、自分たちがどれだけのリスクを抱えているかを見ているんだ。"
  },
  {
    "start": 286856,
    "end": 292488,
    "text": "もちろん、サイバー攻撃者の役に立つようなことはしたくない。"
  },
  {
    "start": 292576,
    "end": 293204,
    "text": "そうだね。"
  },
  {
    "start": 294104,
    "end": 295604,
    "text": "これらのモデルは可能である。"
  },
  {
    "start": 295944,
    "end": 306194,
    "text": "その後、今年1月にコード・ラマの別バージョンである70 Bをリリースした。"
  },
  {
    "start": 306654,
    "end": 308574,
    "text": "これもすべて市販されている。"
  },
  {
    "start": 308694,
    "end": 310830,
    "text": "ツールもあるし、オープンソースのコードもある。"
  },
  {
    "start": 310982,
    "end": 312998,
    "text": "ライセンスは商用利用を認めている。"
  },
  {
    "start": 313126,
    "end": 318834,
    "text": "私たちの許容範囲内の使用ポリシーに従えば、ほとんど何をやってもいい。"
  },
  {
    "start": 319574,
    "end": 321750,
    "text": "タイムライン表示のようなものだ。"
  },
  {
    "start": 321822,
    "end": 326558,
    "text": "オリジナルのラマは2月に研究用としてライセンスされた。"
  },
  {
    "start": 326686,
    "end": 329924,
    "text": "私の昔のチームがやっていたオリジナルのラマについては、楽しい話がたくさんある。"
  },
  {
    "start": 330014,
    "end": 331624,
    "text": "彼らはAIを向上させている。"
  },
  {
    "start": 331744,
    "end": 335080,
    "text": "ミスター・オールをご存知なら、彼らは今やミスター・オールチームだ。"
  },
  {
    "start": 335272,
    "end": 344296,
    "text": "私たちは、650億パラメータモデルのラマ2まで、また商業的に使用可能な7つ、70までリリースした。"
  },
  {
    "start": 344480,
    "end": 346084,
    "text": "当時の最先端。"
  },
  {
    "start": 347264,
    "end": 352824,
    "text": "本当に、今企業のスタートアップで使われている本当に質の高いモデルのようなものだと言える。"
  },
  {
    "start": 352944,
    "end": 354760,
    "text": "本当に景色が変わったよ。"
  },
  {
    "start": 354912,
    "end": 362194,
    "text": "その後、8月と1月にコードラマが来て、そしてもちろんメタラマ3がある。"
  },
  {
    "start": 363054,
    "end": 368814,
    "text": "そう、メタルラマ3だ。"
  },
  {
    "start": 368854,
    "end": 370662,
    "text": "これは絶対的な労作だった。"
  },
  {
    "start": 370718,
    "end": 375174,
    "text": "GitHubのモデルカードを見れば、何人が貢献したかがわかる。"
  },
  {
    "start": 375214,
    "end": 378942,
    "text": "実際、今こうしている間にも、人々は私を忘れている、私を忘れている、私を忘れていると言っている。"
  },
  {
    "start": 378958,
    "end": 385064,
    "text": "私は実際に、この実現に大きな役割を果たした社内の多くの貢献者とともに、このモデルカードを更新している。"
  },
  {
    "start": 385134,
    "end": 388144,
    "text": "ラマ3世をドアから連れ出すには、絶対的な村の力が必要だった。"
  },
  {
    "start": 388604,
    "end": 391052,
    "text": "基本的に2つのバージョンをリリースした。"
  },
  {
    "start": 391068,
    "end": 392404,
    "text": "パラメータは80億。"
  },
  {
    "start": 392484,
    "end": 394064,
    "text": "なぜ7人なのか不思議に思うだろう？"
  },
  {
    "start": 394844,
    "end": 396060,
    "text": "なぜ7人ではなく8人なのか？"
  },
  {
    "start": 396172,
    "end": 401464,
    "text": "まあ、ボキャブラリーが増えたので、パラメーターも増えたが、8点になってしまった。"
  },
  {
    "start": 401844,
    "end": 403356,
    "text": "それなら700億がある。"
  },
  {
    "start": 403500,
    "end": 410364,
    "text": "私たちは、事前に訓練されたベースモデルと、整列されたモデルの両方をリリースしました。"
  },
  {
    "start": 410404,
    "end": 415848,
    "text": "これらは以前のチャットモデルに似ていて、すべてオープンソースだ。"
  },
  {
    "start": 415976,
    "end": 420644,
    "text": "また、ラマガードV2もリリースしたので、それについても話そう。"
  },
  {
    "start": 421264,
    "end": 424000,
    "text": "を少しクリックすると、これらのモデルが表示される。"
  },
  {
    "start": 424152,
    "end": 428688,
    "text": "少なくとも7倍のデータでトレーニングした。"
  },
  {
    "start": 428856,
    "end": 434364,
    "text": "以前のモデルをご存じであれば、事前トレーニングで約2兆個のトークンを使ってトレーニングしています。"
  },
  {
    "start": 434824,
    "end": 441392,
    "text": "この新しいモデルは、微調整のために15兆個以上のトークンをトレーニングしている。"
  },
  {
    "start": 441448,
    "end": 449472,
    "text": "つまり、生成AIモデルのワークフローをご存じであれば、事前トレーニングは教師なし、つまり事後トレーニングではラベルを持たない。"
  },
  {
    "start": 449528,
    "end": 454632,
    "text": "人間のフィードバックによる強化学習、あるいはDPOのようなことをするわけですが、それについてはまたお話しします。"
  },
  {
    "start": 454808,
    "end": 456544,
    "text": "あなたはまた、多くの人間の注釈を持っている。"
  },
  {
    "start": 456624,
    "end": 460764,
    "text": "SFTには100万件のアノテーションがあった。"
  },
  {
    "start": 461664,
    "end": 463744,
    "text": "ラマ2世では、実際にはその10倍だ。"
  },
  {
    "start": 463824,
    "end": 468434,
    "text": "実際、ラナ3ではその10倍強で、より多くの人間がラベル付けしたデータがある。"
  },
  {
    "start": 468514,
    "end": 471974,
    "text": "とても高価だが、素晴らしいモデルができる。"
  },
  {
    "start": 472714,
    "end": 478834,
    "text": "明らかに、私たちはより大きな語彙と新しいトークナイザーを含めた。"
  },
  {
    "start": 478874,
    "end": 479854,
    "text": "最高だよ。"
  },
  {
    "start": 481074,
    "end": 485574,
    "text": "より効率的で、より高性能で、コンテキストウィンドウも2倍になる。"
  },
  {
    "start": 485874,
    "end": 491162,
    "text": "これらのモデルは、ラナ・スリーのごく初期のリリースであることを強調しておきたい。"
  },
  {
    "start": 491298,
    "end": 501984,
    "text": "本当は、プレリリースとかプレビューと呼ぶ予定だったんだ。"
  },
  {
    "start": 502104,
    "end": 507684,
    "text": "正直なところ、私たちはレセプションに圧倒され、他のモデルと比べていかに優れているかに驚かされました。"
  },
  {
    "start": 509384,
    "end": 510248,
    "text": "そういえば。"
  },
  {
    "start": 510336,
    "end": 518200,
    "text": "私たちは実際に、インストラクターやポストトレインモデルのようなものだけでなく、ベースモデルそのものについても評価を行っている。"
  },
  {
    "start": 518312,
    "end": 522048,
    "text": "ベースモデルで十分だ。"
  },
  {
    "start": 522136,
    "end": 526484,
    "text": "例えば、テキストを補完するように。"
  },
  {
    "start": 527184,
    "end": 530776,
    "text": "そうすれば、インストラクターについて考えることができる。"
  },
  {
    "start": 530800,
    "end": 532552,
    "text": "それは人間的なものだ。"
  },
  {
    "start": 532728,
    "end": 539248,
    "text": "実際に質疑応答ができたり、チャットのようなこともできる。"
  },
  {
    "start": 539296,
    "end": 543924,
    "text": "それは、よりアプリケーションタイプのユースケースに沿ったものだ。"
  },
  {
    "start": 544464,
    "end": 549230,
    "text": "左側から見てわかるように、これは本当に、本当に信じられないほど素晴らしい。"
  },
  {
    "start": 549262,
    "end": 559434,
    "text": "クアルコムが量子化し、スナップドラゴンを搭載した携帯電話上で動作させているように、実際に携帯電話でも使用可能な8つのBモデルを比較することができる。"
  },
  {
    "start": 559854,
    "end": 570150,
    "text": "ジェマセブンBやミンストレルセブンBのような他のトップモデルと比較しても、その実、圧倒的な差がある。"
  },
  {
    "start": 570182,
    "end": 580774,
    "text": "スライドにも載っていないことだが、エイトBの数字とパフォーマンスを比較すると、70Bのラマ2モデルよりも優れている。"
  },
  {
    "start": 581114,
    "end": 587890,
    "text": "また、70B以上のモデルをジェミニ・プロ1.5やクラウド・スリー・ソネットとも比較した。"
  },
  {
    "start": 588082,
    "end": 592418,
    "text": "数学のモデューロ・ポインター2で、全体的に勝っているんだ。"
  },
  {
    "start": 592466,
    "end": 597774,
    "text": "明らかにグーグルはミネルバのプロンプトで違うことをしている。"
  },
  {
    "start": 598074,
    "end": 602966,
    "text": "70 Bは絶対にクレイジーで、クレイジーで、オープンソースのモデルだ。"
  },
  {
    "start": 603070,
    "end": 606214,
    "text": "ご覧のように、事前トレーニングのベンチマークも行った。"
  },
  {
    "start": 606334,
    "end": 611874,
    "text": "プレトレーニングの面白いところは、実際にその数字を発表している会社があまりないことだ。"
  },
  {
    "start": 612334,
    "end": 617942,
    "text": "彼らの論文を読み、実際に発表されたものを見れば、ジェミニ・プロ1.0を手に入れることができたのはかなりラッキーだった。"
  },
  {
    "start": 617998,
    "end": 634396,
    "text": "しかし、ベースモデルは1.0モデルや先週発売されたばかりのミックスロール22 B、そして今週のインストラクター・バージョンよりも全体的に優れている。"
  },
  {
    "start": 634580,
    "end": 637052,
    "text": "とんでもなく印象的なモデルたち。"
  },
  {
    "start": 637108,
    "end": 638396,
    "text": "これで遊ぶのは超楽しそうだ。"
  },
  {
    "start": 638420,
    "end": 639384,
    "text": "きっと気に入るよ。"
  },
  {
    "start": 639884,
    "end": 640980,
    "text": "ダウンロードしてください。"
  },
  {
    "start": 641092,
    "end": 641904,
    "text": "楽しんでくれ。"
  },
  {
    "start": 642724,
    "end": 647540,
    "text": "私たちが強く主張し、情熱を注いできたことのひとつは、ベンチマークが楽しいということだ。"
  },
  {
    "start": 647652,
    "end": 654428,
    "text": "MMouやGSmakやこういったものを見せるのは素晴らしいことだし、それに付随するちょっとした駆け引きもある。"
  },
  {
    "start": 654596,
    "end": 662140,
    "text": "実際にこれを人間の手に渡して、彼らが実際にどう動くか、そして彼らがこのモデルをどう好むかを理解するとき、ゴムは道と出会うようなものだ。"
  },
  {
    "start": 662212,
    "end": 677788,
    "text": "私たちが行ったのは、多くのアノテーション・パートナーを獲得し、彼らを通じて規模を拡大し、基本的には彼らと協力して、12のカテゴリーにまたがる人間のプロンプトに基づいて、1800のプロンプト・データセットを作成することだった。"
  },
  {
    "start": 677876,
    "end": 680040,
    "text": "これはコーディングかもしれないし、推論かもしれない。"
  },
  {
    "start": 680212,
    "end": 687856,
    "text": "私たちはGitHubに多くの詳細な情報を掲載し、基本的にすべての人間にその出来を尋ねているんだ。"
  },
  {
    "start": 687880,
    "end": 690296,
    "text": "をクリックすると、勝率、勝率、引き分け率、負け率を見ることができる。"
  },
  {
    "start": 690440,
    "end": 695160,
    "text": "つまり、ベンチマークを信じないなら、人間を信じろ、ということだ。"
  },
  {
    "start": 695192,
    "end": 697520,
    "text": "実際にこれらのモデルで遊び、実際に気に入っている人たちだ。"
  },
  {
    "start": 697552,
    "end": 701928,
    "text": "見てわかるように、比較すると、明らかに、圧倒的に、人々はラマ2よりもラマ3の方が好きなんだ。"
  },
  {
    "start": 701976,
    "end": 704164,
    "text": "ラマ2よりラマ3の方が断然好きだ。"
  },
  {
    "start": 704584,
    "end": 712874,
    "text": "GPD3対GPD3、3.5対ミストラル・ミディアム対クラウドソニック。"
  },
  {
    "start": 714254,
    "end": 715758,
    "text": "本当に勇気づけられたよ。"
  },
  {
    "start": 715806,
    "end": 722726,
    "text": "それでわかったのは、我々がやっているのはベンチマークと同じではなく、モデルが質的に優れているということだ。"
  },
  {
    "start": 722870,
    "end": 727478,
    "text": "実際、素晴らしかったのは、私たちのチームが毎日、これらのモデルで遊んでいたことだ。"
  },
  {
    "start": 727646,
    "end": 735796,
    "text": "朝起きたら、模型で遊んで、質問して、拒否するか、質問に答えてくれるか、おしゃべりしてくれるか見ていた。"
  },
  {
    "start": 735860,
    "end": 740024,
    "text": "これらのモデルのチューニングには本当に多くの時間を費やした。"
  },
  {
    "start": 742124,
    "end": 744796,
    "text": "私たちが実際にどのように開発したのか、そのような詳細には立ち入らない。"
  },
  {
    "start": 744940,
    "end": 750044,
    "text": "私たちが考えたことは、最高レベルで4つある。"
  },
  {
    "start": 750164,
    "end": 751744,
    "text": "ひとつはモデル・アーキテクチャだ。"
  },
  {
    "start": 752164,
    "end": 757188,
    "text": "ひとつは、ラマをご存じならわかると思うが、密な自己回帰変換器を使ったことだ。"
  },
  {
    "start": 757236,
    "end": 764224,
    "text": "2つ目は、これらのモデルにはグループクエリーアテンション、つまりGQAアテンションメカニズムもあった。"
  },
  {
    "start": 764384,
    "end": 770404,
    "text": "今回は新しいトークナイザーを追加した。これについては、もうすぐ発表する論文で詳しく説明する予定だ。"
  },
  {
    "start": 771184,
    "end": 777512,
    "text": "実際のモデル・アーキテクチャーに大きな飛躍はないが、思慮深い変更のようなものもある。"
  },
  {
    "start": 777608,
    "end": 779684,
    "text": "その後、我々はそれを大幅に拡大した。"
  },
  {
    "start": 780344,
    "end": 782968,
    "text": "トレーニングデータは、15兆トークンを超えると申し上げました。"
  },
  {
    "start": 783016,
    "end": 786284,
    "text": "大量のコンピュート、大量のデータ。"
  },
  {
    "start": 786874,
    "end": 792938,
    "text": "事前トレーニングに関しては、2週間前にブログでトレーニング・インフラについて書いたと思う。"
  },
  {
    "start": 792986,
    "end": 798330,
    "text": "私たちは2つのカスタムメイドの24キロ100クラスターを持っていて、それを訓練に使っている。"
  },
  {
    "start": 798442,
    "end": 800546,
    "text": "我々は多くのコンピュート（計算能力）に恵まれている。"
  },
  {
    "start": 800690,
    "end": 801410,
    "text": "ありがとう、ジェンセン。"
  },
  {
    "start": 801442,
    "end": 802294,
    "text": "ありがとう、マーク。"
  },
  {
    "start": 803114,
    "end": 807314,
    "text": "そして最後に、ポストトレーニングで多くの仕事をした。"
  },
  {
    "start": 807474,
    "end": 815374,
    "text": "みんな、事前トレーニングの話が大好きで、どれだけスケールアップするか、何万ものGPUとどれだけのデータを事前トレーニングするかという話をすると思う。"
  },
  {
    "start": 815414,
    "end": 821246,
    "text": "本当に、マジックはポストトレーニングにあると思う。"
  },
  {
    "start": 821430,
    "end": 824326,
    "text": "そこで、私たちは多くの人間による注釈を生み出している。"
  },
  {
    "start": 824430,
    "end": 827646,
    "text": "そこで、私たちは多くのスフティングを行っている。"
  },
  {
    "start": 827790,
    "end": 838948,
    "text": "リジェクト、サンプリング、PPo、DPoのようなことを行い、明らかに大規模なデータ訓練とともに、これらのモデルの使いやすさと人間的な側面のバランスを取ろうとしている。"
  },
  {
    "start": 839116,
    "end": 841584,
    "text": "これが私たちの考えだった。"
  },
  {
    "start": 842604,
    "end": 846452,
    "text": "親切さと安全性にも注目した。"
  },
  {
    "start": 846508,
    "end": 848996,
    "text": "これはつまり、本質的なトレードオフなんだ。"
  },
  {
    "start": 849020,
    "end": 856532,
    "text": "私たちは、これらのモデルがどれだけ役に立つか、どれだけ質問に答えられるか、どれだけ事実に即しているかなど、モデルの有用性を最大化しようとしている。"
  },
  {
    "start": 856668,
    "end": 866096,
    "text": "我々はまた、安全性のバランスをとり、モデルがどのように反応するかを理解したい。"
  },
  {
    "start": 866120,
    "end": 868424,
    "text": "詳しくはモデルカードをご覧ください。"
  },
  {
    "start": 868464,
    "end": 874524,
    "text": "実は今朝、別のブログ記事も投稿している。"
  },
  {
    "start": 876384,
    "end": 877528,
    "text": "最後のピース。"
  },
  {
    "start": 877576,
    "end": 884064,
    "text": "つまり、ジェネイの時代にはレッドチーム化が非常に重要なんだ。"
  },
  {
    "start": 884184,
    "end": 887376,
    "text": "私たちは多くの時間を過ごし、バーはどんどん変わっていった。"
  },
  {
    "start": 887440,
    "end": 887640,
    "text": "そうだね。"
  },
  {
    "start": 887672,
    "end": 891440,
    "text": "レッドチームのあり方や、こういったことをどう考えるかという点で、状況は変化し続けている。"
  },
  {
    "start": 891472,
    "end": 897232,
    "text": "例えば、Cバーニーというのを聞いたことがあるだろうか。"
  },
  {
    "start": 897288,
    "end": 898984,
    "text": "そうか、手が見えないんだ。"
  },
  {
    "start": 899024,
    "end": 903760,
    "text": "なるほど、サイバーや生物学的リスク、核や無線といったものがあるわけだ。"
  },
  {
    "start": 903912,
    "end": 913160,
    "text": "昨年発表された大統領令を読めばわかるように、これらのリスクについては言及されており、我々はこれらのリスクについてモデルを評価している。"
  },
  {
    "start": 913192,
    "end": 917204,
    "text": "例えば、誰かが生物兵器を作り出したいと思ったとき、あなたはどれだけ役に立てるだろうか？"
  },
  {
    "start": 917264,
    "end": 920676,
    "text": "まあ、つまり、そういうことを評価しなければならない。"
  },
  {
    "start": 920700,
    "end": 921236,
    "text": "私たちは理解しなければならない。"
  },
  {
    "start": 921300,
    "end": 922580,
    "text": "グーグルで検索すればいいんだろ？"
  },
  {
    "start": 922612,
    "end": 926156,
    "text": "どうすればいいかを教えてくれるリンクがいくつか見つかるだろう。"
  },
  {
    "start": 926300,
    "end": 931844,
    "text": "モデルは、異なる情報をどの程度統合し、あなたを助けることができるのか？"
  },
  {
    "start": 931884,
    "end": 934748,
    "text": "これらは実際に評価しなければならないことであり、軽減しなければならないことだ。"
  },
  {
    "start": 934796,
    "end": 937944,
    "text": "実際、この目的のために文字通り専門チームがある。"
  },
  {
    "start": 938684,
    "end": 942100,
    "text": "最後に、ライセンスについて簡単に説明すると、あまり大きな変更はない。"
  },
  {
    "start": 942212,
    "end": 944012,
    "text": "研究用と商業用だ。"
  },
  {
    "start": 944148,
    "end": 945646,
    "text": "派生物を作ることもできる。"
  },
  {
    "start": 945780,
    "end": 947970,
    "text": "700MAUというものがある。"
  },
  {
    "start": 948002,
    "end": 952378,
    "text": "もしあなたがとても大きな会社なら、私たちと一緒に働きましょう。"
  },
  {
    "start": 952506,
    "end": 953734,
    "text": "ほとんどの選手がそうだ。"
  },
  {
    "start": 954194,
    "end": 960610,
    "text": "また、多くの企業がllamaを使いたいと言っていたので、正しくブランディングできるように、ブランディングのガイドラインも追加しました。"
  },
  {
    "start": 960642,
    "end": 962054,
    "text": "それが今のライセンスだ。"
  },
  {
    "start": 963434,
    "end": 965810,
    "text": "エコシステムは、先ほども言ったように大きい。"
  },
  {
    "start": 965882,
    "end": 972530,
    "text": "ここでは、NvidiaやIntel、Qualcommといったハードウェアベンダーから参加した人々の一部を紹介する。"
  },
  {
    "start": 972562,
    "end": 977882,
    "text": "私たちは、企業やプラットフォーム・プロバイダーに至るまで、本当に密接に協力している。"
  },
  {
    "start": 978018,
    "end": 979770,
    "text": "本当に素晴らしいエコシステムだ。"
  },
  {
    "start": 979802,
    "end": 983934,
    "text": "これはロゴのサインをもらったものだけだ。"
  },
  {
    "start": 984754,
    "end": 989386,
    "text": "巨大なオープンソースコミュニティもあり、オラマは私の個人的なお気に入りだ。"
  },
  {
    "start": 989410,
    "end": 990654,
    "text": "本当に素晴らしいプロジェクトだ。"
  },
  {
    "start": 991074,
    "end": 993762,
    "text": "もちろん、私たちはGGMalの人々と本当に密接に仕事をしている。"
  },
  {
    "start": 993818,
    "end": 996682,
    "text": "毛糸もまた、文脈の長さを延長するための実にクールなプロジェクトだ。"
  },
  {
    "start": 996778,
    "end": 998694,
    "text": "ぜひいくつかのプロジェクトをチェックしてほしい。"
  },
  {
    "start": 999254,
    "end": 1002834,
    "text": "さて、そろそろ時間がなくなりそうなので、安全性にシフトチェンジしよう。"
  },
  {
    "start": 1003334,
    "end": 1005006,
    "text": "もしかしたら、向こうが僕を行かせてくれるかもしれない。"
  },
  {
    "start": 1005110,
    "end": 1008094,
    "text": "私はパープラマのことを話した。"
  },
  {
    "start": 1008174,
    "end": 1010434,
    "text": "なぜ \"パープラマ \"と呼ぶのか、誰か知ってる？"
  },
  {
    "start": 1011894,
    "end": 1013446,
    "text": "そう、片手もない。"
  },
  {
    "start": 1013630,
    "end": 1014334,
    "text": "赤、青。"
  },
  {
    "start": 1014414,
    "end": 1014894,
    "text": "そうだ。"
  },
  {
    "start": 1014934,
    "end": 1015358,
    "text": "赤、青。"
  },
  {
    "start": 1015406,
    "end": 1016870,
    "text": "じゃあ、レッドチームとブルーチームだね。"
  },
  {
    "start": 1016942,
    "end": 1018486,
    "text": "攻撃と守備。"
  },
  {
    "start": 1018630,
    "end": 1021262,
    "text": "これはサイバーセキュリティの分野から拝借したものだ。"
  },
  {
    "start": 1021318,
    "end": 1026874,
    "text": "実はこの名前は、私たちのサイバーセキュリティ・ゲナイ・チームの科学者の一人が命名した。"
  },
  {
    "start": 1027293,
    "end": 1040725,
    "text": "私たちは両方の指標を管理することが本当に重要だと考えました。例えば、これらの害のいくつかについて、私たちがどのようなことをしているのかを評価し、実際に明確な指標を持つことができるようにすることです。"
  },
  {
    "start": 1040869,
    "end": 1045101,
    "text": "何かを測定できるだけでは不十分で、実際に何かをする必要がある。"
  },
  {
    "start": 1045117,
    "end": 1051873,
    "text": "パープルラマの真骨頂は、このようなことを評価し、実際にフィルタリングできるモデルを展開できるようにすることなんだ。"
  },
  {
    "start": 1052994,
    "end": 1056474,
    "text": "これが、私たちがシステムレベルの安全性について考えるきっかけとなった。"
  },
  {
    "start": 1056514,
    "end": 1059210,
    "text": "私たちがモデルの有用性を最大化すると申し上げた。"
  },
  {
    "start": 1059282,
    "end": 1061694,
    "text": "これは私たちが取った考え方とはまったく異なるものだ。"
  },
  {
    "start": 1062314,
    "end": 1065610,
    "text": "例えば、ラマ2世はとても安全なモデルだ。"
  },
  {
    "start": 1065722,
    "end": 1076458,
    "text": ""
  },
  {
    "start": 1076626,
    "end": 1081590,
    "text": "モデルたちはとても安全だ。"
  },
  {
    "start": 1081742,
    "end": 1087022,
    "text": "同時に、インプットとアウトプットのセーフガードの柔軟性も確保したかった。"
  },
  {
    "start": 1087078,
    "end": 1089110,
    "text": "そうすれば、タクソノミーをカスタマイズすることができる。"
  },
  {
    "start": 1089222,
    "end": 1093774,
    "text": "ある種のリスクを基本的にフィルタリングしたいのであれば、絶対にできる。"
  },
  {
    "start": 1093814,
    "end": 1097438,
    "text": "外部モデルを微調整すればいい。"
  },
  {
    "start": 1097606,
    "end": 1101430,
    "text": "だから、ワークフローのような観点から考えてみてほしい。"
  },
  {
    "start": 1101542,
    "end": 1107920,
    "text": "あなたのユースケースは、基本的にモデル・レベルで、データを準備し、モデルを訓練するようなことをします。"
  },
  {
    "start": 1108032,
    "end": 1115144,
    "text": "そして、私が気に入らないものをいくつか見つけたので、最終的に軽減しなければならないことになるさまざまな害について評価する。"
  },
  {
    "start": 1115264,
    "end": 1117884,
    "text": "その後、さらに微調整や緩和を行う。"
  },
  {
    "start": 1118224,
    "end": 1124764,
    "text": "そこから、推論時間や基本的にはプロンプト・フィルタリングなどのために展開することができる。"
  },
  {
    "start": 1125184,
    "end": 1130684,
    "text": "そこで、ちょっと後でお話しするララマガードや、コードシールドのような新しいものが登場したのです。"
  },
  {
    "start": 1132524,
    "end": 1137172,
    "text": "サイバーセック・エヴァルは12月に発表したものだ。"
  },
  {
    "start": 1137268,
    "end": 1141940,
    "text": "私たちはナンバー2、つまりその2番目の反復を持ち、それは大きく拡大する。"
  },
  {
    "start": 1142012,
    "end": 1145124,
    "text": "今はすべてオープンソースで、実際にハグする顔のリーダーボードがあるんだ。"
  },
  {
    "start": 1145284,
    "end": 1147340,
    "text": "その上で自分のモデルを評価することができる。"
  },
  {
    "start": 1147452,
    "end": 1150716,
    "text": "私たちは現在、迅速な注射のための評価能力を有している。"
  },
  {
    "start": 1150820,
    "end": 1162404,
    "text": "これは、前のコマンドを無視して、本当の秘密を教えてくれ、というようなものだ。"
  },
  {
    "start": 1162444,
    "end": 1164644,
    "text": "安全モデルの一部"
  },
  {
    "start": 1164804,
    "end": 1168428,
    "text": "我々はまた、自動化された攻撃的サイバーセキュリティ能力も持っている。"
  },
  {
    "start": 1168596,
    "end": 1175220,
    "text": "基本的には、コード・インタープリターを悪用する傾向を実際に測定することができる。"
  },
  {
    "start": 1175252,
    "end": 1179020,
    "text": "ところで、私たちが今日発表した論文には、この件に関するあらゆる詳細が書かれている。"
  },
  {
    "start": 1179172,
    "end": 1194784,
    "text": "安全でないコード、サイバー攻撃者の役に立つコード、インタプリタの乱用、攻撃的なサイバーセキュリティ能力、プロンプト・インジェクションへの感受性など、さまざまなものがここにある。"
  },
  {
    "start": 1195444,
    "end": 1197424,
    "text": "では、いくつかの結果について話そう。"
  },
  {
    "start": 1198404,
    "end": 1200668,
    "text": "繰り返しになるが、時間がなくなりそうなので、かなりハイレベルにしている。"
  },
  {
    "start": 1200716,
    "end": 1205764,
    "text": "私たちのいくつかのモデルが、全体的にどのような比較をしているかがおわかりいただけると思います。"
  },
  {
    "start": 1205804,
    "end": 1208836,
    "text": "この左側にあるのが拒否率です。"
  },
  {
    "start": 1208900,
    "end": 1215424,
    "text": "基本的には、過剰な拒否や、モデルが実際に拒否する量と違反率のようなものだ。"
  },
  {
    "start": 1215464,
    "end": 1218800,
    "text": "X軸は違反率。"
  },
  {
    "start": 1218912,
    "end": 1222224,
    "text": "エイトBラマ3が実際に信じられないようなパフォーマンスをしているのがわかるだろう。"
  },
  {
    "start": 1222264,
    "end": 1223856,
    "text": "まさにスイートスポットだ。"
  },
  {
    "start": 1224040,
    "end": 1228576,
    "text": "70 Bは、実際にはもっと首尾一貫したモデル、あるいはもっとスマートなモデルだ。"
  },
  {
    "start": 1228600,
    "end": 1235320,
    "text": "私たちが発見したのは、モデルが強力であればあるほど、実際に違反する可能性が高くなるということです。"
  },
  {
    "start": 1235352,
    "end": 1236136,
    "text": "ならば、軽減しなければならない。"
  },
  {
    "start": 1236160,
    "end": 1238454,
    "text": "だから、ちょうど真ん中に位置している。"
  },
  {
    "start": 1238834,
    "end": 1240410,
    "text": "コード・ラマ70 Bを見ることができる。"
  },
  {
    "start": 1240442,
    "end": 1244338,
    "text": "正直なところ、私たちはそのモデルを緩和していた。"
  },
  {
    "start": 1244466,
    "end": 1249410,
    "text": "実際にかなり高いレベルで拒否しているのがわかるが、ユーザーにとってはちょっと迷惑な話だ。"
  },
  {
    "start": 1249522,
    "end": 1253054,
    "text": "それは私たちが学んだことであり、次の世代で修正していくものだ。"
  },
  {
    "start": 1255034,
    "end": 1261242,
    "text": "ちょっと見づらいグラフだが、基本的にこれが即効性のある注射という点でのモデルのパフォーマンスだ。"
  },
  {
    "start": 1261418,
    "end": 1267096,
    "text": "これは、基本的にプロンプト・インジェクション攻撃のすべての異なるタイプに対するモデルです。"
  },
  {
    "start": 1267120,
    "end": 1272352,
    "text": "これは、トークン攻撃、説得、仮想化、これらすべての異なる方法を繰り返すようなものだ。"
  },
  {
    "start": 1272448,
    "end": 1275968,
    "text": "これらのモデルを脱獄する方法はたくさんある。"
  },
  {
    "start": 1276096,
    "end": 1281964,
    "text": "上の方に、青がいい、青がいい、と書いてある。"
  },
  {
    "start": 1282904,
    "end": 1287644,
    "text": "もし、このことをもっと詳しく知りたい、実際に自分でやってみたいということであれば、一番上にリンクを貼っておく。"
  },
  {
    "start": 1289184,
    "end": 1291032,
    "text": "よし、ラマガルデ、急ごう。"
  },
  {
    "start": 1291088,
    "end": 1294014,
    "text": "また、12月にはランベガルドVをリリースした。"
  },
  {
    "start": 1294144,
    "end": 1295674,
    "text": "繰り返しますが、これはオープンソースモデルです。"
  },
  {
    "start": 1295714,
    "end": 1297882,
    "text": "これは、あなた自身が使えるモデルであり、あなた自身が展開できるものだ。"
  },
  {
    "start": 1297938,
    "end": 1300202,
    "text": "それは7つのBラマ2に基づいている。"
  },
  {
    "start": 1300378,
    "end": 1306254,
    "text": "アマゾンやサジェイメイカーに配備され、一緒に配備され、その他たくさんのデータブリックに配備された。"
  },
  {
    "start": 1306994,
    "end": 1313974,
    "text": "繰り返しになるが、これはコンテンツ・モデレーションAPIに似ている。"
  },
  {
    "start": 1314354,
    "end": 1317274,
    "text": "私たちはラマ3世をベースにしたラマ・ガード2世を作った。"
  },
  {
    "start": 1317314,
    "end": 1318874,
    "text": "よりパワフルなモデルだ。"
  },
  {
    "start": 1318914,
    "end": 1320920,
    "text": "ベンチマークが格段に良くなっているのがわかるだろう。"
  },
  {
    "start": 1321072,
    "end": 1326284,
    "text": "今週、MLコモンズの方針を発表しました。"
  },
  {
    "start": 1326864,
    "end": 1337204,
    "text": "MLコモンズのすべてのパートナーと共同で設計したため、この方針に関しては本当に本当に優れている。"
  },
  {
    "start": 1338264,
    "end": 1340364,
    "text": "また、それはオープンに利用できる。"
  },
  {
    "start": 1341064,
    "end": 1347790,
    "text": "コードシールドは基本的に、サイバーセキュリティのための推論時間入力出力セーフガードツールである。"
  },
  {
    "start": 1347912,
    "end": 1351642,
    "text": "これは基本的に、llmsによって生成された安全でないコードのフィルタリングをサポートする。"
  },
  {
    "start": 1351778,
    "end": 1355362,
    "text": "例えば、フィッシング攻撃を生成するように頼んでも、フィルタリングで除外してしまうだろう。"
  },
  {
    "start": 1355458,
    "end": 1356386,
    "text": "かなりクールだ。"
  },
  {
    "start": 1356450,
    "end": 1361574,
    "text": "これもまたオープンソースで、GitHubにあるので手に入れることができる。"
  },
  {
    "start": 1361914,
    "end": 1367974,
    "text": "基本的には、安全でないコードからコード・インタープリター、安全なコマンド実行、保護、そういったものまでカバーしている。"
  },
  {
    "start": 1369674,
    "end": 1377582,
    "text": "あと1つか2つスライドがあると思うのですが、もしあと1つか2分ほどお時間をいただけるのであれば、本当にクールな話に入りたいと思います。"
  },
  {
    "start": 1377678,
    "end": 1379894,
    "text": "トーチ・チューンを共同デザインした。"
  },
  {
    "start": 1380014,
    "end": 1381166,
    "text": "誰かトーチ・チューンを知っているかい？"
  },
  {
    "start": 1381270,
    "end": 1382566,
    "text": "Pytorchを使っている人はいますか？"
  },
  {
    "start": 1382750,
    "end": 1384174,
    "text": "オーケー。"
  },
  {
    "start": 1384254,
    "end": 1387814,
    "text": "トーチ・チューンは、僕とスミット、そしてチームの愛の結晶なんだ。"
  },
  {
    "start": 1387894,
    "end": 1391022,
    "text": "これは純粋なpytorchの微調整ライブラリです。"
  },
  {
    "start": 1391158,
    "end": 1394270,
    "text": "依存関係は15もない。"
  },
  {
    "start": 1394302,
    "end": 1395710,
    "text": "これは純粋なピトーチだ。"
  },
  {
    "start": 1395822,
    "end": 1397174,
    "text": "これほどクリーンなものはない。"
  },
  {
    "start": 1397254,
    "end": 1400850,
    "text": "Pytorchを使えば、Pythonでこれを構築し、自分で使うことができる。"
  },
  {
    "start": 1400982,
    "end": 1402130,
    "text": "美しいよ。"
  },
  {
    "start": 1402322,
    "end": 1403106,
    "text": "完全な微調整。"
  },
  {
    "start": 1403130,
    "end": 1405042,
    "text": "リャマ3世をサポートしているのは明らかだ。"
  },
  {
    "start": 1405058,
    "end": 1411010,
    "text": "llama2をお見せしているのは、残念ながらここだけだが、実はllama3のサポートが開始されたんだ。"
  },
  {
    "start": 1411042,
    "end": 1413414,
    "text": "ハギング・フェイスやその他多くのライブラリーと統合されている。"
  },
  {
    "start": 1413834,
    "end": 1414418,
    "text": "最高だよ。"
  },
  {
    "start": 1414466,
    "end": 1415334,
    "text": "ぜひご覧あれ。"
  },
  {
    "start": 1416074,
    "end": 1417494,
    "text": "ここにたくさんの資料がある。"
  },
  {
    "start": 1418314,
    "end": 1420346,
    "text": "GitHub llama threeにアクセスする。"
  },
  {
    "start": 1420490,
    "end": 1422386,
    "text": "リャマのレシピもありますから、検索してみてください。"
  },
  {
    "start": 1422490,
    "end": 1428130,
    "text": "それから、スタートノート、ラングチェーン、ラグ・プロンプト・エンジンなど、いろいろなものが入ったビットリーがある。"
  },
  {
    "start": 1428242,
    "end": 1430254,
    "text": "オーケー、もう1分待ってくれ。"
  },
  {
    "start": 1430834,
    "end": 1434634,
    "text": "今朝、マークが話していた。"
  },
  {
    "start": 1434714,
    "end": 1436894,
    "text": "実際には、もっと大規模なモデルトレーニングを行っている。"
  },
  {
    "start": 1437234,
    "end": 1438414,
    "text": "大きな男の子だ。"
  },
  {
    "start": 1438834,
    "end": 1442058,
    "text": "ここでいくつかの指標を紹介したい。"
  },
  {
    "start": 1442186,
    "end": 1447974,
    "text": "今朝、ツイッターでいくつかの比較を見たよ。"
  },
  {
    "start": 1448634,
    "end": 1450490,
    "text": "他のモデルと比較することもできる。"
  },
  {
    "start": 1450522,
    "end": 1452214,
    "text": "このモデルはトレーニングをしていない。"
  },
  {
    "start": 1453014,
    "end": 1456462,
    "text": "まだ少し道半ばだが、いくつかの数字を紹介したい。"
  },
  {
    "start": 1456558,
    "end": 1464598,
    "text": "事前トレーニングと、チェックポイントをつかんで基本的なSFTを行い、アライメントをとって数字を示した。"
  },
  {
    "start": 1464726,
    "end": 1470150,
    "text": "MMoUが86.1であることがわかるだろう。"
  },
  {
    "start": 1470222,
    "end": 1473994,
    "text": "誰かと比べることはできないが、このモデルがどのような傾向にあるかはなんとなくわかるだろう。"
  },
  {
    "start": 1474294,
    "end": 1477214,
    "text": "GSmak94.1に換算して。"
  },
  {
    "start": 1477254,
    "end": 1478402,
    "text": "本当に、本当に強い。"
  },
  {
    "start": 1478438,
    "end": 1482858,
    "text": "われわれのモデルは、まだトレーニングも行っていないし、ポストトレーニングも行っていない。"
  },
  {
    "start": 1482906,
    "end": 1484370,
    "text": "ポストトレーニングは大きく変わるだろう。"
  },
  {
    "start": 1484482,
    "end": 1486650,
    "text": "私たちは、これらの指標の少なくともいくつかをお披露目したかった。"
  },
  {
    "start": 1486682,
    "end": 1488034,
    "text": "かなりエキサイティングだ。"
  },
  {
    "start": 1488194,
    "end": 1490454,
    "text": "これはちょうど3日前のチェックポイントでのものだ。"
  },
  {
    "start": 1492754,
    "end": 1493774,
    "text": "誕生日おめでとう。"
  },
  {
    "start": 1494714,
    "end": 1496474,
    "text": "これからのことをいくつか。"
  },
  {
    "start": 1496594,
    "end": 1503306,
    "text": "もちろん、我々はより大きく、より良いモデルを用意している。"
  },
  {
    "start": 1503410,
    "end": 1506050,
    "text": "多言語、私たちは多くの言語をサポートします。"
  },
  {
    "start": 1506122,
    "end": 1513206,
    "text": "つまり、フェイスブックは私たちの財産であり、私たちのFOAであり、私たちのアプリのファミリーは40億人以上、あるいは40億人前後だと想像できます。"
  },
  {
    "start": 1513390,
    "end": 1514486,
    "text": "私たちはどこにでもいる。"
  },
  {
    "start": 1514550,
    "end": 1517262,
    "text": "メタAIから何から、多言語は超重要だ。"
  },
  {
    "start": 1517358,
    "end": 1519158,
    "text": "我々はそれをリャマにも組み込んでいきたい。"
  },
  {
    "start": 1519286,
    "end": 1525194,
    "text": "マルチモーダル、スマートグラスやRay banを使ったAR VRでやっていることはすべて想像できる。"
  },
  {
    "start": 1525774,
    "end": 1529326,
    "text": "身の回りのすべてを理解する必要があるが、テキストではそれができない。"
  },
  {
    "start": 1529430,
    "end": 1535930,
    "text": "マルチモーダルの登場はもちろん、安全へのコミットメントは、安全性のすべてと安全性に関する多くのものをオープンソース化し続ける。"
  },
  {
    "start": 1536042,
    "end": 1539194,
    "text": "それを中心にコミュニティを作り、安全を中心に標準化を構築する。"
  },
  {
    "start": 1539354,
    "end": 1540786,
    "text": "それは私が超熱中していることなんだ。"
  },
  {
    "start": 1540850,
    "end": 1542402,
    "text": "我々は間違いなくそれを続けていく。"
  },
  {
    "start": 1542458,
    "end": 1546394,
    "text": "最後に、メタAIに行くことを約束する。"
  },
  {
    "start": 1546434,
    "end": 1556094,
    "text": "実際にllama 3を使って遊びたいなら、無料だし、遊べるし、プロンプトも出せるし、私が最初にクールなアニメーション画像を見せたように、実際に画像を生成させることもできる。"
  },
  {
    "start": 1556474,
    "end": 1561138,
    "text": "imagineをクリックし、プロンプトを表示させ、クリックし、アニメートをクリックすれば、何か生成される。"
  },
  {
    "start": 1561226,
    "end": 1563010,
    "text": "メタAIを促すこともできる。"
  },
  {
    "start": 1563042,
    "end": 1566374,
    "text": "あなたは実際にラマを3ベースモデルと呼んでいる。"
  },
  {
    "start": 1567314,
    "end": 1568242,
    "text": "そこで小休止する。"
  },
  {
    "start": 1568298,
    "end": 1568834,
    "text": "本当にありがとう。"
  }
]