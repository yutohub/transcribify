[
  {
    "start": 90,
    "end": 846,
    "text": "皆さん、こんにちは。"
  },
  {
    "start": 1028,
    "end": 3946,
    "text": "今日もマケモアの導入を続けている。"
  },
  {
    "start": 4058,
    "end": 10538,
    "text": "さて、前回の講義では、文字レベルの言語モデリングのために、Benjo Hotel 2003に沿ったマルチリョープ・レセプトロンを実装した。"
  },
  {
    "start": 10634,
    "end": 16906,
    "text": "私たちはこの論文に従って、過去の数文字を取り込み、MLPを使ってシーケンスの次の文字を予測しました。"
  },
  {
    "start": 17098,
    "end": 25960,
    "text": "リカレント・ニューラル・ネットワークやそのバリエーション、例えばGRU LSTMなどです。"
  },
  {
    "start": 26330,
    "end": 31350,
    "text": "さて、その前に、マルチリオン・パーセプトロンのレベルにもう少しこだわる必要がある。"
  },
  {
    "start": 31500,
    "end": 43254,
    "text": "というのも、トレーニング中のニューラルネットの活性化、特に逆流する勾配、そしてそれらがどのように振る舞い、どのように見えるかについて、非常に直感的に理解できるようにしたいからだ。"
  },
  {
    "start": 43372,
    "end": 64478,
    "text": "というのも、リカレント・ニューラル・ネットワークは、普遍的な近似器であり、原理的にはすべてのアルゴリズムを実装できるという点で非常に表現力が豊かである一方、私たちがいつも使っている一次勾配ベースのテクニックでは、とても簡単に最適化できないことがわかるからです。"
  },
  {
    "start": 64644,
    "end": 72514,
    "text": "なぜ簡単に最適化できないのかを理解する鍵は、活性度と勾配を理解し、それらがトレーニング中にどのように振る舞うかを理解することである。"
  },
  {
    "start": 72632,
    "end": 78834,
    "text": "リカレント・ニューラル・ネットワーク以降の多くの変種が、この状況を改善しようとしていることがわかるだろう。"
  },
  {
    "start": 79032,
    "end": 81510,
    "text": "だから、それが私たちが取るべき道なんだ。"
  },
  {
    "start": 81580,
    "end": 82758,
    "text": "さあ、始めよう。"
  },
  {
    "start": 82844,
    "end": 93286,
    "text": "この講義のスタートコードは、ほぼ以前のコードですが、少し整理したので、すべてのtorchとmathplotlibユーティリティをインポートしていることがわかると思います。"
  },
  {
    "start": 93398,
    "end": 95322,
    "text": "前と同じように言葉を読み込んでいる。"
  },
  {
    "start": 95456,
    "end": 96986,
    "text": "これらは8つの例文である。"
  },
  {
    "start": 97088,
    "end": 99066,
    "text": "全部で32,000人だ。"
  },
  {
    "start": 99168,
    "end": 103770,
    "text": "ここでは、すべての小文字と特殊なドット・トークンの語彙を紹介する。"
  },
  {
    "start": 104190,
    "end": 113338,
    "text": "ここでは、データセットを読み込んで処理し、3つの分割（train devとtest split）を作成している。"
  },
  {
    "start": 113514,
    "end": 126030,
    "text": "さて、MLPでは、ここにあったマジックナンバーの束を取り除いた以外は、全く同じMLPです。その代わりに、文字の埋め込み空間の次元数と隠れ層の隠れユニットの数があります。"
  },
  {
    "start": 126110,
    "end": 131350,
    "text": "だから、マジックナンバーを変更する必要がないように、外側に引っ張り出してきたんだ。"
  },
  {
    "start": 131500,
    "end": 138134,
    "text": "11,000のパラメーターを持つ同じニューラルネットを、バッチサイズ32で200,000ステップかけて最適化する。"
  },
  {
    "start": 138252,
    "end": 143562,
    "text": "コードを少しリファクタリングしたことがわかると思うが、機能的な変更はない。"
  },
  {
    "start": 143616,
    "end": 151594,
    "text": "いくつかの変数とコメントを追加し、マジックナンバーをすべて削除しただけで、それ以外はまったく同じです。"
  },
  {
    "start": 151792,
    "end": 155678,
    "text": "そして最適化すると、我々の損失はこのようになった。"
  },
  {
    "start": 155844,
    "end": 161040,
    "text": "列車とバルのロスは約2.16などと見た。"
  },
  {
    "start": 161570,
    "end": 166906,
    "text": "ここでは、任意の分割を評価するためにコードを少しリファクタリングした。"
  },
  {
    "start": 167018,
    "end": 169918,
    "text": "には、どのスプリットを評価したいかを示す文字列を渡す。"
  },
  {
    "start": 170014,
    "end": 175358,
    "text": "そしてここで、train、val、testに応じてインデックスを作成し、正しいスプリットを得る。"
  },
  {
    "start": 175454,
    "end": 179798,
    "text": "となると、これはネットワークのフォワードパスであり、ロスの評価と印刷である。"
  },
  {
    "start": 179964,
    "end": 182470,
    "text": "ただ、より素敵にするだけだ。"
  },
  {
    "start": 182810,
    "end": 191242,
    "text": "トーチ・ノグラードというデコレーターを使っている。"
  },
  {
    "start": 191376,
    "end": 201750,
    "text": "基本的に、このデコレーターは関数の上で何をするかというと、この関数の中で何が起こっても、torchはグラデーションを必要としないことを想定している。"
  },
  {
    "start": 201910,
    "end": 209278,
    "text": "最終的な後方パスを見越して、すべての勾配を記録しておくような記帳は一切しない。"
  },
  {
    "start": 209444,
    "end": 214250,
    "text": "まるで、ここで作られるテンソルは、すべて \"偽 \"の階調を要求されるかのようだ。"
  },
  {
    "start": 214410,
    "end": 216394,
    "text": "だから、すべてがより効率的になる。"
  },
  {
    "start": 216442,
    "end": 220510,
    "text": "なぜなら、あなたはトーチに、私がこの計算をドットバックと呼ぶことはないと言っているのだから。"
  },
  {
    "start": 220590,
    "end": 223090,
    "text": "を使えば、ボンネットの下のグラフをメンテナンスする必要はない。"
  },
  {
    "start": 223590,
    "end": 225330,
    "text": "これがそうだ。"
  },
  {
    "start": 225480,
    "end": 231480,
    "text": "トーチ・ノグラードでコンテキスト・マネージャーを使うこともできる。"
  },
  {
    "start": 232810,
    "end": 244614,
    "text": "先ほどと同じように、モデルからのサンプリング、フォーム、ニューラルネットでのパッシブ、分布の取得、そこからのサンプリング、コンテキストウィンドウの調整、そして特別なエンドトークンが得られるまで繰り返す。"
  },
  {
    "start": 244742,
    "end": 249610,
    "text": "モデルからサンプリングされた言葉が、よりきれいに見えるようになってきているのがわかる。"
  },
  {
    "start": 249760,
    "end": 256160,
    "text": "まだすごいとは言えないし、完全にネームバリューがあるわけでもないが、バイオグラムのモデルよりはずっといい。"
  },
  {
    "start": 257570,
    "end": 259006,
    "text": "それが出発点だ。"
  },
  {
    "start": 259108,
    "end": 261870,
    "text": "さて、最初に精査したいのは初期化だ。"
  },
  {
    "start": 262450,
    "end": 269106,
    "text": "私たちのネットワークは初期化時の設定が非常に不適切で、複数の問題があることがわかる。"
  },
  {
    "start": 269128,
    "end": 270866,
    "text": "まずは最初の1つから。"
  },
  {
    "start": 271048,
    "end": 280002,
    "text": "0回目の反復、つまり最初の反復で、27の損失を記録している。"
  },
  {
    "start": 280136,
    "end": 283800,
    "text": "この値が高すぎるから、初期化がめちゃくちゃなんだとわかる。"
  },
  {
    "start": 284250,
    "end": 290578,
    "text": "ニューラルネットのトレーニングでは、ほとんどの場合、初期化時にどの程度の損失が予想されるか、おおよその見当がつく。"
  },
  {
    "start": 290754,
    "end": 294406,
    "text": "それは損失関数と問題設定による。"
  },
  {
    "start": 294598,
    "end": 299820,
    "text": "この場合、私は27を予想しているのではなく、もっと低い数字を予想している。"
  },
  {
    "start": 300510,
    "end": 308718,
    "text": "基本的には、初期化時に、どのトレーニング例に対しても、次に来る可能性のあるキャラクターが27人いることが望ましい。"
  },
  {
    "start": 308884,
    "end": 313438,
    "text": "初期化の時点では、どの文字が他の文字よりも可能性が高いと信じる理由はない。"
  },
  {
    "start": 313604,
    "end": 322610,
    "text": "したがって、最初に出てくる確率分布は、27文字すべてにほぼ等しい確率を割り当てる一様分布になると予想される。"
  },
  {
    "start": 323350,
    "end": 330360,
    "text": "私たちが望むのは、どのキャラクターの確率もだいたい27分の1になることです。"
  },
  {
    "start": 331770,
    "end": 333558,
    "text": "これが記録すべき確率である。"
  },
  {
    "start": 333724,
    "end": 336338,
    "text": "その場合、損失は負の対数確率となる。"
  },
  {
    "start": 336514,
    "end": 349530,
    "text": "これをテンソルに包んで、その対数を取れば、負の対数確率が予想される損失となる。"
  },
  {
    "start": 349680,
    "end": 356270,
    "text": "つまり、今起きていることは、初期化時にニューラルネットが確率分布を作り出し、それがめちゃくちゃになっているということだ。"
  },
  {
    "start": 356340,
    "end": 360298,
    "text": "自信に満ち溢れたキャラクターもいれば、自信のないキャラクターもいる。"
  },
  {
    "start": 360474,
    "end": 370510,
    "text": "ということは、基本的に起こっていることは、ネットワークが非常に自信を持って間違っているということであり、それが非常に高いロスを記録させているのだ。"
  },
  {
    "start": 370590,
    "end": 373154,
    "text": "この問題の四次元的な例である。"
  },
  {
    "start": 373272,
    "end": 380360,
    "text": "例えば、4文字しかないとして、ニューラルネットから出てくるロジットがゼロに非常に近いとしよう。"
  },
  {
    "start": 380730,
    "end": 384722,
    "text": "そして、すべてのゼロのソフトマックスを取ると、確率が得られる。"
  },
  {
    "start": 384786,
    "end": 390470,
    "text": "分布は拡散しているので、和は1になり、まさに一様である。"
  },
  {
    "start": 390970,
    "end": 399750,
    "text": "この場合、ラベルが例えば2であったとしても、一様分布であるため、ラベルが2であろうが3であろうが1であろうが0であろうが、実際には問題にはならない。"
  },
  {
    "start": 399830,
    "end": 402874,
    "text": "まったく同じ損失、この場合は1.38を記録している。"
  },
  {
    "start": 402992,
    "end": 405918,
    "text": "これは4次元の例で予想される損失である。"
  },
  {
    "start": 406084,
    "end": 412254,
    "text": "もちろん、これらのロジットを操作し始めると、ここでの損失が変わってくるのは目に見えている。"
  },
  {
    "start": 412372,
    "end": 419058,
    "text": "ロックアウトする可能性もあるし、偶然にも5人とか、そういう非常に多い数字になる可能性もある。"
  },
  {
    "start": 419144,
    "end": 433794,
    "text": "その場合、初期化時に偶然正しいラベルに正しい確率を割り当てているため、損失は非常に少なくなる。"
  },
  {
    "start": 433922,
    "end": 436786,
    "text": "そうなれば、より大きな損失を計上することになる。"
  },
  {
    "start": 436978,
    "end": 446570,
    "text": "ロジットがこのように極端な値をとり、大きな損失を記録することがある。"
  },
  {
    "start": 448430,
    "end": 451486,
    "text": "例えば、4つのランダムなトルフがあるとする。"
  },
  {
    "start": 451588,
    "end": 459440,
    "text": "これは一様で、すみません、正規分布の数字です。"
  },
  {
    "start": 460290,
    "end": 466746,
    "text": "そしてここで、そこから出たロジット確率と損失を表示することもできる。"
  },
  {
    "start": 466938,
    "end": 473140,
    "text": "だから、これらのロジットはゼロに近いので、ほとんどの場合、出てくる損失は問題ない。"
  },
  {
    "start": 473750,
    "end": 475894,
    "text": "これを10倍したようなものだろう。"
  },
  {
    "start": 475932,
    "end": 489190,
    "text": "これらはより極端な値であるため、正しいバケットを当てる可能性が非常に低く、自信を持って間違って非常に高い損失を記録することがお分かりいただけただろう。"
  },
  {
    "start": 489530,
    "end": 497610,
    "text": "もし、あなたのロッジがさらに極端に出るようなら、初期化でも無限大のような極めて非常識なロスが出るかもしれない。"
  },
  {
    "start": 500270,
    "end": 501674,
    "text": "基本的にこれは良くない。"
  },
  {
    "start": 501712,
    "end": 507642,
    "text": "ネットワークが初期化されたとき、ロードジットはほぼゼロであってほしい。"
  },
  {
    "start": 507786,
    "end": 511130,
    "text": "実際、ロジットがゼロである必要はなく、等しいだけでいいのだ。"
  },
  {
    "start": 511210,
    "end": 518210,
    "text": "例えば、すべてのロジットが1であれば、ソフトマックス内部で正規化されるため、実際には問題ないことになる。"
  },
  {
    "start": 518360,
    "end": 522018,
    "text": "対称性によって、任意の正負の数にはしたくない。"
  },
  {
    "start": 522104,
    "end": 526238,
    "text": "初期化時に期待する損失を記録し、すべてゼロにしたいだけなのだ。"
  },
  {
    "start": 526334,
    "end": 528274,
    "text": "では、具体的にどこが問題なのかを見てみよう。"
  },
  {
    "start": 528312,
    "end": 531110,
    "text": "この例では、初期化を行っている。"
  },
  {
    "start": 531450,
    "end": 533254,
    "text": "ニューラルネットを再初期化してみよう。"
  },
  {
    "start": 533372,
    "end": 536034,
    "text": "最初のイテレーションが終わったら、ここで休憩させてください。"
  },
  {
    "start": 536082,
    "end": 539734,
    "text": "最初の損失は27だけだ。"
  },
  {
    "start": 539932,
    "end": 541178,
    "text": "それは高すぎる。"
  },
  {
    "start": 541264,
    "end": 544054,
    "text": "直感的に、関係する変数を予想できるようになった。"
  },
  {
    "start": 544182,
    "end": 561886,
    "text": "最初の行を印刷してみると、ロジットが非常に極端な値をとっていることがわかる。"
  },
  {
    "start": 561988,
    "end": 564974,
    "text": "これらのロジットはもっともっとゼロに近いはずだ。"
  },
  {
    "start": 565172,
    "end": 571940,
    "text": "では、このニューラルネットから出てくるロジットを、どうすればもっとゼロに近づけることができるかを考えてみよう。"
  },
  {
    "start": 572390,
    "end": 577350,
    "text": "ここで、ロジットは隠された状態にw 2とb 2を掛けたものとして計算されることがわかる。"
  },
  {
    "start": 577500,
    "end": 583720,
    "text": "まず第一に、現在b2つを適切な大きさのランダムな値として初期化している。"
  },
  {
    "start": 584170,
    "end": 589126,
    "text": "というのも、我々はおおよそゼロが欲しいのであって、実際には乱数のバイアスを加えたくはないからだ。"
  },
  {
    "start": 589228,
    "end": 596730,
    "text": "つまり、初期化時にb2が基本的にゼロであることを確認するために、ここでゼロを足しているのだ。"
  },
  {
    "start": 597310,
    "end": 600250,
    "text": "秒、これはhにwを2倍したものである。"
  },
  {
    "start": 600320,
    "end": 606030,
    "text": "ロジットを非常に小さくしたいのであれば、wを2倍して小さくすることになる。"
  },
  {
    "start": 606850,
    "end": 614330,
    "text": "例えば、W 2をゼロ倍にして、すべての要素を1つずつ縮小していくと、最初の繰り返しだけでいい。"
  },
  {
    "start": 614410,
    "end": 617042,
    "text": "そうすることで、私たちが期待しているものに近づいているんだ。"
  },
  {
    "start": 617176,
    "end": 620210,
    "text": "というわけで、私たちが欲しいのはだいたい3.29くらいだ。"
  },
  {
    "start": 620360,
    "end": 621938,
    "text": "これは4.2だ。"
  },
  {
    "start": 622104,
    "end": 624610,
    "text": "もっと小さくできるかもしれない。"
  },
  {
    "start": 625430,
    "end": 626446,
    "text": "3.32."
  },
  {
    "start": 626488,
    "end": 628482,
    "text": "よし、だんだん近づいてきた。"
  },
  {
    "start": 628626,
    "end": 632360,
    "text": "さて、これをゼロに設定することはできるのだろうか？"
  },
  {
    "start": 632970,
    "end": 637590,
    "text": "そうすれば、もちろん、初期化時に求めていたものが得られる。"
  },
  {
    "start": 638010,
    "end": 642230,
    "text": "僕が普段こういうことをしないのは、とても緊張しているからなんだ。"
  },
  {
    "start": 642310,
    "end": 648380,
    "text": "ニューラルネットのwや重みを正確にゼロに設定したくない理由は、後で説明しよう。"
  },
  {
    "start": 649310,
    "end": 652620,
    "text": "通常は、正確にゼロではなく、小さな数字にしたいものだ。"
  },
  {
    "start": 653230,
    "end": 654438,
    "text": "この出力レイヤーの場合。"
  },
  {
    "start": 654454,
    "end": 657070,
    "text": "この具体的なケースでは、問題ないと思う。"
  },
  {
    "start": 657220,
    "end": 660510,
    "text": "そんなことをすれば、すぐにうまくいかなくなることをすぐにお見せしよう。"
  },
  {
    "start": 660580,
    "end": 662574,
    "text": "0.1で行こう。"
  },
  {
    "start": 662772,
    "end": 666622,
    "text": "その場合、我々の損失は十分に近いが、若干のエントロピーがある。"
  },
  {
    "start": 666686,
    "end": 671806,
    "text": "正確にはゼロではなく、若干のエントロピーがあり、それが対称性の破れに使われる。"
  },
  {
    "start": 671838,
    "end": 677942,
    "text": "すぐにわかるように、ロジットがゼロに近づいている。"
  },
  {
    "start": 678076,
    "end": 692426,
    "text": "これらを消去し、break文を取り除けば、この新しい初期化で最適化を実行することができます。"
  },
  {
    "start": 692528,
    "end": 697260,
    "text": "よし、それで走らせてみたんだけど、最初は良かったんだ。"
  },
  {
    "start": 698110,
    "end": 714774,
    "text": "というのも、基本的にホッケースティックで起こっていることは、損失の最初の数回の繰り返しであり、最適化の間に起こっていることは、最適化によってロジットが押しつぶされ、その後ロジットが並べ替えられるだけだからです。"
  },
  {
    "start": 714922,
    "end": 728790,
    "text": "つまり、基本的には、損失関数の簡単な部分、つまり重みが縮小されるだけの部分を取り除いたのです。そのため、最初のうちは簡単な利益は得られず、実際のニューラルネットを訓練するための難しい利益が得られるだけです。"
  },
  {
    "start": 728860,
    "end": 731030,
    "text": "だから、ホッケースティックは出現しない。"
  },
  {
    "start": 731370,
    "end": 740466,
    "text": "その1、初期化時の損失が予想通りであること、その2、損失がホッケースティックのように見えないこと。"
  },
  {
    "start": 740578,
    "end": 745226,
    "text": "これは、どのニューラルネットを訓練する場合にも当てはまることであり、注意すべきことである。"
  },
  {
    "start": 745408,
    "end": 749398,
    "text": "第二に、出てきた敗戦は実際にはかなり改善されている。"
  },
  {
    "start": 749494,
    "end": 751786,
    "text": "残念なことに、以前ここにあったものを消してしまった。"
  },
  {
    "start": 751888,
    "end": 757054,
    "text": "これが2.12で、これが2.16だったと思う。"
  },
  {
    "start": 757172,
    "end": 759790,
    "text": "少し改善された。"
  },
  {
    "start": 759940,
    "end": 776290,
    "text": "その理由は、ニューラルネットを最適化するためにより多くのサイクル、時間を費やしているからだ。最初の数千回の反復に時間を費やすのではなく、おそらく重みを小さくしているのだろう。"
  },
  {
    "start": 776630,
    "end": 778118,
    "text": "気をつけなければならないことがある。"
  },
  {
    "start": 778204,
    "end": 779878,
    "text": "これがナンバーワンだ。"
  },
  {
    "start": 779964,
    "end": 781494,
    "text": "では、2つ目の問題を見てみよう。"
  },
  {
    "start": 781612,
    "end": 785670,
    "text": "ニューラルネットを再初期化し、break文を再導入しよう。"
  },
  {
    "start": 786010,
    "end": 788514,
    "text": "初期損失はそれなりにある。"
  },
  {
    "start": 788562,
    "end": 796650,
    "text": "しかし、このニューラルネットとその初期化には、まだ深い問題が潜んでいる。"
  },
  {
    "start": 797310,
    "end": 805006,
    "text": "さて、問題はhの値、つまり隠れ状態の活性化である。"
  },
  {
    "start": 805188,
    "end": 810014,
    "text": "さて、このベクトル、すまないがテンソルhを視覚化すると、ちょっと見づらい。"
  },
  {
    "start": 810052,
    "end": 815614,
    "text": "ここで問題になるのは、大雑把に言えば、1かマイナスの要素がいくつあるかということだ。"
  },
  {
    "start": 815812,
    "end": 817922,
    "text": "10時間後にトーチを思い出してほしい。"
  },
  {
    "start": 817976,
    "end": 820274,
    "text": "ten h関数はスカッシュ関数である。"
  },
  {
    "start": 820392,
    "end": 825790,
    "text": "これは任意の数値を受け取り、それをマイナス1と1の範囲に押し込める。"
  },
  {
    "start": 825950,
    "end": 832050,
    "text": "このテンソルの中の値の分布を知るために、hのヒストグラムを見てみよう。"
  },
  {
    "start": 832210,
    "end": 833640,
    "text": "まずはこれをやろう。"
  },
  {
    "start": 834810,
    "end": 839398,
    "text": "hは32の例で200の活性化であることがわかる。"
  },
  {
    "start": 839494,
    "end": 840570,
    "text": "それぞれの例で。"
  },
  {
    "start": 840720,
    "end": 845610,
    "text": "大きなベクトルに引き伸ばすために、マイナス1として見ることができる。"
  },
  {
    "start": 846270,
    "end": 853306,
    "text": "toolistを呼び出すと、これをpythonの大きな浮動小数点数リストに変換することができる。"
  },
  {
    "start": 853498,
    "end": 857402,
    "text": "そして、これをplt histに渡してヒストグラムを作成します。"
  },
  {
    "start": 857546,
    "end": 863200,
    "text": "50ビンが必要で、セミコロンで不要な出力を抑えるとする。"
  },
  {
    "start": 864130,
    "end": 869970,
    "text": "このヒストグラムを見ると、ほとんどの値がマイナス1と1の値を取っていることがわかる。"
  },
  {
    "start": 870040,
    "end": 872846,
    "text": "この10時間はとてもとてもアクティブだ。"
  },
  {
    "start": 873038,
    "end": 877494,
    "text": "また、基本的になぜそうなるのかも見てみよう。"
  },
  {
    "start": 877692,
    "end": 887250,
    "text": "10時間プレーにつながる前活性化を見てみると、前活性化の分布が非常に広いことがわかる。"
  },
  {
    "start": 887330,
    "end": 889862,
    "text": "これらはマイナス15から15の間の数値を取る。"
  },
  {
    "start": 889996,
    "end": 895530,
    "text": "だから、トーチ10時間では、すべてがマイナス1と1の範囲になるようにつぶされ、キャップされているのだ。"
  },
  {
    "start": 895600,
    "end": 898570,
    "text": "ここでは多くの数字が非常に極端な値をとっている。"
  },
  {
    "start": 898990,
    "end": 903178,
    "text": "さて、ニューラルネットワークに慣れていない人は、これが問題だとは思わないかもしれない。"
  },
  {
    "start": 903264,
    "end": 914682,
    "text": "バックプロパゲーションの奥義に精通し、ニューラルネットの中を勾配がどのように流れていくかを直感的に理解している人なら、ここで10時間分の活性度分布を見て汗をかいていることだろう。"
  },
  {
    "start": 914826,
    "end": 916158,
    "text": "その理由をお見せしよう。"
  },
  {
    "start": 916244,
    "end": 924398,
    "text": "バックプロパゲーションでは、マイクログラッドで見たのと同じように、ロスからスタートしてネットワークを逆向きに流れるバックワードパスを行なっていることに留意しなければならない。"
  },
  {
    "start": 924574,
    "end": 928040,
    "text": "特に、10時間かけてバックプロパゲートする。"
  },
  {
    "start": 928570,
    "end": 933426,
    "text": "この層は、それぞれの例に対して200のニューロンで構成されている。"
  },
  {
    "start": 933618,
    "end": 936406,
    "text": "これは、エレメントワイズテンHを実装している。"
  },
  {
    "start": 936508,
    "end": 939478,
    "text": "バックワードパスで10時間後に何が起こるか見てみよう。"
  },
  {
    "start": 939644,
    "end": 946300,
    "text": "実際に、最初の講義で説明したマイクログラッドのコードに戻り、10hをどのように実装したかを見てみよう。"
  },
  {
    "start": 946750,
    "end": 952090,
    "text": "ここでは入力がxであることを確認し、次にtを計算する。"
  },
  {
    "start": 952240,
    "end": 954814,
    "text": "はtであり、tはマイナス1から1の間である。"
  },
  {
    "start": 954852,
    "end": 956254,
    "text": "これは10Hの出力だ。"
  },
  {
    "start": 956372,
    "end": 959774,
    "text": "では、バックワードパスでは、10Hをどのように逆伝播するのか？"
  },
  {
    "start": 959972,
    "end": 963934,
    "text": "その採点値を取り出して、掛け算をする。"
  },
  {
    "start": 963972,
    "end": 968686,
    "text": "これは局所勾配を用いた連鎖法則であり、1マイナスtの2乗という形になる。"
  },
  {
    "start": 968878,
    "end": 973842,
    "text": "tan hの出力がマイナス1または1に非常に近い場合はどうなるのか？"
  },
  {
    "start": 973976,
    "end": 979422,
    "text": "ここでtを1に等しく差し込むと、lグラッドを掛けてゼロになる。"
  },
  {
    "start": 979566,
    "end": 987138,
    "text": "l dot gradが何であろうと、私たちは勾配を殺しているのであり、事実上、この10h単位での逆伝播を止めているのだ。"
  },
  {
    "start": 987314,
    "end": 992490,
    "text": "同様に、tがマイナス1になると、これは再びゼロになり、グラッドは停止する。"
  },
  {
    "start": 992830,
    "end": 996890,
    "text": "これは直感的に理解できる。"
  },
  {
    "start": 997470,
    "end": 1003440,
    "text": "この出力が1に非常に近い場合、我々はこのtan hの最後尾にいることになる。"
  },
  {
    "start": 1003810,
    "end": 1005946,
    "text": "とても変化している。"
  },
  {
    "start": 1006138,
    "end": 1015442,
    "text": "基本的に、入力はテンハイのフラットな領域にあるため、テンハイの出力にあまり影響を与えない。"
  },
  {
    "start": 1015576,
    "end": 1018270,
    "text": "したがって、損失への影響はない。"
  },
  {
    "start": 1018430,
    "end": 1029954,
    "text": "したがって、この10hニューロンの重みとバイアスは、この10hユニットの出力が10hの平坦領域にあるため、損失には影響しない。"
  },
  {
    "start": 1030082,
    "end": 1034578,
    "text": "好きなように、好きなように変えることができる。"
  },
  {
    "start": 1034754,
    "end": 1036438,
    "text": "それを正当化する別の方法だ。"
  },
  {
    "start": 1036524,
    "end": 1039002,
    "text": "実際、勾配は基本的にゼロだろう。"
  },
  {
    "start": 1039056,
    "end": 1040170,
    "text": "消えてしまう。"
  },
  {
    "start": 1040830,
    "end": 1047222,
    "text": "実際、tがゼロに等しいとき、その勾配は1倍になる。"
  },
  {
    "start": 1047366,
    "end": 1054240,
    "text": "hがちょうどゼロの値をとるとき、その勾配はそのまま通過する。"
  },
  {
    "start": 1054690,
    "end": 1064542,
    "text": "基本的にこれは、tがゼロに等しい場合、10h単位はある種の非活動状態になり、勾配はただ通過する。"
  },
  {
    "start": 1064676,
    "end": 1069198,
    "text": "フラットテールになればなるほど、グラデーションはつぶれる。"
  },
  {
    "start": 1069374,
    "end": 1074366,
    "text": "つまり、実際、10hを流れる勾配は減少する一方であることがわかるだろう。"
  },
  {
    "start": 1074478,
    "end": 1084360,
    "text": "その減少量は、この10hのフラットテールのどの程度にいるかに応じて、ここにある2乗で比例する。"
  },
  {
    "start": 1084890,
    "end": 1089014,
    "text": "それがここで、そしてこれを通して起こっていることなんだ。"
  },
  {
    "start": 1089132,
    "end": 1100090,
    "text": "ここで懸念されるのは、これらの出力hがすべてマイナス1と1の平坦領域にある場合、ネットワークを流れる勾配はこの層で破壊されてしまうということだ。"
  },
  {
    "start": 1100910,
    "end": 1107070,
    "text": "さて、ここにはいくつかの救いがある。"
  },
  {
    "start": 1107490,
    "end": 1108974,
    "text": "ここにいくつかのコードを書いた。"
  },
  {
    "start": 1109092,
    "end": 1118590,
    "text": "基本的にここでやりたいことは、hを見て、絶対値をとり、それがどれくらいの頻度で平坦な領域にあるかを見ることだ。"
  },
  {
    "start": 1118670,
    "end": 1121780,
    "text": "ゼロより大きい99"
  },
  {
    "start": 1122150,
    "end": 1124110,
    "text": "その結果得られるものは以下の通りである。"
  },
  {
    "start": 1124270,
    "end": 1125934,
    "text": "これはブール型テンソルである。"
  },
  {
    "start": 1125982,
    "end": 1132146,
    "text": "が真なら白、偽なら黒となる。"
  },
  {
    "start": 1132338,
    "end": 1137266,
    "text": "つまり、基本的にここにあるのは32の例と200の隠れニューロンである。"
  },
  {
    "start": 1137378,
    "end": 1140010,
    "text": "この多くが白人であることがわかる。"
  },
  {
    "start": 1140160,
    "end": 1148490,
    "text": "つまり、これらの10個のhニューロンが非常に活発で、平坦な尾部にあるということだ。"
  },
  {
    "start": 1148910,
    "end": 1155310,
    "text": "このような場合、後方勾配は破壊されることになる。"
  },
  {
    "start": 1156450,
    "end": 1165950,
    "text": "さて、この200個のニューロンのうち、1個でもカラム全体が白であれば、大変なことになる。"
  },
  {
    "start": 1166100,
    "end": 1168590,
    "text": "その場合、いわゆる死んだ神経細胞になってしまうからだ。"
  },
  {
    "start": 1168670,
    "end": 1179734,
    "text": "重みとバイアスの初期化は、1つの例もこの10hをアクティブにしないような10hニューロンである。"
  },
  {
    "start": 1179852,
    "end": 1186230,
    "text": "もしすべての例が尻尾に着けば、このニューロンは自分が死んだニューロンであることを知ることはない。"
  },
  {
    "start": 1186570,
    "end": 1194010,
    "text": "だから、これを精査して真っ白な列を探すだけで、そうではないことがわかる。"
  },
  {
    "start": 1194160,
    "end": 1199066,
    "text": "白一色の神経細胞は一つも見当たらない。"
  },
  {
    "start": 1199248,
    "end": 1208910,
    "text": "したがって、この10時間のニューロンの1つ1つに対して、10時間の活動的な部分でニューロンを活性化させるいくつかの例があることになる。"
  },
  {
    "start": 1208980,
    "end": 1215540,
    "text": "そうすると、ある勾配が流れてきて、このニューロンは学習し、ニューロンは変化し、動き、何かをするようになる。"
  },
  {
    "start": 1216230,
    "end": 1220158,
    "text": "神経細胞が死んでいるようなケースに陥ることがある。"
  },
  {
    "start": 1220254,
    "end": 1231302,
    "text": "これがどのように現れるかというと、10個のhニューロンでは、データセットからどのような入力を差し込んでも、この10個のhニューロンが常に完全に1か完全に負の1を発火する場合である。"
  },
  {
    "start": 1231356,
    "end": 1235960,
    "text": "その場合、すべてのグラデーションがゼロになってしまうため、学習することができない。"
  },
  {
    "start": 1236570,
    "end": 1241122,
    "text": "これは10hだけでなく、ニューラルネットワークで使われる他の多くの非線形性についても言えることだ。"
  },
  {
    "start": 1241186,
    "end": 1243110,
    "text": "確かに10時間はよく使う。"
  },
  {
    "start": 1243180,
    "end": 1247346,
    "text": "シグモイドはニューロンをつぶしてしまうので、まったく同じ問題がある。"
  },
  {
    "start": 1247458,
    "end": 1249834,
    "text": "シグモイドも同様である。"
  },
  {
    "start": 1250002,
    "end": 1256874,
    "text": "シグモイドも基本的には同じである。"
  },
  {
    "start": 1257002,
    "end": 1258954,
    "text": "同じことがレルーにも当てはまる。"
  },
  {
    "start": 1259082,
    "end": 1262960,
    "text": "reluは零下で完全にフラットな領域を持っている。"
  },
  {
    "start": 1263330,
    "end": 1266500,
    "text": "もしリニューロンがあれば、それはパススルーだ。"
  },
  {
    "start": 1267350,
    "end": 1268478,
    "text": "もしポジティブなら"
  },
  {
    "start": 1268574,
    "end": 1272546,
    "text": "もしプリアクティベーションがマイナスであれば、そのままシャットアウトされる。"
  },
  {
    "start": 1272648,
    "end": 1287960,
    "text": "この領域は完全にフラットなので、バックプロパゲーションの間、これは勾配を正確にゼロにすることになる。"
  },
  {
    "start": 1288330,
    "end": 1300730,
    "text": "例えば、死んだレルニューロン、死んだレルニューロンは、基本的には、レル非線形性を持つニューロンが決して活性化しない場合のように見える。"
  },
  {
    "start": 1301070,
    "end": 1305150,
    "text": "データセットにどのような例を差し込んでも、オンになることはない。"
  },
  {
    "start": 1305220,
    "end": 1307130,
    "text": "いつもこの平坦な地域にある。"
  },
  {
    "start": 1307290,
    "end": 1309370,
    "text": "そうなると、このリレー・ニューロンは死んだニューロンということになる。"
  },
  {
    "start": 1309450,
    "end": 1312062,
    "text": "その重みと偏りは決して学ぶことはないだろう。"
  },
  {
    "start": 1312116,
    "end": 1314990,
    "text": "ニューロンは決して活性化しないので、勾配は得られない。"
  },
  {
    "start": 1315490,
    "end": 1322542,
    "text": "初期化時に、重みとバイアスが偶然にいくつかのニューロンを永遠に死なせてしまうことがあるからだ。"
  },
  {
    "start": 1322686,
    "end": 1324702,
    "text": "最適化の最中にも起こりうる。"
  },
  {
    "start": 1324846,
    "end": 1326834,
    "text": "学習率が高すぎるような場合だ。"
  },
  {
    "start": 1326872,
    "end": 1333110,
    "text": "例えば、勾配が強すぎてデータ・マニホールドから外れてしまうニューロンがある。"
  },
  {
    "start": 1333450,
    "end": 1337826,
    "text": "それ以来、どんな例もこのニューロンを活性化させることはない。"
  },
  {
    "start": 1337858,
    "end": 1339362,
    "text": "このニューロンは永遠に死んだままだ。"
  },
  {
    "start": 1339426,
    "end": 1343590,
    "text": "それはネットワークというマインドにおける永久的な脳の損傷のようなものだ。"
  },
  {
    "start": 1343750,
    "end": 1352426,
    "text": "例えば、学習率が非常に高く、再読み込みニューロンを持つニューラルネットを訓練した場合、最後の損失が発生することがあります。"
  },
  {
    "start": 1352538,
    "end": 1361866,
    "text": "そうすると、実際にやることは、トレーニングセット全体を調べ、例を転送して、決して活性化しないニューロンを見つけることだ。"
  },
  {
    "start": 1361978,
    "end": 1366318,
    "text": "ネットワーク内の死んだニューロンだから、そのニューロンがオンになることはない。"
  },
  {
    "start": 1366404,
    "end": 1370622,
    "text": "通常、トレーニング中に起こるのは、これらのリニューロンが変化したり、動いたりすることだ。"
  },
  {
    "start": 1370686,
    "end": 1378290,
    "text": "そして、高い勾配のために、どこかで、偶然に、叩き落とされてしまう。"
  },
  {
    "start": 1378790,
    "end": 1382866,
    "text": "これは、ニューロンの一部に起こりうる永久的な脳の損傷のようなものだ。"
  },
  {
    "start": 1383058,
    "end": 1392330,
    "text": "リーキー・ブレルのような他の非線形性は、平坦なテールを持たず、ほとんど常に勾配を持つことがわかるので、この問題にそれほど悩まされることはないだろう。"
  },
  {
    "start": 1392830,
    "end": 1396154,
    "text": "エルーもかなり頻繁に使われている。"
  },
  {
    "start": 1396352,
    "end": 1399530,
    "text": "また、平らな部分があるため、この問題に悩まされるかもしれない。"
  },
  {
    "start": 1400190,
    "end": 1403966,
    "text": "それはただ、注意すべきことであり、懸念すべきことだ。"
  },
  {
    "start": 1404068,
    "end": 1410314,
    "text": "この場合、極端な値をとる活性化hが多すぎる。"
  },
  {
    "start": 1410442,
    "end": 1414254,
    "text": "白の列がないから大丈夫だと思う。"
  },
  {
    "start": 1414292,
    "end": 1418926,
    "text": "確かに、ネットワークは最適化し、かなりまともなロスを出してくれるが、最適とは言えない。"
  },
  {
    "start": 1418958,
    "end": 1422030,
    "text": "これは、特に初期化中には避けたいことだ。"
  },
  {
    "start": 1422190,
    "end": 1431080,
    "text": "つまり、基本的に起こっていることは、10時間に流れるこのHプレ活性化は、極端すぎる、大きすぎるということだ。"
  },
  {
    "start": 1433370,
    "end": 1437110,
    "text": "10Hの両側で飽和しすぎた配分を生み出している。"
  },
  {
    "start": 1437180,
    "end": 1445590,
    "text": "というのも、ニューロンは更新頻度が低いため、トレーニング量が少なくなってしまうからだ。"
  },
  {
    "start": 1445670,
    "end": 1447002,
    "text": "どうすれば解決できるのか？"
  },
  {
    "start": 1447136,
    "end": 1461342,
    "text": "まあ、hのプリ・アクティベーションはMcATで、これはcから来ているので、これらは一様なガウシアンなのですが、それにwの1プラスbの1が掛けられ、hのプリ・アクトはゼロから離れすぎていて、それが問題を引き起こしているのです。"
  },
  {
    "start": 1461476,
    "end": 1466690,
    "text": "このプリアクティベーションをゼロに近づけたい。"
  },
  {
    "start": 1467270,
    "end": 1470740,
    "text": "ここで私たちが求めているのは、実はとてもよく似たものなのだ。"
  },
  {
    "start": 1471350,
    "end": 1474994,
    "text": "さて、バイアスは非常に小さく設定しても構わない。"
  },
  {
    "start": 1475032,
    "end": 1478630,
    "text": "エントロピーを得るには、それを1倍すればいい。"
  },
  {
    "start": 1479370,
    "end": 1489362,
    "text": "そうすることで、10個のhニューロンの初期化にちょっとしたバリエーションと多様性を持たせることができるんだ。"
  },
  {
    "start": 1489426,
    "end": 1493100,
    "text": "実際にやってみると、それが最適化に少し役立つことがわかる。"
  },
  {
    "start": 1493630,
    "end": 1496262,
    "text": "ウエイトもスカッシュと同じようにできる。"
  },
  {
    "start": 1496326,
    "end": 1498874,
    "text": "すべてをゼロイチにかけてみよう。"
  },
  {
    "start": 1499072,
    "end": 1500890,
    "text": "最初のバッチを再実行しよう。"
  },
  {
    "start": 1501390,
    "end": 1502990,
    "text": "さて、これを見てみよう。"
  },
  {
    "start": 1503140,
    "end": 1511018,
    "text": "wにゼロ1を掛けると、より良いヒストグラムになります。"
  },
  {
    "start": 1511114,
    "end": 1514818,
    "text": "プレアクトがマイナス1.5から1.5になっているからだ。"
  },
  {
    "start": 1514904,
    "end": 1517780,
    "text": "白はもっともっと少ない。"
  },
  {
    "start": 1518470,
    "end": 1520180,
    "text": "よし、白はない。"
  },
  {
    "start": 1520790,
    "end": 1527634,
    "text": "というのも、どちらの方向にもゼロ99以上の飽和状態にあるニューロンは存在しないからだ。"
  },
  {
    "start": 1527762,
    "end": 1530360,
    "text": "実際、かなりまともな場所だ。"
  },
  {
    "start": 1531850,
    "end": 1534760,
    "text": "もう少し上に行けるかもしれない。"
  },
  {
    "start": 1536570,
    "end": 1539030,
    "text": "申し訳ないが、私はここでWを変えているのだろうか？"
  },
  {
    "start": 1539100,
    "end": 1540780,
    "text": "ゼロ2に行けるかもしれない"
  },
  {
    "start": 1542110,
    "end": 1546118,
    "text": "なるほど、このような配分がいいのかもしれない。"
  },
  {
    "start": 1546294,
    "end": 1548986,
    "text": "多分、これが初期化のあるべき姿だろう。"
  },
  {
    "start": 1549088,
    "end": 1562958,
    "text": "では、これらを消去して、初期化から始めて、ブレークなしで完全な最適化を実行してみよう。"
  },
  {
    "start": 1563044,
    "end": 1566010,
    "text": "さて、最適化が終わったので、ロスを再実行した。"
  },
  {
    "start": 1566090,
    "end": 1567834,
    "text": "これが私たちが得た結果である。"
  },
  {
    "start": 1567972,
    "end": 1572126,
    "text": "では、念のため、この講義で以前に見たすべての敗戦を書き留めておこう。"
  },
  {
    "start": 1572318,
    "end": 1575042,
    "text": "しかし、実際に改善されていることがわかる。"
  },
  {
    "start": 1575096,
    "end": 1578850,
    "text": "念のために言っておくが、我々は2.17の検証損失でスタートした。"
  },
  {
    "start": 1578920,
    "end": 1583826,
    "text": "ソフトマックスが確信犯的に間違っていることを修正することから始めると、2.13まで下がった。"
  },
  {
    "start": 1583938,
    "end": 1588550,
    "text": "10時間のレイヤーが飽和しすぎていたのを修正することで、2.10まで下がった。"
  },
  {
    "start": 1588700,
    "end": 1591850,
    "text": "もちろん、このようなことが起こるのは、初期化がうまくいっているからだ。"
  },
  {
    "start": 1591920,
    "end": 1599930,
    "text": "勾配がゼロに設定されているので、あまり生産的でないトレーニングではなく、生産的なトレーニングに多くの時間を費やしている。"
  },
  {
    "start": 1600080,
    "end": 1605290,
    "text": "私たちは、ソフトマックスの過信のような非常に単純なことを学ばなければならない。"
  },
  {
    "start": 1605370,
    "end": 1608510,
    "text": "ウェイトマトリックスを潰すように、我々はサイクルを費やしている。"
  },
  {
    "start": 1608930,
    "end": 1620286,
    "text": "これは、ニューラルネットの内部とその活性度と勾配を意識するだけで、基本的に初期化とそれがパフォーマンスに与える影響を説明するものである。"
  },
  {
    "start": 1620478,
    "end": 1622862,
    "text": "今はとても小さなネットワークで仕事をしている。"
  },
  {
    "start": 1622926,
    "end": 1625374,
    "text": "これは単なる1層の多層的な認識だ。"
  },
  {
    "start": 1625502,
    "end": 1631266,
    "text": "ネットワークが非常に浅いため、最適化問題は実際には非常に簡単で、非常に寛容である。"
  },
  {
    "start": 1631378,
    "end": 1634738,
    "text": "私たちの初期化がひどかったとしても、ネットワークはまだ学習していた。"
  },
  {
    "start": 1634834,
    "end": 1636514,
    "text": "結局、少し悪くなっただけだ。"
  },
  {
    "start": 1636562,
    "end": 1637158,
    "text": "結果"
  },
  {
    "start": 1637324,
    "end": 1639402,
    "text": "しかし、これは一般的なケースではない。"
  },
  {
    "start": 1639456,
    "end": 1650154,
    "text": "実際に50層など、もっと深いネットワークを扱うようになると、物事はもっと複雑になり、問題は山積みになる。"
  },
  {
    "start": 1650352,
    "end": 1657102,
    "text": "そのため、初期化が十分でなければ、ネットワークが基本的にまったくトレーニングしないような状態になることもある。"
  },
  {
    "start": 1657236,
    "end": 1662886,
    "text": "ネットワークが深ければ深いほど、複雑であればあるほど、このようなエラーは許されない。"
  },
  {
    "start": 1663018,
    "end": 1671060,
    "text": "だから、間違いなく注意すべきこと、精査すべきこと、企図すべきこと、注意すべきことがある。"
  },
  {
    "start": 1673430,
    "end": 1675506,
    "text": "なるほど、それでうまくいったのは素晴らしいことだ。"
  },
  {
    "start": 1675608,
    "end": 1679042,
    "text": "今ここにあるのは、ゼロや2といった魔法の数字だ。"
  },
  {
    "start": 1679096,
    "end": 1680462,
    "text": "どこでこんなことを思いつくんだろう？"
  },
  {
    "start": 1680536,
    "end": 1684710,
    "text": "たくさんのレイヤーを持つ大規模なニューラルネットがある場合、どのように設定すればいいのでしょうか？"
  },
  {
    "start": 1685130,
    "end": 1687430,
    "text": "だから、手作業でやる人はいない。"
  },
  {
    "start": 1687500,
    "end": 1693706,
    "text": "実は、このスケールを設定する比較的原則的な方法がいくつかあるので、これから紹介したい。"
  },
  {
    "start": 1693888,
    "end": 1698380,
    "text": "ここで、この議論への動機付けのために用意したコードを貼り付けておこう。"
  },
  {
    "start": 1699310,
    "end": 1705050,
    "text": "ここでやっているのは、ガウシアンから引き出されたランダムな入力、xがあるということだ。"
  },
  {
    "start": 1705210,
    "end": 1708506,
    "text": "10次元の例は1000通りある。"
  },
  {
    "start": 1708618,
    "end": 1714446,
    "text": "次に、重み付けレイヤーがあり、これもここでやったようにガウシアンを使って初期化される。"
  },
  {
    "start": 1714628,
    "end": 1721390,
    "text": "隠れ層のこれらのニューロンは10個の入力を見ており、この隠れ層には200個のニューロンがある。"
  },
  {
    "start": 1721550,
    "end": 1730230,
    "text": "とすると、ここでも、この場合と同じように、xとwの掛け算で、これらのニューロンのプレアクティベーションが得られる。"
  },
  {
    "start": 1730650,
    "end": 1737026,
    "text": "基本的に、ここでの分析は、OK、これらが一様なガウシアンであり、これらの重みが一様なガウシアンであると仮定する。"
  },
  {
    "start": 1737138,
    "end": 1746678,
    "text": "x×wとした場合、バイアスや非線形性はひとまず忘れるとして、これらのガウシアンの平均と標準偏差はいくらか？"
  },
  {
    "start": 1746854,
    "end": 1753358,
    "text": "つまり、ここでは、入力は正規のガウス分布で、平均はゼロ、標準偏差は1である。"
  },
  {
    "start": 1753444,
    "end": 1757470,
    "text": "標準偏差は、繰り返しますが、このガウス分布の広がりを示す指標にすぎません。"
  },
  {
    "start": 1758370,
    "end": 1765550,
    "text": "ここで掛け算をしてyのヒストグラムを見ると、もちろん平均値は変わらない。"
  },
  {
    "start": 1765620,
    "end": 1768686,
    "text": "これは対称的な操作なので、ゼロに等しい。"
  },
  {
    "start": 1768798,
    "end": 1772290,
    "text": "標準偏差が3まで拡大していることがわかる。"
  },
  {
    "start": 1772440,
    "end": 1776082,
    "text": "入力の標準偏差は1だったが、今は3になっている。"
  },
  {
    "start": 1776216,
    "end": 1779990,
    "text": "ヒストグラムを見ると、このガウシアンが拡大していることがわかる。"
  },
  {
    "start": 1780970,
    "end": 1785618,
    "text": "つまり、入力からこのガウシアンを展開しているのだ。"
  },
  {
    "start": 1785714,
    "end": 1786694,
    "text": "そんなことは望んでいない。"
  },
  {
    "start": 1786732,
    "end": 1793642,
    "text": "ニューラルネットの大部分は比較的同じような活性を持つようにしたいので、ニューラルネット全体でガウス単位を大まかに設定する。"
  },
  {
    "start": 1793776,
    "end": 1803150,
    "text": "問題は、この分布を維持し、ガウス分布であり続けるために、これらのwをどのようにスケーリングするかということだ。"
  },
  {
    "start": 1803650,
    "end": 1815738,
    "text": "だから、直感的に、ここでwの要素にもっと大きな数、たとえば5倍をかけると、このガウスは大きくなり、標準偏差が大きくなる。"
  },
  {
    "start": 1815834,
    "end": 1817122,
    "text": "今は15歳だ。"
  },
  {
    "start": 1817256,
    "end": 1822190,
    "text": "つまり、基本的に、この出力Yの数値はどんどん極端な値になっていく。"
  },
  {
    "start": 1822350,
    "end": 1830834,
    "text": "これを例えばゼロ2のように縮小すると、逆にこのガウシアンはどんどん小さくなり、縮小していく。"
  },
  {
    "start": 1830962,
    "end": 1833638,
    "text": "標準偏差が0.6であることがわかるだろう。"
  },
  {
    "start": 1833804,
    "end": 1840460,
    "text": "そこで質問だが、標準偏差を正確に1に保つには、ここに何を掛ければいいのだろうか？"
  },
  {
    "start": 1840830,
    "end": 1852650,
    "text": "この掛け算の分散を計算すると、数学的に正しい答えは、ファンの平方根で割ることになる。"
  },
  {
    "start": 1852800,
    "end": 1857822,
    "text": "ファン・インは基本的に入力エレメントの数で、ここでは10である。"
  },
  {
    "start": 1857956,
    "end": 1860570,
    "text": "を10の平方根で割ることになっている。"
  },
  {
    "start": 1860730,
    "end": 1862506,
    "text": "これは平方根を求める一つの方法である。"
  },
  {
    "start": 1862538,
    "end": 1864274,
    "text": "それをゼロの5乗にするんだ。"
  },
  {
    "start": 1864392,
    "end": 1866878,
    "text": "これは平方根と同じだ。"
  },
  {
    "start": 1867054,
    "end": 1877282,
    "text": "を10の平方根で割ると、出力注意の標準偏差は1であることがわかる。"
  },
  {
    "start": 1877416,
    "end": 1880758,
    "text": "今、当然のことながら、多くの論文がその方法を研究している。"
  },
  {
    "start": 1880924,
    "end": 1883234,
    "text": "ニューラルネットワークを最適に初期化する。"
  },
  {
    "start": 1883362,
    "end": 1888950,
    "text": "多層パーセプトロンの場合、このような非線形性を間に挟んだ、かなり深いネットワークができる。"
  },
  {
    "start": 1889100,
    "end": 1895082,
    "text": "アクティべーションがお行儀よく、無限大に拡大したり、ゼロまで縮小したりしないようにしたい。"
  },
  {
    "start": 1895216,
    "end": 1900490,
    "text": "問題は、どのように重みを初期化すれば、これらの活性化がネットワーク全体で妥当な値になるのか、ということだ。"
  },
  {
    "start": 1900910,
    "end": 1909162,
    "text": "さて、これをかなり詳細に研究した論文で、よく参照されるものに、整流器を深く掘り下げるというカイミングらの論文がある。"
  },
  {
    "start": 1909306,
    "end": 1919678,
    "text": "この場合、彼らはコンボリューション・ニューラル・ネットワークを研究し、特に10hの非直線性の代わりにReluの非直線性とpreluの非直線性を研究する。"
  },
  {
    "start": 1919774,
    "end": 1921522,
    "text": "分析は非常に似ている。"
  },
  {
    "start": 1921656,
    "end": 1935558,
    "text": "基本的に、ここで何が起こるかというと、彼らにとっては、ここでかなり重要視しているレル直列性は、すべての負の数が単にゼロにクランプされるスカッシング関数なのだ。"
  },
  {
    "start": 1935724,
    "end": 1940040,
    "text": "正の数はパススルーだが、負の数はすべてゼロにセットされる。"
  },
  {
    "start": 1940490,
    "end": 1951050,
    "text": "というのも、基本的に分布の半分を捨てているのだから、ニューラルネットの前方活性化を分析した結果、ゲインでそれを補う必要があることがわかった。"
  },
  {
    "start": 1951950,
    "end": 1962990,
    "text": "そこで彼らは、基本的にウエイトを初期化するとき、ゼロ平均のガウシアンで初期化しなければならないことを発見した。"
  },
  {
    "start": 1963330,
    "end": 1968686,
    "text": "ここでは、ガウシアンをファニンの平方根で初期化している。"
  },
  {
    "start": 1968878,
    "end": 1970578,
    "text": "このNLがファン・インだ。"
  },
  {
    "start": 1970664,
    "end": 1977060,
    "text": "というのも、ここで割り算をしているからだ。"
  },
  {
    "start": 1977990,
    "end": 1985222,
    "text": "今、彼らは、基本的に分布の半分を捨て、ゼロで固定するreluのために、この係数2を追加しなければならない。"
  },
  {
    "start": 1985356,
    "end": 1987666,
    "text": "そこで、さらなる要素が加わることになる。"
  },
  {
    "start": 1987858,
    "end": 1997526,
    "text": "さて、それに加えてこの論文では、ニューラルネットのフォワードパスにおけるアクティベーションの挙動だけでなく、バックプロパゲーションについても研究している。"
  },
  {
    "start": 1997638,
    "end": 2005386,
    "text": "最終的にはパラメーターを更新することになるからだ。"
  },
  {
    "start": 2005578,
    "end": 2009534,
    "text": "その分析結果は、ぜひご一読いただきたい。"
  },
  {
    "start": 2009572,
    "end": 2011530,
    "text": "親しみやすいとは言い難い。"
  },
  {
    "start": 2011690,
    "end": 2028070,
    "text": "彼らが発見したのは、基本的に、フォワード・パスを適切に初期化すれば、バックワード・パスも初期化されるということだ。"
  },
  {
    "start": 2029610,
    "end": 2033654,
    "text": "基本的には、これはあまり重要な選択ではないことが経験的に判明している。"
  },
  {
    "start": 2033852,
    "end": 2037922,
    "text": "さて、このチーミング初期化はPytorchにも実装されている。"
  },
  {
    "start": 2037986,
    "end": 2042214,
    "text": "torch nn init documentationを見れば、チャイムが鳴るのは普通だとわかるだろう。"
  },
  {
    "start": 2042342,
    "end": 2046860,
    "text": "私の考えでは、これが現在ニューラルネットワークを初期化する最も一般的な方法だろう。"
  },
  {
    "start": 2047230,
    "end": 2049594,
    "text": "ここではいくつかのキーワード引数を取る。"
  },
  {
    "start": 2049712,
    "end": 2052746,
    "text": "つまり、第一にモードを知りたがっている。"
  },
  {
    "start": 2052858,
    "end": 2056670,
    "text": "アクティベーションを正規化しますか、それともグラデーションを正規化しますか？"
  },
  {
    "start": 2057010,
    "end": 2062442,
    "text": "平均がゼロ、単位または1標準偏差で常に注意すること。"
  },
  {
    "start": 2062586,
    "end": 2071170,
    "text": "というのも、論文によると、これはあまり重要ではなく、ほとんどの人がデフォルトのままにしているからだ。"
  },
  {
    "start": 2071320,
    "end": 2075726,
    "text": "非直線性によって、微妙に異なるゲインを計算する必要があるからだ。"
  },
  {
    "start": 2075838,
    "end": 2082246,
    "text": "つまり、非直線性がリニアなだけで、非直線性がなければ、ここでのゲインは1になる。"
  },
  {
    "start": 2082348,
    "end": 2085958,
    "text": "私たちはここで得たのとまったく同じような方程式を持っている。"
  },
  {
    "start": 2086124,
    "end": 2089554,
    "text": "非直線性が他のものであれば、ゲインはわずかに異なるだろう。"
  },
  {
    "start": 2089682,
    "end": 2096106,
    "text": "例えば、レルーの場合、この利益は2の平方根である。"
  },
  {
    "start": 2096208,
    "end": 2106042,
    "text": "なぜ平方根かというと、この論文では2が平方根の内側にあるからだ。"
  },
  {
    "start": 2106106,
    "end": 2108560,
    "text": "ゲインは2の平方根である。"
  },
  {
    "start": 2108930,
    "end": 2113502,
    "text": "リニアやアイデンティティの場合は、利得が1になるだけだ。"
  },
  {
    "start": 2113636,
    "end": 2118466,
    "text": "ここで使っているのは10時間プレーの場合だが、その場合の利得は5オーバー3となる。"
  },
  {
    "start": 2118648,
    "end": 2122382,
    "text": "直感的に、なぜ初期化の上にゲインが必要なのか？"
  },
  {
    "start": 2122526,
    "end": 2127214,
    "text": "レル同様、ten hも縮約変換だからだ。"
  },
  {
    "start": 2127342,
    "end": 2133446,
    "text": "つまり、行列の掛け算から出力された分布を、何らかの方法でつぶしているのだ。"
  },
  {
    "start": 2133548,
    "end": 2140546,
    "text": "さて、Reluはゼロ以下のすべてを0:10にクランプすることによって、それをつぶしている。"
  },
  {
    "start": 2140578,
    "end": 2144154,
    "text": "尾を引いて、絞り込む。"
  },
  {
    "start": 2144272,
    "end": 2153078,
    "text": "そのため、絞り込みと戦うためには、ウェイトを少し上げて、すべてを標準単位標準偏差に戻す必要がある。"
  },
  {
    "start": 2153254,
    "end": 2156318,
    "text": "だから、ちょっとだけゲインが出るんだ。"
  },
  {
    "start": 2156484,
    "end": 2160682,
    "text": "今、私はこのセクションを少し早足で読み飛ばしているが、実は意図的にそうしている。"
  },
  {
    "start": 2160826,
    "end": 2176914,
    "text": "というのも、この論文が書かれた7年ほど前は、活性度や勾配、その範囲やヒストグラムに細心の注意を払い、ゲインの正確な設定や使用する非線形性の精査などに細心の注意を払わなければならなかったからだ。"
  },
  {
    "start": 2177032,
    "end": 2184594,
    "text": "特にニューラルネットが非常に深い場合、ニューラルネットが学習するためには、すべてが非常に繊細で非常に壊れやすく、適切に配置されていなければならなかった。"
  },
  {
    "start": 2184722,
    "end": 2189490,
    "text": "現代の技術革新は、あらゆるものをより安定させ、より良いものにした。"
  },
  {
    "start": 2189570,
    "end": 2193594,
    "text": "これらのネットワークを正確に初期化することの重要性は低くなっている。"
  },
  {
    "start": 2193792,
    "end": 2206906,
    "text": "例えば、残留コネクション、バッチ正規化、グループ正規化などの正規化レイヤーの使用などだ。"
  },
  {
    "start": 2207018,
    "end": 2208766,
    "text": "これらもたくさん見ていくつもりだ。"
  },
  {
    "start": 2208868,
    "end": 2219454,
    "text": "その3は、より優れたオプティマイザーで、確率的勾配降下法、つまりここで基本的に使っている単純なオプティマイザーだけでなく、RMspropや特にAdamのような少し複雑なオプティマイザーもある。"
  },
  {
    "start": 2219582,
    "end": 2225858,
    "text": "このように、現代の技術革新はすべて、ニューラルネットの初期化を正確に較正することの重要性を低くしている。"
  },
  {
    "start": 2226024,
    "end": 2231714,
    "text": "とはいえ、実際には、ニューラルネットを初期化するときにどうすればいいのか。"
  },
  {
    "start": 2231762,
    "end": 2235638,
    "text": "私は基本的に、ウエイトをファン・インの平方根で正規化するだけだ。"
  },
  {
    "start": 2235804,
    "end": 2249402,
    "text": "つまり、基本的に、ここでやったことは、今私がやっていることであり、もしここで正確を期したいのであれば、チーミングノーマルで行くのであれば、このように実装することになる。"
  },
  {
    "start": 2249536,
    "end": 2255166,
    "text": "標準偏差はファンnの平方根のゲインになるように設定したいんだよね？"
  },
  {
    "start": 2255348,
    "end": 2260590,
    "text": "そこで、ウエイトの標準偏差を設定するために、次のように進める。"
  },
  {
    "start": 2261170,
    "end": 2269870,
    "text": "基本的に、レニンを燃やして1000個の数字を作ったとすると、その標準偏差を見ることができる。"
  },
  {
    "start": 2269950,
    "end": 2272114,
    "text": "これをもう少し大きくして、1つに近づけよう。"
  },
  {
    "start": 2272232,
    "end": 2277778,
    "text": "これは、平均がゼロで単位標準偏差のガウス分布の広がりである。"
  },
  {
    "start": 2277954,
    "end": 2286774,
    "text": "さて、基本的に、これらを例えばゼロ2倍すると、基本的にガウシアンはスケールダウンし、標準偏差はゼロ2になる。"
  },
  {
    "start": 2286892,
    "end": 2291878,
    "text": "つまり、基本的には、ここで掛け合わせた数値が、このガウシアンの標準偏差ということになる。"
  },
  {
    "start": 2292054,
    "end": 2298874,
    "text": "ここで、rw 1をサンプリングすると、これは標準偏差ゼロの2ガウスになる。"
  },
  {
    "start": 2299072,
    "end": 2305520,
    "text": "標準偏差をファン・モードの平方根（ファンn）以上のゲインに設定したい。"
  },
  {
    "start": 2305890,
    "end": 2313300,
    "text": "つまり、ゲインを掛け算するわけだ。"
  },
  {
    "start": 2314230,
    "end": 2316050,
    "text": "5オーバー3がゲイン。"
  },
  {
    "start": 2316550,
    "end": 2331014,
    "text": "でファンの平方根を割る。"
  },
  {
    "start": 2331212,
    "end": 2333638,
    "text": "この例では、ファンが10人だった。"
  },
  {
    "start": 2333724,
    "end": 2341622,
    "text": "ここで気づいたのだが、w1のファン・インはエンベデッド倍ブロック・サイズになっている。"
  },
  {
    "start": 2341756,
    "end": 2343958,
    "text": "それは、それぞれのキャラクターが十人十色だからだ。"
  },
  {
    "start": 2343974,
    "end": 2345882,
    "text": "で、それを3つ連結する。"
  },
  {
    "start": 2345936,
    "end": 2347850,
    "text": "だから、実はここのファンは30歳だったんだ。"
  },
  {
    "start": 2347920,
    "end": 2349850,
    "text": "ここで30を使うべきだったかもしれない。"
  },
  {
    "start": 2350000,
    "end": 2353014,
    "text": "基本的には、30の平方根が欲しい。"
  },
  {
    "start": 2353142,
    "end": 2356910,
    "text": "これがこの数字で、これが標準偏差だ。"
  },
  {
    "start": 2356980,
    "end": 2359214,
    "text": "この数字はゼロ3であることが判明した。"
  },
  {
    "start": 2359412,
    "end": 2365650,
    "text": "一方、ここでは、ただいじくりまわして分布を見て、問題なさそうだと確認しただけで、ゼロ2となった。"
  },
  {
    "start": 2365800,
    "end": 2374922,
    "text": "その代わりに、ここでやりたいことは、標準偏差を3より5にすることだ。"
  },
  {
    "start": 2375006,
    "end": 2381042,
    "text": "この金額をゼロ2の平方根で割る。"
  },
  {
    "start": 2381186,
    "end": 2385954,
    "text": "この括弧はそれほど必要ではないが、わかりやすくするためにここに書いておく。"
  },
  {
    "start": 2386082,
    "end": 2387430,
    "text": "これは基本的に我々が望んでいることだ。"
  },
  {
    "start": 2387500,
    "end": 2392006,
    "text": "これは、私たちの場合、10時間の非直線性についてのチーミングである。"
  },
  {
    "start": 2392118,
    "end": 2394458,
    "text": "これがニューラルネットを初期化する方法である。"
  },
  {
    "start": 2394624,
    "end": 2400140,
    "text": "ということは、ゼロ2を掛ける代わりにゼロ3を掛けることになる。"
  },
  {
    "start": 2400910,
    "end": 2407822,
    "text": "このようにして初期化し、ニューラルネットを訓練して結果を見ることができる。"
  },
  {
    "start": 2407956,
    "end": 2412090,
    "text": "さて、ニューラルネットをトレーニングしてみたところ、ほぼ同じ場所に行き着いた。"
  },
  {
    "start": 2412170,
    "end": 2415038,
    "text": "検証の結果、損失は2.10となった。"
  },
  {
    "start": 2415124,
    "end": 2417342,
    "text": "以前は2.10もあった。"
  },
  {
    "start": 2417476,
    "end": 2421150,
    "text": "多少の差はあるが、それはプロセスのランダム性なのだろう。"
  },
  {
    "start": 2421310,
    "end": 2424270,
    "text": "もちろん、大事なのは同じ場所にたどり着くことだ。"
  },
  {
    "start": 2424350,
    "end": 2432338,
    "text": "ヒストグラムを見たり、チェックを推測したりすることで得られるマジックナンバーを導入する必要はなかった。"
  },
  {
    "start": 2432434,
    "end": 2440018,
    "text": "半原則的なもので、もっと大きなネットワークに拡張できるもの、そしてガイドとして使えるものがある。"
  },
  {
    "start": 2440114,
    "end": 2445798,
    "text": "これらの初期化の正確な設定は、現代のいくつかの技術革新により、今日それほど重要ではないことを述べた。"
  },
  {
    "start": 2445894,
    "end": 2450838,
    "text": "今こそ、そのような現代的なイノベーションのひとつ、バッチ・ノーマライゼーションを導入する絶好の機会だと思う。"
  },
  {
    "start": 2451014,
    "end": 2464882,
    "text": "バッチ正規化は2015年にグーグルのチームから発表されたもので、非常にインパクトのある論文だった。"
  },
  {
    "start": 2465016,
    "end": 2467780,
    "text": "バッチ正規化が何をするのか、それを実装してみよう。"
  },
  {
    "start": 2469830,
    "end": 2472130,
    "text": "基本的に、私たちにはこのような隠れた状態がある。"
  },
  {
    "start": 2472200,
    "end": 2487218,
    "text": "その通りだ。私たちは、これらの活性化前の状態が小さすぎるのは困ると話していたんだ。"
  },
  {
    "start": 2487394,
    "end": 2490422,
    "text": "実際には、ほぼガウシアンであってほしい。"
  },
  {
    "start": 2490486,
    "end": 2495450,
    "text": "少なくとも初期化時には、平均がゼロ、単位が1、または標準偏差が1である。"
  },
  {
    "start": 2495950,
    "end": 2508110,
    "text": "バッシュの正規化論文からの洞察は、隠れた状態があって、それがだいたいガウスになるようにしたいのなら、隠れた状態をガウスになるように正規化すればいいじゃないか、というものだ。"
  },
  {
    "start": 2508690,
    "end": 2519426,
    "text": "というのも、単位がガウシアンになるように隠れた状態を標準化することは、すぐにわかるように、完全に微分可能な操作だからだ。"
  },
  {
    "start": 2519528,
    "end": 2521954,
    "text": "それがこの論文の大きな洞察のようなものだ。"
  },
  {
    "start": 2522072,
    "end": 2526242,
    "text": "最初にこれを読んだとき、私の頭は真っ白になった。"
  },
  {
    "start": 2526296,
    "end": 2533970,
    "text": "もしネットワークに単位ガウス状態が必要なら、少なくとも初期化では、単位ガウスになるように正規化すればいい。"
  },
  {
    "start": 2534130,
    "end": 2536118,
    "text": "どうなるか見てみよう。"
  },
  {
    "start": 2536284,
    "end": 2541078,
    "text": "10時間目に入る直前のプレアクトまでスクロールする。"
  },
  {
    "start": 2541244,
    "end": 2544742,
    "text": "ここでもう一度考えてほしいのは、私たちはこれをほぼガウシアンにしようとしているということだ。"
  },
  {
    "start": 2544886,
    "end": 2550150,
    "text": "というのも、もしこの数字が小さすぎるのであれば、ここでの10hはある意味不活発な数字になってしまうからだ。"
  },
  {
    "start": 2550310,
    "end": 2556266,
    "text": "これらが非常に大きな数字である場合、10時間では飽和しすぎており、流れに勾配がある。"
  },
  {
    "start": 2556378,
    "end": 2558590,
    "text": "私たちは、これがほぼガウシアンであることを望んでいる。"
  },
  {
    "start": 2559010,
    "end": 2564238,
    "text": "バッチ正規化における洞察は、これらのアクティベーションを標準化すればいいということだ。"
  },
  {
    "start": 2564334,
    "end": 2566558,
    "text": "これらは正確にガウシアンである。"
  },
  {
    "start": 2566734,
    "end": 2575646,
    "text": "ここでhpriactは、32×232例×200ニューロンの隠れ層の形状を持つ。"
  },
  {
    "start": 2575838,
    "end": 2580360,
    "text": "基本的に私たちができることは、Hpractをとって平均を計算することです。"
  },
  {
    "start": 2581130,
    "end": 2591100,
    "text": "ゼロ次元をまたいで計算したい平均値をtrueとして保持し、これを簡単に放送できるようにしたい。"
  },
  {
    "start": 2591470,
    "end": 2594762,
    "text": "この形状は200分の1である。"
  },
  {
    "start": 2594816,
    "end": 2600010,
    "text": "つまり、バッチ内の全要素の平均をとっているのである。"
  },
  {
    "start": 2600370,
    "end": 2605230,
    "text": "同様に、これらのアクティベーションの標準偏差を計算することができる。"
  },
  {
    "start": 2606770,
    "end": 2608880,
    "text": "これも200分の1になる。"
  },
  {
    "start": 2609250,
    "end": 2614370,
    "text": "さて、この論文の中で、彼らは処方箋のようなものを示している。"
  },
  {
    "start": 2614520,
    "end": 2623582,
    "text": "ここでは平均を計算しているが、これは単にニューロンの活性化の平均値を取っているだけである。"
  },
  {
    "start": 2623726,
    "end": 2634934,
    "text": "標準偏差は、基本的には、これまで使ってきたスプレッドの尺度のようなもので、平均値からの各値の距離である。"
  },
  {
    "start": 2635052,
    "end": 2641414,
    "text": "それを2乗して平均したものが分散だ。"
  },
  {
    "start": 2641542,
    "end": 2647050,
    "text": "標準偏差を求める場合は、分散を平方根して標準偏差を求めます。"
  },
  {
    "start": 2647630,
    "end": 2649970,
    "text": "この2つを計算している。"
  },
  {
    "start": 2650070,
    "end": 2657390,
    "text": "では、平均を引き、標準偏差で割ることによって、これらのxを正規化または標準化することにしよう。"
  },
  {
    "start": 2657730,
    "end": 2672930,
    "text": "基本的には、Hのプリ・アクトから意味を引いて、標準偏差で割る。"
  },
  {
    "start": 2674230,
    "end": 2677970,
    "text": "これこそが、この2つの標準偏差と平均値を計算していることなのだ。"
  },
  {
    "start": 2678310,
    "end": 2680862,
    "text": "おっと、申し訳ない。"
  },
  {
    "start": 2680936,
    "end": 2682930,
    "text": "これが平均で、これが分散である。"
  },
  {
    "start": 2683010,
    "end": 2685414,
    "text": "シグマが標準偏差であることはおわかりだろう。"
  },
  {
    "start": 2685452,
    "end": 2689430,
    "text": "これはシグマ2乗で、分散は標準偏差の2乗である。"
  },
  {
    "start": 2690730,
    "end": 2692950,
    "text": "これが、これらの値を標準化する方法である。"
  },
  {
    "start": 2693030,
    "end": 2701674,
    "text": "そうすることで、すべてのニューロンの発火率が、少なくともこの32の例では、正確に単位ガウスになる。"
  },
  {
    "start": 2701722,
    "end": 2703146,
    "text": "だからバッチ・ノーマライゼーションと呼ばれるのだ。"
  },
  {
    "start": 2703258,
    "end": 2709438,
    "text": "我々はこれらのバッチを正規化しており、そうすれば原理的にはこれをトレーニングすることができる。"
  },
  {
    "start": 2709524,
    "end": 2713706,
    "text": "平均と標準偏差の計算は、単なる数式であることに注意してほしい。"
  },
  {
    "start": 2713738,
    "end": 2715050,
    "text": "完全に微分可能だ。"
  },
  {
    "start": 2715130,
    "end": 2718260,
    "text": "このすべては完全に微分可能であり、これを訓練すればいい。"
  },
  {
    "start": 2718630,
    "end": 2722866,
    "text": "問題は、これではあまり良い結果が得られないということだ。"
  },
  {
    "start": 2722968,
    "end": 2729190,
    "text": "その理由は、初期化時のみだが、ほぼガウスになるようにしたいからだ。"
  },
  {
    "start": 2729610,
    "end": 2732914,
    "text": "ガウシアンであることを強制されたくない。"
  },
  {
    "start": 2733042,
    "end": 2745206,
    "text": "私たちは常に、ニューラル・ネットがこれを動かせるようにしたいのです。もっと拡散させたり、もっと鋭くしたり、10時間のニューラル・ネットをもっとトリガーにしたり、もっとトリガーハッピーにしたり、もっとトリガーハッピーにしないようにしたり。"
  },
  {
    "start": 2745308,
    "end": 2751760,
    "text": "そして、分布がどのように動き回るべきかを逆伝播に教えてもらいたい。"
  },
  {
    "start": 2752290,
    "end": 2765510,
    "text": "そのため、ネットワーク内のどの点でもアクティベーションを標準化するというこのアイデアに加え、スケールとシフトという追加要素を導入する必要がある。"
  },
  {
    "start": 2765690,
    "end": 2777426,
    "text": "つまり、基本的にやっていることは、正規化された入力を受け取って、さらにゲインでスケーリングし、バイアスでオフセットして、このレイヤーからの最終出力を得ているということだ。"
  },
  {
    "start": 2777618,
    "end": 2780098,
    "text": "つまり、次のようなことだ。"
  },
  {
    "start": 2780274,
    "end": 2786920,
    "text": "バッチ正規化ゲインを一度だけ初期化できるようにする。"
  },
  {
    "start": 2787370,
    "end": 2791050,
    "text": "は、1×nの隠された形になる。"
  },
  {
    "start": 2792270,
    "end": 2797210,
    "text": "その場合、バイアスはbnとなり、ゼロで焼かれることになる。"
  },
  {
    "start": 2797550,
    "end": 2801550,
    "text": "また、n×1×nの隠れる形になる。"
  },
  {
    "start": 2802050,
    "end": 2810080,
    "text": "するとここで、BNゲインがこれを乗算し、BNバイアスがここで相殺する。"
  },
  {
    "start": 2811090,
    "end": 2829714,
    "text": "初期化時にこの値は1に、この値は0に初期化されるので、このバッチ内の各ニューロンの発火値は正確に単位ガウスになり、h事前作用の分布がどのようなものであっても、各ニューロンについて単位ガウスになる。"
  },
  {
    "start": 2829762,
    "end": 2832310,
    "text": "というのが、少なくとも初期化の時点では、私たちが望んでいることだ。"
  },
  {
    "start": 2833770,
    "end": 2839338,
    "text": "そうすれば、最適化中にBngainとBnBasにバックプロパゲートして変更することができる。"
  },
  {
    "start": 2839424,
    "end": 2854510,
    "text": "バックプロパゲーションで学習させるので、ニューラルネットのパラメータにこれらを含めるようにするだけでよい。"
  },
  {
    "start": 2855490,
    "end": 2859680,
    "text": "これを初期化して、トレーニングできるようにしよう。"
  },
  {
    "start": 2865490,
    "end": 2873362,
    "text": "そしてこの行もコピーする。これは、1行のコードで最高の正規化レイヤーである。"
  },
  {
    "start": 2873496,
    "end": 2878680,
    "text": "ここでスイングダウンし、テストタイムでもまったく同じことをする。"
  },
  {
    "start": 2881610,
    "end": 2889510,
    "text": "列車時間と同様に、正規化し、スケーリングすることで、列車と検証ロスが得られる。"
  },
  {
    "start": 2890570,
    "end": 2893090,
    "text": "ちょっと変えてみよう。"
  },
  {
    "start": 2893100,
    "end": 2895194,
    "text": "今のところ、このままにしておくつもりだ。"
  },
  {
    "start": 2895392,
    "end": 2897446,
    "text": "このまま収束するのを待つつもりだ。"
  },
  {
    "start": 2897558,
    "end": 2899882,
    "text": "それで、ニューラルネットがここで収束するようにした。"
  },
  {
    "start": 2899936,
    "end": 2906142,
    "text": "下にスクロールすると、検証の損失が2.10であることがわかる。"
  },
  {
    "start": 2906276,
    "end": 2910906,
    "text": "この結果は、私たちが以前に達成したいくつかの結果に匹敵する。"
  },
  {
    "start": 2911098,
    "end": 2919230,
    "text": "というのも、今回扱っているのは、隠れ層が1層だけの非常にシンプルなニューラルネットだからだ。"
  },
  {
    "start": 2919390,
    "end": 2927250,
    "text": "つまり、隠れ層が1つだけという非常に単純なケースで、wのスケールをどの程度にすべきかを実際に計算することができたのだ。"
  },
  {
    "start": 2927320,
    "end": 2930162,
    "text": "これらの事前活性化は、すでにほぼガウス型をしている。"
  },
  {
    "start": 2930226,
    "end": 2932918,
    "text": "bashの正規化はここではあまり機能していない。"
  },
  {
    "start": 2933084,
    "end": 2952410,
    "text": "さまざまな種類の演算があり、例えば残差接続などもあるような、より深いニューラルネットになると、ニューラルネット全体の活性がほぼガウスになるように、重み行列のスケールを調整するのは基本的に非常に難しくなります。"
  },
  {
    "start": 2952830,
    "end": 2955770,
    "text": "そうなると、あっという間に手に負えなくなる。"
  },
  {
    "start": 2955930,
    "end": 2961790,
    "text": "それに比べれば、ニューラルネット全体にバッチ正規化レイヤーを散りばめるのはずっとずっと簡単だろう。"
  },
  {
    "start": 2961940,
    "end": 2966962,
    "text": "特に、このようにリニアのレイヤーをひとつひとつ見ていくのが一般的だ。"
  },
  {
    "start": 2967016,
    "end": 2979590,
    "text": "これは、重み行列を掛け合わせ、バイアスを加える線形レイヤーである。あるいは、例えばコンボリューション（畳み込み）も後で取り上げるが、基本的には重み行列との掛け算を行うが、より空間的に構造化されたフォーマットである。"
  },
  {
    "start": 2980810,
    "end": 2991554,
    "text": "このような線形レイヤー（畳み込みレイヤー）を取り、その直後にバッシュ正規化レイヤーを追加して、ニューラルネットの各ポイントでこれらのアクティベーションのスケールをコントロールするのが通例である。"
  },
  {
    "start": 2991692,
    "end": 2998362,
    "text": "ニューラルネット全体にバッシュルーム・レイヤーを追加し、ニューラルネット全体のアクティベーションのスケールをコントロールする。"
  },
  {
    "start": 2998496,
    "end": 3009214,
    "text": "ニューラルネットワークに導入したい、さまざまなタイプのニューラルネットワークのレゴブロックの活性化分布を気にしたり、完璧な数学をする必要はない。"
  },
  {
    "start": 3009332,
    "end": 3012126,
    "text": "トレーニングが大幅に安定する。"
  },
  {
    "start": 3012228,
    "end": 3015026,
    "text": "だからこのレイヤーは今、かなり人気がある。"
  },
  {
    "start": 3015048,
    "end": 3018706,
    "text": "バッシュの正規化によってもたらされる安定性は、実はとんでもない代償を伴う。"
  },
  {
    "start": 3018888,
    "end": 3025780,
    "text": "その代償は、ここで起きていることを考えれば、何かひどく奇妙で不自然なことが起きているということだ。"
  },
  {
    "start": 3026310,
    "end": 3034050,
    "text": "以前は、ニューラルネットに入力される1つの例があって、その活性度とロジットを計算していた。"
  },
  {
    "start": 3034210,
    "end": 3037158,
    "text": "これは決定論的なプロセスの一種である。"
  },
  {
    "start": 3037324,
    "end": 3039814,
    "text": "を使うと、この例ではいくつかのロジットが得られる。"
  },
  {
    "start": 3040012,
    "end": 3044742,
    "text": "その後、トレーニングの効率化のために、いきなり例のバッチを使うようになった。"
  },
  {
    "start": 3044806,
    "end": 3049546,
    "text": "これらの例のバッチは独立して処理されており、単なる効率的なものだった。"
  },
  {
    "start": 3049728,
    "end": 3059246,
    "text": "バッチ正規化では、バッチを通して正規化するため、これらの例を数学的に、ニューラルネットのフォワードパスとバックワードパスで結合する。"
  },
  {
    "start": 3059428,
    "end": 3073810,
    "text": "隠れ状態の活性化、hpract、そしてロジットは、どの入力例に対しても、その例とその入力の関数というだけでなく、そのバッチにたまたま入ってきた他のすべての例の関数でもある。"
  },
  {
    "start": 3074310,
    "end": 3076286,
    "text": "これらの例はランダムにサンプリングされている。"
  },
  {
    "start": 3076398,
    "end": 3090150,
    "text": "つまり、例えばHPRを見ると、hに入力される隠しステートのアクティブ度は、どの入力例に対しても、バッチに含まれる他の入力例によって微妙に変化する。"
  },
  {
    "start": 3090730,
    "end": 3103626,
    "text": "他のどのような例がたまたま乗ってくるかによって、hは突然変化し、異なる例をサンプリングすることを想像すれば、平均と標準偏差の統計が影響を受けるので、ジッターのようになりそうだ。"
  },
  {
    "start": 3103818,
    "end": 3107630,
    "text": "だから、hのジッターとロジットのジッターを得ることになる。"
  },
  {
    "start": 3108450,
    "end": 3112314,
    "text": "これはバグか何か好ましくないものだと思うだろう。"
  },
  {
    "start": 3112442,
    "end": 3119506,
    "text": "非常に奇妙なことだが、これはニューラルネットワークのトレーニングにおいて、副次的な効果として良い結果をもたらす。"
  },
  {
    "start": 3119608,
    "end": 3129702,
    "text": "というのも、これは一種のレギュラライザーのようなものだと考えることができるからだ。"
  },
  {
    "start": 3129836,
    "end": 3136354,
    "text": "つまり、入力例のどれかを効果的に水増しし、エントロピーを少し導入しているのだ。"
  },
  {
    "start": 3136482,
    "end": 3142762,
    "text": "水増しされているため、実際にはデータ補強の一種のようなものだ。"
  },
  {
    "start": 3142896,
    "end": 3146810,
    "text": "入力を少し補強するようなもので、ジッターを発生させるんだ。"
  },
  {
    "start": 3146880,
    "end": 3151798,
    "text": "これは、ニューラルネットが具体的な具体例に対してオーバーフィットしにくくするためである。"
  },
  {
    "start": 3151894,
    "end": 3157646,
    "text": "このようなノイズを導入することで、実際に例をパッド化し、ニューラルネットを正則化するのだ。"
  },
  {
    "start": 3157748,
    "end": 3163642,
    "text": "これが、2次効果であるように見せかけて、実は正則化である理由のひとつである。"
  },
  {
    "start": 3163706,
    "end": 3178558,
    "text": "というのも、バッチ内の例が数学的に結合され、フォワードパスで結合されるというこの性質は、基本的に誰も好まない。"
  },
  {
    "start": 3178654,
    "end": 3184678,
    "text": "このことについても少し説明するが、多くのバグなどを引き起こしている。"
  },
  {
    "start": 3184764,
    "end": 3186440,
    "text": "だから、この物件を好きな人はいない。"
  },
  {
    "start": 3186810,
    "end": 3194758,
    "text": "そのため、人々はバッチ正規化の使用を非推奨とし、バッチの例をカップリングしない他の正規化技術に移行しようとしている。"
  },
  {
    "start": 3194854,
    "end": 3199930,
    "text": "例えば、レイヤーの正規化、インスタンスの正規化、グループの正規化などである。"
  },
  {
    "start": 3200000,
    "end": 3202560,
    "text": "これらについては後ほど紹介する。"
  },
  {
    "start": 3204050,
    "end": 3209050,
    "text": "基本的に、長くなるが、バッチ正規化は最初に導入された正規化レイヤーの一種である。"
  },
  {
    "start": 3209130,
    "end": 3210718,
    "text": "非常にうまくいった。"
  },
  {
    "start": 3210884,
    "end": 3213230,
    "text": "それがたまたま、このようなレギュラー化の効果をもたらした。"
  },
  {
    "start": 3213380,
    "end": 3224178,
    "text": "トレーニングを安定させることができたので、人々はそれを取り除き、他のノーマライゼーション技術に移行しようとしてきた。"
  },
  {
    "start": 3224264,
    "end": 3233990,
    "text": "正則化効果があり、活性度とその分布をコントロールするのに非常に効果的だからである。"
  },
  {
    "start": 3234330,
    "end": 3237202,
    "text": "というのは、バッシュ正常化の簡単なストーリーのようなものだ。"
  },
  {
    "start": 3237346,
    "end": 3243430,
    "text": "このカップリングの奇妙な結果のひとつをお見せしよう。"
  },
  {
    "start": 3243590,
    "end": 3250474,
    "text": "以前、検証セットでの敗戦を評価したときに、私がほんの少し触れただけの奇妙な結果のひとつがここにある。"
  },
  {
    "start": 3250672,
    "end": 3261086,
    "text": "基本的には、ニューラルネットをトレーニングしたら、それをある種のセッティングに配備し、個々の例を入力してニューラルネットから予測を得られるようにしたい。"
  },
  {
    "start": 3261268,
    "end": 3269806,
    "text": "フォワードパスでニューラルネットがバッチの平均理解偏差の統計量を推定するとき、ニューラルネットは入力としてバッチを期待する。"
  },
  {
    "start": 3269838,
    "end": 3270274,
    "text": "今すぐだ。"
  },
  {
    "start": 3270392,
    "end": 3273860,
    "text": "どうすれば1つの例を入力し、賢明な結果を得ることができるのか？"
  },
  {
    "start": 3274310,
    "end": 3278546,
    "text": "というわけで、バッチ正規化論文の提案は以下の通りである。"
  },
  {
    "start": 3278738,
    "end": 3291898,
    "text": "ここでやりたいことは、基本的にトレーニングの後に、トレーニング・セットに対して1回だけバッシュの平均と標準偏差を計算して設定するステップを設けたい。"
  },
  {
    "start": 3292064,
    "end": 3298570,
    "text": "そこで、時間の都合上、このコードを書いてみた。バシャームの統計量を校正する、というものだ。"
  },
  {
    "start": 3298910,
    "end": 3306446,
    "text": "基本的には、トーチネット・ノグラードがピトーチに、我々は後方にはドットを呼び出さない。"
  },
  {
    "start": 3306548,
    "end": 3308506,
    "text": "もう少し効率的になるだろう。"
  },
  {
    "start": 3308698,
    "end": 3317858,
    "text": "トレーニングセットを取り出し、すべてのトレーニング例についてプリアクティベーションを取得し、トレーニングセット全体の平均と標準偏差を1回だけ推定する。"
  },
  {
    "start": 3318024,
    "end": 3320830,
    "text": "とすると、bと平均、bと標準偏差が得られることになる。"
  },
  {
    "start": 3320910,
    "end": 3324740,
    "text": "現在、これらはトレーニングセット全体で推定される固定数である。"
  },
  {
    "start": 3325130,
    "end": 3333560,
    "text": "ここでは、動的に推定する代わりに、bn平均を使うことにする。"
  },
  {
    "start": 3334170,
    "end": 3337110,
    "text": "ここではBN標準偏差を使用する。"
  },
  {
    "start": 3337930,
    "end": 3342934,
    "text": "そのため、テスト時にこれらを修正し、クランプして推論時に使用する。"
  },
  {
    "start": 3343062,
    "end": 3348060,
    "text": "これで、基本的に同じ結果が得られることがわかっただろう。"
  },
  {
    "start": 3348830,
    "end": 3357098,
    "text": "というのも、平均と標準偏差が固定したテンソルになったからだ。"
  },
  {
    "start": 3357274,
    "end": 3365486,
    "text": "とはいえ、この平均と標準偏差をニューラルネットワーク学習後の第2段階として推定しようとする人は実際にはいない。"
  },
  {
    "start": 3365598,
    "end": 3376770,
    "text": "つまり、ニューラルネットのトレーニング中に、平均と標準偏差をランニングマターラン方式で推定できるということだ。"
  },
  {
    "start": 3376920,
    "end": 3379926,
    "text": "それなら、単純にトレーニングの段階を1つにすればいい。"
  },
  {
    "start": 3380028,
    "end": 3384306,
    "text": "そのトレーニングの傍らで、走行平均と標準偏差を推定している。"
  },
  {
    "start": 3384418,
    "end": 3386040,
    "text": "それがどんなものか見てみよう。"
  },
  {
    "start": 3386490,
    "end": 3393370,
    "text": "ここでは基本的に、バッチで推定している平均をとり、これを反復のbn平均と呼ぶことにしよう。"
  },
  {
    "start": 3395230,
    "end": 3398090,
    "text": "では、これがBNsTDだ。"
  },
  {
    "start": 3400930,
    "end": 3412702,
    "text": "BNstDはIで、平均はここで、STDはここに来る。"
  },
  {
    "start": 3412836,
    "end": 3414142,
    "text": "だから、今のところ何もしていない。"
  },
  {
    "start": 3414196,
    "end": 3419650,
    "text": "移動して、平均と標準偏差の追加の変数を作り、ここに置いた。"
  },
  {
    "start": 3419720,
    "end": 3421406,
    "text": "だから、今のところ何も変わっていない。"
  },
  {
    "start": 3421518,
    "end": 3426390,
    "text": "これから行うのは、トレーニング中にこれら2つの値の平均を取り続けることだ。"
  },
  {
    "start": 3426540,
    "end": 3431160,
    "text": "ここでスイングして、アンダースコアのBNを作ってみよう。"
  },
  {
    "start": 3431690,
    "end": 3435350,
    "text": "ゼロで初期化するつもりだ。"
  },
  {
    "start": 3435850,
    "end": 3440940,
    "text": "その後、Bnstdを実行し、すぐに初期化する。"
  },
  {
    "start": 3443150,
    "end": 3451126,
    "text": "というのも、最初にw1とb1を初期化したため、hの影響はほぼ単位ガウスになるからだ。"
  },
  {
    "start": 3451158,
    "end": 3454334,
    "text": "平均はほぼゼロ、標準偏差はほぼ1になる。"
  },
  {
    "start": 3454452,
    "end": 3456878,
    "text": "このように初期化するつもりだ。"
  },
  {
    "start": 3457044,
    "end": 3459118,
    "text": "では、これらを更新していこうと思う。"
  },
  {
    "start": 3459284,
    "end": 3467630,
    "text": "Pytorchで実行されている平均と標準偏差は、実際には勾配ベースの最適化の一部ではありません。"
  },
  {
    "start": 3467710,
    "end": 3470354,
    "text": "グラデーションを導き出すつもりはない。"
  },
  {
    "start": 3470552,
    "end": 3472900,
    "text": "彼らはトレーニングの面で更新している。"
  },
  {
    "start": 3473430,
    "end": 3499660,
    "text": "つまり、トーチ・ノグラードがPytorchに、ここでのアップデートはグラフを構築するものではない、なぜなら後方へのダブはないからだ。"
  },
  {
    "start": 3500290,
    "end": 3513600,
    "text": "同じように、BNSTdはほとんど以前のままであるが、現在の標準偏差が何であるかという方向で少し更新される。"
  },
  {
    "start": 3514850,
    "end": 3520958,
    "text": "ここにあるように、このアップデートはグラデーションに基づく最適化の外側で、その側にある。"
  },
  {
    "start": 3521134,
    "end": 3532470,
    "text": "勾配降下法ではなく、ただ単に滑らかな平均値のようなジャンキーな方法で更新されているのだ。"
  },
  {
    "start": 3533130,
    "end": 3545146,
    "text": "そのため、ネットワークがトレーニング中で、バックプロパゲーション中にこれらのプリアクティベーションが変化し、シフトしている間、典型的な平均と標準偏差を追跡し、それらを一度推定しています。"
  },
  {
    "start": 3545328,
    "end": 3551670,
    "text": "今、これを実行するとき、私はこれをランニング形式で記録している。"
  },
  {
    "start": 3551830,
    "end": 3561760,
    "text": "もちろん、BN平均アンダースコア・ランニングとBN平均アンダースコアSTDが、以前ここで計算したものと非常に似ていることを望んでいる。"
  },
  {
    "start": 3562130,
    "end": 3570306,
    "text": "そうすることで、2つ目のステージは必要なくなる。"
  },
  {
    "start": 3570488,
    "end": 3574878,
    "text": "これはPytorchのbash正規化レイヤーでも実装されている方法だ。"
  },
  {
    "start": 3574974,
    "end": 3578774,
    "text": "トレーニング中もまったく同じことが起こる。"
  },
  {
    "start": 3578892,
    "end": 3587080,
    "text": "そして後で推論を使うときには、それらの隠れた状態の平均と標準偏差の両方の推定された実行平均を使う。"
  },
  {
    "start": 3587690,
    "end": 3590082,
    "text": "最適化が収束するのを待とう。"
  },
  {
    "start": 3590146,
    "end": 3593722,
    "text": "走行平均と標準偏差がこの2つにほぼ等しいことを願う。"
  },
  {
    "start": 3593776,
    "end": 3599050,
    "text": "であれば、ここでそれを使うだけでよく、最後に明示的にキャリブレーションを行う段階は必要ない。"
  },
  {
    "start": 3599120,
    "end": 3601050,
    "text": "さて、最適化は終わった。"
  },
  {
    "start": 3601210,
    "end": 3603610,
    "text": "明示的な見積もりをやり直すよ。"
  },
  {
    "start": 3603770,
    "end": 3615540,
    "text": "陽的推定によるBn平均はこちら、最適化中の実行推定によるBn平均はこちらです。"
  },
  {
    "start": 3615990,
    "end": 3620354,
    "text": "同じではないが、同じようにかなり近い。"
  },
  {
    "start": 3620392,
    "end": 3625620,
    "text": "BNSTDはこれであり、BNSTDのランニングはこれである。"
  },
  {
    "start": 3626150,
    "end": 3631320,
    "text": "つまり、同じではないが、かなり近い値であることがわかるだろう。"
  },
  {
    "start": 3631690,
    "end": 3635606,
    "text": "したがって、ここではBn平均の代わりにBn平均を使うことができる。"
  },
  {
    "start": 3635788,
    "end": 3643580,
    "text": "BNSTDの代わりに、BNStDの実行を使えば、うまくいけば、検証のロスはあまり影響しない。"
  },
  {
    "start": 3644350,
    "end": 3646438,
    "text": "オーケー、基本的には同じだね。"
  },
  {
    "start": 3646614,
    "end": 3653914,
    "text": "こうすることで、キャリブレーションという明確な段階を省くことができる。"
  },
  {
    "start": 3654032,
    "end": 3656010,
    "text": "さて、バッチ正規化はほぼ終わった。"
  },
  {
    "start": 3656090,
    "end": 3658334,
    "text": "あと2つだけ注意したいことがある。"
  },
  {
    "start": 3658452,
    "end": 3661966,
    "text": "第一に、このプラス・イプシロンがここで何をしているのかについての議論を飛ばしてしまった。"
  },
  {
    "start": 3662068,
    "end": 3666974,
    "text": "このイプシロンは通常、ある小さな固定数のようなもので、例えばデフォルトでは1eマイナス5である。"
  },
  {
    "start": 3667102,
    "end": 3675682,
    "text": "これは基本的に、バッチの分散が正確にゼロである場合にゼロによる除算を防ぐものである。"
  },
  {
    "start": 3675816,
    "end": 3685394,
    "text": "この場合、通常はゼロによる除算になるが、プラス・イプシロンがあるため、これは代わりに分母の小さな数字になり、物事はよりお行儀よくなる。"
  },
  {
    "start": 3685522,
    "end": 3688998,
    "text": "また、ここにごく小さな数のプラス・イプシロンを加えてもかまわない。"
  },
  {
    "start": 3689084,
    "end": 3691046,
    "text": "実際、結果は大きく変わらない。"
  },
  {
    "start": 3691148,
    "end": 3696010,
    "text": "今回の非常に単純な例では、このようなことは起こりそうもないので省略する。"
  },
  {
    "start": 3696160,
    "end": 3700982,
    "text": "2つ目に気づいてほしいのは、私たちはここで無駄なことをしているということだ。"
  },
  {
    "start": 3701126,
    "end": 3710266,
    "text": "ここでバイアスをhpractに追加しているが、これらのバイアスはhpractに追加しているため、実際には役に立たない。"
  },
  {
    "start": 3710378,
    "end": 3715682,
    "text": "ということは、これらのニューロンの平均を計算し、それを差し引くことになる。"
  },
  {
    "start": 3715816,
    "end": 3720434,
    "text": "ここで加えたバイアスは、ここで差し引かれる。"
  },
  {
    "start": 3720632,
    "end": 3722738,
    "text": "だから、こうしたバイアスは何の役にも立たない。"
  },
  {
    "start": 3722824,
    "end": 3727006,
    "text": "実際、それらは差し引かれており、他の計算には影響しない。"
  },
  {
    "start": 3727118,
    "end": 3733270,
    "text": "というのも、引き算され、実際には何の影響も及ぼさないからだ。"
  },
  {
    "start": 3733420,
    "end": 3744022,
    "text": "だから、バッシュの正規化レイヤーを使うときは、リニアやコンボなどのウェイトレイヤーを使う前に、バイアスを使わないでここに来たほうがいい。"
  },
  {
    "start": 3744166,
    "end": 3750422,
    "text": "バイアスを使いたくないのに、ここではバイアスを加えたくない。"
  },
  {
    "start": 3750566,
    "end": 3761866,
    "text": "その代わりに、ここではbash正規化バイアスがあり、このbash正規化バイアスが、元々ここにあったbの代わりに、この分布のバイアスを担当している。"
  },
  {
    "start": 3762058,
    "end": 3771650,
    "text": "つまり、基本的にバッシュの正規化レイヤーは独自のバイアスを持ち、その前のレイヤーにバイアスを持つ必要はない。"
  },
  {
    "start": 3771800,
    "end": 3773954,
    "text": "これが、気をつけなければならないもうひとつの細かい点だ。"
  },
  {
    "start": 3773992,
    "end": 3776590,
    "text": "致命的なことが起きないこともある。"
  },
  {
    "start": 3776670,
    "end": 3778386,
    "text": "このBは役に立たないだろう。"
  },
  {
    "start": 3778498,
    "end": 3782978,
    "text": "勾配はつかないし、学習もしない。"
  },
  {
    "start": 3783074,
    "end": 3786914,
    "text": "それ以外は特に影響はない。"
  },
  {
    "start": 3787042,
    "end": 3793398,
    "text": "さて、コードをコメントで少し整理したので、bashの正規化レイヤーについて簡単にまとめておこう。"
  },
  {
    "start": 3793574,
    "end": 3799338,
    "text": "ニューラルネットの活性化の統計量を制御するために、bashの正規化を使用している。"
  },
  {
    "start": 3799504,
    "end": 3812400,
    "text": "バッシュ正規化レイヤーはニューラルネット全体に散りばめるのが一般的で、通常は、例えば線形レイヤーや畳み込みレイヤーなど、乗算を行うレイヤーの後に配置する。"
  },
  {
    "start": 3813110,
    "end": 3821330,
    "text": "さて、bashの正規化は内部的にゲインとバイアスのパラメーターを持っており、これらはバックプロパゲーションを使って学習される。"
  },
  {
    "start": 3821670,
    "end": 3824046,
    "text": "また、2つのバッファーを備えている。"
  },
  {
    "start": 3824238,
    "end": 3830578,
    "text": "バッファは、平均と標準偏差、実行平均と標準偏差の実行平均である。"
  },
  {
    "start": 3830754,
    "end": 3832930,
    "text": "これらはバックプロパゲーションを使って学習されていない。"
  },
  {
    "start": 3833010,
    "end": 3838070,
    "text": "これらのトレーニングは、平均値の更新のようなジャンキーな更新を使って行われる。"
  },
  {
    "start": 3838890,
    "end": 3844950,
    "text": "これらはバッシュルーム・レイヤーのパラメーターとバッファーのようなものだ。"
  },
  {
    "start": 3845110,
    "end": 3854190,
    "text": "つまり、バッチルーム層に入力されるアクティベーションの平均と標準偏差を計算しているのだ。"
  },
  {
    "start": 3854850,
    "end": 3863310,
    "text": "そして、単位ガウスになるようにバッチをセンタリングし、学習したバイアスとゲインでオフセットとスケーリングを行う。"
  },
  {
    "start": 3863970,
    "end": 3872430,
    "text": "その上で、入力の平均と標準偏差を記録し、この平均と標準偏差を維持する。"
  },
  {
    "start": 3872590,
    "end": 3878520,
    "text": "平均と標準偏差を常に再推定する必要がないように、これは後に推論で使われる。"
  },
  {
    "start": 3878890,
    "end": 3883894,
    "text": "加えて、テスト時に基本的に個々の例を前進させることができる。"
  },
  {
    "start": 3884092,
    "end": 3885794,
    "text": "これがバッシュの正規化レイヤーだ。"
  },
  {
    "start": 3885842,
    "end": 3890242,
    "text": "かなり複雑なレイヤーだが、これが内部で行われていることだ。"
  },
  {
    "start": 3890386,
    "end": 3897574,
    "text": "では、Resnet（残差ニューラルネットワーク）を検索できるように、実際の例を少しお見せしましょう。"
  },
  {
    "start": 3897702,
    "end": 3901370,
    "text": "これらは、画像分類に使用される一般的なニューラルネットワークのタイプである。"
  },
  {
    "start": 3902030,
    "end": 3907598,
    "text": "もちろん、リスネットについて詳しく説明したわけではないので、そのすべてを説明するつもりはない。"
  },
  {
    "start": 3907684,
    "end": 3912030,
    "text": "とりあえず、この画像はこの上部にあるレスネットにフィードされることだけは覚えておいてほしい。"
  },
  {
    "start": 3912100,
    "end": 3918046,
    "text": "そのイメージの中にあるものを予測するために、繰り返し構造を持つ何層ものレイヤーがある。"
  },
  {
    "start": 3918238,
    "end": 3925090,
    "text": "この繰り返し構造はこれらのブロックで構成され、これらのブロックはこのディープ・ニューラル・ネットワークの中で順次積み重ねられていく。"
  },
  {
    "start": 3925530,
    "end": 3934418,
    "text": "さて、このコード、基本的に使用され、直列に順次繰り返されるブロックは、このボトルネックブロックと呼ばれる。"
  },
  {
    "start": 3934514,
    "end": 3935858,
    "text": "ボトルネック・ブロック"
  },
  {
    "start": 3936034,
    "end": 3937382,
    "text": "ここにはたくさんある。"
  },
  {
    "start": 3937436,
    "end": 3942760,
    "text": "これはすべてピートーチであり、もちろんすべてを網羅しているわけではないが、いくつかの断片を指摘しておきたい。"
  },
  {
    "start": 3943070,
    "end": 3945722,
    "text": "ここでニューラルネットを初期化する。"
  },
  {
    "start": 3945776,
    "end": 3948874,
    "text": "このブロックのコードは、基本的にここでやっていることだ。"
  },
  {
    "start": 3948912,
    "end": 3955578,
    "text": "すべてのレイヤーを初期化し、フォワードでは、実際に入力があったときにニューラルネットがどのように動作するかを指定する。"
  },
  {
    "start": 3955674,
    "end": 3959440,
    "text": "このコードは、我々がここでやっていることに沿ったものだ。"
  },
  {
    "start": 3961410,
    "end": 3968514,
    "text": "今、これらのブロックは複製され、直列に積み重ねられている。"
  },
  {
    "start": 3968712,
    "end": 3971282,
    "text": "だから、ここで何が起こっているかに注目してほしい。"
  },
  {
    "start": 3971336,
    "end": 3974526,
    "text": "ひとつは、畳み込み層だ。"
  },
  {
    "start": 3974718,
    "end": 3981700,
    "text": "このコンボリューションレイヤーは、コンボリューションレイヤーが適用されないことを除けば、基本的にはリニアレイヤーと同じものだ。"
  },
  {
    "start": 3982710,
    "end": 3986418,
    "text": "畳み込み層は画像に使われるため、空間的な構造を持っている。"
  },
  {
    "start": 3986514,
    "end": 3994534,
    "text": "基本的に、この線形乗算とバイアス・オフセットは、全入力の代わりにパッチに対して行われる。"
  },
  {
    "start": 3994662,
    "end": 4003654,
    "text": "というのも、これらの画像には構造があり、空間構造畳み込みは基本的にWx＋bを行うだけだが、入力の重なり合ったパッチに対して行うからだ。"
  },
  {
    "start": 4003782,
    "end": 4006000,
    "text": "それ以外はWXプラスBだ。"
  },
  {
    "start": 4006610,
    "end": 4013630,
    "text": "ノルムレイヤーは、デフォルトでは、次元バッチ正規化レイヤーのバッチノルムとして初期化されている。"
  },
  {
    "start": 4014050,
    "end": 4016622,
    "text": "となると、レルーのような非線形性がある。"
  },
  {
    "start": 4016686,
    "end": 4019630,
    "text": "ここでは \"relu \"を使う。"
  },
  {
    "start": 4019710,
    "end": 4022020,
    "text": "今回は10時間である。"
  },
  {
    "start": 4022470,
    "end": 4027166,
    "text": "どちらも単なる非線形性であり、比較的同じように使うことができる。"
  },
  {
    "start": 4027278,
    "end": 4031080,
    "text": "非常に深いネットワークの場合、Reluは経験的に少しうまく機能する。"
  },
  {
    "start": 4031610,
    "end": 4039250,
    "text": "コンボリューション・バッシュ・ノーマライゼーション、relu、コンボリューション・バッシュ・ノーマライゼーション、relu......といったモチーフが繰り返されている。"
  },
  {
    "start": 4039330,
    "end": 4042634,
    "text": "それから、これはまだカバーしていない残留コネクションだ。"
  },
  {
    "start": 4042832,
    "end": 4045274,
    "text": "基本的には、これとまったく同じパターンだ。"
  },
  {
    "start": 4045392,
    "end": 4059418,
    "text": "コンボリューション（畳み込み）のような、あるいはリニア（線形）層のようなウェイト層があり、バッチ正規化、そして非直線性の10hがあるが、基本的にはウェイト層、正規化層、そして非直線性である。"
  },
  {
    "start": 4059514,
    "end": 4065310,
    "text": "ディープ・ニューラル・ネットワークを作るときに積み重ねるモチーフは、まさにここにある通りだ。"
  },
  {
    "start": 4065460,
    "end": 4074114,
    "text": "もうひとつ注目してほしいのは、カンプレイヤーを初期化するとき、たとえばカンプをひとつずつ初期化するとき、その深さはここにある。"
  },
  {
    "start": 4074312,
    "end": 4078798,
    "text": "つまり、Pytorchの畳み込みレイヤーであるdにnn個のcompを初期化している。"
  },
  {
    "start": 4078894,
    "end": 4082086,
    "text": "ここには、まだ説明するつもりはないキーワードの議論がたくさんある。"
  },
  {
    "start": 4082188,
    "end": 4084562,
    "text": "バイアスがいかに虚偽に等しいかわかるだろう。"
  },
  {
    "start": 4084706,
    "end": 4089958,
    "text": "バイアスが偽に等しいのは、バイアスが我々のケースで使われていないのとまったく同じ理由である。"
  },
  {
    "start": 4090044,
    "end": 4091894,
    "text": "バイアスの使い方を消したのはおわかりだろう。"
  },
  {
    "start": 4092022,
    "end": 4100502,
    "text": "なぜなら、このウェイトレイヤーの後にバッシュの正規化があり、バッシュの正規化はそのバイアスを差し引き、さらに独自のバイアスを持つからだ。"
  },
  {
    "start": 4100566,
    "end": 4105466,
    "text": "このような偽のパラメーターを導入する必要はないし、パフォーマンスが落ちるわけでもない。"
  },
  {
    "start": 4105658,
    "end": 4112960,
    "text": "だから、彼らはバシオンリロというモチーフを持っているから、ここにバイアスをかける必要はない。"
  },
  {
    "start": 4114690,
    "end": 4116914,
    "text": "ところで、この例はとても見つけやすい。"
  },
  {
    "start": 4116952,
    "end": 4121522,
    "text": "Pytorchをresnetすると、このような例がある。"
  },
  {
    "start": 4121656,
    "end": 4126270,
    "text": "これは、Pytorchの残差ニューラルネットワークの純正実装のようなものだ。"
  },
  {
    "start": 4126350,
    "end": 4127986,
    "text": "こちらでご覧いただけます。"
  },
  {
    "start": 4128088,
    "end": 4130422,
    "text": "もちろん、まだ多くの部分を取材したわけではない。"
  },
  {
    "start": 4130556,
    "end": 4136774,
    "text": "また、これらのパイトーチ・レイヤーの定義とパラメータについても簡単に説明したい。"
  },
  {
    "start": 4136892,
    "end": 4142794,
    "text": "さて、畳み込みレイヤーの代わりに線形レイヤーを見てみよう。"
  },
  {
    "start": 4142832,
    "end": 4146026,
    "text": "これはリニアレイヤーで、コンボリューションについてはまだ説明していない。"
  },
  {
    "start": 4146128,
    "end": 4150250,
    "text": "さっきも言ったように、コンボリューションはパッチ以外は基本的にリニアレイヤーだ。"
  },
  {
    "start": 4151070,
    "end": 4157150,
    "text": "リニアレイヤーはWX＋Bを行うが、ここではWAトランスポーズを呼び出す。"
  },
  {
    "start": 4158530,
    "end": 4161294,
    "text": "というのは、ここでやったのと同じように、WXプラスBを計算するからだ。"
  },
  {
    "start": 4161412,
    "end": 4169346,
    "text": "このレイヤーを初期化するには、ファンインとファンアウトを知る必要がある。"
  },
  {
    "start": 4169448,
    "end": 4174900,
    "text": "これはファン・インとファン・アウトであり、ウェイトマトリックスの大きさがわかる。"
  },
  {
    "start": 4175350,
    "end": 4178738,
    "text": "バイアスをかけるかどうかも入力する必要がある。"
  },
  {
    "start": 4178914,
    "end": 4183670,
    "text": "falseに設定すると、このレイヤーにはバイアスがかからない。"
  },
  {
    "start": 4184250,
    "end": 4190860,
    "text": "バッチ・ノルムのような正規化レイヤーがレイヤーの後に続く場合は、今回のケースとまったく同じようにすることができる。"
  },
  {
    "start": 4191550,
    "end": 4194198,
    "text": "これにより、基本的にバイアスを無効にすることができる。"
  },
  {
    "start": 4194374,
    "end": 4200854,
    "text": "さて、初期化という点では、ここを下に振ると、このリニア・レイヤー内部で使用される変数が報告される。"
  },
  {
    "start": 4200982,
    "end": 4205642,
    "text": "ここでの線形レイヤーは、重みとバイアスの2つのパラメーターを持つ。"
  },
  {
    "start": 4205786,
    "end": 4211482,
    "text": "同じようにウェイトとバイアスがあり、それをデフォルトで初期化する方法について話している。"
  },
  {
    "start": 4211626,
    "end": 4220210,
    "text": "デフォルトでは、Pytorchはファンを取り込み、平方根でファンを1つオーバーすることでウェイトを初期化します。"
  },
  {
    "start": 4220710,
    "end": 4225486,
    "text": "となると、正規分布の代わりに一様分布を使っていることになる。"
  },
  {
    "start": 4225678,
    "end": 4230534,
    "text": "でも、5対3ではなく、1対1を使っている。"
  },
  {
    "start": 4230572,
    "end": 4232502,
    "text": "ここで計算されているのは利得ではない。"
  },
  {
    "start": 4232556,
    "end": 4239320,
    "text": "ゲインは1だけだが、それ以外はファンnの平方根に対してちょうど1である。"
  },
  {
    "start": 4240330,
    "end": 4244982,
    "text": "kの平方根上の1がウェイトのスケールである。"
  },
  {
    "start": 4245126,
    "end": 4251270,
    "text": "彼らが数字を描くとき、デフォルトではガウス分布ではなく、一様分布を使っている。"
  },
  {
    "start": 4251350,
    "end": 4255886,
    "text": "つまり、kの負の平方根からkの平方根まで一様に描く。"
  },
  {
    "start": 4256068,
    "end": 4262906,
    "text": "この講義で見てきたことに関しては、まったく同じことで、同じ動機だ。"
  },
  {
    "start": 4263018,
    "end": 4271694,
    "text": "なぜこのようなことをするかというと、入力がほぼガウスであれば、このレイヤーからの出力がほぼガウスになるようにするためである。"
  },
  {
    "start": 4271822,
    "end": 4277522,
    "text": "であれば、基本的にはファン・インの平方根を1倍してウエイトをスケーリングすることで達成できる。"
  },
  {
    "start": 4277656,
    "end": 4279240,
    "text": "これがそうだ。"
  },
  {
    "start": 4279930,
    "end": 4283122,
    "text": "次に、バッシュの正規化レイヤーである。"
  },
  {
    "start": 4283186,
    "end": 4285670,
    "text": "それがPytorchではどのように見えるか見てみよう。"
  },
  {
    "start": 4286010,
    "end": 4290518,
    "text": "ここでは1次元のバッシュ正規化レイヤーを使っている。"
  },
  {
    "start": 4290684,
    "end": 4293354,
    "text": "そこには、多くのキーワードの議論も含まれている。"
  },
  {
    "start": 4293472,
    "end": 4297194,
    "text": "私たちにとって必要な機能の数は200です。"
  },
  {
    "start": 4297312,
    "end": 4306110,
    "text": "ゲイン、バイアス、平均と標準偏差のバッファを初期化するために必要なパラメータである。"
  },
  {
    "start": 4306850,
    "end": 4309678,
    "text": "それなら、ここでイプシロンの値を知る必要がある。"
  },
  {
    "start": 4309844,
    "end": 4311742,
    "text": "デフォルトではマイナス5分の1である。"
  },
  {
    "start": 4311796,
    "end": 4313678,
    "text": "普通はあまり変えない。"
  },
  {
    "start": 4313844,
    "end": 4315678,
    "text": "そして、その勢いを知る必要がある。"
  },
  {
    "start": 4315854,
    "end": 4322542,
    "text": "ここでのモメンタムは、彼らが説明するように、基本的にこれらの実行平均と実行標準偏差に使用されます。"
  },
  {
    "start": 4322686,
    "end": 4324866,
    "text": "デフォルトでは、ここでの運動量はゼロ1である。"
  },
  {
    "start": 4324968,
    "end": 4328840,
    "text": "この例で使用している運動量は0.1である。"
  },
  {
    "start": 4329690,
    "end": 4333398,
    "text": "基本的には、時々変更した方がいいかもしれない。"
  },
  {
    "start": 4333564,
    "end": 4342714,
    "text": "大雑把に言えば、バッチサイズが非常に大きい場合、通常、バッチサイズごとに平均と標準偏差を推定すると、次のようになります。"
  },
  {
    "start": 4342752,
    "end": 4345754,
    "text": "十分な大きさがあれば、ほぼ同じ結果が得られるだろう。"
  },
  {
    "start": 4345952,
    "end": 4350620,
    "text": "そのため、ゼロ1のように少し高い運動量を使うことができる。"
  },
  {
    "start": 4350990,
    "end": 4361722,
    "text": "バッチサイズが32と小さい場合、平均と標準偏差は微妙に異なる数値になるかもしれない。"
  },
  {
    "start": 4361786,
    "end": 4363982,
    "text": "価値が大きく変化している。"
  },
  {
    "start": 4364116,
    "end": 4374818,
    "text": "もし運動量が0であれば、この値がトレーニングセット全体の実際の平均値と標準偏差に落ち着き、収束するには十分ではないかもしれない。"
  },
  {
    "start": 4374984,
    "end": 4379682,
    "text": "だから基本的に、バッチサイズが非常に小さい場合、0.1の勢いは潜在的に危険である。"
  },
  {
    "start": 4379746,
    "end": 4387670,
    "text": "そうすると、トレーニング中に平均と標準偏差が乱高下しすぎて、うまく収束しないことがある。"
  },
  {
    "start": 4389290,
    "end": 4398230,
    "text": "Afineはtrueに等しく、このバッチ正規化レイヤーが学習可能なアフィン・パラメーター、ゲインとバイアスを持つかどうかを決定する。"
  },
  {
    "start": 4398390,
    "end": 4400650,
    "text": "これはほとんど常に真実である。"
  },
  {
    "start": 4400720,
    "end": 4404190,
    "text": "なぜこれをfalseに変えたいのか、実はよくわからない。"
  },
  {
    "start": 4406290,
    "end": 4406654,
    "text": "それからだ。"
  },
  {
    "start": 4406692,
    "end": 4412000,
    "text": "実行中の統計トラックは、pytorchのbash正規化レイヤーがこれを行うかどうかを決定しています。"
  },
  {
    "start": 4412770,
    "end": 4423106,
    "text": "走るスタッツを省略したい理由のひとつは、たとえば、このようにステージ2として最後に推定したい場合があるからだ。"
  },
  {
    "start": 4423208,
    "end": 4427860,
    "text": "その場合、バッシュの正規化レイヤーに、使うつもりのない余計な計算をさせたくない。"
  },
  {
    "start": 4428810,
    "end": 4436694,
    "text": "最後に、このバッチ正規化を実行するデバイスがCPUかGPUか、そしてデータ型はどうするかを知る必要がある。"
  },
  {
    "start": 4436812,
    "end": 4439900,
    "text": "半精度、単精度、倍精度など。"
  },
  {
    "start": 4440830,
    "end": 4443834,
    "text": "それはバッシュの正規化レイヤーであり、それ以外は論文にリンクしている。"
  },
  {
    "start": 4443872,
    "end": 4449820,
    "text": "私たちが導入したのと同じ方式で、すべてがここでやったこととまったく同じだ。"
  },
  {
    "start": 4450670,
    "end": 4453610,
    "text": "さて、これで今回の講義で取り上げたかったことはすべて終わった。"
  },
  {
    "start": 4453770,
    "end": 4460442,
    "text": "私がお話ししたかったのは、ニューラルネットワークにおける活性度と勾配、そしてそれらの統計量を理解することの重要性です。"
  },
  {
    "start": 4460506,
    "end": 4465310,
    "text": "特に、ニューラルネットワークをより大きく、より大きく、より深くしていくにつれて、その重要性は増していく。"
  },
  {
    "start": 4465670,
    "end": 4477342,
    "text": "私たちは、基本的に出力層での分布を見て、最後の層でアクティブ度が乱れすぎているために予測を誤ると、このようなホッケースティック・ロスになることがわかりました。"
  },
  {
    "start": 4477486,
    "end": 4483574,
    "text": "これを修正すれば、トレーニングが無駄な仕事をしなくなり、トレーニング終了時の損失が改善される。"
  },
  {
    "start": 4483772,
    "end": 4486002,
    "text": "それから、アクティベーションをコントロールする必要があることもわかった。"
  },
  {
    "start": 4486066,
    "end": 4490594,
    "text": "ゼロにつぶれたり、無限大に爆発したりすることは望んでいない。"
  },
  {
    "start": 4490722,
    "end": 4495750,
    "text": "というのも、ニューラルネットの非線形性には多くの問題があるからだ。"
  },
  {
    "start": 4495830,
    "end": 4498954,
    "text": "基本的には、ニューラルネット全体が均質であることが望ましい。"
  },
  {
    "start": 4498992,
    "end": 4501820,
    "text": "ニューラルネット全体がほぼガウス状に活性化するようにしたい。"
  },
  {
    "start": 4502350,
    "end": 4511440,
    "text": "それから、もし活性度をほぼガウス型にしたい場合、ニューラルネットの初期化時に重み行列とバイアスをどのようにスケーリングするかについて話し合った。"
  },
  {
    "start": 4513110,
    "end": 4515380,
    "text": "すべてが可能な限りコントロールされている。"
  },
  {
    "start": 4517190,
    "end": 4519742,
    "text": "その結果、改善効果が大きくなった。"
  },
  {
    "start": 4519886,
    "end": 4541050,
    "text": "というのも、さまざまなタイプのレイヤーを持つ、より深いニューラルネットの場合、アクティブ度がニューラルネット全体でほぼ均一になるように重みとバイアスを正確に設定するのが本当に本当に難しくなるからです。"
  },
  {
    "start": 4541200,
    "end": 4544326,
    "text": "そこで私は、正規化レイヤーという概念を導入した。"
  },
  {
    "start": 4544438,
    "end": 4547750,
    "text": "今、人々が実際に使っている正規化レイヤーはたくさんある。"
  },
  {
    "start": 4547830,
    "end": 4552454,
    "text": "バッシュ正規化レイヤー、正規化インスタンス、正規化グループ正規化。"
  },
  {
    "start": 4552582,
    "end": 4559950,
    "text": "まだほとんど取り上げていませんが、最初に紹介したものと、最初に登場したと思われるバッチ・ノーマライゼーションと呼ばれるものを紹介しました。"
  },
  {
    "start": 4560530,
    "end": 4562686,
    "text": "バッチ正規化がどのように機能するかを見た。"
  },
  {
    "start": 4562868,
    "end": 4566162,
    "text": "このレイヤーは、ディープ・ニューラル・ネット全体に散りばめることができる。"
  },
  {
    "start": 4566296,
    "end": 4576306,
    "text": "基本的な考え方は、おおよそガウス型の活性を求めるのであれば、活性を取り、平均と標準偏差を取り、データを中心に置くということです。"
  },
  {
    "start": 4576488,
    "end": 4580870,
    "text": "センタリング演算は微分可能だからだ。"
  },
  {
    "start": 4581210,
    "end": 4584882,
    "text": "そのうえで、私たちは実際には多くのベルやホイッスルを追加しなければならなかった。"
  },
  {
    "start": 4585026,
    "end": 4590870,
    "text": "これで、bashの正規化レイヤーの複雑さがお分かりいただけたと思う。"
  },
  {
    "start": 4590940,
    "end": 4594890,
    "text": "突然、ゲインやバイアスが必要になった。"
  },
  {
    "start": 4595630,
    "end": 4600934,
    "text": "では、すべてのトレーニング例を結合しているのだから、突然、推論をどうするかが問題になる。"
  },
  {
    "start": 4601062,
    "end": 4602730,
    "text": "推論はどこで行うのか？"
  },
  {
    "start": 4602810,
    "end": 4611530,
    "text": "この平均と標準偏差を訓練セット全体にわたって一度推定し、推論時に使用する必要がある。"
  },
  {
    "start": 4611690,
    "end": 4613598,
    "text": "そうなると、誰も第2ステージをやりたがらない。"
  },
  {
    "start": 4613684,
    "end": 4622210,
    "text": "その代わりに、トレーニング中にすべてをバッチ正規化レイヤーに組み込み、ランニング方式でこれらを推定することで、すべてが少しシンプルになるようにしている。"
  },
  {
    "start": 4622550,
    "end": 4625170,
    "text": "バッチ正規化レイヤーを提供する。"
  },
  {
    "start": 4626250,
    "end": 4629234,
    "text": "さっきも言ったように、このレイヤーが好きな人はいない。"
  },
  {
    "start": 4629362,
    "end": 4631510,
    "text": "大量のバグを引き起こす。"
  },
  {
    "start": 4632410,
    "end": 4638598,
    "text": "直感的には、フォワードパッシブとニューラルネットの例をカップリングしているからだろう。"
  },
  {
    "start": 4638764,
    "end": 4647500,
    "text": "私は自分の人生で何度も何度もこのレイヤーで自分の足を撃ってきた。"
  },
  {
    "start": 4648190,
    "end": 4650780,
    "text": "基本的には、できるだけ避けるようにする。"
  },
  {
    "start": 4651870,
    "end": 4662814,
    "text": "これらのレイヤーに代わるものとして、例えばグループ正規化やレイヤー正規化などがあり、これらは最近のディープラーニングでは一般的になってきているが、まだ取り上げていない。"
  },
  {
    "start": 4663012,
    "end": 4675150,
    "text": "というのも、より深いニューラルネットを確実にトレーニングできるようになったのはこれが初めてだったからだ。"
  },
  {
    "start": 4675230,
    "end": 4682600,
    "text": "基本的に、この層はニューラルネットの活性化の統計量をコントロールするのに効果的だったからだ。"
  },
  {
    "start": 4683050,
    "end": 4687750,
    "text": "というのがここまでの話で、これが私が取り上げたいことのすべてだ。"
  },
  {
    "start": 4687820,
    "end": 4691446,
    "text": "今後の講義では、できればリカレント・ニューラルネットに踏み込んでいきたい。"
  },
  {
    "start": 4691558,
    "end": 4701274,
    "text": "リカレント・ニューラル・ネットは、これから説明するように、非常に深いネットワークである。"
  },
  {
    "start": 4701402,
    "end": 4712538,
    "text": "そこで、活性化統計やすべての正規化レイヤーに関する多くの分析が、優れたパフォーマンスを発揮するために非常に重要になる。"
  },
  {
    "start": 4712634,
    "end": 4714014,
    "text": "それはまた次の機会に。"
  },
  {
    "start": 4714132,
    "end": 4714938,
    "text": "さようなら。"
  },
  {
    "start": 4715114,
    "end": 4716250,
    "text": "オーケー、だから私は嘘をついた。"
  },
  {
    "start": 4716330,
    "end": 4719022,
    "text": "おまけとして、もうひとつまとめておきたい。"
  },
  {
    "start": 4719166,
    "end": 4723758,
    "text": "この講義で紹介したことをもう1度まとめておくと便利だと思う。"
  },
  {
    "start": 4723854,
    "end": 4730082,
    "text": "また、Pytorchで遭遇するようなものに近づけるために、コードを少し修正することから始めたい。"
  },
  {
    "start": 4730226,
    "end": 4738466,
    "text": "リニア・モジュールとバッチトーム・モジュールのように、コードをモジュールで構成することがわかるだろう。"
  },
  {
    "start": 4738578,
    "end": 4744806,
    "text": "Pytorchでニューラルネットワークを構築するのと同じように、ニューラルネットワークを構築できるように、これらのモジュールの中にコードを入れている。"
  },
  {
    "start": 4744838,
    "end": 4746378,
    "text": "詳しく説明しよう。"
  },
  {
    "start": 4746544,
    "end": 4748300,
    "text": "ニューラルネットを作成する。"
  },
  {
    "start": 4748670,
    "end": 4752234,
    "text": "そして、前回と同じように最適化ループを行う。"
  },
  {
    "start": 4752432,
    "end": 4759102,
    "text": "そしてもうひとつ、前方パスと後方パスの両方で活性化統計を見てみたい。"
  },
  {
    "start": 4759236,
    "end": 4762606,
    "text": "そして、ここでも前と同じように評価とサンプリングが行われる。"
  },
  {
    "start": 4762788,
    "end": 4766510,
    "text": "ここまで巻き戻して、もう少しゆっくり行こう。"
  },
  {
    "start": 4766670,
    "end": 4769054,
    "text": "ここではリニアレイヤーを作成している。"
  },
  {
    "start": 4769182,
    "end": 4774862,
    "text": "トーチNnにはたくさんの種類のレイヤーがあり、そのうちのひとつがリニアレイヤーであることにお気づきだろう。"
  },
  {
    "start": 4775006,
    "end": 4783670,
    "text": "トーチNnリニアは、入力フィーチャー、出力フィーチャー、バイアスの有無、そしてこのレイヤーを配置したいデバイスとデータタイプを受け取る。"
  },
  {
    "start": 4783820,
    "end": 4788150,
    "text": "この2つは省略するが、それ以外はまったく同じものだ。"
  },
  {
    "start": 4788300,
    "end": 4795062,
    "text": "ファンイン（入力数）、ファンアウト（出力数）、そしてバイアスを使うかどうかだ。"
  },
  {
    "start": 4795206,
    "end": 4798422,
    "text": "内部的には、このレイヤーの内部にはウェイトとバイアスがある。"
  },
  {
    "start": 4798486,
    "end": 4805818,
    "text": "お望みなら、ガウスから引いた乱数を使ってウェイトを初期化するのが一般的だ。"
  },
  {
    "start": 4805914,
    "end": 4814454,
    "text": "この講義ですでに説明したタイミングの初期化ですが、これは良いデフォルトで、Pytorchが使っているデフォルトでもあります。"
  },
  {
    "start": 4814602,
    "end": 4818046,
    "text": "デフォルトでは、バイアスは通常ゼロに初期化される。"
  },
  {
    "start": 4818238,
    "end": 4824558,
    "text": "このモジュールを呼び出すと、基本的にw×xにnbがあればbを足したものを計算する。"
  },
  {
    "start": 4824734,
    "end": 4831938,
    "text": "このモジュールのパラメータを呼び出すと、このレイヤーのパラメータであるテンソルが返される。"
  },
  {
    "start": 4832114,
    "end": 4834498,
    "text": "さて、次はバッシュの正規化レイヤーだ。"
  },
  {
    "start": 4834594,
    "end": 4843660,
    "text": "これはPytorchnのbash norm 1dレイヤーとよく似ている。"
  },
  {
    "start": 4844350,
    "end": 4856990,
    "text": "次元数、分割に使用するイプシロン、そして平均と分散を記録する際に使用するモメンタムだ。"
  },
  {
    "start": 4857970,
    "end": 4862170,
    "text": "今、Pytorchは実際にはもっと多くのものを取っているが、私は彼らの設定のいくつかを想定している。"
  },
  {
    "start": 4862250,
    "end": 4863870,
    "text": "我々にとっては、アフィネが真になる。"
  },
  {
    "start": 4863940,
    "end": 4866186,
    "text": "つまり、ガンマとベータを使うことになる。"
  },
  {
    "start": 4866218,
    "end": 4869522,
    "text": "正規化後は、トラックランニングのスタッツが真になる。"
  },
  {
    "start": 4869576,
    "end": 4874100,
    "text": "ここでは、バッシュ・ノルムの実行平均と実行分散を記録する。"
  },
  {
    "start": 4874470,
    "end": 4881320,
    "text": "デフォルトのデバイスはCPUで、デフォルトのデータ型はfloat、float 32である。"
  },
  {
    "start": 4882010,
    "end": 4883394,
    "text": "これがデフォルトだ。"
  },
  {
    "start": 4883442,
    "end": 4887298,
    "text": "そうでなければ、このバッチルームのレイヤーで同じパラメーターをすべて取ることになる。"
  },
  {
    "start": 4887394,
    "end": 4889240,
    "text": "まずは取っておく。"
  },
  {
    "start": 4889770,
    "end": 4890970,
    "text": "さて、ここで新しい発見がある。"
  },
  {
    "start": 4891040,
    "end": 4893306,
    "text": "ドット・トレーニングがあり、デフォルトでは真になる。"
  },
  {
    "start": 4893408,
    "end": 4896842,
    "text": "pytorchnnモジュールもこの属性トレーニングを持っている。"
  },
  {
    "start": 4896976,
    "end": 4912350,
    "text": "というのも、多くのモジュールとバッチノルムは、ニューラルネットをトレーニングするのか、評価モードで実行して評価損失を計算するのか、あるいはテスト例に対する推論に使用するのかで、動作が異なるからだ。"
  },
  {
    "start": 4912690,
    "end": 4919502,
    "text": "バッチノルムはこの例で、トレーニングの際には、現在のバッチから推定された平均と分散を使用することになるからだ。"
  },
  {
    "start": 4919646,
    "end": 4923646,
    "text": "推論中、我々は実行平均と実行分散を使っている。"
  },
  {
    "start": 4923838,
    "end": 4931410,
    "text": "トレーニングの場合は平均と分散を更新しますが、テストの場合は更新されず、固定されたままです。"
  },
  {
    "start": 4931570,
    "end": 4933314,
    "text": "だから、このフラグは必要なのだ。"
  },
  {
    "start": 4933362,
    "end": 4940300,
    "text": "デフォルトはtrueで、現在のPytorchと同じように、beshambandiのパラメータはガンマとベータである。"
  },
  {
    "start": 4941550,
    "end": 4947130,
    "text": "その場合、実行平均と実行分散はpytorch命名法ではバッファと呼ばれます。"
  },
  {
    "start": 4947470,
    "end": 4953218,
    "text": "これらのバッファは、ここでは明示的に指数移動平均を使って学習される。"
  },
  {
    "start": 4953334,
    "end": 4956890,
    "text": "これらはバックプロパゲーションや確率的勾配降下法には含まれない。"
  },
  {
    "start": 4956970,
    "end": 4959690,
    "text": "それらは、このレイヤーのパラメーターのようなものではない。"
  },
  {
    "start": 4959770,
    "end": 4966478,
    "text": "そのため、ここでパラメータを指定した場合、ガンマとベータのみを返し、平均と分散は返しません。"
  },
  {
    "start": 4966574,
    "end": 4973810,
    "text": "これは、指数移動平均を使用した前方パスごとに、内部で訓練されているようなものだ。"
  },
  {
    "start": 4974310,
    "end": 4975890,
    "text": "これが初期化だ。"
  },
  {
    "start": 4976630,
    "end": 4983074,
    "text": "さて、フォワードパスでは、もしトレーニング中であれば、バッチによって推定された平均と分散を使用する。"
  },
  {
    "start": 4983202,
    "end": 4988610,
    "text": "平均と分散を計算します。"
  },
  {
    "start": 4988770,
    "end": 4997814,
    "text": "さて、上では標準偏差を推定し、分散ではなく標準偏差を記録していた。"
  },
  {
    "start": 4997942,
    "end": 4999850,
    "text": "論文に正確に従おう。"
  },
  {
    "start": 5000000,
    "end": 5003722,
    "text": "ここでは標準偏差の2乗である分散を計算する。"
  },
  {
    "start": 5003786,
    "end": 5008670,
    "text": "標準偏差の代わりに分散が記録されるのだ。"
  },
  {
    "start": 5009570,
    "end": 5017890,
    "text": "トレーニングをしていない場合は、平均と分散を使い、正規化する。"
  },
  {
    "start": 5018870,
    "end": 5025218,
    "text": "そしてこのレイヤーの出力を計算し、dot outというアトリビュートに割り当てている。"
  },
  {
    "start": 5025384,
    "end": 5029382,
    "text": "さて、ドットアウトはこのモジュールで使っているものだ。"
  },
  {
    "start": 5029516,
    "end": 5031426,
    "text": "これはピトーチにはないものだ。"
  },
  {
    "start": 5031458,
    "end": 5033046,
    "text": "私たちはそこから少し逸脱している。"
  },
  {
    "start": 5033148,
    "end": 5041226,
    "text": "ドットアウトを作成しているのは、これらの変数を非常に簡単に管理し、統計を作成してプロットできるようにしたいからだ。"
  },
  {
    "start": 5041328,
    "end": 5044890,
    "text": "Pytorchとinモジュールはdot out属性を持たない。"
  },
  {
    "start": 5045230,
    "end": 5052774,
    "text": "最後に、ここでも申し上げたように、提供されたモメンタムを考慮し、指数移動平均を使ってバッファを更新しています。"
  },
  {
    "start": 5052902,
    "end": 5057094,
    "text": "重要なのは、私がトーチ・ノグラードのコンテキスト・マネージャーを使っていることだ。"
  },
  {
    "start": 5057222,
    "end": 5067710,
    "text": "もしこれを使わなければ、Pytorchはテンソルから計算グラフ全体を作り始めることになる。"
  },
  {
    "start": 5067870,
    "end": 5072446,
    "text": "平均と分散を含むものをドットバックと呼ぶことはない。"
  },
  {
    "start": 5072558,
    "end": 5079810,
    "text": "だからコンテキスト・マネージャーを使う必要があるのだ。"
  },
  {
    "start": 5080230,
    "end": 5081842,
    "text": "そうすれば、より効率的になる。"
  },
  {
    "start": 5081906,
    "end": 5084050,
    "text": "ピトルヒに \"後戻りはしない \"と言っているだけだ。"
  },
  {
    "start": 5084130,
    "end": 5086342,
    "text": "我々はテンソルの束を持っていて、それらを更新したいだけなのだ。"
  },
  {
    "start": 5086396,
    "end": 5087320,
    "text": "それだけだ。"
  },
  {
    "start": 5087690,
    "end": 5089240,
    "text": "そして、戻る。"
  },
  {
    "start": 5090090,
    "end": 5092710,
    "text": "さて、下にスクロールすると、10Hレイヤーがある。"
  },
  {
    "start": 5092790,
    "end": 5100298,
    "text": "これはtorch ten hととてもよく似ていて、あまり多くのことはできない。"
  },
  {
    "start": 5100464,
    "end": 5104970,
    "text": "このレイヤーにはパラメーターがない。"
  },
  {
    "start": 5105130,
    "end": 5112240,
    "text": "これらはレイヤーであるため、基本的に単なるリストに積み上げるのは非常に簡単だ。"
  },
  {
    "start": 5113250,
    "end": 5116354,
    "text": "我々は、慣れ親しんだすべての初期化を行うことができる。"
  },
  {
    "start": 5116392,
    "end": 5121810,
    "text": "最初の埋め込み行列のようなものがあれば、レイヤーができ、それらを順次呼び出すことができる。"
  },
  {
    "start": 5122150,
    "end": 5125986,
    "text": "トルチャト・ノグラードでは、ここでいくつかのイニシャライズがある。"
  },
  {
    "start": 5126088,
    "end": 5130102,
    "text": "出力されるソフトマックスは、先ほど見たように、もう少し自信のないものにしたい。"
  },
  {
    "start": 5130236,
    "end": 5134710,
    "text": "というのも、ここでは6層の多層パーセプトロンを使っているからだ。"
  },
  {
    "start": 5134780,
    "end": 5138550,
    "text": "リニア・テンH、リニア・テンH、その他をどう積み重ねているかわかるだろう。"
  },
  {
    "start": 5139130,
    "end": 5142842,
    "text": "ここでゲインを使うつもりで、ちょっと遊んでみようと思う。"
  },
  {
    "start": 5142896,
    "end": 5146490,
    "text": "これを変更すると、統計がどうなるかは見ての通りだ。"
  },
  {
    "start": 5147070,
    "end": 5152198,
    "text": "最後に、パラメータは基本的に埋め込み行列とすべての層のすべてのパラメータである。"
  },
  {
    "start": 5152294,
    "end": 5156206,
    "text": "ここで私はダブル・ブリッツを使用している。"
  },
  {
    "start": 5156228,
    "end": 5164530,
    "text": "レイヤーのレイヤーごとに、そしてレイヤーごとのパラメーターごとに、すべてのピースとパラメーターを積み上げていく。"
  },
  {
    "start": 5164870,
    "end": 5172530,
    "text": "今、合計で46,000のパラメーターがあり、それらすべてに勾配が必要であることをPytorchに伝えている。"
  },
  {
    "start": 5175910,
    "end": 5178370,
    "text": "そして、ここにすべてがある。"
  },
  {
    "start": 5178440,
    "end": 5180390,
    "text": "ここでの私たちは、実はほとんど慣れている。"
  },
  {
    "start": 5180540,
    "end": 5183366,
    "text": "バッチをサンプリングし、フォワードパスをしている。"
  },
  {
    "start": 5183468,
    "end": 5189122,
    "text": "現在のフォワードパスは、すべてのレイヤーを順番にリニアに適用し、その後にクロスエントロピーを適用するだけである。"
  },
  {
    "start": 5189266,
    "end": 5197098,
    "text": "そして後方パスでは、レイヤーごとにすべての出力を繰り返し、グラデーションを保持するようにPytorchに指示していることに気づくだろう。"
  },
  {
    "start": 5197264,
    "end": 5201558,
    "text": "であれば、ここではすでにすべてのグラデーションが「なし」に設定されている。"
  },
  {
    "start": 5201654,
    "end": 5208458,
    "text": "勾配を埋めるために後戻りを行い、確率的勾配を用いて更新を行い、そしていくつかの統計を追跡する。"
  },
  {
    "start": 5208634,
    "end": 5211802,
    "text": "となると、1回繰り返しただけで壊れそうだ。"
  },
  {
    "start": 5211946,
    "end": 5221566,
    "text": "このセルで、この図で、私はフォワード・パスの活性化のヒストグラムを視覚化している。"
  },
  {
    "start": 5221758,
    "end": 5228690,
    "text": "基本的にソフトマックスレイヤーだけである最後のレイヤーを除いて、すべてのレイヤーを反復する。"
  },
  {
    "start": 5230310,
    "end": 5231682,
    "text": "10時間のレイヤーだ。"
  },
  {
    "start": 5231746,
    "end": 5235414,
    "text": "10Hレイヤーを使っているのは、出力が有限で、1対1のマイナスだからだ。"
  },
  {
    "start": 5235452,
    "end": 5237302,
    "text": "だから、ここはとてもイメージしやすい。"
  },
  {
    "start": 5237356,
    "end": 5241354,
    "text": "マイナス1対1が見える。"
  },
  {
    "start": 5241552,
    "end": 5250298,
    "text": "そのレイヤーのアウト・テンソルをtに取り込み、tの平均、標準偏差、飽和パーセントを計算している。"
  },
  {
    "start": 5250464,
    "end": 5255274,
    "text": "私が飽和パーセントを定義する方法は、ドットの絶対値がゼロより大きいことだ。"
  },
  {
    "start": 5255392,
    "end": 5258478,
    "text": "ということは、私たちは10Hの最後尾にいるということだ。"
  },
  {
    "start": 5258564,
    "end": 5262634,
    "text": "10Hのテールにいるとき、実際にグラデーションが止まることを覚えておいてほしい。"
  },
  {
    "start": 5262762,
    "end": 5264800,
    "text": "あまり高くしたくない。"
  },
  {
    "start": 5265490,
    "end": 5270942,
    "text": "ここでTorch histogramを呼び出し、このヒストグラムをプロットしている。"
  },
  {
    "start": 5271086,
    "end": 5275186,
    "text": "基本的に、これはすべての異なるタイプのレイヤーで、すべてのレイヤーが異なる色を持っているということです。"
  },
  {
    "start": 5275288,
    "end": 5283878,
    "text": "私たちは、これらのテンソルのうち、この軸上の以下の値のいずれかを取る値がいくつあるかを見ている。"
  },
  {
    "start": 5284044,
    "end": 5287990,
    "text": "最初の層は20％とかなり飽和している。"
  },
  {
    "start": 5288060,
    "end": 5292318,
    "text": "尾を引いているのがわかると思うが、その後、すべてが安定する。"
  },
  {
    "start": 5292434,
    "end": 5300220,
    "text": "もしここにもっと多くの層があれば、実際には標準偏差がゼロの65あたりで安定し、飽和度はおよそ5％になるだろう。"
  },
  {
    "start": 5300590,
    "end": 5307200,
    "text": "これが安定し、きれいな分布が得られるのは、ゲインが3より5に設定されているからだ。"
  },
  {
    "start": 5307730,
    "end": 5315182,
    "text": "さて、このゲインだが、デフォルトではファンの平方根を1オーバーした値で初期化されているのがわかるだろう。"
  },
  {
    "start": 5315316,
    "end": 5318734,
    "text": "そして、初期化の間に、すべてのレイヤーを反復処理する。"
  },
  {
    "start": 5318782,
    "end": 5321810,
    "text": "リニアレイヤーならゲインでブーストする。"
  },
  {
    "start": 5322310,
    "end": 5324290,
    "text": "今、私たちはそれを見た。"
  },
  {
    "start": 5324440,
    "end": 5328120,
    "text": "基本的に、もしゲインを使わなければどうなるのか？"
  },
  {
    "start": 5328650,
    "end": 5336678,
    "text": "これを再描画すると、標準偏差が縮小し、彩度がゼロに近づいているのがわかるだろう。"
  },
  {
    "start": 5336844,
    "end": 5344650,
    "text": "基本的に何が起こっているかというと、最初のレイヤーはかなりまともだが、それ以降のレイヤーはゼロに縮小しているようなものだ。"
  },
  {
    "start": 5344800,
    "end": 5347370,
    "text": "徐々にではあるが、ゼロに近づいている。"
  },
  {
    "start": 5347520,
    "end": 5361870,
    "text": "その理由は、線形レイヤーだけでサンドイッチしている場合、前に見たこの方法で重みを初期化すれば、標準偏差を1だけ節約できるからだ。"
  },
  {
    "start": 5362020,
    "end": 5369390,
    "text": "というのも、そこには10hのレイヤーが散りばめられていて、この10hのレイヤーが機能をつぶしているのだ。"
  },
  {
    "start": 5369470,
    "end": 5372818,
    "text": "だから、彼らはあなたの配給を受け取り、それをわずかにつぶす。"
  },
  {
    "start": 5372904,
    "end": 5379030,
    "text": "そのため、押し潰されに対抗するためには、それを拡大し続けるために多少の利得が必要なのだ。"
  },
  {
    "start": 5379850,
    "end": 5383270,
    "text": "ただ、3試合より5試合の方が価値があることがわかった。"
  },
  {
    "start": 5383420,
    "end": 5392634,
    "text": "小さすぎるもの、例えば1であれば、物事はゼロに向かって進むと見たが、高すぎるものであれば、2にしてみよう。"
  },
  {
    "start": 5392672,
    "end": 5393900,
    "text": "ここで私たちはそれを目の当たりにする。"
  },
  {
    "start": 5396590,
    "end": 5399062,
    "text": "では、もう少し極端なことをしよう。"
  },
  {
    "start": 5399206,
    "end": 5400374,
    "text": "もう少し見えやすい。"
  },
  {
    "start": 5400422,
    "end": 5401500,
    "text": "3つ試してみよう。"
  },
  {
    "start": 5402350,
    "end": 5405680,
    "text": "つまり、飽和度が大きすぎるのだ。"
  },
  {
    "start": 5406210,
    "end": 5410350,
    "text": "なるほど、3人ではアクティベーションが飽和しすぎてしまう。"
  },
  {
    "start": 5410690,
    "end": 5417582,
    "text": "3より5が、10hのアクティベーションを持つリニアレイヤーのサンドイッチには良い設定だ。"
  },
  {
    "start": 5417726,
    "end": 5422180,
    "text": "標準偏差を妥当なところでおおよそ安定させる。"
  },
  {
    "start": 5422550,
    "end": 5425522,
    "text": "正直なところ、5オーバーがどこから来たのか見当もつかない。"
  },
  {
    "start": 5425576,
    "end": 5436466,
    "text": "Pytorchでは、私たちが来る初期化を見ていたとき、私は経験的に、この線形と10時間のサンドイッチを安定させ、飽和が良い範囲にあることを見ました。"
  },
  {
    "start": 5436658,
    "end": 5439554,
    "text": "これが数学の公式から導き出されたものなのかどうかは分からない。"
  },
  {
    "start": 5439602,
    "end": 5444458,
    "text": "これがどこから来たのか簡単に検索してみたが、何も見つからなかった。"
  },
  {
    "start": 5444624,
    "end": 5447270,
    "text": "確かに、経験的にこれらの範囲は非常に素晴らしいものであることがわかる。"
  },
  {
    "start": 5447350,
    "end": 5450682,
    "text": "飽和度はおよそ5％で、これはかなりいい数字だ。"
  },
  {
    "start": 5450816,
    "end": 5454954,
    "text": "これは、この文脈におけるゲインの良い設定である。"
  },
  {
    "start": 5455082,
    "end": 5458042,
    "text": "同様に、グラデーションでもまったく同じことができる。"
  },
  {
    "start": 5458186,
    "end": 5461310,
    "text": "10時間なら、まったく同じループだ。"
  },
  {
    "start": 5461380,
    "end": 5466862,
    "text": "レイヤーを取り出す代わりに、グラッドを取り出し、平均と標準偏差も表示しています。"
  },
  {
    "start": 5467006,
    "end": 5469630,
    "text": "これらの値のヒストグラムをプロットしている。"
  },
  {
    "start": 5469790,
    "end": 5473310,
    "text": "だから、勾配分布がかなり合理的であることがわかるだろう。"
  },
  {
    "start": 5473390,
    "end": 5479458,
    "text": "特に、私たちが求めているのは、このサンドイッチのすべての異なるレイヤーがほぼ同じグラデーションであることだ。"
  },
  {
    "start": 5479554,
    "end": 5481698,
    "text": "物事は縮小も爆発もしない。"
  },
  {
    "start": 5481874,
    "end": 5487174,
    "text": "例えば、ここに来て、このゲインが小さすぎた場合にどうなるかを見てみることができる。"
  },
  {
    "start": 5487292,
    "end": 5488262,
    "text": "これはゼロだった。"
  },
  {
    "start": 5488316,
    "end": 5488920,
    "text": "5人だ。"
  },
  {
    "start": 5490410,
    "end": 5494138,
    "text": "するとまず、すべてのアクティベーションがゼロに縮小しているのがわかる。"
  },
  {
    "start": 5494224,
    "end": 5496230,
    "text": "また、グラデーションが変なことになっている。"
  },
  {
    "start": 5496310,
    "end": 5500460,
    "text": "グラデーションはここから始まって、今は広がっているような感じだ。"
  },
  {
    "start": 5501070,
    "end": 5508286,
    "text": "同様に、例えばゲインが高すぎる場合、例えば3だとすると、グラデーションも高くなる。"
  },
  {
    "start": 5508308,
    "end": 5513802,
    "text": "より深い層に行くにつれて、活性化も変化するという非対称性が起こっている。"
  },
  {
    "start": 5513946,
    "end": 5515342,
    "text": "だから、それは我々が望んでいることではない。"
  },
  {
    "start": 5515396,
    "end": 5527378,
    "text": "この場合、今行っているようにバッチタームを使用しなければ、フォワードパスとバックワードパスの両方で素晴らしいアクティベーションを得るためには、非常に慎重にゲインを設定しなければならないことがわかった。"
  },
  {
    "start": 5527464,
    "end": 5529810,
    "text": "さて、バッチ正規化に移る前に。"
  },
  {
    "start": 5529970,
    "end": 5533766,
    "text": "また、10Hユニットがない場合にどうなるかも見てみたい。"
  },
  {
    "start": 5533868,
    "end": 5541878,
    "text": "つまり、10Hの非線形性をすべて消し去り、利得を3より5のままにしておけば、巨大な線形サンドイッチができあがるというわけだ。"
  },
  {
    "start": 5541974,
    "end": 5543850,
    "text": "アクティベーションがどうなるか見てみよう。"
  },
  {
    "start": 5544270,
    "end": 5549558,
    "text": "前に見たように、ここでの正しいゲインは標準偏差を維持したゲインである。"
  },
  {
    "start": 5549654,
    "end": 5553278,
    "text": "1.667は高すぎる。"
  },
  {
    "start": 5553444,
    "end": 5556510,
    "text": "だから、これから起こることは次のようなことだ。"
  },
  {
    "start": 5557170,
    "end": 5562350,
    "text": "10時間レイヤーがなくなったので、リニアに変更せざるを得なかった。"
  },
  {
    "start": 5562790,
    "end": 5564980,
    "text": "これもリニアに変更しよう。"
  },
  {
    "start": 5565830,
    "end": 5575022,
    "text": "私たちが見ているのは、青から始まった活性化が、第4層では非常に拡散しているということだ。"
  },
  {
    "start": 5575086,
    "end": 5577320,
    "text": "アクティベーションに何が起きているかというと、こういうことだ。"
  },
  {
    "start": 5577690,
    "end": 5587458,
    "text": "一番上のレイヤーのグラデーションでは、グラデーションの統計は紫色で、レイヤーの奥に行くにつれて薄くなっていく。"
  },
  {
    "start": 5587554,
    "end": 5590678,
    "text": "だから基本的には、ニューラルネットのように非対称性がある。"
  },
  {
    "start": 5590764,
    "end": 5598506,
    "text": "非常に深いニューラルネットワーク、例えば50層とか、そういうものを使っている場合、これはあまり良い場所ではないと想像するかもしれない。"
  },
  {
    "start": 5598688,
    "end": 5603982,
    "text": "そのため、バッチ正規化以前は、この設定は非常に厄介だった。"
  },
  {
    "start": 5604116,
    "end": 5611182,
    "text": "特に、ゲインが大きすぎるとこうなり、ゲインが小さすぎるとこうなる。"
  },
  {
    "start": 5611316,
    "end": 5621954,
    "text": "その反対は、基本的には縮小と拡散である。"
  },
  {
    "start": 5622152,
    "end": 5624114,
    "text": "だから、これはあなたが望んでいることではない。"
  },
  {
    "start": 5624152,
    "end": 5629954,
    "text": "この場合、初期化時に行っているように、ゲインの正しい設定はちょうど1である。"
  },
  {
    "start": 5630082,
    "end": 5635970,
    "text": "ということは、フォワードパスとバックワードパスの統計はお行儀が良いことがわかる。"
  },
  {
    "start": 5636130,
    "end": 5650380,
    "text": "なぜこれをお見せしたいかというと、基本的には、正規化レイヤーの前や、まだ説明する必要のあるatomのような高度なオプティマイザーを使う前、そして残差接続などを使う前に、ニューラルネストにトレーニングさせるようなものだからです。"
  },
  {
    "start": 5650750,
    "end": 5652954,
    "text": "ニューラルネットのトレーニングは基本的に次のようになる。"
  },
  {
    "start": 5653152,
    "end": 5654910,
    "text": "完全なバランス感覚のようなものだ。"
  },
  {
    "start": 5654980,
    "end": 5661146,
    "text": "すべてが正確にオーケストレーションされていることを確認しなければならないし、アクティベーションやグラジエント、その統計にも気を配らなければならない。"
  },
  {
    "start": 5661258,
    "end": 5663262,
    "text": "それなら、何かトレーニングできるかもしれない。"
  },
  {
    "start": 5663396,
    "end": 5665726,
    "text": "非常に深いネットワークを訓練することは基本的に不可能だった。"
  },
  {
    "start": 5665758,
    "end": 5668002,
    "text": "これが根本的な理由だ。"
  },
  {
    "start": 5668136,
    "end": 5670690,
    "text": "初期化には細心の注意が必要だ。"
  },
  {
    "start": 5672070,
    "end": 5680180,
    "text": "ところで、なぜ10層ものレイヤーが必要なのか？"
  },
  {
    "start": 5680550,
    "end": 5692058,
    "text": "もちろん、その理由は、ただ線形レイヤーを積み重ねただけなら、非常に簡単に素晴らしい活性化などが得られるからだ。"
  },
  {
    "start": 5692224,
    "end": 5694454,
    "text": "これはただの巨大なリニアサンドイッチだ。"
  },
  {
    "start": 5694502,
    "end": 5699450,
    "text": "その結果、表現力という点では1つの線形レイヤーに集約されることが判明した。"
  },
  {
    "start": 5699600,
    "end": 5704238,
    "text": "出力を入力の関数としてプロットした場合、一次関数が得られるだけだ。"
  },
  {
    "start": 5704324,
    "end": 5708906,
    "text": "直線的なレイヤーをいくら重ねても、結局は直線的な変換にしかならない。"
  },
  {
    "start": 5709018,
    "end": 5716420,
    "text": "すべてのWX＋Bは、わずかに異なるWとわずかに異なるBを持つ大きなWX＋Bに崩壊するだけだ。"
  },
  {
    "start": 5717350,
    "end": 5728498,
    "text": "興味深いことに、逆伝播と後方パスのダイナミクスのために前方パスは単なる線形レイヤーに崩壊するにもかかわらず、実際には最適化は同一ではない。"
  },
  {
    "start": 5728594,
    "end": 5737718,
    "text": "チェーンルールの計算方法のおかげで、バックワードパスにはいろいろな面白いダイナミクスが生まれるんだ。"
  },
  {
    "start": 5737804,
    "end": 5746618,
    "text": "つまり、リニアレイヤーを単体で最適化するのと、10個のリニアレイヤーでサンドイッチにしたものを最適化するのでは、どちらもフォワードパスでの線形変換にすぎない。"
  },
  {
    "start": 5746704,
    "end": 5754318,
    "text": "実際、無限に階層化された線形レイヤーなどを分析した論文もある。"
  },
  {
    "start": 5754404,
    "end": 5757600,
    "text": "だから、そこで遊べることはたくさんある。"
  },
  {
    "start": 5758530,
    "end": 5774820,
    "text": "基本的には、10×非線形性によって、このサンドイッチを単なる線形関数から、原理的には任意の関数を近似できるニューラルネットワークに変えることができる。"
  },
  {
    "start": 5775350,
    "end": 5781766,
    "text": "さて、これで以前のようにリニア10hサンドを使うようにコードをリセットし、すべてをリセットした。"
  },
  {
    "start": 5781868,
    "end": 5783670,
    "text": "で5点差。"
  },
  {
    "start": 5783820,
    "end": 5790230,
    "text": "最適化を1ステップ実行し、フォワードパスとバックワードパスの活性化統計を見ることができる。"
  },
  {
    "start": 5790380,
    "end": 5795914,
    "text": "ニューラルネットをトレーニングする際に、もうひとつ重要なプロットを追加しました。"
  },
  {
    "start": 5796112,
    "end": 5804026,
    "text": "結局のところ、私たちがやっているのはニューラルネットのパラメーターを更新することなので、パラメーターとその値、勾配が気になる。"
  },
  {
    "start": 5804218,
    "end": 5814570,
    "text": "ここで私がやっているのは、実際に利用可能なすべてのパラメーターを反復処理し、それを2次元のパラメーターに限定している。"
  },
  {
    "start": 5814650,
    "end": 5821890,
    "text": "バイアスは省略し、バッシュルームでのガンマとベータも省略している。"
  },
  {
    "start": 5822310,
    "end": 5824162,
    "text": "そちらもご覧ください。"
  },
  {
    "start": 5824216,
    "end": 5828120,
    "text": "ウェイトで何が起こっているかは、それだけで有益だ。"
  },
  {
    "start": 5828810,
    "end": 5832070,
    "text": "ここにすべての異なるウェイトとその形状がある。"
  },
  {
    "start": 5832650,
    "end": 5837458,
    "text": "これがエンベディングレイヤーで、最初のリニアレイヤーから最後のリニアレイヤーまである。"
  },
  {
    "start": 5837554,
    "end": 5842694,
    "text": "そして、これらすべてのパラメーターの平均と標準偏差、ヒストグラムが得られる。"
  },
  {
    "start": 5842742,
    "end": 5844810,
    "text": "を見ると、実際にはそれほど素晴らしいものではないことがわかる。"
  },
  {
    "start": 5844880,
    "end": 5846486,
    "text": "パラダイスでトラブルが起きている。"
  },
  {
    "start": 5846598,
    "end": 5850250,
    "text": "このグラデーションは問題ないように見えたが、何か変だ。"
  },
  {
    "start": 5850320,
    "end": 5851770,
    "text": "それについてはまた後で。"
  },
  {
    "start": 5851920,
    "end": 5855642,
    "text": "そして最後に、グラデーションとデータの比率である。"
  },
  {
    "start": 5855786,
    "end": 5865414,
    "text": "グラデーションのスケールと実際の数値のスケールを比較することで、グラデーションのスケールを知ることができるからだ。"
  },
  {
    "start": 5865562,
    "end": 5869490,
    "text": "これは重要なことだ。"
  },
  {
    "start": 5870470,
    "end": 5873810,
    "text": "つまり、学習率にデータへの勾配をかけたものである。"
  },
  {
    "start": 5873960,
    "end": 5881410,
    "text": "だから、もし勾配の大きさが大きすぎたら、もしそこにある数字がデータの数字に比べて大きすぎたら、問題になる。"
  },
  {
    "start": 5881570,
    "end": 5885190,
    "text": "この場合、データへの勾配は我々の低い数字である。"
  },
  {
    "start": 5885340,
    "end": 5893340,
    "text": "gradの中にある値は、これらのウェイトの中にあるデータよりも1000倍小さい。"
  },
  {
    "start": 5893790,
    "end": 5896966,
    "text": "さて、最後のレイヤーに関しては、それは真実ではない。"
  },
  {
    "start": 5897078,
    "end": 5915666,
    "text": "というのも、ピンクの最後のレイヤーは、ニューラルネット内部のいくつかの値よりもはるかに大きな値を取るからです。"
  },
  {
    "start": 5915768,
    "end": 5925582,
    "text": "最後の層の勾配の標準偏差が1～2であることを除けば、標準偏差は全体を通してほぼ1～3である。"
  },
  {
    "start": 5925726,
    "end": 5935622,
    "text": "つまり、最後のレイヤーの勾配は、ニューラルネット内の他のウェイトの約100倍、申し訳ないが10倍になっている。"
  },
  {
    "start": 5935756,
    "end": 5946650,
    "text": "というのも、単純な確率的勾配センスのセットアップでは、初期化時に他のレイヤーをトレーニングするよりも、この最後のレイヤーを約10倍速くトレーニングすることになるからだ。"
  },
  {
    "start": 5947070,
    "end": 5951162,
    "text": "さて、実はこの問題は、もう少し長くトレーニングすれば少しは解決する。"
  },
  {
    "start": 5951216,
    "end": 5955520,
    "text": "だから、例えば1000を超えたら、その時だけ休憩を入れる。"
  },
  {
    "start": 5956050,
    "end": 5959802,
    "text": "再初期化させてから、1000ステップやらせてくれ。"
  },
  {
    "start": 5959946,
    "end": 5963806,
    "text": "1000歩進んだら、フォワードパスを見ることができる。"
  },
  {
    "start": 5963988,
    "end": 5975650,
    "text": "ニューロンは少し飽和気味で、バックワードパスも見ることができますが、それ以外は良好で、ほぼ等しく、ゼロに縮んだり、無限大に爆発したりすることはありません。"
  },
  {
    "start": 5975990,
    "end": 5980422,
    "text": "ウェイトも少し安定してきているのがわかるだろう。"
  },
  {
    "start": 5980476,
    "end": 5985938,
    "text": "最後のピンクのレイヤーのテールは、最適化中に実際に入ってきている。"
  },
  {
    "start": 5986114,
    "end": 5994982,
    "text": "atomのような最新のオプティマイザーではなく、確率的勾配降下のような非常に単純な更新ルールを使っている場合は特にそうだ。"
  },
  {
    "start": 5995126,
    "end": 5998886,
    "text": "さて、ニューラルネットワークを訓練するときに私がいつも見ているプロットをもうひとつお見せしよう。"
  },
  {
    "start": 5998998,
    "end": 6012586,
    "text": "なぜなら、最終的に重要なのは勾配対データ比ではなく、更新対データ比だからだ。"
  },
  {
    "start": 6012778,
    "end": 6019170,
    "text": "ここで、データレシオの新しいアップデートを紹介したい。"
  },
  {
    "start": 6019590,
    "end": 6022690,
    "text": "リスト化し、反復するたびに作り上げていくつもりだ。"
  },
  {
    "start": 6023030,
    "end": 6029030,
    "text": "ここでは、基本的に反復ごとに比率を記録しておきたい。"
  },
  {
    "start": 6030010,
    "end": 6037990,
    "text": "勾配がない場合は、学習率に勾配をかけた更新を比較する。"
  },
  {
    "start": 6038810,
    "end": 6041770,
    "text": "これが、すべてのパラメーターに適用されるアップデートだ。"
  },
  {
    "start": 6042430,
    "end": 6047014,
    "text": "ほら、すべてのパラメーターを反復して、基本的に更新の標準偏差を取っている。"
  },
  {
    "start": 6047062,
    "end": 6055630,
    "text": "それを実際の内容、そのパラメーターのデータとその標準偏差で割るのだ。"
  },
  {
    "start": 6055970,
    "end": 6062042,
    "text": "これは、基本的に、これらのテンソルの値に対する更新の大きさの比率である。"
  },
  {
    "start": 6062186,
    "end": 6063538,
    "text": "それからログを取るんだ。"
  },
  {
    "start": 6063544,
    "end": 6069490,
    "text": "本当はログ10を撮りたいんだけどね。"
  },
  {
    "start": 6070230,
    "end": 6079034,
    "text": "我々は基本的に、この除算の指数と、フロートをポップアウトする項目を見ている。"
  },
  {
    "start": 6079182,
    "end": 6083954,
    "text": "このUdテンソルは、全パラメータを記録し、Udテンソルに追加する。"
  },
  {
    "start": 6084082,
    "end": 6087190,
    "text": "では、再初期化して1000回反復してみよう。"
  },
  {
    "start": 6087530,
    "end": 6093962,
    "text": "先ほどと同じように、アクティベーション、グラジエント、パラメーターのグラジエントを見ることができる。"
  },
  {
    "start": 6094096,
    "end": 6096570,
    "text": "さて、ここでもうひとつ紹介したい筋書きがある。"
  },
  {
    "start": 6097310,
    "end": 6103870,
    "text": "ここで起こっているのは、すべてのパラメーターを反復しているのだが、ここでやったように、またウェイトだけに制約をかけている。"
  },
  {
    "start": 6104530,
    "end": 6107582,
    "text": "これらのセンサーの次元数は2である。"
  },
  {
    "start": 6107716,
    "end": 6113040,
    "text": "そして、私は基本的に、これらの更新比率をすべて経時的にプロットしている。"
  },
  {
    "start": 6114290,
    "end": 6120222,
    "text": "この比率をプロットすると、初期化中に時間とともに変化していくのがわかる。"
  },
  {
    "start": 6120286,
    "end": 6125380,
    "text": "そして、これらのアップデートは、通常トレーニング中に安定し始める。"
  },
  {
    "start": 6125750,
    "end": 6129350,
    "text": "もうひとつは、おおよその値をプロットしています。"
  },
  {
    "start": 6129420,
    "end": 6135190,
    "text": "これがおおよその目安で、だいたいマイナス3分の1くらいになるはずだ。"
  },
  {
    "start": 6135340,
    "end": 6150250,
    "text": "つまり、基本的にこのテンソルにはいくつかの値があり、それらは特定の値をとり、反復ごとに更新される値は、それらのテンソルの実際の大きさのおよそ1000分の1以下ということだ。"
  },
  {
    "start": 6150750,
    "end": 6159982,
    "text": "もしこれがもっと大きかったら、例えばこの対数がマイナス1とかだったら、これは実際にかなり値を更新していることになる。"
  },
  {
    "start": 6160036,
    "end": 6161774,
    "text": "彼らは多くの変化を経験している。"
  },
  {
    "start": 6161972,
    "end": 6173490,
    "text": "最後のレイヤーが異常値である理由は、ソフトマックスの自信を失わせないために、このレイヤーが人為的に縮小されたからである。"
  },
  {
    "start": 6174310,
    "end": 6183590,
    "text": "ここでは、最後のレイヤーの予測の信頼性を低くするために、初期化で重みを0.1倍していることがわかる。"
  },
  {
    "start": 6185210,
    "end": 6189126,
    "text": "そのため、人為的にテンソルの中の値が低くなりすぎた。"
  },
  {
    "start": 6189228,
    "end": 6191990,
    "text": "だから一時的に非常に高い比率になっているんだ。"
  },
  {
    "start": 6192070,
    "end": 6197626,
    "text": "その体重が学習し始めると、時間の経過とともに安定するのがわかるだろう。"
  },
  {
    "start": 6197808,
    "end": 6208910,
    "text": "基本的に、私は通常、すべてのパラメータについてこの更新比率の推移を見たい。"
  },
  {
    "start": 6209570,
    "end": 6212762,
    "text": "この対数プロットではマイナス3前後。"
  },
  {
    "start": 6212906,
    "end": 6217090,
    "text": "マイナス3以下であれば、通常はパラメーターのトレーニング速度が十分でないことを意味する。"
  },
  {
    "start": 6217240,
    "end": 6220530,
    "text": "もし学習率が非常に低かったら、その実験をしてみよう。"
  },
  {
    "start": 6221590,
    "end": 6227094,
    "text": "初期化して、実際に学習率を1eマイナス3にしてみよう。"
  },
  {
    "start": 6227292,
    "end": 6229238,
    "text": "だから0.1だ。"
  },
  {
    "start": 6229404,
    "end": 6236086,
    "text": "学習率が低すぎる場合、このプロットでそれが明らかになる。"
  },
  {
    "start": 6236268,
    "end": 6240122,
    "text": "これらのアップデートがいかに小さすぎるかがわかるだろう。"
  },
  {
    "start": 6240256,
    "end": 6250538,
    "text": "更新のサイズは基本的に、テンソルに含まれる数値のサイズの10,000倍になる。"
  },
  {
    "start": 6250624,
    "end": 6253450,
    "text": "これは遅すぎるトレーニングの症状だ。"
  },
  {
    "start": 6254430,
    "end": 6258926,
    "text": "これは、学習率を設定し、その学習率がどの程度であるべきか感覚をつかむためのもうひとつの方法である。"
  },
  {
    "start": 6259028,
    "end": 6262080,
    "text": "結局のところ、これはあなたが記録しておくものだ。"
  },
  {
    "start": 6264870,
    "end": 6273954,
    "text": "というのも、マイナス3という黒い線の上にいるからだ。"
  },
  {
    "start": 6273992,
    "end": 6275794,
    "text": "マイナス2.5といったところだ。"
  },
  {
    "start": 6275912,
    "end": 6279626,
    "text": "でも、すべてがある程度安定している。"
  },
  {
    "start": 6279678,
    "end": 6283846,
    "text": "というわけで、学習率などはかなりまともな設定のようだ。"
  },
  {
    "start": 6283948,
    "end": 6285270,
    "text": "これは見ておくべきことだ。"
  },
  {
    "start": 6285340,
    "end": 6288214,
    "text": "誤算があれば、すぐにわかる。"
  },
  {
    "start": 6288332,
    "end": 6292138,
    "text": "例えば、すべてがうまくいっているように見えるだろう？"
  },
  {
    "start": 6292224,
    "end": 6296202,
    "text": "比較対象として、物事が適切に調整されていない場合、それはどのように見えるか？"
  },
  {
    "start": 6296336,
    "end": 6301418,
    "text": "ここに来て、例えば、どうするとしよう。"
  },
  {
    "start": 6301584,
    "end": 6305610,
    "text": "正規化でこのファンを適用するのを忘れたとしよう。"
  },
  {
    "start": 6305690,
    "end": 6310030,
    "text": "線形レイヤー内の重みは、すべてのステージでガウスからのサンプルである。"
  },
  {
    "start": 6311970,
    "end": 6313920,
    "text": "何かがおかしいとどうやって気づくのか？"
  },
  {
    "start": 6314370,
    "end": 6318526,
    "text": "活性化プロットを見ればわかるだろう。"
  },
  {
    "start": 6318718,
    "end": 6320820,
    "text": "グラデーションがめちゃくちゃになってしまう。"
  },
  {
    "start": 6321190,
    "end": 6325058,
    "text": "これらのウェイトのヒストグラムもめちゃくちゃになる。"
  },
  {
    "start": 6325144,
    "end": 6326926,
    "text": "非対称性が多い。"
  },
  {
    "start": 6327038,
    "end": 6330534,
    "text": "となると、ここもまた、かなり混乱しているのではないだろうか。"
  },
  {
    "start": 6330652,
    "end": 6338390,
    "text": "これらのレイヤーの学習速度にはかなりのばらつきがあり、中には学習速度が速すぎるレイヤーもある。"
  },
  {
    "start": 6338540,
    "end": 6343990,
    "text": "マイナス1、マイナス1.5、この比率から見れば非常に大きな数字だ。"
  },
  {
    "start": 6344150,
    "end": 6347820,
    "text": "繰り返しになるが、マイナス3くらいで、それ以上にはならないはずだ。"
  },
  {
    "start": 6348350,
    "end": 6352666,
    "text": "こうして神経網の誤作動が現れるのだ。"
  },
  {
    "start": 6352778,
    "end": 6364174,
    "text": "このようなプロットは、誤算に気づかせ、それに対処させる良い方法だ。"
  },
  {
    "start": 6364292,
    "end": 6375534,
    "text": "さて、ここまでで、この線形10時間サンドイッチがあれば、実際にゲインを正確に較正し、活性化、勾配、パラメーター、更新をすべてかなりまともにできることがわかった。"
  },
  {
    "start": 6375662,
    "end": 6380930,
    "text": "確かに、指の上で鉛筆のバランスを取っているような感じだ。"
  },
  {
    "start": 6381090,
    "end": 6385570,
    "text": "なぜなら、このゲインは非常に正確に調整されなければならないからだ。"
  },
  {
    "start": 6385730,
    "end": 6393020,
    "text": "では、バッシュの正規化レイヤーをミックスに導入して、この問題を解決する方法を見てみよう。"
  },
  {
    "start": 6393790,
    "end": 6400634,
    "text": "ここで、バッシュルーム1Dクラスを取り、中に配置し始める。"
  },
  {
    "start": 6400832,
    "end": 6409050,
    "text": "先に述べたように、標準的な典型的な配置はリニアレイヤーの間、つまりリニアレイヤーの直後でノンリニアレイヤーの前だ。"
  },
  {
    "start": 6409130,
    "end": 6416430,
    "text": "実際、非線形性の後に配置しても、よく似た結果を得ることができる。"
  },
  {
    "start": 6417570,
    "end": 6424594,
    "text": "もうひとつ言っておきたいのは、最後の線形レイヤーの後、損失関数の前に配置してもまったく問題ないということだ。"
  },
  {
    "start": 6424712,
    "end": 6427560,
    "text": "これも問題ないだろう。"
  },
  {
    "start": 6428570,
    "end": 6432680,
    "text": "この場合、出力はアップサイズになる。"
  },
  {
    "start": 6434010,
    "end": 6440438,
    "text": "さて、最後のレイヤーはバシャームなので、ソフトマックスの確信度を下げるためにウェイトを変更することはないだろう。"
  },
  {
    "start": 6440534,
    "end": 6455194,
    "text": "ガンマを変更することになる。バッシュルームでは、ガンマは正規化の出力と乗算的に相互作用する変数なので、このサンドイッチを初期化することができる。"
  },
  {
    "start": 6455242,
    "end": 6461502,
    "text": "これでトレーニングができるようになり、アクティベーションがとても良くなるのがわかる。"
  },
  {
    "start": 6461636,
    "end": 6469186,
    "text": "なぜなら、10時間のレイヤーの前に、バッシュのノルムが正規化されるからだ。"
  },
  {
    "start": 6469288,
    "end": 6472994,
    "text": "これは意外なことに、すべてがかなり良く見える。"
  },
  {
    "start": 6473032,
    "end": 6479714,
    "text": "標準偏差はほぼゼロで、65％、全層にわたってほぼ同じ標準偏差になる。"
  },
  {
    "start": 6479762,
    "end": 6482070,
    "text": "すべてが非常に均質に見える。"
  },
  {
    "start": 6482490,
    "end": 6493510,
    "text": "勾配はよく見え、重みもよく見え、その分布と更新もかなり合理的に見える。"
  },
  {
    "start": 6494010,
    "end": 6497642,
    "text": "マイナス3を少し上回っているが、それほど大きくはない。"
  },
  {
    "start": 6497776,
    "end": 6502880,
    "text": "ここでは、すべてのパラメーターがほぼ同じ割合でトレーニングされている。"
  },
  {
    "start": 6504530,
    "end": 6513966,
    "text": "今、私たちが得たものは、これらの獲得に関して、少しもろくなったということだ。"
  },
  {
    "start": 6514068,
    "end": 6522100,
    "text": "例えば、ゲインを0.2にすることができる。"
  },
  {
    "start": 6522790,
    "end": 6526370,
    "text": "これからわかるように、アクティベーションはまったく影響を受けない。"
  },
  {
    "start": 6526710,
    "end": 6529378,
    "text": "それは、やはり、この明示的な正規化のためだ。"
  },
  {
    "start": 6529554,
    "end": 6536120,
    "text": "グラデーションは問題なく見え、ウェイトグラデーションも問題なく見える。"
  },
  {
    "start": 6536810,
    "end": 6555962,
    "text": "つまり、バッチノルムの後方パスと、バッチノルムとその後方パスで入力されるアクティブ度のスケールがどのように相互作用するかによって、前方パスと後方パスが非常に大きく変化しても、実際にはこれらのパラメーターの更新のスケールが変化しているのだ。"
  },
  {
    "start": 6556106,
    "end": 6559306,
    "text": "これらの重みの勾配は影響を受ける。"
  },
  {
    "start": 6559498,
    "end": 6564818,
    "text": "この国ではまだ、恣意的なウェイトでのパスが完全にフリーパスというわけではない。"
  },
  {
    "start": 6564984,
    "end": 6572814,
    "text": "それ以外は、前方、後方、ウェイトの勾配という点で、かなりロバストである。"
  },
  {
    "start": 6572942,
    "end": 6581234,
    "text": "ただ、バッチ規範に入ってくるアクティベーションのスケールを十分に変更する場合は、学習レートを再チューニングする必要があるかもしれない。"
  },
  {
    "start": 6581362,
    "end": 6590120,
    "text": "例えば、ここではリニアレイヤーのゲインを大きく変更した。"
  },
  {
    "start": 6591530,
    "end": 6596518,
    "text": "最後に、バッチ規範を使う場合は、必ずしもその必要はない。"
  },
  {
    "start": 6596614,
    "end": 6597802,
    "text": "これを1つにリセットしよう。"
  },
  {
    "start": 6597856,
    "end": 6598982,
    "text": "得るものはない。"
  },
  {
    "start": 6599126,
    "end": 6603262,
    "text": "私たちは、時にはファンによって正常化する必要さえない。"
  },
  {
    "start": 6603396,
    "end": 6611610,
    "text": "ファンを取り除くと、ランダムなガウシアンになる。"
  },
  {
    "start": 6611770,
    "end": 6623458,
    "text": "スティックはもちろん、フォワードパスもよく見えるし、グラデーションもよく見える。"
  },
  {
    "start": 6623624,
    "end": 6629078,
    "text": "レイヤーの一部に少し脂肪の尾を引くと、これも問題なさそうだ。"
  },
  {
    "start": 6629164,
    "end": 6633606,
    "text": "ご覧の通り、マイナス3を大きく下回っている。"
  },
  {
    "start": 6633628,
    "end": 6638882,
    "text": "このバッチルームの学習率を上げて、より適切なトレーニングができるようにしなければならない。"
  },
  {
    "start": 6638946,
    "end": 6645420,
    "text": "特に、これを大まかに見ると、学習率を10倍にして、マイナス3分の1にする必要があるようだ。"
  },
  {
    "start": 6646510,
    "end": 6651006,
    "text": "私たちはここに来て、これを1.0のアップデートに変更する。"
  },
  {
    "start": 6651188,
    "end": 6661600,
    "text": "再初期化すれば、もちろんすべてうまくいく。"
  },
  {
    "start": 6662290,
    "end": 6666778,
    "text": "まあまあのトレーニングになるだろう。"
  },
  {
    "start": 6666964,
    "end": 6676030,
    "text": "つまり、手短に言えば、ファンを投入してゲインを変えるかどうかにかかわらず、これらのリニアレイヤーのゲインにはかなり強くなっているのだ。"
  },
  {
    "start": 6676190,
    "end": 6683974,
    "text": "実際には、更新スケールに少し気を配り、学習率が適切に調整されていることを確認する必要がある。"
  },
  {
    "start": 6684092,
    "end": 6693980,
    "text": "フォワード・バックワード・パスのアクティベーションとアップデートは、ここで調整される可能性のあるグローバル・スケールを除けば、かなりお行儀よくなっているように見える。"
  },
  {
    "start": 6694510,
    "end": 6696230,
    "text": "さて、では要約しよう。"
  },
  {
    "start": 6696390,
    "end": 6699366,
    "text": "このセクションで達成したかったことが3つある。"
  },
  {
    "start": 6699478,
    "end": 6709406,
    "text": "その1、バッシュ正規化について紹介したい。バッシュ正規化は、ディープ・ニューラル・ネットワークとそのトレーニングを安定させるのに役立つ、私たちが注目している最初の現代的イノベーションの1つだ。"
  },
  {
    "start": 6709588,
    "end": 6715806,
    "text": "バッシュの正規化がどのように機能し、ニューラルネットワークでどのように使われるのか、ご理解いただけただろうか。"
  },
  {
    "start": 6715998,
    "end": 6721742,
    "text": "その2、私たちのコードの一部をパイトール化して、これらのモジュールにまとめたいと思っていました。"
  },
  {
    "start": 6721806,
    "end": 6724734,
    "text": "リニア・バシャーム、1D、10Hなど。"
  },
  {
    "start": 6724782,
    "end": 6730738,
    "text": "これらはレイヤーまたはモジュールで、レゴの積み木のように積み上げてニューラルネットにすることができる。"
  },
  {
    "start": 6730914,
    "end": 6734786,
    "text": "これらのレイヤーは実際にパイトークに存在する。"
  },
  {
    "start": 6734898,
    "end": 6744490,
    "text": "もしtorch nnをインポートすれば、私が構築した方法では、これらの異なるレイヤーすべてにnnをプリペンドするだけで、Pytorchを使うことができる。"
  },
  {
    "start": 6745070,
    "end": 6752422,
    "text": "私がここで開発したAPIは、Pytorchが使っているAPIと同じだからだ。"
  },
  {
    "start": 6752566,
    "end": 6758010,
    "text": "この実装も、私が知る限り、基本的にはPytorchのものと同じだ。"
  },
  {
    "start": 6758170,
    "end": 6766094,
    "text": "その3は、ニューラルネットワークが良い状態にあるかどうかを動的に理解するための診断ツールを紹介する。"
  },
  {
    "start": 6766222,
    "end": 6787494,
    "text": "そして、確率的勾配上昇の一部として更新される重みを調べ、その平均値、標準偏差、さらにデータに対する勾配の比率、あるいはデータに対する更新の比率を見ています。"
  },
  {
    "start": 6787692,
    "end": 6793542,
    "text": "私たちは通常、ある特定の反復において、時間を凍結させた単一のスナップショットとしては見ていないことを見た。"
  },
  {
    "start": 6793686,
    "end": 6797578,
    "text": "一般的には、私がここでやったように、時間をかけて見ていくものだ。"
  },
  {
    "start": 6797664,
    "end": 6801242,
    "text": "彼らはデータに対する更新比率を見て、すべてが問題ないことを確認する。"
  },
  {
    "start": 6801376,
    "end": 6811566,
    "text": "特に、1対マイナス3、つまり、基本的にロックのスケールでマイナス3が、この比率をどうしたいかの大まかな目安になると言った。"
  },
  {
    "start": 6811668,
    "end": 6816350,
    "text": "もしそれが高すぎるなら、おそらく学習率か更新が少し大きすぎるのだろう。"
  },
  {
    "start": 6816420,
    "end": 6819378,
    "text": "もしそれが小さすぎるなら、学習率が小さすぎるのだろう。"
  },
  {
    "start": 6819544,
    "end": 6825860,
    "text": "というのは、ニューラルネットワークをうまく機能させようとするときに、遊んでみたくなるようなことのほんの一部である。"
  },
  {
    "start": 6826630,
    "end": 6829058,
    "text": "今、私が達成しようとしなかったことがいくつもある。"
  },
  {
    "start": 6829154,
    "end": 6833842,
    "text": "私は、バッシュ・ノーム・レイヤーを導入することで、例として以前のパフォーマンスを打ち負かそうとはしなかった。"
  },
  {
    "start": 6833986,
    "end": 6839766,
    "text": "実はやってみたんだ。前に説明した学習率を見つけるメカニズムを使ってね。"
  },
  {
    "start": 6839868,
    "end": 6847914,
    "text": "バッシュ・ノルム・レイヤー、つまりバッシュ・ノルム・ニューラルネットを訓練してみたが、結果は以前得たものと非常によく似ていた。"
  },
  {
    "start": 6848112,
    "end": 6855022,
    "text": "というのも、最適化によってパフォーマンスがボトルネックになることがなくなったからだ。"
  },
  {
    "start": 6855156,
    "end": 6861738,
    "text": "この段階でのパフォーマンスは、コンテキストの長さがネックになっていると思われる。"
  },
  {
    "start": 6861914,
    "end": 6864702,
    "text": "現在、私たちは4人目を予想するために3人の人物を取り上げている。"
  },
  {
    "start": 6864756,
    "end": 6875780,
    "text": "リカレント・ニューラル・ネットワークやトランスフォーマーのような、より強力なアーキテクチャに目を向ける必要がある。"
  },
  {
    "start": 6876390,
    "end": 6885266,
    "text": "また、これらのアクティベーション、グラディエント、バックワードパス、そしてこれらすべてのグラディエントの統計について完全な説明をしようとはしなかった。"
  },
  {
    "start": 6885378,
    "end": 6895562,
    "text": "ここでゲインを変えたら、どうして別の学習レートが必要になるのか。"
  },
  {
    "start": 6895616,
    "end": 6901114,
    "text": "なぜなら、これらの異なるレイヤーの後方経路を実際に見て、すべての仕組みについて直感的に理解する必要があるからだ。"
  },
  {
    "start": 6901232,
    "end": 6903846,
    "text": "今回の講義では、それについては触れなかった。"
  },
  {
    "start": 6903958,
    "end": 6915438,
    "text": "しかし、初期化、バックワードパス、そしてそれらすべてがどのように相互作用するのかを理解するために、直感的なレベルではまだ多くの作業が残っている。"
  },
  {
    "start": 6915614,
    "end": 6922722,
    "text": "正直なところ、私たちはこの分野の最先端に到達しつつあるのだから。"
  },
  {
    "start": 6922856,
    "end": 6927966,
    "text": "確かに、初期化は解決していないし、逆伝播も解決していない。"
  },
  {
    "start": 6928078,
    "end": 6930646,
    "text": "これらはまだ非常に活発な研究分野である。"
  },
  {
    "start": 6930748,
    "end": 6937414,
    "text": "人々はまだ、これらのネットワークを初期化する最良の方法は何か、使用する最良の更新ルールは何か、などを解明しようとしている。"
  },
  {
    "start": 6937452,
    "end": 6943890,
    "text": "どれも本当の解決には至っていないし、これらすべての事件に対するすべての答えを持っているわけでもない。"
  },
  {
    "start": 6944050,
    "end": 6950800,
    "text": "少なくとも前進はしているし、物事が正しい方向に進んでいるかどうかを教えてくれるツールもある。"
  },
  {
    "start": 6951650,
    "end": 6956940,
    "text": "この講義で、私たちは前向きな進歩を遂げることができたと思う。"
  }
]