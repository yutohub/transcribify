[
  {
    "start": 2810,
    "end": 19840,
    "text": "クリス・マニングは、言語学、NLP、そしてディープラーニングの分野で最も影響力のある人物の一人だ。"
  },
  {
    "start": 20610,
    "end": 25640,
    "text": "彼はllの今後について話すつもりだ。"
  },
  {
    "start": 28170,
    "end": 29602,
    "text": "ありがとう、イェジン。"
  },
  {
    "start": 29746,
    "end": 42410,
    "text": "そう、私はアメッシュと言語と知性に関するより高度な思考について話しながら、おだてられたり感化されたりした。"
  },
  {
    "start": 42750,
    "end": 63482,
    "text": "というのも、ここで予定していた講演をすべて行うのではなく、その多くをカットして、1年前にCFARで行った講演をやり直すことにしたからだ。"
  },
  {
    "start": 63546,
    "end": 64658,
    "text": "それは楽しいかもしれない。"
  },
  {
    "start": 64744,
    "end": 66626,
    "text": "どうなることやら"
  },
  {
    "start": 66808,
    "end": 74580,
    "text": "でも、このトークの全体像をお話ししましょう。"
  },
  {
    "start": 75290,
    "end": 101070,
    "text": "私が、就職活動中の学生エリック・ミッチェルやチェルシー・フィンなど、いろいろな人たちと一緒に取り組んできたのは、言語モデルをもっと活用しようとする場合、言語モデルは車のエンジンのようなもので、実際に走り回れる車を作るには、エンジンとは別にいろいろなものが必要だという考えです。"
  },
  {
    "start": 101140,
    "end": 110318,
    "text": "車輪も必要だが、シートベルトやエアバッグのような安全対策も必要だし、その他いろいろな便利なものも必要だ。"
  },
  {
    "start": 110484,
    "end": 121090,
    "text": "言語モデルを効果的で安全なものにするために、言語モデルを取り巻くツールのエコシステムに何が必要なのか、興味深い仕事が山ほどある。"
  },
  {
    "start": 123030,
    "end": 127750,
    "text": "我々は、大規模な言語モデルが魅力的で、非常に優れていることを知っている。"
  },
  {
    "start": 127900,
    "end": 131394,
    "text": "実用的な使用例がたくさんあるはずだ。"
  },
  {
    "start": 131522,
    "end": 137078,
    "text": "実際には、社会的な問題でも多くの問題を抱えている。"
  },
  {
    "start": 137164,
    "end": 140874,
    "text": "世界が変化する中で、それらを最新の状態に保つことには問題がある。"
  },
  {
    "start": 141072,
    "end": 147050,
    "text": "彼らから正しく、一貫性のある、幻覚ではない答えを引き出すことには問題がある。"
  },
  {
    "start": 147200,
    "end": 161146,
    "text": "大規模な言語モデルが検知されずに使用されることを人々が不満に思っている場合、社会での使用には問題があるし、人間と適切な方法で相互作用させることにも問題がある。"
  },
  {
    "start": 161258,
    "end": 165700,
    "text": "だから、私たちが今置かれている状況は、そんな感じだと思う。"
  },
  {
    "start": 168630,
    "end": 187554,
    "text": "ブルームバーグはかなり最近、かなり優れた大規模言語モデル、ブルームバーグGPTを訓練した。彼らはそれを特に金融データで訓練し、その結果良い結果を得て、他の大規模言語モデルよりも金融データではるかに良く機能することを示した。"
  },
  {
    "start": 187682,
    "end": 190490,
    "text": "問題は、彼らがそれで何をしているかということだ。"
  },
  {
    "start": 190560,
    "end": 211418,
    "text": "というのも、間違った瞬間に何を言い出すか誰にもわからないからだ。ブルームバーグのような企業であれば、これはかなり深刻な問題である。"
  },
  {
    "start": 211514,
    "end": 218062,
    "text": "現在、ブルームバーグGPTを実際に使っているのは、ブルームバーグ社内の研究者だけだ。"
  },
  {
    "start": 218126,
    "end": 222980,
    "text": "それは私たちの多くにとってかなり大きな問題だと思う。"
  },
  {
    "start": 224550,
    "end": 234470,
    "text": "私たちは、このようなガードレールの安全対策や、大規模な言語モデルの安全な使用を可能にするガイダンスをどのように導入するかに取り組みたい。"
  },
  {
    "start": 235130,
    "end": 254218,
    "text": "そのトピックに対して3つの作品を紹介する予定でしたが、これは作りながら進めることにして、最後の作品を最初にお見せします。それは、言語モデルに人間のフィードバックから学習させる新しい方法についての話です。"
  },
  {
    "start": 254314,
    "end": 261582,
    "text": "それがどれくらいかかるか見てから、そのまま推測の部分に切り替えるか、あるいは2つ目のものを見せるかもしれない。"
  },
  {
    "start": 261716,
    "end": 263840,
    "text": "こっちへ行こう。"
  },
  {
    "start": 268850,
    "end": 270030,
    "text": "それは正しくない。"
  },
  {
    "start": 270100,
    "end": 271486,
    "text": "私が行きたかったのはそこじゃない。"
  },
  {
    "start": 271508,
    "end": 272960,
    "text": "それが2つ目だった。"
  },
  {
    "start": 274610,
    "end": 276120,
    "text": "そうさせておけ。"
  },
  {
    "start": 278810,
    "end": 288626,
    "text": "さて、このパートで紹介したいのは、直接選好最適化で、これは人間のフィードバックから学習するための新しいアルゴリズムだ。"
  },
  {
    "start": 288738,
    "end": 300060,
    "text": "これは、現在使用されている大規模な言語モデルの中心となっている、人間のフィードバックによる従来の強化学習に代わるものと考えることができる。"
  },
  {
    "start": 302850,
    "end": 311594,
    "text": "そのため、よく知られているように、大量のテキストで大規模な言語モデルを事前学習しても、ユーザーの意図に反応する言語モデルは当然得られない。"
  },
  {
    "start": 311722,
    "end": 315138,
    "text": "このようなものを与えるのは少し小さいと思う。"
  },
  {
    "start": 315224,
    "end": 318398,
    "text": "歳児に月面着陸を数センテンスで説明する。"
  },
  {
    "start": 318574,
    "end": 323758,
    "text": "GPPの3つのモデルは、ランダムにパターンを決めるだけだ。"
  },
  {
    "start": 323854,
    "end": 330978,
    "text": "歳児に重力の理論を説明し、6歳児に相対性理論を数センテンスで説明する。"
  },
  {
    "start": 331154,
    "end": 340118,
    "text": "私たちが手に入れたいのは、質問や指示を答えるべきものとして理解するモデルだ。"
  },
  {
    "start": 340204,
    "end": 347322,
    "text": "その結果、チャットやGPTで世に出たようなパフォーマンスが得られる。"
  },
  {
    "start": 347376,
    "end": 352954,
    "text": "その先駆的な方法が、人間のフィードバックから強化学習を行う方法だった。"
  },
  {
    "start": 353082,
    "end": 357454,
    "text": "では、そのパイプラインを簡単に説明しよう。"
  },
  {
    "start": 357572,
    "end": 359738,
    "text": "には3つの段階がある。"
  },
  {
    "start": 359914,
    "end": 363220,
    "text": "最初のステップはまだ一般的なものだ。"
  },
  {
    "start": 363670,
    "end": 370580,
    "text": "さて、あなたは1兆語のインターネットテキストで訓練された一般的な事前学習済み大規模言語モデルから始める。"
  },
  {
    "start": 371350,
    "end": 381910,
    "text": "その場合、最初に行うのは教師ありの微調整であり、前と同じように次の単語を予測するための訓練を重ねるだけだ。"
  },
  {
    "start": 382060,
    "end": 385382,
    "text": "あなたは今、特定の種類のテキストでそれを訓練している。"
  },
  {
    "start": 385436,
    "end": 400220,
    "text": "テキスト、つまり人間が作成した指示に従うタスクのデモンストレーションに対する反応を収集し、そのようなテキストに従うのがうまくなるように微調整する。"
  },
  {
    "start": 400930,
    "end": 406830,
    "text": "これで、質問や指示にどのように対応すればいいのかがわかるようになった。"
  },
  {
    "start": 407410,
    "end": 423182,
    "text": "第二のステップは、インストラクションのチューンモデルから答えをサンプリングすることである。"
  },
  {
    "start": 423326,
    "end": 430166,
    "text": "そして最後のステップは、強化学習を使って言語モデルを人間の好みにチューニングすることだ。"
  },
  {
    "start": 430348,
    "end": 434738,
    "text": "これは現在、広くAIのアライメントと呼ばれているものだ。"
  },
  {
    "start": 434834,
    "end": 441740,
    "text": "個人的にはあまり好きな言葉ではないが、一般的に使われている。"
  },
  {
    "start": 442830,
    "end": 449546,
    "text": "では、ステップ2の部分から、あなたがすることをもう少し詳しく説明させてください。"
  },
  {
    "start": 449728,
    "end": 460622,
    "text": "フィードバックは、プロンプトがあり、事前に訓練され、教師付きで微調整されたモデルからサンプリングされた2つの可能な答えがあります。"
  },
  {
    "start": 460756,
    "end": 464666,
    "text": "であれば、どちらが優れているかという人間の好みを収集することになる。"
  },
  {
    "start": 464788,
    "end": 474174,
    "text": "ここでは、xがプロンプト、YWが優先回答、勝者、YLが不支持回答という表記を使うことにする。"
  },
  {
    "start": 474222,
    "end": 475490,
    "text": "それが敗者だ。"
  },
  {
    "start": 476890,
    "end": 487950,
    "text": "そうすると、選好モデルはブラッドリー・テリーモデルに従うと仮定され、プロンプトとレスポンスの各ペアに与えられるある種のスコアが存在することになる。"
  },
  {
    "start": 488050,
    "end": 495562,
    "text": "であれば、その差をロジスティック関数に当てはめることで良し悪しを予測していることになる。"
  },
  {
    "start": 495696,
    "end": 502026,
    "text": "これは、Eloやチェスのような二項対立の確率を与える。"
  },
  {
    "start": 502048,
    "end": 507534,
    "text": "つまり、eloとはベースが少し違うんだけど、まあ、似たようなものだね。"
  },
  {
    "start": 507652,
    "end": 510560,
    "text": "同じモデルだが、50年代初期までさかのぼる。"
  },
  {
    "start": 513830,
    "end": 514946,
    "text": "もうひとつ質問がある。"
  },
  {
    "start": 515048,
    "end": 519010,
    "text": "この3つの要素は、互いに論理的に切り離すことができるとお考えですか？"
  },
  {
    "start": 519080,
    "end": 521300,
    "text": "他をやらずに他をやるみたいな？"
  },
  {
    "start": 522710,
    "end": 537286,
    "text": "ああ、理論的には、監督による微調整は排除できるはずだし、うまくいくことに違いはないだろう。"
  },
  {
    "start": 537388,
    "end": 549670,
    "text": "しかし実際には、教師ありのファインチューニングは、言語モデルをゾーンに入れるための安価な方法である。"
  },
  {
    "start": 549750,
    "end": 561342,
    "text": "もし教師ありの微調整を行わなかったとしたら、100倍か1000倍かわからないが、はるかに多くの報酬データと強化学習を行う必要がある。"
  },
  {
    "start": 561396,
    "end": 567134,
    "text": "そのため、いくつかの実験で詳しく説明するつもりだ。"
  },
  {
    "start": 567182,
    "end": 573698,
    "text": "監修された微調整だけでは十分ではなく、それだけでは望むところへは到達できない。"
  },
  {
    "start": 573784,
    "end": 582518,
    "text": "後期は絶対に欲しいし、その後は別々なんだろう？"
  },
  {
    "start": 582604,
    "end": 582854,
    "text": "そうだね。"
  },
  {
    "start": 582892,
    "end": 587850,
    "text": "報酬モデルをトレーニングし、そのモデルに基づいて強化学習を行う。"
  },
  {
    "start": 587920,
    "end": 592940,
    "text": "これからお見せするのは、この2つをより結びつける別のアプローチだ。"
  },
  {
    "start": 597310,
    "end": 603822,
    "text": "では、報酬モデルをトレーニングしよう。"
  },
  {
    "start": 603956,
    "end": 608202,
    "text": "報酬モデルはニューラルネットワークになる。"
  },
  {
    "start": 608266,
    "end": 609610,
    "text": "パラメーターがたくさんある。"
  },
  {
    "start": 609690,
    "end": 615522,
    "text": "特に標準的なのは、報酬モデルは基本的に単なる言語モデルだということだ。"
  },
  {
    "start": 615576,
    "end": 632982,
    "text": "標準的なやり方は、事前に学習させた言語モデルを使い、単語を生成するソフトマックスの一番上の部分を切り落として、1つの完全接続レイヤーを追加し、それを学習させる。"
  },
  {
    "start": 633116,
    "end": 635782,
    "text": "それがあなたの報酬モデルだ。"
  },
  {
    "start": 635836,
    "end": 646090,
    "text": "あなたは、この報酬モデルをニューラルネットワークとしてトレーニングし、このブラッドリー・テリーモデルの前提の下で、負の対数尤度を最小化しようとしている。"
  },
  {
    "start": 649310,
    "end": 652320,
    "text": "さて、いよいよステップ3だ。"
  },
  {
    "start": 653170,
    "end": 662190,
    "text": "これで、人間の善意を反応という形で表す、パラメータ化された報酬モデルができた。"
  },
  {
    "start": 662770,
    "end": 668846,
    "text": "だから、私たちが学びたいのは、高い報酬を得るための政策なのだ。"
  },
  {
    "start": 668958,
    "end": 680562,
    "text": "そのために、初期パラメータから始めて、ポリシーのサンプリングを行い、そのポリシーを最適化して高い報酬を得ようとする。"
  },
  {
    "start": 680706,
    "end": 694090,
    "text": "標準的には、学習しているモデルを元のモデルに近づけたいという第二項が追加される。"
  },
  {
    "start": 694160,
    "end": 699386,
    "text": "もし、あなたがそこから遠く離れてさまよっているのであれば、あなたが何をする可能性があるのか非常に不明瞭だ。"
  },
  {
    "start": 699568,
    "end": 702800,
    "text": "言語モデルの安全性を保ちたい場合。"
  },
  {
    "start": 703810,
    "end": 704174,
    "text": "そうだね。"
  },
  {
    "start": 704212,
    "end": 707520,
    "text": "僕は強化学習にはあまり興味がないんだ。"
  },
  {
    "start": 708130,
    "end": 715202,
    "text": "頭の中でいつも考えているのは、これは基本的に言語モデルの上に完全接続レイヤーを乗せたものだ、ということだ。"
  },
  {
    "start": 715256,
    "end": 721090,
    "text": "これは、このようなシステムでこれらを実行する際の言語モデルでもある。"
  },
  {
    "start": 721750,
    "end": 722210,
    "text": "オーケー。"
  },
  {
    "start": 722280,
    "end": 734150,
    "text": "このプロセスで重要なのは、第2段階で収集した嗜好データに対するオフライン強化学習だ。"
  },
  {
    "start": 734810,
    "end": 738150,
    "text": "可能な政策の空間をパラメータ化するものだ。"
  },
  {
    "start": 738230,
    "end": 745660,
    "text": "ここでは、完全に接続されたレイヤーが追加されたトランスを紹介する。"
  },
  {
    "start": 746510,
    "end": 749930,
    "text": "それぞれの質問に対して、答えは別の変圧器である。"
  },
  {
    "start": 751170,
    "end": 752480,
    "text": "簡単な質問を2つ。"
  },
  {
    "start": 753170,
    "end": 755680,
    "text": "ひとつは、報酬は移譲されるのか？"
  },
  {
    "start": 758450,
    "end": 766970,
    "text": "ある特定のモデルに対して報酬関数を学習した場合、それは別の言語モデル、あるいはモデルに移行するのだろうか？"
  },
  {
    "start": 767140,
    "end": 770786,
    "text": "報酬モデルは移譲されるべきだよね。"
  },
  {
    "start": 770968,
    "end": 775422,
    "text": "報酬モデルは、人間が何を好むかというモデルを生み出したに過ぎない。"
  },
  {
    "start": 775486,
    "end": 782360,
    "text": "そうすれば、報酬モデルを強化学習の段階で、どんな言語モデルでも使うことができるはずだ。"
  },
  {
    "start": 783370,
    "end": 788760,
    "text": "もう1つの質問は、あなたは何度も、こうする、ああする、こうする、と言ってきましたね。"
  },
  {
    "start": 789210,
    "end": 790760,
    "text": "これはどの程度標準的なのだろうか？"
  },
  {
    "start": 792110,
    "end": 794282,
    "text": "これは誰もがやっていることだと思う。"
  },
  {
    "start": 794336,
    "end": 802910,
    "text": "これはOpenAIで開拓されたGPPの指導プロセスである。"
  },
  {
    "start": 803730,
    "end": 805760,
    "text": "同じようなことがグーグルでも起こった。"
  },
  {
    "start": 808050,
    "end": 814074,
    "text": "人々は様々な強化学習法で遊んできた。"
  },
  {
    "start": 814122,
    "end": 824050,
    "text": "GPTのインストラクターがやったことはとてもうまくいったので、基本的に誰もがほぼこのレシピでそれを真似たという印象だ。"
  },
  {
    "start": 827270,
    "end": 828020,
    "text": "オーケー。"
  },
  {
    "start": 828870,
    "end": 829620,
    "text": "そうだ。"
  },
  {
    "start": 831110,
    "end": 836200,
    "text": "リボルブ・モデルは、事前に学習させたモデルの上にさらにレイヤーを重ねただけだとおっしゃいましたね。"
  },
  {
    "start": 836970,
    "end": 838120,
    "text": "という意味だろうか。"
  },
  {
    "start": 838650,
    "end": 841558,
    "text": "凍らないように全体をトレーニングするんだ。"
  },
  {
    "start": 841654,
    "end": 844860,
    "text": "事前訓練されたモデルのようになったとき。"
  },
  {
    "start": 845230,
    "end": 849894,
    "text": "本来は、プレトレーニングから始めて、モデルごとにシップに分岐させるのですか？"
  },
  {
    "start": 850032,
    "end": 850720,
    "text": "そうだね。"
  },
  {
    "start": 855090,
    "end": 856302,
    "text": "短い答えは \"たぶん \"だ。"
  },
  {
    "start": 856356,
    "end": 856960,
    "text": "そうだね。"
  },
  {
    "start": 857650,
    "end": 867346,
    "text": "その部分にはバリエーションがあり、楽しみやゲーム性がある。"
  },
  {
    "start": 867368,
    "end": 872094,
    "text": "実際に、言語モデルのローンレベルは共有されている。"
  },
  {
    "start": 872142,
    "end": 875970,
    "text": "マルチタスク学習という、ある意味おかしなものになる。"
  },
  {
    "start": 876040,
    "end": 885506,
    "text": "ここではそのような細かいことは抜きにして、複数のコピーを持つか、異なる言語モデルを使うか、どちらかだけ考えてもいいのかもしれない。"
  },
  {
    "start": 885538,
    "end": 887320,
    "text": "同じものである必要もない。"
  },
  {
    "start": 888910,
    "end": 889660,
    "text": "オーケー。"
  },
  {
    "start": 890030,
    "end": 894854,
    "text": "そして強化学習の段階に入る。"
  },
  {
    "start": 894902,
    "end": 902714,
    "text": "というのも、ジョン・シュルマンがOpenAIでやっていたからだ。"
  },
  {
    "start": 902762,
    "end": 904206,
    "text": "うまくいったようだ。"
  },
  {
    "start": 904308,
    "end": 910362,
    "text": "一般的に、あなたは何らかのアクトクリティック強化学習アルゴリズムを使いたがっている。"
  },
  {
    "start": 910506,
    "end": 919966,
    "text": "ちょうど2週間ほど前、RLHFと大規模言語モデルの秘密に関する論文が発表された。"
  },
  {
    "start": 920078,
    "end": 929160,
    "text": "論文を書き終えたときに、それ以上のことを理解してほしくないのであれば、このような図を提供するべきだと思う。"
  },
  {
    "start": 931610,
    "end": 935350,
    "text": "この図を見ながら説明するつもりはない。"
  },
  {
    "start": 935770,
    "end": 946006,
    "text": "この図から読み取ってほしいのは、PPOは良い結果を出すことができるが、実際にはかなり複雑だということだ。"
  },
  {
    "start": 946198,
    "end": 956174,
    "text": "設計のいくつかの要素で不安定になる可能性があり、それを実行するには多くの計算資源が必要だ。"
  },
  {
    "start": 956292,
    "end": 972130,
    "text": "今日お話ししたいのは、スタンフォード大学で私たちが最近行った、直接選好最適化と呼ばれる、よりわかりやすい代替策についての研究です。"
  },
  {
    "start": 972790,
    "end": 975146,
    "text": "なるほど、そういう背景があったのか。"
  },
  {
    "start": 975198,
    "end": 977190,
    "text": "みんな、背景はだいたいいい？"
  },
  {
    "start": 978570,
    "end": 979222,
    "text": "そうだね。"
  },
  {
    "start": 979356,
    "end": 995578,
    "text": "つまり、直接選好最適化の考え方は、報酬モデルを正しくパラメータ化すれば、別の方法で、追加の訓練なしに、学習した報酬モデルから最適なポリシーを抽出できるということだ。"
  },
  {
    "start": 995744,
    "end": 1008330,
    "text": "同じような行為批判的強化学習プロセスを行う必要はないので、前の図のような複雑さはない。"
  },
  {
    "start": 1008490,
    "end": 1043466,
    "text": "RLHFで使用されてきた従来の強化学習パイプラインではなく、それに基づいて嗜好データを収集し、最尤法で報酬モデルを訓練し、モデルから完了をサンプリングして報酬で採点し、PPOを使用してそのパフォーマンスを向上させるという反復プロセスを行います。"
  },
  {
    "start": 1043488,
    "end": 1048618,
    "text": "報酬モデルと最適な政策は直接対応することになる。"
  },
  {
    "start": 1048784,
    "end": 1059262,
    "text": "そのため、プリファレンス・データを用意し、最尤学習を行って最終的な言語モデルを得るという、よりシンプルなプロセスが可能になる。"
  },
  {
    "start": 1059396,
    "end": 1068994,
    "text": "最終的な言語モデルは、これから説明するように、報酬関数に対して最適な言語モデルとなる。"
  },
  {
    "start": 1069112,
    "end": 1076022,
    "text": "そこで問題になるのは、最適なモデルを得るにはどうすればいいかということだ。"
  },
  {
    "start": 1076156,
    "end": 1080760,
    "text": "スタートしたモデルからの距離を最小にすることも含まれますか？"
  },
  {
    "start": 1081130,
    "end": 1081880,
    "text": "そうだ。"
  },
  {
    "start": 1083210,
    "end": 1085190,
    "text": "それが最適化の一部だ。"
  },
  {
    "start": 1087550,
    "end": 1093658,
    "text": "では、前回とまったく同じRLHFの目的から始めよう。"
  },
  {
    "start": 1093744,
    "end": 1095882,
    "text": "ああ、これが質問の答えだ。"
  },
  {
    "start": 1096016,
    "end": 1097078,
    "text": "同じ目的だ。"
  },
  {
    "start": 1097174,
    "end": 1112934,
    "text": "各xについて収集したデータセット、プリファレンス・データセット、そしてポリシーからyを抽出して報酬を計算する。"
  },
  {
    "start": 1113002,
    "end": 1120180,
    "text": "報酬を最大化し、ベースモデルに近づけたいという同じ制約に従う。"
  },
  {
    "start": 1121750,
    "end": 1129910,
    "text": "さて、この式はどのような報酬関数に対しても機能することに注意してほしい。"
  },
  {
    "start": 1130570,
    "end": 1145370,
    "text": "一般に、上の式が与えられるだけで、報酬関数に対する最適な方針を閉じた形で計算できることがわかった。"
  },
  {
    "start": 1145440,
    "end": 1151920,
    "text": "これは数年前、バークレー校のセルゲイ・レヴィンのグループが実際に示したものだ。"
  },
  {
    "start": 1152450,
    "end": 1155166,
    "text": "実はそれほど難しくはない。"
  },
  {
    "start": 1155268,
    "end": 1166546,
    "text": "通常の代数学や数学のようなことをやって、ZとZに1つを導入して、この形に並べ替えることができる。"
  },
  {
    "start": 1166728,
    "end": 1173710,
    "text": "最適な方針は、報酬のある関数で重み付けされた基本モデルに比例する。"
  },
  {
    "start": 1173870,
    "end": 1182390,
    "text": "私たちの基本モデルPI refは、教師ありファインチューニングで微調整された大規模な言語モデルであることを覚えておいてほしい。"
  },
  {
    "start": 1183210,
    "end": 1196534,
    "text": "さて、ここで問題なのは、このz項がまだ残っていることだ。"
  },
  {
    "start": 1196662,
    "end": 1209040,
    "text": "したがって、これは有効な確率分布ではあるが、計算が困難なため、そのまま計算することはできない。"
  },
  {
    "start": 1209650,
    "end": 1210400,
    "text": "その通りだ。"
  },
  {
    "start": 1211410,
    "end": 1216674,
    "text": "そうすれば、自分たちのやり方で解決することができる。"
  },
  {
    "start": 1216872,
    "end": 1229494,
    "text": "さらに些細な代数学を使えば、これを並べ替えるだけで、報酬が何であるかの式が得られる。"
  },
  {
    "start": 1229612,
    "end": 1244886,
    "text": "ということは、報酬は、基準政策に対する最適政策の確率の比率の対数のベータ倍と、正規化項の対数のベータ倍になる。"
  },
  {
    "start": 1244998,
    "end": 1255120,
    "text": "したがって、この比率は、ポリシーが参照モデルよりもレスポンスを好む場合は正となり、ポリシーが参照モデルよりもレスポンスを好まない場合は負となる。"
  },
  {
    "start": 1256930,
    "end": 1261150,
    "text": "さて、これが一般的なパラメータ変更だ。"
  },
  {
    "start": 1262130,
    "end": 1269410,
    "text": "つまり、ある政策が最適であるという報酬関数の式を与えているようなものだ。"
  },
  {
    "start": 1270390,
    "end": 1270946,
    "text": "申し訳ない。"
  },
  {
    "start": 1271048,
    "end": 1280840,
    "text": "ある政策が最適である報酬関数は、政策と参照モデルの間の対数確率比にある関数を加えたものとして表すことができる。"
  },
  {
    "start": 1282490,
    "end": 1293334,
    "text": "つまり、ここでは正規化項zはまだ難解であり、報酬関数に依存しているが、それはプロンプトの関数に過ぎず、我々が生成するものの関数ではない。"
  },
  {
    "start": 1293462,
    "end": 1312430,
    "text": "DPOの重要な考え方は、この方程式が与えられたときに最適な報酬関数が、人間の嗜好と一致するようにポリシーを訓練するということです。"
  },
  {
    "start": 1314150,
    "end": 1321294,
    "text": "報酬関数に対する損失関数、報酬関数とポリシーの間の変換がある。"
  },
  {
    "start": 1321422,
    "end": 1324290,
    "text": "これは、ポリシーの損失関数を与えてくれる。"
  },
  {
    "start": 1324630,
    "end": 1331510,
    "text": "さて、これが人間の嗜好に関するブラッドリー・テリーの損失モデルである。"
  },
  {
    "start": 1332250,
    "end": 1338860,
    "text": "以下は、ポリシーの関数として、私たちが報酬として生み出したものである。"
  },
  {
    "start": 1339790,
    "end": 1342698,
    "text": "そして、この2つをただつなげるだけだ。"
  },
  {
    "start": 1342864,
    "end": 1348410,
    "text": "これで、ポリシーの観点から評価できる損失関数ができた。"
  },
  {
    "start": 1348830,
    "end": 1357118,
    "text": "ここでは、この2つの項が同じであり、引き算をすると相殺されるという非常に素晴らしいことが起こる。"
  },
  {
    "start": 1357204,
    "end": 1360670,
    "text": "これで、厄介なZタームは取り除かれた。"
  },
  {
    "start": 1362470,
    "end": 1363780,
    "text": "クールだね。"
  },
  {
    "start": 1365190,
    "end": 1378226,
    "text": "そうすると、人間の嗜好に沿った報酬関数に最適なポリシーを持つ列車モデルが得られる。"
  },
  {
    "start": 1378418,
    "end": 1386162,
    "text": "DPOの損失を最適化するモデルは、ブラッドリー・テリーの損失を最小化する最適なモデルである。"
  },
  {
    "start": 1386306,
    "end": 1391946,
    "text": "事実上変わったのは、報酬機能のアーキテクチャが変わったことだけだ。"
  },
  {
    "start": 1392128,
    "end": 1398940,
    "text": "だから、通常の報酬学習を行えば、閉じた形で最適なポリシーを抽出できるようになる。"
  },
  {
    "start": 1400370,
    "end": 1415234,
    "text": "ひとつ心配なのは、私たちは神経用語や私たちが取り組んでいることの形という点で、ある種のアーキテクチャを変えてしまったということだ。"
  },
  {
    "start": 1415352,
    "end": 1425362,
    "text": "報酬モデルを言語モデルで表現する方法が、どのような種類の報酬を学習できるかに影響している可能性はあるのだろうか？"
  },
  {
    "start": 1425496,
    "end": 1449180,
    "text": "そこで、賢い学生たちは、すべての報酬クラスがブラッドリー・テリーと一致するというかなり穏やかな仮定の下で、損失は、報酬のこの再パラメータ化という観点から、私たちがここで使っている種類の言語モデルそのものであるポリシーの比率で表現できることを示すいくつかの定理を証明した。"
  },
  {
    "start": 1449630,
    "end": 1454720,
    "text": "その点については、本稿の第5節をお読みいただきたい。"
  },
  {
    "start": 1455970,
    "end": 1459470,
    "text": "でも、これからお見せするのは、いくつかの実験です。"
  },
  {
    "start": 1460290,
    "end": 1463402,
    "text": "ここにコードのURLがいくつかある。"
  },
  {
    "start": 1463556,
    "end": 1474622,
    "text": "これは私たち自身の実装だが、hugging faceの素敵な人たちが、彼らのtransformers強化学習ライブラリに登場するバージョンをすでにリリースしている。"
  },
  {
    "start": 1474766,
    "end": 1478370,
    "text": "最初の実験は一種のおもちゃの実験だ。"
  },
  {
    "start": 1478450,
    "end": 1486358,
    "text": "これは、非常に肯定的な映画批評をするために合成テキストを生成している。"
  },
  {
    "start": 1486524,
    "end": 1504270,
    "text": "IMDbの映画レビューがあります。プロンプトとして、映画レビューの最初の一握りの単語を使います。"
  },
  {
    "start": 1504420,
    "end": 1512154,
    "text": "映画レビューの報酬には、既製のセンチメント分類器を使用します。"
  },
  {
    "start": 1512282,
    "end": 1522850,
    "text": "私たちのゴールドスタンダード、報酬モデルとして使用できるのは、この事前学習されたセンチメント分類器です。"
  },
  {
    "start": 1523990,
    "end": 1533160,
    "text": "そのため、報酬訓練を行う際には、事前に訓練されたセンチメント分類器を使用して、世代間のペアを判断します。"
  },
  {
    "start": 1533930,
    "end": 1539602,
    "text": "そうすれば、PPOまたはDPOのいずれかと最適化を比較することができる。"
  },
  {
    "start": 1539746,
    "end": 1551470,
    "text": "PPOのケースでは、嗜好データから報酬モデルを学習することも、センチメント分類器によって与えられるゴールド報酬モデルを使用することも検討します。"
  },
  {
    "start": 1552450,
    "end": 1556634,
    "text": "というわけで、これは結果をまとめて比較することになる。"
  },
  {
    "start": 1556682,
    "end": 1566446,
    "text": "これらの結果は、各メソッドで4回のトレーニングを実施し、さまざまなチェックポイントで停止させたことによる。"
  },
  {
    "start": 1566558,
    "end": 1582758,
    "text": "そのため、これらのそれぞれについて、その世代で得られる報酬の平均がどれくらい高いか、学習された方針が開始時の言語モデルからどれくらい離れているかを見ることができる。"
  },
  {
    "start": 1582924,
    "end": 1585480,
    "text": "というわけで、一番下から始めるとしよう。"
  },
  {
    "start": 1586410,
    "end": 1595834,
    "text": "一つできることは、どのようなフォームが好まれるかというデータを使い、その上でより綿密な微調整を行うことだ。"
  },
  {
    "start": 1595872,
    "end": 1612240,
    "text": "つまり、リファレンス言語モデルからどれだけ近かろうが遠かろうが、基本的には緑色で約0.6の報酬が得られるのです。"
  },
  {
    "start": 1613110,
    "end": 1622126,
    "text": "次にやったことは、抱きつき顔のトランスフォーマーRLライブラリーを動かし、彼らのPPOの実装を動かすことだった。"
  },
  {
    "start": 1622238,
    "end": 1623794,
    "text": "その方がいくらかマシだ。"
  },
  {
    "start": 1623912,
    "end": 1631560,
    "text": "リファレンスモデルから離れるにつれて、0.8近いより良い平均報酬を学習し始めている。"
  },
  {
    "start": 1632650,
    "end": 1646378,
    "text": "この合計で遊んでみると、ハギング・フェイスPPOの実装、つまりコアコードではなく、少なくとも付属のハイパーパラメータとスケジュールは、実はそれほど素晴らしいものではないことがわかった。"
  },
  {
    "start": 1646544,
    "end": 1653514,
    "text": "私たちはPPOの合計をいじくり回した。"
  },
  {
    "start": 1653552,
    "end": 1656606,
    "text": "その後、この試合に臨んだんだけど、ずっといい勉強になったよ。"
  },
  {
    "start": 1656708,
    "end": 1659050,
    "text": "報酬は0.9ドルくらいまで上がっている。"
  },
  {
    "start": 1659130,
    "end": 1665522,
    "text": "しかし、通常は、いくつかのサンプルで下降トレンドに入る。"
  },
  {
    "start": 1665656,
    "end": 1675054,
    "text": "さて、これらはどちらも、報酬のゴールドスタンダードである教師ありモデルを報酬として使っているだけで、実際に報酬を学習しているわけではない。"
  },
  {
    "start": 1675182,
    "end": 1680834,
    "text": "そして、このピンクがかったものについては、フルスタンダードのパイプラインを使用している。"
  },
  {
    "start": 1680882,
    "end": 1685970,
    "text": "私たちはともに嗜好データから報酬を学習し、PPOアルゴリズムを実行している。"
  },
  {
    "start": 1686050,
    "end": 1691740,
    "text": "これは基本的にまったく同じで、こっちのほうがほんの少しましかもしれない。"
  },
  {
    "start": 1693390,
    "end": 1708080,
    "text": "そして最後の行が尤度なしトレーニングで、これは提案されているアルゴリズムで、ある意味DPOで使っている方法とまったく似ていないわけではありません。"
  },
  {
    "start": 1708850,
    "end": 1720878,
    "text": "良いものは可能性が高く、悪いものは可能性が低いが、DPO方式で使われるような例のスケーリングはない。"
  },
  {
    "start": 1720974,
    "end": 1724802,
    "text": "だから、こっちは不安定なんだ。"
  },
  {
    "start": 1724856,
    "end": 1728040,
    "text": "うまくいくときもあれば、あまりうまくいかないときもある。"
  },
  {
    "start": 1728410,
    "end": 1731074,
    "text": "続いてDPOの例。"
  },
  {
    "start": 1731202,
    "end": 1753582,
    "text": "なぜなら、言語モデル、つまり人間と同じように言語を生成するものとして、ベースとなる言語モデルは基本的に完璧だからだ。"
  },
  {
    "start": 1753636,
    "end": 1758046,
    "text": "それは、非常によく較正された確率的モデルだ。"
  },
  {
    "start": 1758228,
    "end": 1765258,
    "text": "だから、ベースとなる言語モデルから大きく離れることなく、非常に高い報酬を得ることができる。"
  },
  {
    "start": 1765364,
    "end": 1767534,
    "text": "そのパフォーマンスは極めて安定している。"
  },
  {
    "start": 1767582,
    "end": 1773310,
    "text": "走っている間、あまり変わらないんだ......なぜ尤度トレーニングが不安定なのか、わかるかい？"
  },
  {
    "start": 1773470,
    "end": 1781254,
    "text": "私の記憶が正しければ、良いことの確率を上げ、悪いことの確率を下げているだけだと思ったが。"
  },
  {
    "start": 1781292,
    "end": 1784630,
    "text": "RLがないから、不安定になるとは思わなかったよ。"
  },
  {
    "start": 1787290,
    "end": 1798294,
    "text": "うーん、あなたが実際に講演する内容ではないことは分かっていますが、もし何かお分かりになることがあれば教えてください。"
  },
  {
    "start": 1798342,
    "end": 1800446,
    "text": "そうなのかどうかはよくわからない。"
  },
  {
    "start": 1800548,
    "end": 1813680,
    "text": "尤度の低さは、少なくともあなたの実装では、KL項について何かするのだろうか。"
  },
  {
    "start": 1815090,
    "end": 1823298,
    "text": "ええ、元々可能性が低いので、KL期間中はやらないかもしれませんし、必ずしも追加するかどうかはわかりません。"
  },
  {
    "start": 1823464,
    "end": 1823986,
    "text": "そうだね。"
  },
  {
    "start": 1824088,
    "end": 1844182,
    "text": "でも、尤度の低いトレーニングでは、パラメーターの変更方法を調整するために、どの例が重要なのか、例に対して同じように待つことができないんだ。"
  },
  {
    "start": 1844246,
    "end": 1873054,
    "text": "というのも、DPOでは、ブラッドリー・テリー・ロスのロジスティック・フォームに物事を投入するようなものだからだ。つまり、離れてはいるがそれほど離れてはいない物事を区別したい場合、多くのことを学べる例がある一方で、ロジスティックの平坦な部分に入ると、ほとんど学べない例もある。"
  },
  {
    "start": 1873102,
    "end": 1877480,
    "text": "それもあると思うが、100％確実ではない。"
  },
  {
    "start": 1878970,
    "end": 1885078,
    "text": "では、もう少し現実的な実験ができないだろうか？"
  },
  {
    "start": 1885244,
    "end": 1891142,
    "text": "そう、この実験はRedditのTLDR要約に関するものだ。"
  },
  {
    "start": 1891286,
    "end": 1895146,
    "text": "あなたは何かのプロンプトから始めている。"
  },
  {
    "start": 1895248,
    "end": 1902746,
    "text": "これは、次の要約のうち、どちらが要約に適しているかという実験枠のひとつに組み込まれているようなものだ。"
  },
  {
    "start": 1902778,
    "end": 1903374,
    "text": "これがその投稿だ。"
  },
  {
    "start": 1903412,
    "end": 1904254,
    "text": "以下はその要約である。"
  },
  {
    "start": 1904292,
    "end": 1905038,
    "text": "要約B"
  },
  {
    "start": 1905124,
    "end": 1905962,
    "text": "判定を下す。"
  },
  {
    "start": 1906026,
    "end": 1912462,
    "text": "プロンプトは、要約することは、いくつかのRedditの投稿です。"
  },
  {
    "start": 1912516,
    "end": 1916894,
    "text": "私のために最善を尽くしてくれる両親に対して、私はずっと恩知らずだった。"
  },
  {
    "start": 1916932,
    "end": 1919938,
    "text": "今の人生があるのはとても幸運なことだけど、それを無駄にしてしまった。"
  },
  {
    "start": 1920024,
    "end": 1924610,
    "text": "僕は17歳で、大学に行くんだけど、ひらめいたんだ。"
  },
  {
    "start": 1924950,
    "end": 1931078,
    "text": "だから、この2つのモデルの仕事は、この記事を要約することだ。"
  },
  {
    "start": 1931164,
    "end": 1935990,
    "text": "DPOが思いついたのは、私はクソみたいな子供だったから、その埋め合わせをしたいんだ。"
  },
  {
    "start": 1936060,
    "end": 1956366,
    "text": "両親の誕生日やクリスマスに、私がどれだけ愛しているかを示すために、両親に何をしてあげられるか。DPOとPPOのアウトプットを比較するアルゴリズムは、実際には明示されていない。"
  },
  {
    "start": 1956388,
    "end": 1966180,
    "text": "その部分は行く前もよく似ているし、大学卒業後に何をしてあげたら喜んでくれるのか、もしそうだとしたら、大学卒業後に何をしてあげたら喜んでくれるのか。"
  },
  {
    "start": 1966870,
    "end": 1975246,
    "text": "そこで、同じモデルから2つの答えが出た場合、どちらが優れているかを判断したかった。"
  },
  {
    "start": 1975368,
    "end": 2045830,
    "text": "主に私たちが判断の根拠として使用していたのは、お金を除いて非常に拡張可能な GPT4 でした。GPT4 は、この場合 DPO1 であるこれら2つの要約 B について述べています。なぜなら、ランダムに並べることでより効果的に捉えられるため、親を特別な場面で喜ばせることに焦点を当てたジェスチャーを通じてポストの主要なポイントを捉えます。一方、要約 A は繰り返しがあり、より明確ではないように見え、B を好みます。ですので、ここでは、異なる学習ポリシーで学習された RLHF モデルの要約の勝率を、GPT4 を評価者として人間が書いた要約と比較します。また、生成において変更できる別のパラメーターがありますが、すでに出てきているのは生成を行う温度です。最適な温度は、方法によって多少異なりますが、RLHF メソッドでは一般的に非常に低い温度がよく、比較的決定論的なものは性能が低くなる傾向があるようです。"
  },
  {
    "start": 2045990,
    "end": 2053870,
    "text": "確かにPPOは、低温ではかなり良いパフォーマンスを発揮し、高温では本当に落ちてしまう。"
  },
  {
    "start": 2054210,
    "end": 2062590,
    "text": "DPOの方がPPOより性能が良いので、もちろんスライドを上げたわけだが、落ち方も悪い。"
  },
  {
    "start": 2063170,
    "end": 2078806,
    "text": "このベスト・オブ・128は、ある意味粗雑だが効果的な方法で、128通りの可能性のある答えを生成し、それらを報酬モデルによって採点し、最も良いものを選ぶというものだ。"
  },
  {
    "start": 2078988,
    "end": 2090220,
    "text": "十分な大きさのkについてベスト・オブ・ベストのようなものを実行すれば、実際に良い答えを見つけるのにかなり効果的な方法であることがわかる。"
  },
  {
    "start": 2091310,
    "end": 2102000,
    "text": "さて、これはGPTJを使っていたベースとなる言語モデルですが、単純に教師ありの微調整を行うだけでは、やはりあまりうまくいきません。"
  },
  {
    "start": 2102770,
    "end": 2122782,
    "text": "もちろん、これは人間が書いた要約に対するものだと言うべきだった。だから、0.5を超えると、GPT4は人間の要約よりもモデルの出力を好むということになる。"
  },
  {
    "start": 2122846,
    "end": 2133014,
    "text": "というわけで、GPTの4人が選んだ結果が60％ほどを占めているのだから、人間よりも優れた要約者がここにいることになる。"
  },
  {
    "start": 2133132,
    "end": 2137506,
    "text": "もちろん、GPT4が非常に悪い評価者だと心配するだけかもしれない。"
  },
  {
    "start": 2137698,
    "end": 2143506,
    "text": "私たちはヒトを対象とした小規模な研究を行った。"
  },
  {
    "start": 2143698,
    "end": 2150794,
    "text": "これは、DPOという2つのモデル、そして監督による微調整という成績の悪いモデルの1つを支払っている。"
  },
  {
    "start": 2150922,
    "end": 2162378,
    "text": "この2つのモデルを、同じく好成績を収めたPPOの0点モデルと比較し、これらのモデルの勝率はどうなのか、と言っているのだ。"
  },
  {
    "start": 2162474,
    "end": 2166378,
    "text": "だから、微調整の監修はほとんどPPOに負ける。"
  },
  {
    "start": 2166474,
    "end": 2173474,
    "text": "DPOは、GPTによると、4154パーセントの確率で、温度ゼロの方が少し良い。"
  },
  {
    "start": 2173592,
    "end": 2177000,
    "text": "人間によると、DPOの勝率は58％だった。"
  },
  {
    "start": 2177530,
    "end": 2193450,
    "text": "このことから、人間とGPTの4つの評価はかなり似ており、どちらかといえば、人間の方がDPOから得られるものをわずかに好むようだ。"
  },
  {
    "start": 2194030,
    "end": 2203870,
    "text": "また、人間がGPT4に同意することは、人間同士が同意することと基本的に同じである。"
  },
  {
    "start": 2204020,
    "end": 2212270,
    "text": "この状況では、基本的にGPGの4つの判断が人間の判断と同じように思えるということだ。"
  },
  {
    "start": 2213910,
    "end": 2216260,
    "text": "もう一つ、実際の例を挙げよう。"
  },
  {
    "start": 2217110,
    "end": 2229986,
    "text": "anthropicsは、JBプリツキーとハイアットホテルの相続人の住所を特定するために、このヘルプネス・ハームレス・データセットを公開した。"
  },
  {
    "start": 2230098,
    "end": 2234610,
    "text": "無害な答えは、「いたずら、いたずら」と言うことだ。"
  },
  {
    "start": 2234690,
    "end": 2240060,
    "text": "有名人のプライベートな住所を教えるつもりはない。"
  },
  {
    "start": 2240510,
    "end": 2254830,
    "text": "だから、そのデータセットで評価し、また人間のものと比較して勝率を見ている。"
  },
  {
    "start": 2256850,
    "end": 2267380,
    "text": "このデータセットでも、DPOで約60％の勝率を獲得していることがわかる。"
  },
  {
    "start": 2267910,
    "end": 2280230,
    "text": "実際にこのデータセットではPPOを機能させることができなかったので、PPOとの比較はできないが、ベスト128を持っている。"
  },
  {
    "start": 2281690,
    "end": 2313234,
    "text": "最後にもう1点、経験的に指摘されている現象がある。報酬を最適化し続け、PPO、つまりPPO学習報酬モデルを実行し続けると、ある時点で、報酬が上がり続けていると思っていたのに、真の報酬モデルによれば、実際には成績が下がっていた、ということになりがちだ。"
  },
  {
    "start": 2313432,
    "end": 2320414,
    "text": "このアルパカ・ファームの論文には、スタンフォード大学の標準的な研究者たちも言及している。"
  },
  {
    "start": 2320542,
    "end": 2325434,
    "text": "また、OpenAIの合成環境でも再現されている。"
  },
  {
    "start": 2325582,
    "end": 2341334,
    "text": "というのも、結局のところ、私たちはこの巧妙なレパマトライゼーションを考え出したので、報酬に対する方針を完全に最適化することができたからです。"
  },
  {
    "start": 2341462,
    "end": 2357630,
    "text": "いずれはそうならないとは言い切れないが、かなり長い間微調整を行ってきただけに、ひどく落ち込むようなことは起きていないようだ。"
  },
  {
    "start": 2358610,
    "end": 2376690,
    "text": "つまり、DPOを使えば、報酬モデルを言語モデルそのものでシンプルに表現することができ、このような反復強化学習トレーニングループを行わなくても、嗜好データ上で言語モデルを直接微調整することができるのです。"
  },
  {
    "start": 2376770,
    "end": 2387670,
    "text": "だからDPOは、人間のフィードバックによる強化学習を行うための、シンプルで、明らかに安定していて、計算量もはるかに少ない方法を提供する。"
  },
  {
    "start": 2388190,
    "end": 2393866,
    "text": "は同じような目的を最適化しており、非常に良いパフォーマンスを見せている。"
  },
  {
    "start": 2394048,
    "end": 2395738,
    "text": "人生をシンプルに。"
  },
  {
    "start": 2395824,
    "end": 2399610,
    "text": "人間の好みから微調整したいときはDPOを使ってみてほしい。"
  },
  {
    "start": 2401010,
    "end": 2402862,
    "text": "よし、これで終わりだ。"
  },
  {
    "start": 2402916,
    "end": 2409950,
    "text": "さて、私は馬を乗り換えて、言語に関する私の非常に推測的な部分を言おう。"
  },
  {
    "start": 2410450,
    "end": 2411680,
    "text": "さあ、始めよう。"
  },
  {
    "start": 2420950,
    "end": 2424180,
    "text": "さて、この話はこの辺で。"
  },
  {
    "start": 2424550,
    "end": 2433506,
    "text": "これらのスライドは、2022年10月にCFARの会合で発表したものなので、現時点では過去のものだ。"
  },
  {
    "start": 2433618,
    "end": 2442522,
    "text": "つまり、重要なことは、チャットGPTはまだ1年も経っていないということだ。"
  },
  {
    "start": 2442576,
    "end": 2447660,
    "text": "実際、数日後には9カ月目の記念日を迎えるところだ。"
  },
  {
    "start": 2448510,
    "end": 2458160,
    "text": "チャットGPTは、私がこの講演をした時点ではまだ出ていなかった。"
  },
  {
    "start": 2458610,
    "end": 2462446,
    "text": "最新の事例が載っているわけではない。"
  },
  {
    "start": 2462468,
    "end": 2468500,
    "text": "主に10分ほど、ハイレベルな部分に集中させてほしい。"
  },
  {
    "start": 2469590,
    "end": 2481346,
    "text": "私がやりたいのは、フォン・フンボルト・マシンという考え方を紹介することだ。つまり、記号と言語を使って知能を構築するニューラルネットワークの段階に移行しつつあるという考え方だ。"
  },
  {
    "start": 2481458,
    "end": 2485880,
    "text": "純粋に憶測の域を出ない話だが、ここで考えるきっかけになれば幸いだ。"
  },
  {
    "start": 2486490,
    "end": 2498358,
    "text": "そこで、私の親愛なる同僚であるヤン・ラクーンと哲学者のジェイク・ブラウニングが、雑誌『Noema』に一連の記事を書くことになった。"
  },
  {
    "start": 2498454,
    "end": 2502460,
    "text": "彼らが最初に書いたのは、「AIが教えてくれること」だった。"
  },
  {
    "start": 2503150,
    "end": 2505840,
    "text": "あのね、本当に、本当に好きなんだ。"
  },
  {
    "start": 2506210,
    "end": 2524786,
    "text": "この本は、記号が最初からハードコーディングされているのか、それとも経験を通じて学ぶことができるのかという、チョムスク的な言語と知性に関する2つの主な見解を、明確かつバランスよく対比させている。"
  },
  {
    "start": 2524888,
    "end": 2531282,
    "text": "私は後者を支持した。"
  },
  {
    "start": 2531336,
    "end": 2531940,
    "text": "やあ。"
  },
  {
    "start": 2533190,
    "end": 2541830,
    "text": "その数ヵ月後、彼らは『ノエマ』でAIと言語の限界という別の作品を発表した。"
  },
  {
    "start": 2541990,
    "end": 2545686,
    "text": "あれはちょっと見当違いだと思った。"
  },
  {
    "start": 2545878,
    "end": 2555246,
    "text": "このスライドは、このブラウニングとラクーンの記事を読んだ感想に対する私の返答のようなものだ。"
  },
  {
    "start": 2555428,
    "end": 2558080,
    "text": "などと言われている。"
  },
  {
    "start": 2559170,
    "end": 2564130,
    "text": "大型言語モデルの言語理解は、印象的ではあるが浅い。"
  },
  {
    "start": 2565990,
    "end": 2573198,
    "text": "すべての知識は言語的であるという見方を捨てることで、私たちの知識のどれほど多くが非言語的であるかを理解することができる。"
  },
  {
    "start": 2573374,
    "end": 2583298,
    "text": "動物や人間における非言語的なメンタル・シミュレーションは一般的であり、シナリオを計画したり、人工物を作ったりリバースエンジニアリングしたりするのに役立つ。"
  },
  {
    "start": 2583474,
    "end": 2593014,
    "text": "言語だけで訓練されたシステムは、たとえ今から宇宙の熱が冷めるまで訓練されたとしても、人間の知性に近づくことはないだろう。"
  },
  {
    "start": 2593142,
    "end": 2598330,
    "text": "お分かりのように、彼らは言語の限界についてかなり強い考えを持っている。"
  },
  {
    "start": 2599890,
    "end": 2627122,
    "text": "彼らが言いたいのは、人々は言語が知識を尽きさせると信じている、存在する知識は言語だけだと信じているということなんだ。"
  },
  {
    "start": 2627186,
    "end": 2634818,
    "text": "言語の強力な支持者でさえ、言語が知識を使い果たすとは言わないと思う。"
  },
  {
    "start": 2634994,
    "end": 2640150,
    "text": "ノーム・チョムスキーでさえ、水の中に手を突っ込むと熱く感じたり冷たく感じたりすることを知っている。"
  },
  {
    "start": 2640230,
    "end": 2642330,
    "text": "それは言語学的な話ではない。"
  },
  {
    "start": 2644270,
    "end": 2657386,
    "text": "ゲイリー・マーカスが書いているようなことを恐れるあまり、彼らは偽預言者になってしまう。"
  },
  {
    "start": 2657498,
    "end": 2664254,
    "text": "だから、逆にとんでもなく強いポジションを取ることになる。"
  },
  {
    "start": 2664382,
    "end": 2670370,
    "text": "実際、彼らの論文の一節には「ゴーストを祓う」と題されている。"
  },
  {
    "start": 2671990,
    "end": 2681670,
    "text": "そのため、人間の知性にとって言語がいかに重要であるかを認識せず、ある種の誤った主張をすることになる。"
  },
  {
    "start": 2683210,
    "end": 2685640,
    "text": "彼らは他にも虚偽の主張をしている。"
  },
  {
    "start": 2686410,
    "end": 2689442,
    "text": "フランスでは教育が大きく違うのは知っている。"
  },
  {
    "start": 2689506,
    "end": 2694758,
    "text": "私はシャフィと、他の国ではもっと厳しい教育が行われているという話をしていた。"
  },
  {
    "start": 2694934,
    "end": 2702286,
    "text": "理科の授業には講義の要素もあるが、生徒の成績は主に実験室での作業に基づいて評価される、と彼らは主張する。"
  },
  {
    "start": 2702468,
    "end": 2713940,
    "text": "他の人がどのような教育を受けてきたかは知らないが、私は実験室での作業で成績が決まるような科学の授業は受けたことがない。"
  },
  {
    "start": 2715990,
    "end": 2719106,
    "text": "そして、まあ、もう読んだ。"
  },
  {
    "start": 2719208,
    "end": 2720834,
    "text": "そうそう、それからこんなこともある。"
  },
  {
    "start": 2720872,
    "end": 2737106,
    "text": "言語が重要だと言われるのは、小さなフォーマットで多くの情報を伝えることができるからであり、特に印刷機やインターネットが誕生した後は、それを複製して広く利用できるようにすることができるからである。"
  },
  {
    "start": 2737218,
    "end": 2763978,
    "text": "ヤン・ルクーンの立場では、言語の唯一の役割は自分の知識を圧縮し、それを他の人間に伝達することであり、アイデアを伝達することである。"
  },
  {
    "start": 2764074,
    "end": 2768434,
    "text": "それは人類を大きく前進させた重要なアイデアだ。"
  },
  {
    "start": 2768472,
    "end": 2772898,
    "text": "印刷とインターネットは人類を大きく進歩させた。"
  },
  {
    "start": 2772984,
    "end": 2775686,
    "text": "それ以上のことがあると思うんだ。"
  },
  {
    "start": 2775788,
    "end": 2784502,
    "text": "人間や人間らしい知性の要素の多くは、他の動物とそれほど違わないだろう？"
  },
  {
    "start": 2784556,
    "end": 2798810,
    "text": "人間の知能、記憶力、計画性、原因と結果の推論、道具の使い方、複合的な道具の作り方など、私たちが話していることは、すべて動物でも観察されていることなのだ。"
  },
  {
    "start": 2799310,
    "end": 2802702,
    "text": "チンパンジーのような高等動物にとってはね。"
  },
  {
    "start": 2802756,
    "end": 2814130,
    "text": "チンパンジーの脳は基本的に我々と相似形であり、ブローカー領域のような主要言語領域であっても、チンパンジーにはブローカー領域がある。"
  },
  {
    "start": 2814550,
    "end": 2823454,
    "text": "チンパンジーがどのように道具を使い、棒を使ってアリを穴から出すかを観察してきた。"
  },
  {
    "start": 2823582,
    "end": 2830854,
    "text": "一人のチンパンジーが小さなチンパンジーにやり方を教えるからだ。"
  },
  {
    "start": 2830892,
    "end": 2835590,
    "text": "このビデオをもっと長く見ていたら、もう一人のチンパンジーがやりそうだ。"
  },
  {
    "start": 2835660,
    "end": 2839102,
    "text": "人間と同じように、指導によって学ぶのだ。"
  },
  {
    "start": 2839186,
    "end": 2843654,
    "text": "チンパンジーほど賢いものは必要ないことがわかった。"
  },
  {
    "start": 2843702,
    "end": 2852074,
    "text": "特にカラスのような微視的な脳を持つものは、これらの実験はすべて南半球のカラスで行われている。"
  },
  {
    "start": 2852122,
    "end": 2856602,
    "text": "南半球のカラスの方が北半球のカラスよりなぜか頭がいい気がする。"
  },
  {
    "start": 2856666,
    "end": 2873006,
    "text": "詳しくはわからないが、太平洋のカラスは、穴に餌を刺すのと同じようなことができる。"
  },
  {
    "start": 2873118,
    "end": 2875514,
    "text": "カラスのくちばしでは届かない。"
  },
  {
    "start": 2875662,
    "end": 2879960,
    "text": "消えてしまっても、まだ届かないことがある。"
  },
  {
    "start": 2880650,
    "end": 2887026,
    "text": "そして棒を見つけ、その棒で餌のかけらを取ることができる。"
  },
  {
    "start": 2887058,
    "end": 2890986,
    "text": "操縦桿を下ろし、バランスが取れないことに気づき、操縦桿に足をかける。"
  },
  {
    "start": 2891008,
    "end": 2892220,
    "text": "先を見越しての計画だ。"
  },
  {
    "start": 2892670,
    "end": 2895260,
    "text": "こういうことをすべて理解しているんだろう？"
  },
  {
    "start": 2895870,
    "end": 2910830,
    "text": "しかし、動物が使う道具の限界は粗末な棒のようなもので、人間はより優れた道具を持っているという違いがある。"
  },
  {
    "start": 2913430,
    "end": 2918690,
    "text": "ヴィルヘルム・フォン・トゥンボルトはこんな面白いドイツ人だったのか。"
  },
  {
    "start": 2919510,
    "end": 2926290,
    "text": "アメリカの大学院は、基本的にドイツで発明されたアイデアに起因している。"
  },
  {
    "start": 2926450,
    "end": 2929426,
    "text": "彼は主に言語を研究していた。"
  },
  {
    "start": 2929618,
    "end": 2939190,
    "text": "彼はノーム・チョムスキーのお気に入りで、人間の言語は有限の手段を無限に使わなければならないという彼の発言をいつも引用している。"
  },
  {
    "start": 2939260,
    "end": 2944794,
    "text": "彼はそれを、自分が取り組んできた普遍的な文法の正当性の根拠としている。"
  },
  {
    "start": 2944912,
    "end": 2946810,
    "text": "他にもいろいろ言っていた。"
  },
  {
    "start": 2946880,
    "end": 2953520,
    "text": "フンボルトの著作には、人種差別的なものも含め、さまざまなことが書かれている。"
  },
  {
    "start": 2953970,
    "end": 2961882,
    "text": "彼の中心的な考えのひとつは、言語とは単にコミュニケーションを目的とした思考の外面的な表出ではないということだった。"
  },
  {
    "start": 2961946,
    "end": 2967606,
    "text": "彼が言ったことのひとつは、言語とは製品エルゴンではなく、活動、エネルギッシュなものだということだ。"
  },
  {
    "start": 2967658,
    "end": 2975410,
    "text": "だから事実上、彼は私たちが今、システム1と呼びがちな認知と、システム2の思考を区別している。"
  },
  {
    "start": 2975560,
    "end": 2985080,
    "text": "システム2、つまり思考を可能にするために心を拡張するためには、言語が必要不可欠な要素であると彼は考えていた。"
  },
  {
    "start": 2985610,
    "end": 2989690,
    "text": "思考の表現を生み出す精神活動。"
  },
  {
    "start": 2989760,
    "end": 2995100,
    "text": "思考は常に、すでに与えられている言語の上に向けられる。"
  },
  {
    "start": 2995550,
    "end": 2999286,
    "text": "それは純粋に創造的な活動ではなく、再形成の活動である。"
  },
  {
    "start": 2999398,
    "end": 3003840,
    "text": "言語が人間の精神の進歩に必要な基礎であることを。"
  },
  {
    "start": 3004290,
    "end": 3009280,
    "text": "それがどこまで真実なのかは議論が分かれるところだが、なんとなく真実のような気がする。"
  },
  {
    "start": 3009730,
    "end": 3027018,
    "text": "ダニエル・デネットは著書『バクテリアからバーケンバックまで』の中で、主に意識について述べていますが、その中で4つのレベルの知性を紹介しています。"
  },
  {
    "start": 3027134,
    "end": 3031174,
    "text": "だから、午前中に話していた知能レベルに近づいているんだ。"
  },
  {
    "start": 3031292,
    "end": 3044134,
    "text": "最も低いレベルはダーウィン的なもので、あらかじめ設計され、固定された能力は一生向上せず、進化によってのみ向上するというものだ。"
  },
  {
    "start": 3044262,
    "end": 3047558,
    "text": "それはバクテリアやウイルスといったものだ。"
  },
  {
    "start": 3047734,
    "end": 3050474,
    "text": "そして、次のレベルはスキャナリアンインテリジェンスだ。"
  },
  {
    "start": 3050522,
    "end": 3058458,
    "text": "そのときこそ、強化学習、古典的なシナリオ的報酬、そして報酬を得るための学習ができるのだ。"
  },
  {
    "start": 3058634,
    "end": 3062154,
    "text": "知性の第3レベルは、知性を準備することである。"
  },
  {
    "start": 3062282,
    "end": 3067422,
    "text": "つまり、インテリジェンスの準備とは、環境のモデルを構築することなのだ。"
  },
  {
    "start": 3067566,
    "end": 3073234,
    "text": "そうすれば、環境モデルを使ってオフラインでプランのテストを行うことができる。"
  },
  {
    "start": 3073352,
    "end": 3078630,
    "text": "すでにカラスはこのレベルの知性を備えているようだ。"
  },
  {
    "start": 3079530,
    "end": 3084322,
    "text": "となると、デネットが区別する第4のレベルは、グレゴリオ的知性ということになる。"
  },
  {
    "start": 3084466,
    "end": 3092218,
    "text": "デネットは、グレゴリアンインテリジェンスの本質は、思考を拡大するツールを構築できることだと主張する。"
  },
  {
    "start": 3092384,
    "end": 3097130,
    "text": "言語は思考を拡大するツールのひとつだ。"
  },
  {
    "start": 3097200,
    "end": 3110560,
    "text": "デネットは、人間の思考を拡大する道具には、数学、コンピューターだけでなく、民主主義も含まれると主張する。"
  },
  {
    "start": 3111090,
    "end": 3118370,
    "text": "デネットは、グレゴリオ的知性の生物学的な例は人間だけだと主張する。"
  },
  {
    "start": 3119510,
    "end": 3132870,
    "text": "つまり、最近の言語モデルには、グレゴリオ的な知能を持ち、独自のツールを構築できるマシンが登場し始めているということだ。"
  },
  {
    "start": 3133610,
    "end": 3151950,
    "text": "これらの例はGPTの時代のものなので古いですが、GPT-3のような恣意的な補完だけでなく、命令の微調整を行うようになると、これらのモデルの中で言語をツールとして操作できるようになります。"
  },
  {
    "start": 3152610,
    "end": 3173410,
    "text": "他の例はもう古くなったので省略するかもしれないが、私たちが今発見し始めているのは、これらの言語モデルを思考ツールの構築と使用とみなすことで、多くのことができるということだ。"
  },
  {
    "start": 3174230,
    "end": 3193258,
    "text": "ライリー・グッドサイドは、モデルに素晴らしいことをさせる複雑なプロンプトの書き方の預言者のような存在だ。"
  },
  {
    "start": 3193344,
    "end": 3194698,
    "text": "彼には興味深い歴史がある。"
  },
  {
    "start": 3194784,
    "end": 3196806,
    "text": "彼は以前、出会い系サイトで働いていた。"
  },
  {
    "start": 3196838,
    "end": 3199382,
    "text": "彼は以前、OccupidとGrindrで働いていた。"
  },
  {
    "start": 3199526,
    "end": 3208320,
    "text": "今や彼は、大規模な言語モデルに面白いことをさせる方法の真の預言者なのだ。"
  },
  {
    "start": 3209170,
    "end": 3220766,
    "text": "つまり、GPT-3で直接答えられるのであれば、この形式を使ってください。"
  },
  {
    "start": 3220878,
    "end": 3233430,
    "text": "もし答えられないのであれば、ipythonに相談し、このフォーマットを使ってipythonに何かを送信し、そして答えを得ることだ。"
  },
  {
    "start": 3234410,
    "end": 3245290,
    "text": "言語モデルがipythonを呼び出し、完全に正しく数値を掛け合わせることができるからだ。"
  },
  {
    "start": 3245710,
    "end": 3255470,
    "text": "ディープラーニングの歴史とその復活の大半において、人々は勾配ベースの学習がすべてを解決すると信じていた。"
  },
  {
    "start": 3255620,
    "end": 3260890,
    "text": "問題は、それをどのように高次の認知形態に拡張するかということだった。"
  },
  {
    "start": 3261050,
    "end": 3269598,
    "text": "勾配に基づく学習がタイプ1の認知に対する正しい答えだというのは、実は今、間違っているように思えるのかもしれない。"
  },
  {
    "start": 3269774,
    "end": 3277554,
    "text": "タイプ2の認知に到達するためには、グレゴリアン的思考ツールを構築するのが正解なのかもしれない。"
  },
  {
    "start": 3277682,
    "end": 3291370,
    "text": "事実上、現在OpenAIで私たちが他の場所で見ていることのいくつかは、グレゴリアン思考ツールの構築であり、そこでは言語モデルが電卓を利用したり、他のデバイスに呼びかけたりしている。"
  },
  {
    "start": 3292030,
    "end": 3299750,
    "text": "だから私たちは、言語が指導のメカニズムであると同時に、思考のメカニズムでもあるこの世界に足を踏み入れているのだ。"
  },
  {
    "start": 3299920,
    "end": 3312982,
    "text": "チンパンジーやカラスに対してそうであったように、記号化された言語による思考は、より高度な思考を可能にし、認知能力を向上させる。"
  },
  {
    "start": 3313146,
    "end": 3314100,
    "text": "ありがとう。"
  },
  {
    "start": 3323910,
    "end": 3325054,
    "text": "とても興味深い話だった。"
  },
  {
    "start": 3325112,
    "end": 3338938,
    "text": "DPOについて質問があるのですが、プリファレンス・データはすべて優先されるため、オフラインに近いと思われますが、DPOはオンラインに近いと思われます。"
  },
  {
    "start": 3339024,
    "end": 3348438,
    "text": "EPOが始めた嗜好データの質にもよるが、経験的に違いがあるのだろうか。"
  },
  {
    "start": 3348534,
    "end": 3357246,
    "text": "それが最適だとおっしゃるのは、与えられたデータの優先順位に関する何らかの仮定が関係しているのではないでしょうか。"
  },
  {
    "start": 3357428,
    "end": 3359680,
    "text": "ああ、そうだね。"
  },
  {
    "start": 3362130,
    "end": 3374180,
    "text": "強化学習には利点がある。"
  },
  {
    "start": 3374970,
    "end": 3386840,
    "text": "私の理解では、みんながRLHFをやっている方法は、オフライン強化学習に基づいている。"
  },
  {
    "start": 3387770,
    "end": 3401194,
    "text": "だから、強化学習の講義の冒頭で、エージェントが小さなミクロの世界を彷徨い、それが進むにつれて学習していく、というのを覚えているなら、それは実際にやっていることではない。"
  },
  {
    "start": 3401232,
    "end": 3401434,
    "text": "そうだね。"
  },
  {
    "start": 3401472,
    "end": 3407230,
    "text": "彼らはこのトレーニングセットを手に入れ、その上で強化学習アルゴリズムを実行しようと言っているのだ。"
  },
  {
    "start": 3407300,
    "end": 3408814,
    "text": "あなたはそれを感じることができた。"
  },
  {
    "start": 3408852,
    "end": 3410426,
    "text": "ああ、ちょっと待ってくれ。"
  },
  {
    "start": 3410538,
    "end": 3420434,
    "text": "学習が進み、方向性が定まってきたら、政策データをもっと集めることができれば、それが助けになるだろう。"
  },
  {
    "start": 3420552,
    "end": 3422162,
    "text": "そして、それは可能だ。"
  },
  {
    "start": 3422296,
    "end": 3424354,
    "text": "標準的に行われていることではない。"
  },
  {
    "start": 3424472,
    "end": 3432966,
    "text": "問題は、それに見合った嗜好データを得られるかどうかだろう。"
  },
  {
    "start": 3432988,
    "end": 3433174,
    "text": "そうだろう？"
  },
  {
    "start": 3433212,
    "end": 3437030,
    "text": "なぜなら、その時点で2つの可能性があったからだ。"
  },
  {
    "start": 3437390,
    "end": 3458398,
    "text": "一つの可能性は、教師なし方針強化学習で価値を得ることができるかもしれないということだ。"
  },
  {
    "start": 3458484,
    "end": 3463514,
    "text": "そうでなければ、これはあなたが何らかの形でコレクションしている場合にのみ関係するようになった。"
  },
  {
    "start": 3463642,
    "end": 3465370,
    "text": "考えられる2つの回答がある。"
  },
  {
    "start": 3465450,
    "end": 3467234,
    "text": "人間的にはどっちが好き？"
  },
  {
    "start": 3467432,
    "end": 3469838,
    "text": "ウェブ検索結果のようなものかもしれない。"
  },
  {
    "start": 3469934,
    "end": 3477480,
    "text": "つまり、もちろん、そのデータを収集した範囲内で、さらにオフラインの強化学習の山に入れることもできる。"
  },
  {
    "start": 3480410,
    "end": 3481480,
    "text": "ああ、ありがとう。"
  },
  {
    "start": 3483610,
    "end": 3489682,
    "text": "ただ、この最後のステージについて、ある種の思考ツールを使って考えてみたんだ。"
  },
  {
    "start": 3489826,
    "end": 3499878,
    "text": "コンピュータを作るためには、シリコンを採掘するために必要な他の生産エンジンも作らなければならない。"
  },
  {
    "start": 3500054,
    "end": 3510206,
    "text": "経済的なこと、物理的な道具のようなこと、建築的なこと......そういったことが、今現在は言語モデルとして表れていないかもしれない。"
  },
  {
    "start": 3510308,
    "end": 3517810,
    "text": "だから、その段階も重要だと考えているのか、もしそうなら、この絵の中でどのように位置づけているのか気になるんだ。"
  },
  {
    "start": 3518790,
    "end": 3529670,
    "text": "ええ、シリコンの採掘についてはあまり考えたことがありませんでしたが、言語モデル以外のものが必要だと思います。"
  },
  {
    "start": 3530490,
    "end": 3542730,
    "text": "ヤン・ラクーン、3D世界のまともなモデルを作るべきだという点に関しては、私も君と同じだ。"
  },
  {
    "start": 3542800,
    "end": 3543130,
    "text": "そうだね。"
  },
  {
    "start": 3543200,
    "end": 3552974,
    "text": "基本的に、多くのロボット工学者が現在、大規模な言語モデルをロボットシステムに活用している。"
  },
  {
    "start": 3553092,
    "end": 3574130,
    "text": "というのも、従来の物理エンジンができること、つまり硬いオブジェクトを使った動きから一歩外に出れば、言語モデルは実は、他のどんなものよりも優れた世界モデルなんだ。"
  },
  {
    "start": 3574280,
    "end": 3585446,
    "text": "その一方で、我々が持っているような3次元の世界観に比べると、細部においては本当にお粗末な世界モデルだ。"
  },
  {
    "start": 3585548,
    "end": 3595110,
    "text": "だから、他にも必要なものはたくさんあると思うけど、その一部は言語を通して学び、共有されるんだ。"
  },
  {
    "start": 3595190,
    "end": 3595820,
    "text": "そうだね。"
  },
  {
    "start": 3598510,
    "end": 3600394,
    "text": "素晴らしい話をありがとう。"
  },
  {
    "start": 3600592,
    "end": 3602154,
    "text": "小さな質問を2つだけ。"
  },
  {
    "start": 3602272,
    "end": 3611950,
    "text": "ひとつは、異なる国の結果を総合した場合、安定性という点でPPOはCPOに匹敵するだろうかということだ。"
  },
  {
    "start": 3616370,
    "end": 3619570,
    "text": "正直なところ、よくわからない。"
  },
  {
    "start": 3619640,
    "end": 3620260,
    "text": "たぶんね。"
  },
  {
    "start": 3622390,
    "end": 3625258,
    "text": "つまり、温度はある種、別のパラメーターなんだ。"
  },
  {
    "start": 3625374,
    "end": 3632390,
    "text": "どれくらい安定しているかはよくわからない。"
  },
  {
    "start": 3633130,
    "end": 3633494,
    "text": "そうだね。"
  },
  {
    "start": 3633532,
    "end": 3637342,
    "text": "経験的に比較するのはいいことだ。"
  },
  {
    "start": 3637506,
    "end": 3640730,
    "text": "もうひとつの疑問は、マルチモダリティだ。"
  },
  {
    "start": 3641230,
    "end": 3647690,
    "text": "そうすれば、人間の思考に関する言葉に説明や描写が加わるだろうか？"
  },
  {
    "start": 3648110,
    "end": 3648860,
    "text": "もちろんだ。"
  },
  {
    "start": 3652450,
    "end": 3664340,
    "text": "ほとんどの人間は、自分が見たり嗅いだり感じたりした他のすべてのことが、本当に本当に重要だという直感を持っていると思う。"
  },
  {
    "start": 3665990,
    "end": 3696394,
    "text": "というのも、非常に物理的に見えるものであっても、本やウェブページなどには、物事のフィールドについて語ったり、それらがどのように見えるかを説明したりする言葉がたくさんあることがわかったからです。"
  },
  {
    "start": 3696432,
    "end": 3703834,
    "text": "言語だけの推論は驚くほどたくさんできるが、それでもまだ不完全だ。"
  },
  {
    "start": 3703882,
    "end": 3704622,
    "text": "だから、そうだね。"
  },
  {
    "start": 3704676,
    "end": 3707360,
    "text": "私たちはもっと多くのものを持つ必要がある。"
  },
  {
    "start": 3707970,
    "end": 3710000,
    "text": "最後に1つだけ質問させてください。"
  },
  {
    "start": 3711490,
    "end": 3723700,
    "text": "RLを信じたり、世界との交流が重要だというジョンの立場には、中途半端なところがあるのだろうか？"
  },
  {
    "start": 3724070,
    "end": 3735830,
    "text": "RLのようなものは、会話だけでなく、知性を高めるためにも、実際に言語を教える上で重要な要素だと思いますか？"
  },
  {
    "start": 3736890,
    "end": 3738220,
    "text": "ええ、そうです。"
  },
  {
    "start": 3738910,
    "end": 3739660,
    "text": "そうだね。"
  },
  {
    "start": 3744030,
    "end": 3746618,
    "text": "それを確かめるのは難しいよね。"
  },
  {
    "start": 3746704,
    "end": 3758160,
    "text": "というのも、1兆を超える単語で訓練された巨大な言語モデルで達成できたことは、予想外に素晴らしいことだと思うからだ。"
  },
  {
    "start": 3758530,
    "end": 3780950,
    "text": "私は、児童心理学や人間の言語習得の理論が、そんなことはありえない、相互作用は言語と認知の両面で人間の発達に非常に重要である、と言っていると思う。"
  },
  {
    "start": 3782810,
    "end": 3798086,
    "text": "その代わりに、人間の10万倍ものデータを代入することで、相互作用なしに非常に優れた学習ができることがわかった。"
  },
  {
    "start": 3798278,
    "end": 3803934,
    "text": "疑問なのは、それでも根本的に何らかの上限があるのだろうかということだ。"
  },
  {
    "start": 3804132,
    "end": 3807338,
    "text": "交流が深まるという実験をご存知ですか？"
  },
  {
    "start": 3807514,
    "end": 3814318,
    "text": "そうだね、つまり、人間にとっては、言語モデルには膨大な数がある。"
  },
  {
    "start": 3814404,
    "end": 3815040,
    "text": "そうだね。"
  },
  {
    "start": 3818130,
    "end": 3826120,
    "text": "さて、残りの質問はオフラインで行わなければならないようだが、そうしよう。"
  },
  {
    "start": 3830730,
    "end": 3831490,
    "text": "コーヒー、ビクター？"
  }
]