[
  {
    "start": 250,
    "end": 14014,
    "text": "さて、前回のビデオでは、検索QAにBGEのエンベッディングを追加することについて見てきましたが、今までのラグ・モデルは、このllama2700億モデルを使っていました。"
  },
  {
    "start": 14052,
    "end": 20330,
    "text": "BGEのエンベッディングを追加しても、高品質の出力が得られることがわかるだろう。"
  },
  {
    "start": 20490,
    "end": 25654,
    "text": "正しい答えを得るために、いくつかのアーティファクトが発生しているのは確かだ。"
  },
  {
    "start": 25772,
    "end": 28982,
    "text": "となると、この後も不正解が続くことになる。"
  },
  {
    "start": 29116,
    "end": 32406,
    "text": "そこでこのビデオでは、2つのことをやってみたい。"
  },
  {
    "start": 32508,
    "end": 34920,
    "text": "ひとつは、もっと小型のモデルにできないか？"
  },
  {
    "start": 35370,
    "end": 41510,
    "text": "その後、このようなことが起きないように、実際に改善することができるだろうか？"
  },
  {
    "start": 41580,
    "end": 50202,
    "text": "ここでは、ラマ2のコンテキスト・ウィンドウが展開され、これが正しい答えであることがわかる。"
  },
  {
    "start": 50256,
    "end": 53950,
    "text": "正しい答えを教えてくれたのに、それを続けている。"
  },
  {
    "start": 54100,
    "end": 55470,
    "text": "まずは、見てみよう。"
  },
  {
    "start": 55540,
    "end": 61946,
    "text": "では、このようなノートは、例えば130億人分のモデルに変えた場合、どの程度機能するのだろうか？"
  },
  {
    "start": 62058,
    "end": 65230,
    "text": "よし、これがラマ2130億のモデルだ。"
  },
  {
    "start": 65380,
    "end": 71090,
    "text": "今のところ、一緒にAPIを使っているのがわかるだろう。"
  },
  {
    "start": 71240,
    "end": 74274,
    "text": "モデルのインスタンス化には同じクラスを使っている。"
  },
  {
    "start": 74392,
    "end": 81638,
    "text": "デフォルトでは700億に設定されているが、ここでは130億でインスタンス化する。"
  },
  {
    "start": 81804,
    "end": 83622,
    "text": "ここではすべてが同じだ。"
  },
  {
    "start": 83676,
    "end": 86102,
    "text": "我々はBGEのエンベデッドを進めている。"
  },
  {
    "start": 86156,
    "end": 94694,
    "text": "いろいろなものを持ち込み、チェーンを作り、130億でかなりまともな答えを出している。"
  },
  {
    "start": 94822,
    "end": 98730,
    "text": "しかし、そのすべてが完全に正しいわけではないことがわかる。"
  },
  {
    "start": 98800,
    "end": 107582,
    "text": "llama 2のコンテキストウィンドウは、学習前のコーパスに比べて40％大きくなっていると言っているだけなので、その1つを正しく理解するのに失敗している。"
  },
  {
    "start": 107716,
    "end": 111082,
    "text": "2兆個のトークンでトレーニングされたことがわかる。"
  },
  {
    "start": 111226,
    "end": 112830,
    "text": "いい答えが返ってくるのがわかる。"
  },
  {
    "start": 112900,
    "end": 117338,
    "text": "ラマ3はまだ出ていない。"
  },
  {
    "start": 117514,
    "end": 120130,
    "text": "でも、他の選手と比べると、かなりいい仕事をしているよ。"
  },
  {
    "start": 120200,
    "end": 127106,
    "text": "実際、これほど悪いAdd-on出力はない。"
  },
  {
    "start": 127288,
    "end": 130630,
    "text": "これを使って遊んでいると、何人かいるんだ。"
  },
  {
    "start": 130700,
    "end": 133126,
    "text": "70億のモデルでやってみたらどうだろう？"
  },
  {
    "start": 133228,
    "end": 136360,
    "text": "よし、これで70億のモデルを使ったことになる。"
  },
  {
    "start": 136730,
    "end": 140374,
    "text": "もう一度見てほしい。すべてまったく同じだ。"
  },
  {
    "start": 140412,
    "end": 146010,
    "text": "ここでの唯一の変更点は、ここでのラマ2・7・Bチャットに移行したことだ。"
  },
  {
    "start": 146160,
    "end": 147162,
    "text": "それはわかる。"
  },
  {
    "start": 147216,
    "end": 154174,
    "text": "さて、ここで正しい答えに戻ったが、その後、役に立たない答えや他のものが出てきた。"
  },
  {
    "start": 154292,
    "end": 160094,
    "text": "なぜ、役に立たない回答ばかりを続けたがるのかわからない。"
  },
  {
    "start": 160212,
    "end": 162990,
    "text": "そうすれば、この答えは明らかに間違っていることがわかる。"
  },
  {
    "start": 163060,
    "end": 165906,
    "text": "ラマ2のコンテキストウィンドウは？"
  },
  {
    "start": 166088,
    "end": 169954,
    "text": "には、基本的に使用される安全性データの50％だと書かれている。"
  },
  {
    "start": 170072,
    "end": 180166,
    "text": "現段階では、BGEのエンベディッドから得られる状況は、700億ドルや300億ドルの時と同じだとわかっている。"
  },
  {
    "start": 180268,
    "end": 183638,
    "text": "問題はおそらく、私たちが取り戻そうとしている文脈にはない。"
  },
  {
    "start": 183724,
    "end": 188774,
    "text": "それはおそらく言語モデルの中にあるもので、それをここに取り込んで使うのだろう。"
  },
  {
    "start": 188812,
    "end": 195322,
    "text": "さて、私たちは、この注釈の多くが、この答えが情報に基づいているにもかかわらず、まだ多くの答えを正解にしていることを見ることができる。"
  },
  {
    "start": 195456,
    "end": 199658,
    "text": "私たちは、おそらくそこに入れたくない余分なものをたくさん持っている。"
  },
  {
    "start": 199824,
    "end": 208574,
    "text": "これを解決する次の方法は、QAチェーンを見て、そこにあるプロンプトに取り組むことだ。"
  },
  {
    "start": 208612,
    "end": 216350,
    "text": "次のパートでは、70億パラメーターのラマ2モデルにこだわるつもりだが、プロンプトを弄り、プロンプトを変更するつもりだ。"
  },
  {
    "start": 216430,
    "end": 217746,
    "text": "その話に飛び込もう。"
  },
  {
    "start": 217848,
    "end": 220002,
    "text": "さて、ここでもう一度。"
  },
  {
    "start": 220056,
    "end": 225234,
    "text": "私たちは、すべてをラマ2 70億チャットモデルに設定しただけです。"
  },
  {
    "start": 225272,
    "end": 227814,
    "text": "BGEのエンベッディングも入っているんだ。"
  },
  {
    "start": 227852,
    "end": 228662,
    "text": "以前のようにね。"
  },
  {
    "start": 228796,
    "end": 231800,
    "text": "前と同じようにクロマDBをセットアップしている。"
  },
  {
    "start": 232250,
    "end": 238486,
    "text": "私たちはレトリバーに5つのコンテクストを戻させている。"
  },
  {
    "start": 238588,
    "end": 242982,
    "text": "デフォルトのラマ2プロンプトスタイルの例です。"
  },
  {
    "start": 243046,
    "end": 251366,
    "text": "私がllama 2についてたくさん話したビデオに戻ると、ここにはシステムプロンプトとインストラクタープロンプトの両方があることを思い出してほしい。"
  },
  {
    "start": 251488,
    "end": 253994,
    "text": "これがデフォルトのシステム・プロンプトだ。"
  },
  {
    "start": 254042,
    "end": 257050,
    "text": "あなたは親切で尊敬できる誠実なアシスタントだ。"
  },
  {
    "start": 257210,
    "end": 260842,
    "text": "常に安全でありながら、できるだけ親切に答える。"
  },
  {
    "start": 260906,
    "end": 267698,
    "text": "回答には、有害、非倫理的、人種差別的、性差別的、有害、危険、または違法な内容を含めてはなりません。"
  },
  {
    "start": 267864,
    "end": 271966,
    "text": "回答は、社会的に偏りのない、前向きなものにしてください。"
  },
  {
    "start": 272078,
    "end": 275762,
    "text": "これは大きく斜めになっている。"
  },
  {
    "start": 275816,
    "end": 281202,
    "text": "質問が意味をなさないか、事実と一貫性がない場合について、私たちはいいものを手に入れた。"
  },
  {
    "start": 281266,
    "end": 284278,
    "text": "正しくないことを答えるのではなく、その理由を説明する。"
  },
  {
    "start": 284364,
    "end": 287510,
    "text": "もし答えを知らないのであれば、嘘の情報をシェアしないでほしい。"
  },
  {
    "start": 287660,
    "end": 301594,
    "text": "私はこの最後の部分が好きだが、政治的に正しいと思われたり、ある意味、有害で危険なものをすべて取り除こうとして混乱したりするのは避けたい。"
  },
  {
    "start": 301632,
    "end": 303502,
    "text": "ここにあるものが欲しくなるだろう。"
  },
  {
    "start": 303556,
    "end": 311870,
    "text": "常に外すべきだとは言わないが、それ自体が返ってくる答えに影響を与えることになる。"
  },
  {
    "start": 311940,
    "end": 315274,
    "text": "このシステム・プロンプトを本当に作りたいのか？"
  },
  {
    "start": 315322,
    "end": 345674,
    "text": "何度か試してみて、何が効果的だったかを確認した結果、私が選んだのは、できるだけデフォルトに近づけようとするものです。しかし、私たちのコンテクストからの情報のみを使用し、基本的にはそのコンテクストに忠実で、70億ドルモデルや70億ドルモデル、そして130億ドルモデルでも少しあったような余計なものは一切出さないようにしたいのです。"
  },
  {
    "start": 345872,
    "end": 349942,
    "text": "私が求めているプロンプト、システムプロンプトは、あなたが親切で、敬意を払い、正直であるということです。"
  },
  {
    "start": 350016,
    "end": 356890,
    "text": "アシスタントは常に、提供された文脈のテキストを使って、できる限り役立つアンプを使用する。"
  },
  {
    "start": 356970,
    "end": 357598,
    "text": "そうだね。"
  },
  {
    "start": 357764,
    "end": 361118,
    "text": "そこに入れたコンテキスト・テキストにフラグを立てる。"
  },
  {
    "start": 361204,
    "end": 365710,
    "text": "答えは一度だけでよいので、二度は答えないでください。"
  },
  {
    "start": 365790,
    "end": 366322,
    "text": "そうだね。"
  },
  {
    "start": 366456,
    "end": 369922,
    "text": "回答が終わった後にテキストがない。"
  },
  {
    "start": 370056,
    "end": 376418,
    "text": "これは、あとからただ物事を進めるのを止めようとする私のやり方だ。"
  },
  {
    "start": 376504,
    "end": 383398,
    "text": "質問が意味をなしていなかったり、事実に一貫性がなかったりする場合は、正しくないことを答えるのではなく、その理由を説明すること。"
  },
  {
    "start": 383564,
    "end": 386950,
    "text": "質問の答えを知らないのであれば、嘘の情報をシェアしないでください。"
  },
  {
    "start": 387020,
    "end": 390170,
    "text": "あれはメタが持っていたものとまったく同じにしてある。"
  },
  {
    "start": 390320,
    "end": 398666,
    "text": "さて、インストラクションの中で、コンテキストを新たに2行渡し、そこにコンテキストを渡します。"
  },
  {
    "start": 398768,
    "end": 401274,
    "text": "それから質問、そして質問だ。"
  },
  {
    "start": 401392,
    "end": 409374,
    "text": "最終的なプロンプトはこのようになり、システムプロンプトを含むインストラクションプロンプトのフォーマットになっているのがわかるだろう。"
  },
  {
    "start": 409572,
    "end": 411982,
    "text": "そこに我々の文脈がある。"
  },
  {
    "start": 412036,
    "end": 413966,
    "text": "そこに我々の質問がある。"
  },
  {
    "start": 414148,
    "end": 418354,
    "text": "よし、さっきと同じようにラマ2モデルをインスタンス化しよう。"
  },
  {
    "start": 418472,
    "end": 424194,
    "text": "この場合、温度を変えようとはしていないし、何か派手なことをしようともしていない。"
  },
  {
    "start": 424232,
    "end": 431042,
    "text": "このプロンプトをかなりしっかりしたものにしようとしているだけで、あとは基本的にプロンプト・テンプレートをインスタンス化する必要がある。"
  },
  {
    "start": 431106,
    "end": 433910,
    "text": "これが私たちのプロンプトテンプレートです。"
  },
  {
    "start": 433980,
    "end": 436982,
    "text": "ここでプロンプトのテンプレートに渡して再生成しているだけだ。"
  },
  {
    "start": 437116,
    "end": 441900,
    "text": "入力変数として入れ替えるのは、コンテキストと質問の2つだ。"
  },
  {
    "start": 442350,
    "end": 457102,
    "text": "さて、実際のプロンプト・テンプレートを生成するには、基本的に関数get promptを使って命令テンプレートとシステム・テンプレートを渡し、そこから2つの入力変数をこれらに渡すことにする。"
  },
  {
    "start": 457156,
    "end": 460800,
    "text": "これが文脈であり、質問となる。"
  },
  {
    "start": 461170,
    "end": 462446,
    "text": "そして、基本的にはこうするつもりだ。"
  },
  {
    "start": 462548,
    "end": 464734,
    "text": "僕はこれをラマ・プロンプトと呼んでいるんだ。"
  },
  {
    "start": 464772,
    "end": 468290,
    "text": "ここでは、リャマ・ボロ・プロンプトとでも呼ぼうか。"
  },
  {
    "start": 468440,
    "end": 474798,
    "text": "というのも、これはチェーンに渡す追加の引数になるからだ。"
  },
  {
    "start": 474894,
    "end": 478034,
    "text": "今回はここにプロンプトを渡す。"
  },
  {
    "start": 478152,
    "end": 486518,
    "text": "チェーンを生成するときに、LLMを渡しているのがわかるだろう。"
  },
  {
    "start": 486604,
    "end": 488982,
    "text": "そこには私たちのLLMがある。"
  },
  {
    "start": 489116,
    "end": 494362,
    "text": "私たちはレトリーバーを持っていて、毎回5つの例をパスバックする。"
  },
  {
    "start": 494416,
    "end": 496586,
    "text": "ちょっと確認しよう。"
  },
  {
    "start": 496688,
    "end": 502406,
    "text": "そして、ここに他の引数を渡すのだが、これが本当にプロンプトを渡す場所なのだ。"
  },
  {
    "start": 502448,
    "end": 511022,
    "text": "ソース・ドキュメントをtrueで返し、ソースを引用できるようにする。"
  },
  {
    "start": 511156,
    "end": 511550,
    "text": "そうだろう？"
  },
  {
    "start": 511620,
    "end": 515130,
    "text": "出典の引用を助けるためのものを手に入れただけだ。"
  },
  {
    "start": 515290,
    "end": 519250,
    "text": "そして今、それを実行すると、フラッシュが何に注目しているのかがわかる。"
  },
  {
    "start": 519670,
    "end": 522302,
    "text": "フラッシュが注目される理由を教えてくれる。"
  },
  {
    "start": 522446,
    "end": 529218,
    "text": "同じBGEエンベッディングを使用しているため、これらすべてが同じであることがわかります。"
  },
  {
    "start": 529314,
    "end": 537366,
    "text": "だから、この後の答えはない。"
  },
  {
    "start": 537468,
    "end": 553734,
    "text": "コンテクスト・ウィンドウは、以前は何度も間違えていたウィンドウのひとつだった。"
  },
  {
    "start": 553862,
    "end": 555582,
    "text": "ここで、それが機能していることがわかる。"
  },
  {
    "start": 555636,
    "end": 559198,
    "text": "llama 2のコンテキストウィンドウは4096。"
  },
  {
    "start": 559364,
    "end": 569330,
    "text": "プロンプトが、答えが終わったら1つだけ答えろ、基本的に他には何も言うな、と言っているからだ。"
  },
  {
    "start": 569400,
    "end": 569970,
    "text": "そうだね。"
  },
  {
    "start": 570120,
    "end": 572642,
    "text": "ラマ2は何枚のトークンで調教されたのですか？"
  },
  {
    "start": 572776,
    "end": 574062,
    "text": "2兆トークン。"
  },
  {
    "start": 574126,
    "end": 580342,
    "text": "繰り返しになるが、この後に出てきたような役に立たない回答やその他のクズは一切ない。"
  },
  {
    "start": 580476,
    "end": 581174,
    "text": "オーケー、申し訳ない。"
  },
  {
    "start": 581212,
    "end": 582806,
    "text": "ラマ3の発売はいつですか？"
  },
  {
    "start": 582908,
    "end": 586578,
    "text": "質問の答えは、あなたが提供したテキストでは得られません。"
  },
  {
    "start": 586754,
    "end": 590322,
    "text": "ほら、こう書いてあるのは、そういう文脈を使っていることを示しているんだ。"
  },
  {
    "start": 590386,
    "end": 594010,
    "text": "ラマ3の発売日については一切触れていない。"
  },
  {
    "start": 594160,
    "end": 600998,
    "text": "テキストは、llamaの現在のバージョンであるllama 2と、様々なベンチマークでのパフォーマンスについての情報のみを提供する。"
  },
  {
    "start": 601094,
    "end": 609854,
    "text": "混乱させようとして何かをでっち上げようとするよりも、この方がずっと簡潔で、真実味のある答えになる。"
  },
  {
    "start": 609892,
    "end": 612442,
    "text": "では、メタコードの新モデルとは？"
  },
  {
    "start": 612506,
    "end": 617858,
    "text": "この場合、すべてのラマ2が引っ張られているので、ラマ2と表示されると予想される。"
  },
  {
    "start": 618024,
    "end": 621666,
    "text": "この論文をもう一度チェックする必要がありそうだ。"
  },
  {
    "start": 621768,
    "end": 625874,
    "text": "ここでは大文字のLと小文字のMAは使わない。"
  },
  {
    "start": 625992,
    "end": 627362,
    "text": "新聞もよくわからない。"
  },
  {
    "start": 627416,
    "end": 631542,
    "text": "実際、彼らはこのような言い方にデフォルトで戻っているのだろうか？"
  },
  {
    "start": 631596,
    "end": 632274,
    "text": "可能性はある。"
  },
  {
    "start": 632402,
    "end": 634710,
    "text": "よし、ではもう一度、前者のツールだが、いい答えが出た。"
  },
  {
    "start": 634780,
    "end": 637974,
    "text": "ここでも他には何も出てこない。"
  },
  {
    "start": 638092,
    "end": 649466,
    "text": "そして、このプロンプトが役立つと思う面白い点は、複数の文脈から答えの一部を引き出したい場合、それも可能だということだ。"
  },
  {
    "start": 649568,
    "end": 653530,
    "text": "各ツールにいくつの例を用意する必要があるか、ここでわかるだろう。"
  },
  {
    "start": 653680,
    "end": 659838,
    "text": "各ツールに必要な例数は、ツールの複雑さなどさまざまな要因によって異なります。"
  },
  {
    "start": 659924,
    "end": 661422,
    "text": "これはいい答えだ。"
  },
  {
    "start": 661556,
    "end": 670766,
    "text": "というのも、以前はツールから、ラマ2からは、llmsの増強から、いくつか興味深いものを得ることができたからだ。"
  },
  {
    "start": 670878,
    "end": 675906,
    "text": "そのすべてをこの答えに組み込んでいるんだ。"
  },
  {
    "start": 676008,
    "end": 681494,
    "text": "今となっては、完璧な答えではないかもしれないし、700億の方がずっといいのかもしれない。"
  },
  {
    "start": 681612,
    "end": 687654,
    "text": "700億をこの新しいプロンプトで試してみて、同様にどうなるか見てみるといい。"
  },
  {
    "start": 687852,
    "end": 695718,
    "text": "今回の回答で、私たちはllmsの検索補強でより質の高い回答ができるようになったと思う。"
  },
  {
    "start": 695814,
    "end": 703594,
    "text": "ここでは、補強用llmsがサーベイ・ペーパーであること、つまり、多くの異なるタイプのものをカバーしていることをより多く得ている。"
  },
  {
    "start": 703712,
    "end": 708426,
    "text": "ここでは数種類を引っ張り出し、それをカバーしている。"
  },
  {
    "start": 708458,
    "end": 710314,
    "text": "密集レトリーバーと疎密レトリーバーがいる。"
  },
  {
    "start": 710362,
    "end": 712362,
    "text": "我々は外部の知識ソースを持っている。"
  },
  {
    "start": 712426,
    "end": 715938,
    "text": "そのために役立ついろいろなものがここにある。"
  },
  {
    "start": 716104,
    "end": 718450,
    "text": "最後に、リアクションとは何か。"
  },
  {
    "start": 718520,
    "end": 725506,
    "text": "繰り返しになるが、不謹慎な回答は一切していない。"
  },
  {
    "start": 725608,
    "end": 734982,
    "text": "このプロンプトを微調整したり、このプロンプトをチューニングしたり。"
  },
  {
    "start": 735036,
    "end": 738154,
    "text": "文字通り、そこに座って遊んでいるんだ。"
  },
  {
    "start": 738192,
    "end": 745820,
    "text": "うまく機能するプロンプトができるまで、全体の結果を大きく改善することができる。"
  },
  {
    "start": 747870,
    "end": 750646,
    "text": "プロンプトをカスタマイズするべきだ。"
  },
  {
    "start": 750678,
    "end": 758298,
    "text": "これはよくあることなのですが、新しいスキルや何かをお見せするときに、デフォルトのプロンプトを使います。"
  },
  {
    "start": 758394,
    "end": 768770,
    "text": "もし私がプロダクション用に何かを作るとしたら、プロンプトと戯れ、プロンプトを手に入れ、そのプロンプトに関連するすべてのことに取り組み、かなりの時間を費やすだろう。"
  },
  {
    "start": 769110,
    "end": 771022,
    "text": "とにかく、このノートを君にあげよう。"
  },
  {
    "start": 771086,
    "end": 772306,
    "text": "自分自身と遊ぶことができる。"
  },
  {
    "start": 772408,
    "end": 773486,
    "text": "ノートをお忘れなく。"
  },
  {
    "start": 773518,
    "end": 778542,
    "text": "私はいつもノートを説明文に載せているので、実際にノートを読み込むことができる。"
  },
  {
    "start": 778606,
    "end": 783366,
    "text": "質問の中で、このプロンプトを試してみることはできますか？"
  },
  {
    "start": 783548,
    "end": 784742,
    "text": "できるよね？"
  },
  {
    "start": 784796,
    "end": 786294,
    "text": "開けてみて、やってみて。"
  },
  {
    "start": 786332,
    "end": 788274,
    "text": "特に最近やっているのはこれだ。"
  },
  {
    "start": 788402,
    "end": 791574,
    "text": "無料のAPIキーを取得することができます。"
  },
  {
    "start": 791772,
    "end": 798022,
    "text": "私が使っているのはT fourで、Colabで使えるフリーのGPUです。"
  },
  {
    "start": 798156,
    "end": 800766,
    "text": "これらのことはすべて、自分で遊びながらできることだ。"
  },
  {
    "start": 800948,
    "end": 805982,
    "text": "とにかく、いつものように、何か質問やコメントがあれば、下のコメント欄に書いてください。"
  },
  {
    "start": 806116,
    "end": 815598,
    "text": "もしこのビデオが役に立ったと思われたなら、「いいね！」をクリックしたり、ソーシャル・メディアでビデオをシェアしてください。"
  },
  {
    "start": 815684,
    "end": 816800,
    "text": "とりあえず、さようなら。"
  }
]