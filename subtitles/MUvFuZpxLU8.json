[
  {
    "start": 570,
    "end": 3470,
    "text": "みんなが元気で起きていることを願っている。"
  },
  {
    "start": 3890,
    "end": 8154,
    "text": "最初のスピーカーは、ディープマインドのヤスミン・バリです。"
  },
  {
    "start": 8282,
    "end": 19194,
    "text": "彼女は、物理学と学習理論とディープラーニングの交差点で、ディープラーニングの特徴を理解しようと、多くのクールな仕事をしてきた。"
  },
  {
    "start": 19322,
    "end": 25286,
    "text": "今日、彼女は皆さんにニューラル・スケーリング法について話してくれる。"
  },
  {
    "start": 25308,
    "end": 26118,
    "text": "聞いてるか？"
  },
  {
    "start": 26284,
    "end": 26806,
    "text": "素晴らしい。"
  },
  {
    "start": 26908,
    "end": 36950,
    "text": "ご丁寧な紹介と、本当に素晴らしいスピーカーが揃ったこのワークショップへの招待に感謝する。"
  },
  {
    "start": 38270,
    "end": 54842,
    "text": "今日お話ししたいのは、スケーリング法則を第一原理から理解しようとする私たちの古い研究の一部です。"
  },
  {
    "start": 54906,
    "end": 59200,
    "text": "それは私がとても気にかけていることだ。"
  },
  {
    "start": 59970,
    "end": 65650,
    "text": "というわけで、まずは直接、素材に飛び込んでみよう。"
  },
  {
    "start": 66390,
    "end": 71140,
    "text": "機械学習で使える基本的なスケーリング・ノブはたくさんある。"
  },
  {
    "start": 71830,
    "end": 78374,
    "text": "トレーニングデータの量、モデルのサイズ、計算機の量、時間などだ。"
  },
  {
    "start": 78492,
    "end": 92278,
    "text": "というわけで、この講演の最初の部分で取り組む質問は、機械学習モデルの平均的な性能は、これらの基本的なスケーリング変数の関数としてどの程度なのか？"
  },
  {
    "start": 92454,
    "end": 106814,
    "text": "確かにこの問題のある側面は、古典的な学習理論に当てはまる。特に、学習データの量を増やすとMLモデルがどのように改善されるかに注目した場合だ。"
  },
  {
    "start": 106932,
    "end": 117982,
    "text": "おそらく新しい材料のひとつは、これらの変数のいくつかを総合的に理解し、ある意味で実験とのギャップを埋めることだろう。"
  },
  {
    "start": 118046,
    "end": 120260,
    "text": "なぜ今これを研究するのか？"
  },
  {
    "start": 121350,
    "end": 128600,
    "text": "これは大規模なモデルにおける経験的な観察に基づくもので、ビジョンのような設定でも、またそのような設定でも同じだ。"
  },
  {
    "start": 129610,
    "end": 145660,
    "text": "数年前にカプランとマッキャンリッシュが発表したこの論文は、このことを実証的なレベルではっきりと指摘している。"
  },
  {
    "start": 146270,
    "end": 166580,
    "text": "彼らは、これらの基本変数の関数として、大規模言語モデルにおけるテスト損失のスケーリングを調べ、これらの3つの基本変数の関数として、ここに列挙したようなべき乗則の指数を持つ、ある種の滑らかなトレンドに従う傾向があることを見た。"
  },
  {
    "start": 166950,
    "end": 185478,
    "text": "データの質やデータセットの多様性など、理論的にも実践的にも理解することが重要な、他の非常に重要なツマミについてはお話しできません。"
  },
  {
    "start": 185644,
    "end": 188966,
    "text": "スケーリング法則の例をもっと見る"
  },
  {
    "start": 188998,
    "end": 190662,
    "text": "これが今、指標を変えようとしている。"
  },
  {
    "start": 190726,
    "end": 192534,
    "text": "正確さを重視している。"
  },
  {
    "start": 192662,
    "end": 210160,
    "text": "これはGPT-3の論文からの引用ですが、コンテキスト学習のように問題のモデルサイズを大幅に変更した場合に、パフォーマンスがどのように変化するか、このような挙動を強調しています。"
  },
  {
    "start": 210610,
    "end": 223746,
    "text": "ここでは、モデルサイズの関数として、幅広いベンチマークにおけるGPT-3の総合的な動作パフォーマンスを見ることができます。"
  },
  {
    "start": 223928,
    "end": 241746,
    "text": "現在、スケールの最適な方法を見出そうとする実証的な研究がたくさん行われている。最適な方法は、実際に作業する環境にもよるが、多くの場合、スケールに最適な計算方法を意味する。"
  },
  {
    "start": 241858,
    "end": 251534,
    "text": "ある量の計算機へのアクセスがあり、その計算機をデータ量やモデルサイズなどにどのように割り振るかを考えたい。"
  },
  {
    "start": 251652,
    "end": 263390,
    "text": "そのため、経験論的な文献では、研究者がしばしば次のような関数形を使用している。"
  },
  {
    "start": 263460,
    "end": 278214,
    "text": "2つの変数dとp、トレーニングデータの量とモデルのサイズについて考えるだけで、一定のシフトに加え、べき乗則のような項があるかもしれないし、もっと複雑な形やデータへのフィットもあるかもしれない。"
  },
  {
    "start": 278332,
    "end": 285046,
    "text": "私たちは、このことについて理解し、何か言えることがあるかどうか確かめたい。"
  },
  {
    "start": 285148,
    "end": 288490,
    "text": "申し訳ありませんが、計算量も計算式に入りますか？"
  },
  {
    "start": 292910,
    "end": 300682,
    "text": "計算量は、訓練するモデルによって違うかもしれないが、データ量とパラメーターの数に関係する。"
  },
  {
    "start": 300746,
    "end": 309882,
    "text": "だから、それを制約条件として差し込み、最適値のようなものを求めることができる。"
  },
  {
    "start": 310026,
    "end": 316374,
    "text": "確かに、コンピュート（計算量）は、我々が実際に見ることになる主要な変数のひとつであることが多い。"
  },
  {
    "start": 316442,
    "end": 326660,
    "text": "これからお話しするのは、主にトレーニングデータとモデルのサイズについてですが、これからお話しする設定で計算を組み込むのは簡単です。"
  },
  {
    "start": 327370,
    "end": 335030,
    "text": "では、私が興味深いと思う具体的な質問に分けてみよう。"
  },
  {
    "start": 335100,
    "end": 339330,
    "text": "では、こうした経験的な傾向について、これ以上何か言えることはあるのだろうか？"
  },
  {
    "start": 339490,
    "end": 342410,
    "text": "どのような質問をしたいですか？"
  },
  {
    "start": 342560,
    "end": 349546,
    "text": "例えば、データとモデルのどのような側面が関数形を決定するのか、それについて何か言えることがあるとすれば。"
  },
  {
    "start": 349648,
    "end": 355438,
    "text": "例えば、データとモデルのスケーリングの指数は同じなのか、それとも常に違うのか？"
  },
  {
    "start": 355524,
    "end": 363810,
    "text": "この問題は、パラメータ効率よりもサンプル効率の方が高いのでしょうか？"
  },
  {
    "start": 364310,
    "end": 367220,
    "text": "それとも、これらには対称性があるのだろうか？"
  },
  {
    "start": 367990,
    "end": 372050,
    "text": "スケーリング体制はすべて同じなのか、それとも違うのか？"
  },
  {
    "start": 372120,
    "end": 375750,
    "text": "分類学や分類に近いものだ。"
  },
  {
    "start": 377130,
    "end": 401726,
    "text": "同じというのは、パフォーマンスを向上させる原動力となるものが、理論的には同じ根本的な原因を持っているのか、あるいはもっとメカニズム的に同じなのか、あるいはもっと明らかに同じなのか、あるいはもっと別の、もっとメカニズム的な理由が背後にあるのか、ということだ。"
  },
  {
    "start": 401748,
    "end": 406430,
    "text": "異なるスケーリング体制には異なる起源があると思う。"
  },
  {
    "start": 407250,
    "end": 412042,
    "text": "では、例えば経験的に観察可能な普遍的な行動というのはあるのだろうか？"
  },
  {
    "start": 412106,
    "end": 418174,
    "text": "それとも、この問題全体が、微細な、引用符で囲んだような、ミクロの細部に依存しすぎているのだろうか？"
  },
  {
    "start": 418302,
    "end": 432662,
    "text": "つまり、特定のセットアップの場合、問題は持っているトレーニングデータのタイプやモデルのアーキテクチャなどに大きく依存するため、この問題に取り組もうとしても意味がないということだ。"
  },
  {
    "start": 432716,
    "end": 436198,
    "text": "理屈の上では、指数を測ってくるだけでいい。"
  },
  {
    "start": 436284,
    "end": 438178,
    "text": "あるいは関数形を測定する。"
  },
  {
    "start": 438274,
    "end": 446566,
    "text": "問題のすべての側面がミクロの細部に大きく依存しているわけではないと信じるとき、あなたは通常理論を行う。"
  },
  {
    "start": 446598,
    "end": 451100,
    "text": "この背後にもっと一般的な原則があるかどうかを理解したいだろう。"
  },
  {
    "start": 451710,
    "end": 470340,
    "text": "注意点として、この話の多くは、少し古い研究で、少し理論的なことを使っているため、ドメインとして画像に焦点をあてています。"
  },
  {
    "start": 473030,
    "end": 488070,
    "text": "そのうちの2人、イーサン・ダイアー、ジェフン・リー、ジャレッド・カプラン、ウドカル・シャルマはジョンズ・ホプキンスにいた。"
  },
  {
    "start": 488150,
    "end": 500780,
    "text": "だから、私たち全員が理論物理学のバックグラウンドを持っていて、私たちが使うツールやアイデアの種類は限られているか、それにインスパイアされているかのどちらかだ。"
  },
  {
    "start": 502670,
    "end": 515250,
    "text": "この講演で私がとっている、そしてもっと一般的に私が好きなアプローチは、問題をうまく扱えるような場所でシンプルな理論から始めることです。"
  },
  {
    "start": 515320,
    "end": 521010,
    "text": "理論がないところでは、いくつかのことを経験的に試してみることができる。"
  },
  {
    "start": 521750,
    "end": 525402,
    "text": "だから、シンプルなものから始めて、徐々に複雑にしていく。"
  },
  {
    "start": 525486,
    "end": 535590,
    "text": "私たちは基本的に、より単純な設定を研究し、他の体制にも一般化できるものを見つけ出すことでこれを実現した。"
  },
  {
    "start": 536410,
    "end": 546678,
    "text": "僕は、理論的なことと、それを確認するための実験と、理論へのフィードバックの間を行ったり来たりするのが好きなんだ。"
  },
  {
    "start": 546854,
    "end": 556314,
    "text": "では、問題のセットアップがどのようなものなのか、ほんの数枚のスライドでお見せしましょう。"
  },
  {
    "start": 556362,
    "end": 564260,
    "text": "理論的な問題は、聴衆の何人かが知っているような古典的なものだ。"
  },
  {
    "start": 565270,
    "end": 571666,
    "text": "データ分布pに対して、学習可能なパラメータθを持つモデルf sub thetaを学習したい。"
  },
  {
    "start": 571768,
    "end": 573780,
    "text": "古典的な教師あり学習だ。"
  },
  {
    "start": 574950,
    "end": 582882,
    "text": "この設定では、例えば画像について考えていて、トレーニングデータセットがあり、経験的な損失がある。"
  },
  {
    "start": 582946,
    "end": 589990,
    "text": "MScと書いたが、クロスエントロピーの可能性もあり、それを最小化してパラメーターを求める。"
  },
  {
    "start": 590070,
    "end": 592300,
    "text": "シータ、ハット、学習モデル。"
  },
  {
    "start": 592910,
    "end": 595478,
    "text": "個体数の減少を評価したい。"
  },
  {
    "start": 595574,
    "end": 604494,
    "text": "この誤差を、実際にはわからない真の分布で評価する。"
  },
  {
    "start": 604692,
    "end": 618706,
    "text": "例えば、最適化のために初期化する際に使用するパラメータの初期条件θノットや、トレーニングデータセットの描画などである。"
  },
  {
    "start": 618808,
    "end": 631346,
    "text": "トレーニングセットの固定されたドローに焦点を当て、ワーストケースの境界を求めることもある。"
  },
  {
    "start": 631378,
    "end": 635590,
    "text": "完全に平均化された量を考えたい。"
  },
  {
    "start": 636250,
    "end": 645130,
    "text": "これは、データ量とモデル・パラメーターの数の関数としての損失である。"
  },
  {
    "start": 645790,
    "end": 651138,
    "text": "では、さまざまなアーキテクチャについて見ていこう。"
  },
  {
    "start": 651334,
    "end": 659274,
    "text": "例えば、バニラで完全に接続されたネットワークを頭の片隅に置いておくことはできるが、完全に接続されている必要はない。"
  },
  {
    "start": 659322,
    "end": 662430,
    "text": "また、他のアーキテクチャーについても話し合うことができる。"
  },
  {
    "start": 662770,
    "end": 679750,
    "text": "モデルの最も基本的な特徴としては、深さがあり、隠れ層の幅がある。"
  },
  {
    "start": 681130,
    "end": 686310,
    "text": "ここでも、θは学習可能なパラメーターの集合を意味し、パラメーターの総数はp個である。"
  },
  {
    "start": 687130,
    "end": 694090,
    "text": "ディープラーニングの理論から得られた、異なるモデルタイプに適用可能ないくつかの結果に頼ろうと思う。"
  },
  {
    "start": 694830,
    "end": 700986,
    "text": "完全連結層だけでなく、畳み込み層、残差層、さらには注意層である可能性もある。"
  },
  {
    "start": 701098,
    "end": 725746,
    "text": "アーキテクチャーに注意が必要なモデルについての注意点については、また後ほど説明するが、モデルについての最も漫画的な基本的な考え方から言えば、奥行きが深くなったり、幅が広くなったり、関節のスケーリングがあるために、パラメータpの数が増えている可能性がある。"
  },
  {
    "start": 725858,
    "end": 735910,
    "text": "だから、理想的には、スケーリングの方法、スケーリングを理解する方法として、これらすべてを取り入れたいんだ。"
  },
  {
    "start": 738190,
    "end": 744010,
    "text": "繰り返しになるが、この数量は完全に平均的なものと考えている。"
  },
  {
    "start": 744910,
    "end": 752270,
    "text": "パラメーターの数は、申し上げたように、このジョイント・スケーリングによって2つの異なる方法、あるいはおそらく複数の方法で増やすことができる。"
  },
  {
    "start": 752850,
    "end": 767054,
    "text": "ディープラーニング理論でよく理解されていることについては、理論的な限界があるため、データセットのサイズと幅の関数としての損失を研究することにする。"
  },
  {
    "start": 767102,
    "end": 777910,
    "text": "パラメタリゼーションの過不足は、ネットワークの隠れ層のサイズだけを変え、深さは固定にすることで生じる。"
  },
  {
    "start": 778250,
    "end": 788762,
    "text": "私たちが抽出できるのは、べき乗則の項、あるいはこのようなべき乗則の形である。"
  },
  {
    "start": 788816,
    "end": 804922,
    "text": "トレーニングデータの量に応じたスケーリングに注目して、その指数を調べたり、幅やモデルサイズに応じたスケーリングに注目して、その指数を抽出したりする。"
  },
  {
    "start": 804986,
    "end": 809246,
    "text": "私は主にこれらの指数、アルファDとアルファWに焦点を当てるつもりだ。"
  },
  {
    "start": 809348,
    "end": 815726,
    "text": "アルファWは幅やパラメータを示すものなので、アルファPと同じ意味で使うこともある。"
  },
  {
    "start": 815838,
    "end": 824946,
    "text": "この2つの指数を使って、さまざまな体制や価値観について述べることができるだろう。"
  },
  {
    "start": 825128,
    "end": 830200,
    "text": "dとnの間にこれ以上の相互作用がないという仮定も検証しているのですか？"
  },
  {
    "start": 830810,
    "end": 836402,
    "text": "残念ながら、理論側から共同スケーリングにアクセスすることはできない。"
  },
  {
    "start": 836546,
    "end": 838726,
    "text": "それが限界だ。"
  },
  {
    "start": 838838,
    "end": 845338,
    "text": "ええ、理想的な治療法は、DNNの両方のスケーリングに集中することです。"
  },
  {
    "start": 845424,
    "end": 854640,
    "text": "背景には、ディープラーニング理論で知られていることとして、その限界は難しいということがある。"
  },
  {
    "start": 855410,
    "end": 857280,
    "text": "彼らについてはあまり知られていない。"
  },
  {
    "start": 859730,
    "end": 862650,
    "text": "また、幅だけに焦点を当てることについても聞いていいですか？"
  },
  {
    "start": 862810,
    "end": 869038,
    "text": "理論的な側面から言えば、大きな幅を持つ普遍的な関数近似器が得られることがわかっているので、幅は簡単だ。"
  },
  {
    "start": 869134,
    "end": 877670,
    "text": "ディープラーニングと実践の側にとって、幅の話ばかりで深さの話をしないのは、なんだか面白くないような気がする。"
  },
  {
    "start": 878330,
    "end": 880840,
    "text": "いい質問だね。"
  },
  {
    "start": 883050,
    "end": 892054,
    "text": "背景には、何を学んだかという結果に頼ることになるから、普遍的な近似性よりも少し鋭いかもしれないね。"
  },
  {
    "start": 892182,
    "end": 903466,
    "text": "このような大きな幅の限界で何が学べるのか、それについてはもっと多くのことが分かっている。"
  },
  {
    "start": 903498,
    "end": 907630,
    "text": "カーネルとの関係や、その他のことについてもお話しします。"
  },
  {
    "start": 907780,
    "end": 922050,
    "text": "この方法は、これからお話しするような深さの面を確かに捉えていますが、制約の種類という点では、普遍的な近似性よりも具体的だと思います。"
  },
  {
    "start": 926330,
    "end": 938200,
    "text": "また、理論的な側面から深さを無視するのは妥当なのかという疑問もある。"
  },
  {
    "start": 940990,
    "end": 942582,
    "text": "深さは分析に現れる。"
  },
  {
    "start": 942646,
    "end": 943260,
    "text": "そうだね。"
  },
  {
    "start": 944590,
    "end": 947750,
    "text": "深さはあるが、スケールはない。"
  },
  {
    "start": 947910,
    "end": 949654,
    "text": "ええ、深さが無視されているとは思いません。"
  },
  {
    "start": 949702,
    "end": 951798,
    "text": "ただ、無限大に受け止められていないだけだと思う。"
  },
  {
    "start": 951974,
    "end": 965726,
    "text": "例えば、奥行きと幅が一緒にスケーリングされるような興味深い限界が他にもあることが重要で、トレーニングデータはもっと少なくなるか、スケーリングされるかもしれない。"
  },
  {
    "start": 965838,
    "end": 974850,
    "text": "しかし、理論的にはまだ解明できていない。"
  },
  {
    "start": 978810,
    "end": 989420,
    "text": "では、どのような結果が出るかを強調しておくと、スケール除去かスケーリングのどちらかを見ることになり、両方を一緒に見ることはない。"
  },
  {
    "start": 990670,
    "end": 996380,
    "text": "このような実証的な結果を目指して頑張りたい。"
  },
  {
    "start": 997070,
    "end": 1001642,
    "text": "これらの異なるプロットで何がプロットされているかを説明しよう。"
  },
  {
    "start": 1001706,
    "end": 1008750,
    "text": "これは、先ほど私が言及した分類法のようなものだ。"
  },
  {
    "start": 1009810,
    "end": 1022498,
    "text": "この2×2のブロックは、ここにある2×2のグリッドに対応する、一種の実験のようなものだ。"
  },
  {
    "start": 1022664,
    "end": 1050726,
    "text": "この分類学では、1つの変数、例えばトレーニングデータ量、あるいはモデルサイズなどのスケーリングを見ている場合、理論側、あるいは我々のセットアップでは、基本的に2つのレジームがあり、トレーニングデータ量が2つの変数のうち小さい方がボトルネックになるか、トレーニングデータ量が2つの変数のうち大きい方がボトルネックになるかのどちらかになります。"
  },
  {
    "start": 1050758,
    "end": 1062400,
    "text": "dをスケーリングしていくと、nよりはるかに小さいdの領域から別の領域へと移行し、その間に理論的にはアクセスできないクロスオーバーが存在する。"
  },
  {
    "start": 1063190,
    "end": 1078882,
    "text": "どの変数がボトルネックで、どの変数がボトルネックでないかという基本的な違いによって、少し正当化する。"
  },
  {
    "start": 1078946,
    "end": 1083602,
    "text": "変数のスケーリングを研究する場合、それがボトルネックになる。"
  },
  {
    "start": 1083666,
    "end": 1098200,
    "text": "私たちはこれを解像度に制限のある領域と名づけたが、それはいくつかのデータ、多様体、そしてこれから少し話すスペクトル特性のような性質と関係があるからだ。"
  },
  {
    "start": 1098650,
    "end": 1106122,
    "text": "より大きな変数をスケーリングする場合は、別の限界によってコントロールされる。"
  },
  {
    "start": 1106186,
    "end": 1114142,
    "text": "だから、あなたが観察している指数は、実際には単なる整数のようなものだ。"
  },
  {
    "start": 1114286,
    "end": 1117780,
    "text": "彼らは問題の微細さにはそれほど影響されない。"
  },
  {
    "start": 1122550,
    "end": 1130466,
    "text": "経験的な結果というのは、現実的に興味深い学習課題があるということだろう。"
  },
  {
    "start": 1130578,
    "end": 1132706,
    "text": "理論的にはどうなのか？"
  },
  {
    "start": 1132738,
    "end": 1135480,
    "text": "どのような学習課題に注目していますか？"
  },
  {
    "start": 1138350,
    "end": 1143094,
    "text": "これらの実験はすべて画像分類で、ベンチマーク・タスクのようなものだ。"
  },
  {
    "start": 1143142,
    "end": 1154134,
    "text": "実際、その一環として、さまざまなタスクを集約し、そのうちのいくつかが普遍的な指数を持つことを示した。"
  },
  {
    "start": 1154182,
    "end": 1157230,
    "text": "ある体制では、学習問題のためにそうでない体制もある。"
  },
  {
    "start": 1157380,
    "end": 1167858,
    "text": "私たちがやっていることの一部は、仮定することではなく、部分的に一般化することだ。"
  },
  {
    "start": 1167944,
    "end": 1169822,
    "text": "私たちは生徒の教師という設定で研究しています。"
  },
  {
    "start": 1169886,
    "end": 1173502,
    "text": "別のネットワークで生成された教師データがある。"
  },
  {
    "start": 1173646,
    "end": 1181080,
    "text": "それを除けば、データの特定の量におけるべき乗則以外に、他に強い仮定はない。"
  },
  {
    "start": 1181610,
    "end": 1188840,
    "text": "それは、例えば、非常に特定のタスクによって指定されているわけではない。"
  },
  {
    "start": 1190110,
    "end": 1204030,
    "text": "分布の仮定を仮定して、ある種のテンソル行列や量の崩壊や性質について考えるというアプローチです。"
  },
  {
    "start": 1205250,
    "end": 1210560,
    "text": "そこでは、基本的にデータの仮定が背景に隠されている。"
  },
  {
    "start": 1211250,
    "end": 1211902,
    "text": "それでもだ。"
  },
  {
    "start": 1211956,
    "end": 1220834,
    "text": "フォローするようで申し訳ないが、どういうわけかあなたは、学ぶのが難しい問題があるという事実を避けている。"
  },
  {
    "start": 1220952,
    "end": 1223394,
    "text": "あなたが持っている学習の代用品があるはずだ。"
  },
  {
    "start": 1223432,
    "end": 1225540,
    "text": "そのプロキシはどのようなものですか？"
  },
  {
    "start": 1228150,
    "end": 1242374,
    "text": "この問題を定義するときに、いくつかのステップを踏めばはっきりするかもしれませんが、生徒の教師という設定で、信頼できる教師のネットワークがあり、それがデータを生成し、学習するモデルと同じような形をしています。"
  },
  {
    "start": 1242502,
    "end": 1254122,
    "text": "そのうえで、これらのレジームのパワーログに関連するデータの特性について、いくつかの仮定が必要になる。"
  },
  {
    "start": 1254186,
    "end": 1261726,
    "text": "文字通り、トレーニング例の数とモデルの幅を比較しているのですか？"
  },
  {
    "start": 1261908,
    "end": 1264346,
    "text": "もしそうなら、トレーニング例をどのように定義するのですか？"
  },
  {
    "start": 1264388,
    "end": 1269794,
    "text": "勾配を計算する入力ターゲットのペアは何でもいいのですか？"
  },
  {
    "start": 1269912,
    "end": 1271570,
    "text": "そう、その通りだ。"
  },
  {
    "start": 1271640,
    "end": 1285862,
    "text": "それはいい質問だ。なぜなら、これはnのオーダーのdという設定で戻ってくるからだ。"
  },
  {
    "start": 1285996,
    "end": 1299450,
    "text": "この移行が起こるような複雑なアーキテクチャーでは、どの程度が小さく、どの程度が大きいかは何とも言えない。"
  },
  {
    "start": 1299520,
    "end": 1301660,
    "text": "それはまた戻ってくることになるだろう。"
  },
  {
    "start": 1303330,
    "end": 1312122,
    "text": "ウメッシュが言いたかったのは、たとえば擬似ランダム関数を学習しようとした場合、これらの損失曲線はただ平坦になる、ということだと思う。"
  },
  {
    "start": 1312266,
    "end": 1313470,
    "text": "彼らは決して行かないだろう。"
  },
  {
    "start": 1313620,
    "end": 1323730,
    "text": "それから、実用的な学習問題の大きなクラスがあり、そこでは損失曲線はこのようになるとか、そういう主張がある。"
  },
  {
    "start": 1323800,
    "end": 1335030,
    "text": "ええ、正式には明言されていませんが、私たちがフルハンドルを握っているのは学生教師という設定なのです。"
  },
  {
    "start": 1336810,
    "end": 1339642,
    "text": "データ自体は教師モデルによって生成される。"
  },
  {
    "start": 1339696,
    "end": 1350490,
    "text": "つまり、生徒の先生には機能があり、誰かがあなたのインプット・アウトプットを生み出しているのだ。"
  },
  {
    "start": 1353970,
    "end": 1364482,
    "text": "というのも、もしかしたら定理文までやらせて、それでもまだ謎が残るようなら、そうすることもできるかもしれない。"
  },
  {
    "start": 1364616,
    "end": 1370660,
    "text": "でも、ランダムな関数ではないんだ。"
  },
  {
    "start": 1371830,
    "end": 1377140,
    "text": "B、それは構造的な特性を持っている。"
  },
  {
    "start": 1378310,
    "end": 1391210,
    "text": "理論的に、経験的に観察されたことと一致させるために、私たちがこれから導き出そうとしている結果は何かということに戻ると、基本的には、この2対2のような分類である。"
  },
  {
    "start": 1391630,
    "end": 1398090,
    "text": "指数が実際に変化し、問題にかなり依存する領域がある。"
  },
  {
    "start": 1398160,
    "end": 1401050,
    "text": "ご覧の通り、これらは異なるデータセットである。"
  },
  {
    "start": 1401390,
    "end": 1405834,
    "text": "また、ここではさまざまな種類のアーキテクチャがプロットされており、損失も異なる。"
  },
  {
    "start": 1405962,
    "end": 1407658,
    "text": "これらは指数が異なる。"
  },
  {
    "start": 1407754,
    "end": 1413678,
    "text": "ということは、これらの異なるセッティングの指数がすべて1に近い領域もあるということだ。"
  },
  {
    "start": 1413844,
    "end": 1430854,
    "text": "同様に、幅のスケーリングを見てみると、幅が2つの変数のうち小さいほうの領域からスタートした場合、それがボトルネックとなり、指数が1に近づくある時点で、分散制限と呼ぶ別の領域に移行する。"
  },
  {
    "start": 1430892,
    "end": 1441194,
    "text": "つまり、これらは異なる要因によって支配されており、私たちが名前をつけようとした2つの異なる体制を動かしているだけなのだ。"
  },
  {
    "start": 1441392,
    "end": 1443900,
    "text": "これについて何か質問は？"
  },
  {
    "start": 1444270,
    "end": 1449610,
    "text": "解像度の制限と分散の制限とはどういう意味ですか？"
  },
  {
    "start": 1449970,
    "end": 1455566,
    "text": "ああ、その用語の由来については、もう少し詳しく話そう。"
  },
  {
    "start": 1455668,
    "end": 1466622,
    "text": "この体制では、解像度が高くなればなるほど、データの多様性がよりよく解像されていくようなものだ、という考え方に突き動かされたんだ。"
  },
  {
    "start": 1466686,
    "end": 1487314,
    "text": "dがボトルネックとなる変数で、ddが増加する場合、ddはこのデータ多様体を解決するのに役立つが、ある時点でそこから得られるものが飽和し、別の限界に近づく。"
  },
  {
    "start": 1487362,
    "end": 1491450,
    "text": "オーバーフィッティングのような現象ですか？"
  },
  {
    "start": 1492990,
    "end": 1494380,
    "text": "それは分からない。"
  },
  {
    "start": 1496430,
    "end": 1499450,
    "text": "オーバーフィッティングとは関係ないと思う。"
  },
  {
    "start": 1501250,
    "end": 1506350,
    "text": "それよりも、何が限界へのアプローチの原動力になっているかということだ。"
  },
  {
    "start": 1507970,
    "end": 1517274,
    "text": "このような分類があるのかどうか、あるいはそのような分類法が存在するのかどうか、よくわからなかったのだ。"
  },
  {
    "start": 1517402,
    "end": 1522000,
    "text": "私たちは、この単純な問題からの理解に基づいて、これらの名前をつけた。"
  },
  {
    "start": 1524610,
    "end": 1529080,
    "text": "では、この件に関して他に質問は？"
  },
  {
    "start": 1529850,
    "end": 1540938,
    "text": "ディープラーニングの理論から何を頼りにすることができるのか。これはかなり難しい問題だからだ。"
  },
  {
    "start": 1541024,
    "end": 1557770,
    "text": "この分野にあまり馴染みのない人のために少し触れておくと、ここ数年のディープラーニング理論の進歩は、ニューラルネットワークの大きな幅の限界を研究することにあった。"
  },
  {
    "start": 1557850,
    "end": 1560800,
    "text": "nが無限大になるのか、それともnがかなり大きくなるのか？"
  },
  {
    "start": 1561810,
    "end": 1585538,
    "text": "この限界に近づくと、モデルはますます線形モデルのようになり、初期化時にニューラルネットワークから得られるランダムな特徴を持つパラメータに関して線形になる。"
  },
  {
    "start": 1585634,
    "end": 1595350,
    "text": "しかしそれでも、パラメータが線形であるモデルがある。"
  },
  {
    "start": 1595430,
    "end": 1609870,
    "text": "別の言い方をすれば、例えば二乗損失を使う場合、カーネル回帰と関係があり、これらはニューラルネットワーク・アーキテクチャから派生した、一種のリッチな構成カーネルです。"
  },
  {
    "start": 1611250,
    "end": 1626098,
    "text": "これはダイナミックな現象であり、ニューラルネットワークを特定のスケーリングで初期化する方法と関係している。"
  },
  {
    "start": 1626194,
    "end": 1629000,
    "text": "この制限はある時点で発動する。"
  },
  {
    "start": 1632010,
    "end": 1639862,
    "text": "この関係は、基本的に、例えば二乗損失がある場合、問題を解析的に、そしてむしろ一般的に解くことができることを意味する。"
  },
  {
    "start": 1639926,
    "end": 1653760,
    "text": "例えば、これはその連続時間バージョンだが、解、つまり得られる予測関数は、この線形オードを解くだけであり、閉形式でそれを行うことができる。"
  },
  {
    "start": 1654930,
    "end": 1669038,
    "text": "このオードにはkという量があり、これはカーネルで、初期化時にランダム・ニューラル・ネットワークに由来するものなので、kをゼロに切り詰めた。"
  },
  {
    "start": 1669214,
    "end": 1684550,
    "text": "いずれにせよ、これは、アーキテクチャに依存する再帰関係が存在するさまざまなカーネルを駆動し、研究することができる一連の作業の要約のようなものだ。"
  },
  {
    "start": 1684970,
    "end": 1692326,
    "text": "この限界は、基本的に多くの異なるネットワークタイプに存在し、そこには自然な幅の概念がある。"
  },
  {
    "start": 1692438,
    "end": 1702662,
    "text": "完全連結層、畳み込み層、畳み込みフィルターの数、残差ネットワーク、注意層などだ。"
  },
  {
    "start": 1702726,
    "end": 1709530,
    "text": "アテンションの場合は、限界はアテンションほど自然ではないと思うからだ。"
  },
  {
    "start": 1709690,
    "end": 1716610,
    "text": "同じような限界を知るには、多くの異なるアテンションヘッドの限界を研究する必要がある。"
  },
  {
    "start": 1717110,
    "end": 1724820,
    "text": "だから、他の建築タイプとはちょっと違うんだと思う。"
  },
  {
    "start": 1727610,
    "end": 1736338,
    "text": "我々の理論的なツールでこれらを正確に解くことができることの意味は何だろうか？"
  },
  {
    "start": 1736354,
    "end": 1742490,
    "text": "予測関数の厳密な解が得られ、それを使って計算ができるからだ。"
  },
  {
    "start": 1742640,
    "end": 1749462,
    "text": "また、学習理論のような研究の多くにもつながっている。"
  },
  {
    "start": 1749526,
    "end": 1751014,
    "text": "例えば、カーネル回帰。"
  },
  {
    "start": 1751072,
    "end": 1761274,
    "text": "これは基本的に、二乗損失を使用しているときのカーネル回帰であり、kが何であるかも知っている。"
  },
  {
    "start": 1761322,
    "end": 1763380,
    "text": "再帰関係のようなものを書くことができる。"
  },
  {
    "start": 1764390,
    "end": 1769186,
    "text": "これは特別な解決策だ。"
  },
  {
    "start": 1769208,
    "end": 1778680,
    "text": "より一般的な問題は、実際には微分方程式の階層であり、ディープ・ニューラル・ネットワークのために解かなければならない。"
  },
  {
    "start": 1780570,
    "end": 1782902,
    "text": "ちょっと面白い質問をしてもいいかな？"
  },
  {
    "start": 1782956,
    "end": 1786562,
    "text": "昨日イリヤが言っていたこととどう関係があるのだろう？"
  },
  {
    "start": 1786706,
    "end": 1790730,
    "text": "実際にこれらの最小化などができたと仮定しよう。"
  },
  {
    "start": 1791870,
    "end": 1795100,
    "text": "この制限と何か関係があるのか？"
  },
  {
    "start": 1796030,
    "end": 1798118,
    "text": "どの最小化を行うのか？"
  },
  {
    "start": 1798294,
    "end": 1803962,
    "text": "最短のプログラムを計算したり、コールド・マクグラスの複雑さを計算したり。"
  },
  {
    "start": 1804026,
    "end": 1809514,
    "text": "ここで、閉じた形の解を与えることができる。"
  },
  {
    "start": 1809562,
    "end": 1813380,
    "text": "と何か関係があるのだろうか？"
  },
  {
    "start": 1818230,
    "end": 1820340,
    "text": "ちょっと違うかもしれない。"
  },
  {
    "start": 1823350,
    "end": 1825850,
    "text": "私たちは、これらのモードが非常に最適でないことを知っている。"
  },
  {
    "start": 1826030,
    "end": 1829970,
    "text": "また、これらの結果は学習ダイナミクスを利用している。"
  },
  {
    "start": 1830050,
    "end": 1834550,
    "text": "あるサイズでSGDをやることが重要なんだ。"
  },
  {
    "start": 1834620,
    "end": 1834902,
    "text": "もちろんだ。"
  },
  {
    "start": 1834956,
    "end": 1837880,
    "text": "何でもかんでも議論しているわけではない。"
  },
  {
    "start": 1838890,
    "end": 1840950,
    "text": "我々はそれが非常に微妙なものだと知っている。"
  },
  {
    "start": 1843790,
    "end": 1854640,
    "text": "たぶん、ひとつ言えることは、おそらく、おそらく、最適でないことは、アクセスできるデータの量に左右されると思う。"
  },
  {
    "start": 1855410,
    "end": 1863070,
    "text": "例えば、そうでないモデル、これについてもう少し言ってみる。"
  },
  {
    "start": 1864450,
    "end": 1874094,
    "text": "一般的に、モデルには特徴学習があり、特徴学習とは、このkという量が本当に動的変数であるという意味で定義しようと思います。"
  },
  {
    "start": 1874142,
    "end": 1882710,
    "text": "tに依存するし、力学を支配する方程式もあるから、方程式一式を解くにはそれを解かなければならない。"
  },
  {
    "start": 1884570,
    "end": 1889078,
    "text": "さて、学習体制が異なれば、その量も異なるかもしれない。"
  },
  {
    "start": 1889244,
    "end": 1897354,
    "text": "異なる体制では、ネットワークの深さ、広さ、トレーニングデータの量によって、将来の学習量が異なるかもしれない。"
  },
  {
    "start": 1897472,
    "end": 1901182,
    "text": "これらはすべて、また別の学習体制なのかもしれない。"
  },
  {
    "start": 1901316,
    "end": 1915346,
    "text": "私がここで要約しようとしているのは、これは基本的に厳密解であり、データ量など問題の他の側面に比べてnがかなり大きい場合に有効だということです。"
  },
  {
    "start": 1915448,
    "end": 1920980,
    "text": "ジェイコブも言っていたと思うが、学習率にも制約がある。"
  },
  {
    "start": 1921830,
    "end": 1924334,
    "text": "最も有名なものの一つだ。"
  },
  {
    "start": 1924382,
    "end": 1928130,
    "text": "つまり、多くの場所で使われてきたという意味でね。"
  },
  {
    "start": 1928210,
    "end": 1931510,
    "text": "私が関連性があると思う他の場所についても話そう。"
  },
  {
    "start": 1932170,
    "end": 1937842,
    "text": "良いところは、かなり一般的なソリューションだということだ。"
  },
  {
    "start": 1937906,
    "end": 1949610,
    "text": "そうすれば、データの特性とカーネルの特性を分離することもできるし、この制限を設定することで、基本的に両者がどのように結びついているかを確認することもできる。"
  },
  {
    "start": 1951890,
    "end": 1953280,
    "text": "他に質問は？"
  },
  {
    "start": 1957330,
    "end": 1973342,
    "text": "実際に、経験的な研究から、kが固定されたモデルとkが動的に変化するモデルのパフォーマンスを比較したプロットをお見せしましょう。"
  },
  {
    "start": 1973406,
    "end": 1982294,
    "text": "超低データ領域であれば、これらのモデルがいかに優れているかわかるだろう。"
  },
  {
    "start": 1982332,
    "end": 1987346,
    "text": "要は、ただ勉強することから何が学べるか、ということだ。"
  },
  {
    "start": 1987378,
    "end": 1997894,
    "text": "このモデルは大きな幅で正確であり、必ずしも正確な予測を得るためではなく、この分類法を理解するためなのだ。"
  },
  {
    "start": 1998022,
    "end": 1999420,
    "text": "そういうことだ。"
  },
  {
    "start": 2003650,
    "end": 2013598,
    "text": "では、次の数枚のスライドをもっと手短に説明しよう。"
  },
  {
    "start": 2013694,
    "end": 2024094,
    "text": "だから、正確に言うと、私たちがもう少し研究している体制は、生徒指導体制だ。"
  },
  {
    "start": 2024142,
    "end": 2032840,
    "text": "これは、機械学習を研究するための学習理論や統計物理学的アプローチにおける古典的なセットアップの一種である。"
  },
  {
    "start": 2034490,
    "end": 2046330,
    "text": "我々のセットアップでは、あるデータを生成し、d個のサンプルで基礎となるデータ分布を生成する教師モデルがある。"
  },
  {
    "start": 2047150,
    "end": 2050938,
    "text": "生徒モデルはこのトレーニングデータセットから学習する。"
  },
  {
    "start": 2051104,
    "end": 2057434,
    "text": "ここでは、生徒モデルと教師モデルがともにパラメータにおいて線形であり、任意のランダムな特徴を使用すると仮定する。"
  },
  {
    "start": 2057482,
    "end": 2062838,
    "text": "最後のスライドでも触れたように、このクラスのモデルだ。"
  },
  {
    "start": 2062954,
    "end": 2076978,
    "text": "つまり、ランダムなニューラルネットワークから導き出された、固定されたランダムな特徴量ファイがあり、そのランダムな特徴量からカーネルを構成することができる。"
  },
  {
    "start": 2077074,
    "end": 2080822,
    "text": "具体的には、教師がその模範となる。"
  },
  {
    "start": 2080956,
    "end": 2093638,
    "text": "オメガMは教師重みであり、ランダムに一度だけ描かれ、その後固定される。"
  },
  {
    "start": 2093734,
    "end": 2108526,
    "text": "特定のクラスの生徒モデルは、教師特徴のサブセットを使用し、その上で線形モデルを学習することによっても構築される。"
  },
  {
    "start": 2108548,
    "end": 2115850,
    "text": "これはすべて、ランダムを含む線形モデルのクラス内である。"
  },
  {
    "start": 2116020,
    "end": 2126470,
    "text": "シャフィとスコットの先ほどの質問に続くような形で、一方向性関数がどのように私に支配されたかを理解することができたら、これが正しいかどうか教えてほしい。"
  },
  {
    "start": 2126540,
    "end": 2142518,
    "text": "つまり、私たちは暗黙のうちに、データがランダムな重みで開始され、小さな学習率制限を行うことによって得られた、ある無限の幅を持つニューラルネットワークによって生成されたと仮定しているようなものだ。"
  },
  {
    "start": 2142614,
    "end": 2151622,
    "text": "この極限では、すべてがランダムな特徴において漸近的に線形になるため、一方向性関数を構成することはできない。"
  },
  {
    "start": 2151766,
    "end": 2157034,
    "text": "ということは、もし最悪の場合、全体的に無限の幅を持つネットワークがあったとしたら、私たちはある意味台無しになってしまうということだ。"
  },
  {
    "start": 2157082,
    "end": 2165726,
    "text": "教師モデルは暗黙のうちにランダムな重みを持つようなものだから、このような一方向の関数は得られない。"
  },
  {
    "start": 2165758,
    "end": 2168914,
    "text": "こうやって悪いケースを除外していくのか。"
  },
  {
    "start": 2169112,
    "end": 2170002,
    "text": "そうかもしれない。"
  },
  {
    "start": 2170056,
    "end": 2173102,
    "text": "確かに波はランダムに描かれている。"
  },
  {
    "start": 2173166,
    "end": 2176440,
    "text": "特定の問題に対して微調整されているわけではない。"
  },
  {
    "start": 2177210,
    "end": 2192326,
    "text": "教師ネットワークは無限に多くの特徴を持つことも、有限個の特徴を持つこともできるが、それはランダムな特徴を持つ線形モデルであり、これらのランダムな特徴に関連するカーネルにはべき乗則の仮定が存在することになる。"
  },
  {
    "start": 2192358,
    "end": 2195866,
    "text": "それも理由のひとつだと思う。"
  },
  {
    "start": 2196048,
    "end": 2196650,
    "text": "なるほど。"
  },
  {
    "start": 2196720,
    "end": 2197290,
    "text": "ありがとう。"
  },
  {
    "start": 2197440,
    "end": 2200762,
    "text": "それでも、どんな機能を表現できるのかという疑問は残る。"
  },
  {
    "start": 2200826,
    "end": 2204400,
    "text": "もしウェイトを調整することができたら、何でもその方法で表現できるだろうか？"
  },
  {
    "start": 2206610,
    "end": 2213570,
    "text": "私の理解が正しければ、それが無限に大きい場合は、普遍的な近似に頼ることができると思う。"
  },
  {
    "start": 2214390,
    "end": 2217540,
    "text": "観客の中にも、もしかしたらそういう人がいるかもしれない。"
  },
  {
    "start": 2219190,
    "end": 2220260,
    "text": "オーケー、ありがとう。"
  },
  {
    "start": 2221670,
    "end": 2225986,
    "text": "スペクトルのべき乗則という重要な仮定がある。"
  },
  {
    "start": 2226018,
    "end": 2231490,
    "text": "べき乗則を仮定することは、どのような種類の関数に関係しているのだろうか？"
  },
  {
    "start": 2231570,
    "end": 2232358,
    "text": "私はそう思う。"
  },
  {
    "start": 2232524,
    "end": 2233640,
    "text": "分からないよ。"
  },
  {
    "start": 2237930,
    "end": 2240634,
    "text": "彼らは、基本的にそうなんだ。"
  },
  {
    "start": 2240752,
    "end": 2241930,
    "text": "ええ、基本的にはそうです。"
  },
  {
    "start": 2242080,
    "end": 2246230,
    "text": "まあ、無限にあれば完全な基礎になるはずだからね。"
  },
  {
    "start": 2246310,
    "end": 2251760,
    "text": "つまり、例えば最悪のケースを想定した場合、これらのうち1つでも最悪のケースを想定した関数になるということだ。"
  },
  {
    "start": 2252770,
    "end": 2258030,
    "text": "Fmは左側の微分方程式の解のようなものだったと思う。"
  },
  {
    "start": 2258100,
    "end": 2265678,
    "text": "いや、あれはニューラルネットワークの予測だったんだ。"
  },
  {
    "start": 2265774,
    "end": 2297500,
    "text": "これらのfは、仮に基底関数の集合だとしましょう。そして、おそらく1つ、ここに1つの制約があります。それは、本質的にデータに対する分布の仮定のようなもので、これらの特徴関数からカーネルを構成するとき、これはあるカーネル関数であるということです。"
  },
  {
    "start": 2298750,
    "end": 2311438,
    "text": "カーネル行列の固有値のスペクトルを見れば、データからこのべき乗則に従うことがわかるだろう。"
  },
  {
    "start": 2311604,
    "end": 2319940,
    "text": "これはある種のFには当てはまり、他のFには当てはまらない。"
  },
  {
    "start": 2321110,
    "end": 2325726,
    "text": "これはもしかしたら、背景を推測する助けになるのだろうか？"
  },
  {
    "start": 2325758,
    "end": 2341722,
    "text": "つまり、これがべき乗則である場合、これは一般的な性質のようなものだが、理論的な設定では、もう少し病的なバージョンを研究することもある。"
  },
  {
    "start": 2341776,
    "end": 2348234,
    "text": "例えば、これらの固有値縮退などには対称性があるかもしれない。"
  },
  {
    "start": 2348352,
    "end": 2352074,
    "text": "私たちが言いたいことのひとつは、これは一般的ではないということだ。"
  },
  {
    "start": 2352202,
    "end": 2358746,
    "text": "一般的に、この種の量には素晴らしいパワーロス・スペクトルがある。"
  },
  {
    "start": 2358938,
    "end": 2366980,
    "text": "次のスライドを想定して、漸近的に持つ指数を1＋αkとしました。"
  },
  {
    "start": 2369750,
    "end": 2370994,
    "text": "これについて何か質問は？"
  },
  {
    "start": 2371032,
    "end": 2380630,
    "text": "例えば、これは画像の解像度を変える実験です。"
  },
  {
    "start": 2381850,
    "end": 2395658,
    "text": "問題をコース分けすると、より単純になり、このカーネル指数、固有値、スペクトルが変化する。"
  },
  {
    "start": 2395824,
    "end": 2396982,
    "text": "さまざまなカーブとは？"
  },
  {
    "start": 2397046,
    "end": 2406026,
    "text": "それは固定されたアーキテクチャ、あるいは固定されたセットで、完全に接続されたネットワークだと思う。"
  },
  {
    "start": 2406058,
    "end": 2411978,
    "text": "ダウンサンプリングのようなもので、画像を平均化する。"
  },
  {
    "start": 2412074,
    "end": 2414880,
    "text": "問題の難易度を調整する。"
  },
  {
    "start": 2415970,
    "end": 2417706,
    "text": "そのスペクトルさえもだ。"
  },
  {
    "start": 2417748,
    "end": 2420690,
    "text": "そして、それはスペクトラムにも反映されている。"
  },
  {
    "start": 2421510,
    "end": 2441094,
    "text": "難しい問題ほど、プールの大きさが小さい画像、つまり粒子が粗い画像となり、解像度が高く、傾斜が急な画像となる。"
  },
  {
    "start": 2441222,
    "end": 2443500,
    "text": "浅はかな結果だ。"
  },
  {
    "start": 2444270,
    "end": 2450514,
    "text": "じゃあ、このセクションをちょっと進めてみようか。"
  },
  {
    "start": 2450582,
    "end": 2456378,
    "text": "データ上のオンサイトが与えられれば、それは実験によって動機づけられたものである。"
  },
  {
    "start": 2456394,
    "end": 2470546,
    "text": "これは、データのある側面を現実的にモデル化しようとしているもので、データのスケーリング指数はαkであり、ある種の一次関数であり、幅のスケーリングもαkであることを導き出すことができる。"
  },
  {
    "start": 2470648,
    "end": 2476580,
    "text": "したがって、この設定では、指数αDとαwは実際には同じであることがわかる。"
  },
  {
    "start": 2478310,
    "end": 2481362,
    "text": "彼らはただ、現れた1つの量によってコントロールされているようなものだ。"
  },
  {
    "start": 2481426,
    "end": 2482546,
    "text": "私たちはこれを二重性と呼んでいる。"
  },
  {
    "start": 2482578,
    "end": 2485910,
    "text": "このことについては、いくつかのスライドで実験してみることにしよう。"
  },
  {
    "start": 2486970,
    "end": 2501294,
    "text": "このように解像度を制限したレジームを設定することで、このスケーリングが、データとモデルの間に存在するパワーロットに由来するものであることを、少なくともよりよく理解することができる。"
  },
  {
    "start": 2501412,
    "end": 2509178,
    "text": "データ単体やモデル単体の話ではなく、登場するこの1つの量だけの話なのだ。"
  },
  {
    "start": 2509274,
    "end": 2509946,
    "text": "誤字がある。"
  },
  {
    "start": 2509978,
    "end": 2515140,
    "text": "アルファDが、アルファDがデータに入っていると思う。"
  },
  {
    "start": 2519510,
    "end": 2522740,
    "text": "アルファDはアルファKだと言っているのだ。"
  },
  {
    "start": 2523590,
    "end": 2524946,
    "text": "ああ、そういうことですか？"
  },
  {
    "start": 2525048,
    "end": 2526262,
    "text": "そう、それが結果だ。"
  },
  {
    "start": 2526316,
    "end": 2530680,
    "text": "αw、この指数はαkにも等しい。"
  },
  {
    "start": 2532810,
    "end": 2538422,
    "text": "ラムダIとは何か、これは生徒や教師についての仮定である。"
  },
  {
    "start": 2538566,
    "end": 2540630,
    "text": "同じ機能を使っている。"
  },
  {
    "start": 2540710,
    "end": 2550422,
    "text": "生徒は投影のようなもので、より小さな特徴の集合である可能性があるが、特徴の集合全体はこのような形をしている。"
  },
  {
    "start": 2550576,
    "end": 2552320,
    "text": "このベースはf."
  },
  {
    "start": 2554770,
    "end": 2559882,
    "text": "では、時間の都合上、他のセクションは省略させてもらおう。"
  },
  {
    "start": 2559946,
    "end": 2564150,
    "text": "それが、私たちが「決議制限体制」と呼んでいたものだ。"
  },
  {
    "start": 2564250,
    "end": 2571390,
    "text": "また、ボトルネックになっていない変数のスケーリングを研究する、分散制限レジームも研究した。"
  },
  {
    "start": 2571550,
    "end": 2592074,
    "text": "有限サイズの共分散行列が極限に近づく方法を見てみると、一次展開のようなものは指数1を持つもので、それが損失にどのように影響するかまで追っていくと、指数1、整数指数1になる。"
  },
  {
    "start": 2592192,
    "end": 2599050,
    "text": "つまり、異なる体制では、異なるものに支配されるということだ。"
  },
  {
    "start": 2599120,
    "end": 2601274,
    "text": "ひとつはデータやモデルに大きく依存する。"
  },
  {
    "start": 2601322,
    "end": 2609934,
    "text": "もうひとつは、基本的にnの1乗、nの2乗の1乗、といった具合にコントロールする。"
  },
  {
    "start": 2609972,
    "end": 2611550,
    "text": "限界についての拡大。"
  },
  {
    "start": 2613010,
    "end": 2615266,
    "text": "このスライドも省略させてください。"
  },
  {
    "start": 2615288,
    "end": 2624750,
    "text": "これは、生徒の教師モデルのカーネルを見るという、この特定の種類のセットアップの正気度チェックのようなものだ。"
  },
  {
    "start": 2624830,
    "end": 2631074,
    "text": "冒頭で示した実験の特殊なケースなので、これ以上は触れない。"
  },
  {
    "start": 2631122,
    "end": 2644486,
    "text": "そして、この生徒指導の場では、これらの指数がアルファkに直接関係し、両者が等しいことを発見した。"
  },
  {
    "start": 2644678,
    "end": 2648166,
    "text": "では、より一般的な設定を。"
  },
  {
    "start": 2648198,
    "end": 2650786,
    "text": "一般化できる2つの側面がある。"
  },
  {
    "start": 2650838,
    "end": 2660254,
    "text": "A、生徒の先生という設定にとどまらず、データを生成する一般的な機能を持つこともできる。"
  },
  {
    "start": 2660372,
    "end": 2665202,
    "text": "もうひとつは、線形モデルを超えて、特徴学習のこの重要な側面を含めることだ。"
  },
  {
    "start": 2665256,
    "end": 2667460,
    "text": "それについては最後にコメントする。"
  },
  {
    "start": 2669430,
    "end": 2678514,
    "text": "つまり、この2つの体制について言えば、分散限定体制は、詳細は省くが、より一般的な条件下で示すことができるものだ。"
  },
  {
    "start": 2678562,
    "end": 2680742,
    "text": "この指数は1に等しい。"
  },
  {
    "start": 2680796,
    "end": 2688522,
    "text": "特徴学習とは基本的に無関係なので、カーネルの限界に依存することはない。"
  },
  {
    "start": 2688576,
    "end": 2694890,
    "text": "それは、前述したように、素敵な極限に関するこれらの一次的な用語に由来する。"
  },
  {
    "start": 2697070,
    "end": 2705706,
    "text": "このことを示すために、私たちが頼りにしているのは、無限幅のネットワークに対する有限幅の補正である。"
  },
  {
    "start": 2705738,
    "end": 2707322,
    "text": "それも理解している。"
  },
  {
    "start": 2707466,
    "end": 2712570,
    "text": "これは、解決策を限定した体制にとって、より強固な結果である。"
  },
  {
    "start": 2712650,
    "end": 2714446,
    "text": "ニューラルネットワークの場合。"
  },
  {
    "start": 2714478,
    "end": 2723010,
    "text": "特徴学習がある野生では、予測変数の一般的な形がないので、単なる線形モデルにはならない。"
  },
  {
    "start": 2723160,
    "end": 2729910,
    "text": "その代わりに、実験で検証できる仮説に頼ろうとしている。"
  },
  {
    "start": 2730970,
    "end": 2744954,
    "text": "その話をする前に、かなり現実的というか、よく出てくる設定なんだけど、この結果が持ち越されることがあるんだ。"
  },
  {
    "start": 2744992,
    "end": 2748170,
    "text": "それこそ、事前のトレーニングや微調整が必要なんだ。"
  },
  {
    "start": 2748590,
    "end": 2761662,
    "text": "例えば、imagenetで学習した埋め込みをCFaR tenで微調整したとしよう。"
  },
  {
    "start": 2761796,
    "end": 2767066,
    "text": "これは我々の理論から外れた実験のようなものだ。"
  },
  {
    "start": 2767178,
    "end": 2772270,
    "text": "正しいラベルを生成する教師はいないし、学習された埋め込みも使う。"
  },
  {
    "start": 2772930,
    "end": 2781458,
    "text": "今話した指数に関する結果は、基本的にロバストであることがわかった。"
  },
  {
    "start": 2781554,
    "end": 2786760,
    "text": "ここではもちろん、これらのアルファは1と等しく、1に近い。"
  },
  {
    "start": 2787210,
    "end": 2795260,
    "text": "驚くべきことは、このレジームでは、データセットのスケーリングやモデルサイズのスケーリングを見ても、指数が同じだということだ。"
  },
  {
    "start": 2796190,
    "end": 2802350,
    "text": "だから、私が言ったこの二元性は、このトレーニング前の微調整のセットアップでは保持する必要はなかった。"
  },
  {
    "start": 2803330,
    "end": 2814394,
    "text": "我々が実際にコントロールできる体制ではないが、ここにあるように、アルファPとアルファDはかなり近いことがわかる。"
  },
  {
    "start": 2814532,
    "end": 2824414,
    "text": "データセットのサイズを変えると、この曲線の片側になる。"
  },
  {
    "start": 2824462,
    "end": 2829830,
    "text": "だからこそ、データセットのサイズがボトルネックとなるレジームなのだ。"
  },
  {
    "start": 2830170,
    "end": 2843286,
    "text": "ある時点で、問題が最適に正則化されていない場合、二重降下が起こり、その後、指数が分散限定スケーリングによって制御されるこの領域に渡る。"
  },
  {
    "start": 2843318,
    "end": 2850426,
    "text": "私たちは二重降下曲線の2つの異なる側面を描写しているのだ。"
  },
  {
    "start": 2850608,
    "end": 2853678,
    "text": "ここで歴史的な疑問があるんだ。"
  },
  {
    "start": 2853764,
    "end": 2864142,
    "text": "私の理解が正しければ、これは、計算量が一定であれば、nとdを常に同じ割合でスケーリングすべきだということを意味している。"
  },
  {
    "start": 2864196,
    "end": 2865200,
    "text": "そうなのか？"
  },
  {
    "start": 2867830,
    "end": 2870366,
    "text": "最適なスケーリングを考えているか？"
  },
  {
    "start": 2870478,
    "end": 2879574,
    "text": "ええ、この論文はチンチラ論文のかなり前に発表されたもので、OpenAIはこの論文の共同執筆者でしたが、彼らはまだ間違ったスケーリング則を使っていました。"
  },
  {
    "start": 2879612,
    "end": 2883106,
    "text": "私はただ、この3つの事実がどう整合性があるのかを理解しようとしているだけだ。"
  },
  {
    "start": 2883218,
    "end": 2887960,
    "text": "チンチラ法に近いものがある。"
  },
  {
    "start": 2888410,
    "end": 2894646,
    "text": "私たちはスケーリング法を定めているわけではないので、そのスケーリング法について断定的なことは言えないと思う。"
  },
  {
    "start": 2894758,
    "end": 2899766,
    "text": "我々の結果は、ジョイント・スケーリングではなく、一度に1つの変数のスケーリングを行った場合のものである。"
  },
  {
    "start": 2899878,
    "end": 2911690,
    "text": "ということは、DNPをスケールアップする最適な方法は何なのかを解くためには、dコンマpの完全形lの両方にアクセスする必要があるのではないだろうか？"
  },
  {
    "start": 2911850,
    "end": 2917490,
    "text": "この設定で指数が同じなのは興味深い。"
  },
  {
    "start": 2921430,
    "end": 2922740,
    "text": "質問をありがとう。"
  },
  {
    "start": 2923190,
    "end": 2927140,
    "text": "続けさせてくれ。"
  },
  {
    "start": 2928010,
    "end": 2930706,
    "text": "だから、このような設定になっているのは意外だ。"
  },
  {
    "start": 2930738,
    "end": 2932802,
    "text": "申し上げたように、それは理論外のことだ。"
  },
  {
    "start": 2932946,
    "end": 2936434,
    "text": "我々は学習したモデルを持っており、本物の埋め込みを持っている。"
  },
  {
    "start": 2936482,
    "end": 2945290,
    "text": "これらは教師が生成したものではないが、これらの指数と解像度に制限のあるレジームとの間の二重性を観察することはできる。"
  },
  {
    "start": 2945950,
    "end": 2947018,
    "text": "質問がある。"
  },
  {
    "start": 2947104,
    "end": 2952270,
    "text": "つまり、生徒指導モデルでは、無限大の極限で損失はゼロになる。"
  },
  {
    "start": 2952340,
    "end": 2952960,
    "text": "その通りだ。"
  },
  {
    "start": 2953810,
    "end": 2958414,
    "text": "あなたが示している現実のケースでは、損失がゼロになることはない。"
  },
  {
    "start": 2958532,
    "end": 2962866,
    "text": "分散があるから、いつかは底を打つでしょう？"
  },
  {
    "start": 2962888,
    "end": 2963860,
    "text": "そうなのか？"
  },
  {
    "start": 2965110,
    "end": 2983350,
    "text": "これらの損失のうち、損失がゼロになるのか、それともある時点で底を打つのか。"
  },
  {
    "start": 2984490,
    "end": 3000890,
    "text": "例えば、この分散限定レジームのスケーリングを調べると、プロットの1つで漸近値を引いているのがわかると思います。"
  },
  {
    "start": 3000970,
    "end": 3002960,
    "text": "それだと限界がある。"
  },
  {
    "start": 3006610,
    "end": 3009840,
    "text": "私には5分か10分しかない。"
  },
  {
    "start": 3010690,
    "end": 3019358,
    "text": "ええ、質問の時間を残すために、省略させてください。"
  },
  {
    "start": 3019444,
    "end": 3026814,
    "text": "これらの結果のいくつかを一般化しようとする方法については、オフラインでもっと話すことができる。"
  },
  {
    "start": 3026862,
    "end": 3041690,
    "text": "そして、モデルが線形モデルとして振る舞わない場合、これらの指数をデータ多様体の次元の数値的尺度に関連付けようとする。"
  },
  {
    "start": 3042030,
    "end": 3046646,
    "text": "私たちはその方向でいくつかのことを行った。"
  },
  {
    "start": 3046838,
    "end": 3053198,
    "text": "最後に、私たちがやりたかった実験に話を戻そう。"
  },
  {
    "start": 3053284,
    "end": 3058750,
    "text": "私たちは、このアルファが1に等しい指数がどこから来るのかを実際に説明した。"
  },
  {
    "start": 3058900,
    "end": 3067794,
    "text": "解像度に制限のある設定では、我々はある程度コントロールすることができ、なぜこれらの指数がそのような値をとるのかをある程度理解することができる。"
  },
  {
    "start": 3067832,
    "end": 3074210,
    "text": "αwとαdの間には何らかの関係があり、それは例えばプレトレーニングや微調整で観察することができる。"
  },
  {
    "start": 3078490,
    "end": 3082306,
    "text": "たぶん、もう少し実験を飛ばすだろう。"
  },
  {
    "start": 3082338,
    "end": 3087922,
    "text": "だから、経験的にタスクを変えようとする実験を見てきた。"
  },
  {
    "start": 3087986,
    "end": 3107914,
    "text": "例えば、コースの階層やラベルが細かい場合にタスクをスーパークラス化すると、指数がどのように変化するかというと、例えば指数は不変のまま、プロットが上下にずれるだけです。"
  },
  {
    "start": 3107962,
    "end": 3115222,
    "text": "おそらくひとつの仮説は、ニューラルネットワークは主に入力データ多様体をモデル化し、分類タスクはあまりモデル化しないというものだ。"
  },
  {
    "start": 3115386,
    "end": 3128738,
    "text": "また、例えばモデルの幅を変えるなど、アーキテクチャーを変えることに注目すると、例えば深さを変えることに比べて、指数に大きな変化が見られる。"
  },
  {
    "start": 3128834,
    "end": 3132754,
    "text": "このようなことは理解できる。"
  },
  {
    "start": 3132802,
    "end": 3139818,
    "text": "これは、私が説明したカーネルの限界に近づいているモデルにつながっている。"
  },
  {
    "start": 3139984,
    "end": 3144810,
    "text": "特徴学習があれば、よりサンプル効率やパラメータ効率が良くなる。"
  },
  {
    "start": 3146270,
    "end": 3148902,
    "text": "では、ディープラーニング理論に欠けているものは何だろう？"
  },
  {
    "start": 3148966,
    "end": 3162206,
    "text": "私たちのツールを使ってアナリティクスができる理由の多くは、この問題を解決できるからだと申し上げましたが、それはある限度では正確なのですが、この側面が欠けているのです。"
  },
  {
    "start": 3162228,
    "end": 3166660,
    "text": "kは初期値から変化する力学的変数であることを述べた。"
  },
  {
    "start": 3167270,
    "end": 3181862,
    "text": "特徴学習がある場合、この問題に完全に一般的に取り組むのは難しいし、非常に一般的な方法で一般化を研究するのが扱いやすいかどうかもわからない。"
  },
  {
    "start": 3181996,
    "end": 3199162,
    "text": "聴衆の中に物理学のバックグラウンドを持つ人が何人かいるので、物理学の言葉で言っておくと、このカーネル極限は非相互作用問題や正確に解ける問題のようなもので、ガウス場の理論や線形問題に関連している。"
  },
  {
    "start": 3199216,
    "end": 3204270,
    "text": "これらはすべて私たちが解決できることであり、他の限界は強い相互作用がある。"
  },
  {
    "start": 3208210,
    "end": 3214882,
    "text": "人々は物理学のテクニックを使って、機能学習があるかもしれない他の種類の限界を研究しようとしてきた。"
  },
  {
    "start": 3214936,
    "end": 3229634,
    "text": "相関物理学のテクニックでは、ある種の分割関数を定義し、これを解けるように自己整合的な平均場の理論を持とうとするが、完全な一般性で解くのは難しい。"
  },
  {
    "start": 3229682,
    "end": 3238422,
    "text": "そのため、特定のアーキテクチャや特定のタイプのデータに対して近似値を作成した場合、それを他の問題にどのように一般化するかは明確ではないかもしれない。"
  },
  {
    "start": 3238556,
    "end": 3244806,
    "text": "このシンプルなセッティングを研究することで、私たちが得られるものがあることを、少しでも納得していただけたなら幸いだ。"
  },
  {
    "start": 3244918,
    "end": 3249190,
    "text": "普遍的な行動がある体制は理解できる。"
  },
  {
    "start": 3249350,
    "end": 3257358,
    "text": "プレトレーニングや微調整に影響する二元性の側面を理解することはできるが、すべてを把握することはできない。"
  },
  {
    "start": 3257444,
    "end": 3260222,
    "text": "例えば、特徴学習を完全に理解する。"
  },
  {
    "start": 3260276,
    "end": 3266962,
    "text": "これは特徴学習の効果をプロットしたもので、最適か否かの問題に戻る。"
  },
  {
    "start": 3267016,
    "end": 3276718,
    "text": "このプロットは、これらのカーネルのひとつと、有限サイズのニューラルネットワークを訓練したものとの性能差のようなものを示している。"
  },
  {
    "start": 3276894,
    "end": 3278854,
    "text": "青いカーブと緑のカーブだ。"
  },
  {
    "start": 3278972,
    "end": 3298540,
    "text": "ある種のデータセットサイズ、重要なデータセットサイズがあり、そこではニューラルネットワークの性能はカーネルよりも優れているが、それは同等のカーネルであり、非常に小さなトレーニングデータセット、トレーニングデータサイズの領域では、カーネルの方が優れていることがある。"
  },
  {
    "start": 3298910,
    "end": 3304160,
    "text": "一方が他方を追い越すようなクロスオーバーもある。"
  },
  {
    "start": 3304930,
    "end": 3313490,
    "text": "さて、このスライドは要約のようなもので、冒頭で質問したことをもう一度確認するためのものです。"
  },
  {
    "start": 3313830,
    "end": 3326114,
    "text": "おそらく、異なるスケーリング体制は異なる原動力に支配されている、あるいは異なる要因によって推進されている、と考えていただければと思う。"
  },
  {
    "start": 3326162,
    "end": 3339426,
    "text": "ですから、さまざまなスケーリング・レジームについての分類法ができますし、未解決の質問もたくさんあります。"
  },
  {
    "start": 3339548,
    "end": 3346540,
    "text": "本当は、エマージェンスと相転移について話したかったんだけど、もうちょっとで終わりそうなんだ。"
  },
  {
    "start": 3348350,
    "end": 3350700,
    "text": "質問を受け付けよう"
  },
  {
    "start": 3351470,
    "end": 3355550,
    "text": "いや、たくさん質問した。"
  },
  {
    "start": 3355620,
    "end": 3357134,
    "text": "オーケー、どうぞ。"
  },
  {
    "start": 3357172,
    "end": 3362560,
    "text": "申し訳ないが、この件についてもっと話したい。"
  },
  {
    "start": 3365990,
    "end": 3373422,
    "text": "まあ、今僕がカバーしていることの多くは、直接的にllmsを研究しているわけではなかったんだ。"
  },
  {
    "start": 3373486,
    "end": 3376930,
    "text": "もっと第一原理的な理論やシンプルなセットアップをやろうとしていた。"
  },
  {
    "start": 3377010,
    "end": 3388280,
    "text": "昨日の会話に関連した議論に軸足を移したかったのですが、それと同時に、昨日の話にも関連させたいと思います。"
  },
  {
    "start": 3389130,
    "end": 3396970,
    "text": "私の専門は物性物理学なので、そういう観点から物事を見るのが好きなんだ。"
  },
  {
    "start": 3397870,
    "end": 3400720,
    "text": "それについて少し申し上げたい。"
  },
  {
    "start": 3401250,
    "end": 3401710,
    "text": "オーケー。"
  },
  {
    "start": 3401780,
    "end": 3413280,
    "text": "分配の一般化から外れているわけではなく、分配の問題のようなものです。"
  },
  {
    "start": 3413890,
    "end": 3420226,
    "text": "LLMSの時代には、能力だけでなく、さまざまな指標や精度も重視されるようになった。"
  },
  {
    "start": 3420248,
    "end": 3423650,
    "text": "例えば、私は問題の正確さについては話していない。"
  },
  {
    "start": 3423800,
    "end": 3436310,
    "text": "スケーリング変数の関数としての損失は非常に素晴らしい改善が見られるかもしれないが、精度の関数としてのパフォーマンスを研究すると、かなりシャープになるかもしれない。"
  },
  {
    "start": 3437930,
    "end": 3445702,
    "text": "私はただ、さまざまな機関にまたがるこの大きな取り組みから得られた結果のいくつかを強調したかっただけだ。"
  },
  {
    "start": 3445766,
    "end": 3447062,
    "text": "あれはビッグベンチと呼ばれた。"
  },
  {
    "start": 3447126,
    "end": 3449622,
    "text": "ビッグベンチについてどれだけの人が知っているだろうか？"
  },
  {
    "start": 3449766,
    "end": 3450830,
    "text": "オーケー、多くの人たちだ。"
  },
  {
    "start": 3450900,
    "end": 3462502,
    "text": "その方法について、いくつかのスライドをお見せしたいと思います。"
  },
  {
    "start": 3462586,
    "end": 3473710,
    "text": "これは可用性に関する主張を、より科学的に測定可能な量、つまり評価できるものに変えるための大きな努力のようなものだと思う。"
  },
  {
    "start": 3473870,
    "end": 3481574,
    "text": "どうすればスケーラブルな方法で、興味深く多様なタスクを生み出すことができるかという興味深い問題を提起していると思う。"
  },
  {
    "start": 3481692,
    "end": 3492662,
    "text": "例えば、これらのタスクの多くは、個人によってプログラム的に生成される。"
  },
  {
    "start": 3492726,
    "end": 3503834,
    "text": "だから、このようなことをテストしたいのであれば、画像などに人間が注釈をつけるのはごく自然なことかもしれない。"
  },
  {
    "start": 3503882,
    "end": 3520686,
    "text": "例えば、もっと複雑な方法で科学的な推論をテストしたいのであれば、高校の教科書を見て、教科書から質問の答えをキュレートすることでできるだろう。"
  },
  {
    "start": 3520798,
    "end": 3521778,
    "text": "難しいよ。"
  },
  {
    "start": 3521864,
    "end": 3529990,
    "text": "手作業でなければ、より深い科学的推論に迫る多様なベンチマークを作成するのは難しい。"
  },
  {
    "start": 3530060,
    "end": 3536440,
    "text": "そう考えると、ちょっと興味深いことが浮かび上がってくると思う。"
  },
  {
    "start": 3537210,
    "end": 3546774,
    "text": "この論文は、タスクが異なる行動をとりうるという事実を浮き彫りにした。"
  },
  {
    "start": 3546822,
    "end": 3550986,
    "text": "あるものは、X軸上の何かの関数として直線的に振る舞うかもしれない。"
  },
  {
    "start": 3551018,
    "end": 3562590,
    "text": "これらのうちいくつかはモデルのサイズかもしれないし、いくつかは引用でブレークスルーを起こすかもしれない。"
  },
  {
    "start": 3564870,
    "end": 3588150,
    "text": "これは別の論文のものだが、X軸のスケーリングの関数として、さまざまなタスクに対する創発的な振る舞いと呼ばれるものを示している。"
  },
  {
    "start": 3589790,
    "end": 3600118,
    "text": "ベンチで発表された大きな論文に書かれていることだが、シェーファーらの論文ではさらに深く研究されている。"
  },
  {
    "start": 3600224,
    "end": 3605630,
    "text": "このような創発的な能力は、単に指標を変えることに依存しているようなものだ。"
  },
  {
    "start": 3606530,
    "end": 3619010,
    "text": "文字列の完全一致を得るような、非線形または不連続な非常に鋭いもので測定する場合、メトリックを変えるとかなり鋭くなることがある。"
  },
  {
    "start": 3619430,
    "end": 3632034,
    "text": "彼らは、bigbenchで観察されたこの種の鋭敏な挙動の多くは、単にメトリックを変えれば低減できることを発見した。"
  },
  {
    "start": 3632162,
    "end": 3639734,
    "text": "どのような場合に実際にシャープな挙動を示すのか、非分析性の原因は何なのか、興味深い問題を提起していると思う。"
  },
  {
    "start": 3639862,
    "end": 3644140,
    "text": "それについて少しコメントしたかったんだ。"
  },
  {
    "start": 3645070,
    "end": 3654474,
    "text": "だから、物理学で出てくるような概念と自然につながっている、2種類の概念があると思う。"
  },
  {
    "start": 3654522,
    "end": 3660750,
    "text": "この文献では、例えば相転移との関係について主張されていることがある。"
  },
  {
    "start": 3661170,
    "end": 3679110,
    "text": "むしろ、非公式なレベルではあるが、物理学で観測される非解析性の起源は、熱力学的な限界に対処するためであるという事実について、その理由と少し触れておきたかったのだ。"
  },
  {
    "start": 3679180,
    "end": 3690170,
    "text": "例えば、物理学や統計力学では、解析関数の有限和である分割関数がある。"
  },
  {
    "start": 3690910,
    "end": 3701062,
    "text": "この熱力学的極限では、システムのサイズ、粒子の数、状態の数が無限大になる。"
  },
  {
    "start": 3701126,
    "end": 3709326,
    "text": "まず、移行期のチューニングのようなものを研究する前に、例えば、臨界温度のデータを通してチューニングを行う。"
  },
  {
    "start": 3709508,
    "end": 3714154,
    "text": "それが、この種のトランジションにおける非分析性の原因なのだ。"
  },
  {
    "start": 3714202,
    "end": 3719834,
    "text": "数値計算や実験でそれを慎重に確立するには、通常、有限サイズのスケーリングを行う。"
  },
  {
    "start": 3719882,
    "end": 3730710,
    "text": "システムの大きさが変わると、観測される変数、測定される変数がどのようにシャープになっていくかを研究するのだ。"
  },
  {
    "start": 3731370,
    "end": 3747878,
    "text": "このような観測をより厳密なものにするためには、そのような形で、非分析性が本当に存在するのであれば、その起源を特定するようなことが必要かもしれない。"
  },
  {
    "start": 3747974,
    "end": 3750838,
    "text": "メートル法のせいかもしれない。"
  },
  {
    "start": 3751014,
    "end": 3757802,
    "text": "恐らく、それはメートル法のせいではなく、確かに非分析的な変数が他にもあるのだろう。"
  },
  {
    "start": 3757866,
    "end": 3766050,
    "text": "正しい極限を取ることで、非分析的な挙動を得ることができるからだ。"
  },
  {
    "start": 3766390,
    "end": 3780520,
    "text": "それはおそらく、鋭さについてのコメントで、実際に鋭さのような挙動が見られるのか、それとも単に実験では観測されない、あるいは適切な量を測定していないだけで、漸進的な挙動が見られるのかということだ。"
  },
  {
    "start": 3782410,
    "end": 3792460,
    "text": "最後に、この講演で私が取ったアプローチと、皆さんが取ろうとするかもしれない他のアプローチとを対比させて、終わりにしたいと思います。"
  },
  {
    "start": 3793070,
    "end": 3835522,
    "text": "最近、LLMの文献には、物性物理学の精神を定義するようなこの決定的な論文や、さまざまな科学にわたる創発の考え方がたくさん紹介されていますが、どのレベルでも、スケールが上がるにつれて、どの段階でも、質的に新しい振る舞いが生じる可能性があります。"
  },
  {
    "start": 3835586,
    "end": 3845580,
    "text": "例えば、固体物理学や多体物理学は素粒子物理学から、化学は多体物理学から、といった具合だ。"
  },
  {
    "start": 3847950,
    "end": 3850554,
    "text": "これについては、後で他の登壇者が話すと思う。"
  },
  {
    "start": 3850672,
    "end": 3857950,
    "text": "創発の概念として適切なもの、有用なもの、洞察に満ちた形式化されたものは何なのか、考えるのは興味深い時期だ。"
  },
  {
    "start": 3858610,
    "end": 3873262,
    "text": "この講演で私がとったアプローチは、私たちが知っている理論を使って、物事について第一原理で考えようとすることと、いくつかの段階で、ギャップを埋めるために帰納法を試みることの組み合わせだった。"
  },
  {
    "start": 3873326,
    "end": 3876914,
    "text": "答えを推測し、それを実験で試してみる。"
  },
  {
    "start": 3877042,
    "end": 3890722,
    "text": "今のロムスの時代には、異なる概念や理論的なプリミティブから始めて、有用な予測的なことを言う方が適切かもしれない。"
  },
  {
    "start": 3890866,
    "end": 3894040,
    "text": "だから、そこに行ってみようかな。"
  },
  {
    "start": 3907890,
    "end": 3912640,
    "text": "サーシャがセットアップしている間に、1つだけ燃えるような質問があればいいかもしれない。"
  },
  {
    "start": 3917090,
    "end": 3922780,
    "text": "ああ、だから、つまり、数年前にntksの仕事をしていて、あまりに忙しかったんだ。"
  }
]