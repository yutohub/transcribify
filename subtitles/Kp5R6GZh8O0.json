[
  {
    "start": 250,
    "end": 11920,
    "text": "トレーニング前のセットアップをひとつ、考えてみよう。"
  },
  {
    "start": 15490,
    "end": 35298,
    "text": "事前トレーニングのファインチューニングのセットアップで、カーネルについて考えること、ファインチューニングのセットアップで、カーネルについて考えること、学習済みカーネルをプラグインすること、固定カーネルについて何かを導き出すかもしれませんが、学習済みカーネルの特性をプラグインして、それが成立するかどうかを確認することを考えます。"
  },
  {
    "start": 35394,
    "end": 53786,
    "text": "しかし、例えば、これらの相関する次元の特徴学習について、第一原理から完全に導出することは難しいと思います。"
  },
  {
    "start": 53898,
    "end": 63482,
    "text": "物理学の観点からは、効果的な理論を持つことが目標のひとつだと思う。"
  },
  {
    "start": 63626,
    "end": 72494,
    "text": "理論的な説明の中で、重みの数、パラメーターの数、変数のセットをすべて持ちたいとは思わないだろう。"
  },
  {
    "start": 72542,
    "end": 82038,
    "text": "より高いレベルで何かを語りたい場合、通常は新しい変数セットを定義する。"
  },
  {
    "start": 82124,
    "end": 94330,
    "text": "データの構造に関する洞察と、特徴学習の限界を設定することで、基本的に連立方程式を解くことができる、より小さなセットを導き出すことができるだろう。"
  },
  {
    "start": 95310,
    "end": 101120,
    "text": "一般的に普遍的なことと、それに大きく左右されることを知るのは難しいことだと思う。"
  },
  {
    "start": 103330,
    "end": 105550,
    "text": "かなり挑戦的だと思う。"
  },
  {
    "start": 107490,
    "end": 109280,
    "text": "ヤスミンにもう一度感謝しよう。"
  },
  {
    "start": 115910,
    "end": 121634,
    "text": "続いて、コーネルからサーシャ・ラッシュがハグ顔で登場。"
  },
  {
    "start": 121832,
    "end": 135890,
    "text": "私がサーシャを本当に気に入っているのは、彼が科学者の好奇心の果てをブレンドしているようなところがある一方で、物事を本当にうまく動かすことができ、本当に大きなスケールのモデルをトレーニングしてきたことだ。"
  },
  {
    "start": 136050,
    "end": 139946,
    "text": "今日はどの組み合わせで話してくれるのか、実はわからないんだ。"
  },
  {
    "start": 140048,
    "end": 142378,
    "text": "いずれにせよ、本当に面白くなると思う。"
  },
  {
    "start": 142544,
    "end": 144330,
    "text": "サーシャを歓迎しよう。"
  },
  {
    "start": 147550,
    "end": 148326,
    "text": "ありがとう、ジェイコブ。"
  },
  {
    "start": 148358,
    "end": 153450,
    "text": "ああ、だから今日は、このワークショップの理論的な側面にも触れてみようと思う。"
  },
  {
    "start": 153610,
    "end": 163594,
    "text": "私は自然言語処理の研究者なので、欠点もあればプラス面もある。"
  },
  {
    "start": 163722,
    "end": 167342,
    "text": "神経のスケーリング法則という同じトピックに関わるつもりだ。"
  },
  {
    "start": 167406,
    "end": 177030,
    "text": "同じネタをたくさん使うつもりだが、幸運にもほとんど同じクリップアートを使ったので、この構成でとてもうまくいくだろう。"
  },
  {
    "start": 178010,
    "end": 182210,
    "text": "今日は主にニコラス・モイニングホフが行った仕事を紹介する。"
  },
  {
    "start": 182370,
    "end": 190546,
    "text": "これは、Hugging Faithや他の多くの人たちとの共同研究であり、データ制約付き言語モデルのスケーリングと呼ばれている。"
  },
  {
    "start": 190658,
    "end": 192554,
    "text": "1カ月ほど前にアーカイブにアップしたばかりだ。"
  },
  {
    "start": 192592,
    "end": 194138,
    "text": "比較的最近の作品だ。"
  },
  {
    "start": 194224,
    "end": 198780,
    "text": "コメントや質問があればぜひ聞かせてほしい。"
  },
  {
    "start": 199470,
    "end": 199882,
    "text": "クールだ。"
  },
  {
    "start": 199936,
    "end": 202250,
    "text": "ログのスケーリングについて話そう。"
  },
  {
    "start": 202830,
    "end": 206462,
    "text": "もう一度言うが、私はより現実的な観点からこのことを考えている。"
  },
  {
    "start": 206516,
    "end": 216820,
    "text": "私は、大規模な言語モデルを構築する際に、どのような現実的な疑問があるのか、そしてスケーリング法則がそのような疑問に対する答えにどのように役立つのかに興味があります。"
  },
  {
    "start": 217190,
    "end": 220690,
    "text": "具体的には、この講演では3つの用語を使う。"
  },
  {
    "start": 220840,
    "end": 224958,
    "text": "モデル・サイズ、トレーニング・データ、そしてトレーニング・コンピュートだ。"
  },
  {
    "start": 225054,
    "end": 228802,
    "text": "私は大規模な言語モデルのトレーニングに集中するつもりだ。"
  },
  {
    "start": 228866,
    "end": 232150,
    "text": "というのが、この講演の主な内容だ。"
  },
  {
    "start": 232300,
    "end": 237170,
    "text": "トランスフォーマーである以上、そこには係数が存在する。"
  },
  {
    "start": 237250,
    "end": 242060,
    "text": "この話は無視してもいいんだけど、実際には6つくらいあるんだ。"
  },
  {
    "start": 242670,
    "end": 247690,
    "text": "モデルサイズが与えられれば、トレーニングデータが与えられれば、トレーニング計算量が得られる。"
  },
  {
    "start": 248990,
    "end": 258026,
    "text": "自然言語処理におけるさまざまなモデルは、これらの係数をさまざまな方法で変化させており、その結果、トレーニングの計算量も異なっている。"
  },
  {
    "start": 258138,
    "end": 268610,
    "text": "モデル・サイズはパラメータ数で、トレーニング・データはトークン数で、トレーニング計算量は浮動小数点演算で測定する。"
  },
  {
    "start": 268950,
    "end": 286802,
    "text": "2018年のBertのようなモデルは、NLPの観点からは絶対に革命的だったが、約109,000,000のパラメータ、250,000,000,000のトレーニングトークンを持ち、20回目の計算フロップに1.6eを要した。"
  },
  {
    "start": 286866,
    "end": 287480,
    "text": "申し訳ない。"
  },
  {
    "start": 288010,
    "end": 293260,
    "text": "ということは、その期間、約4日間で約64Tpusということになる。"
  },
  {
    "start": 294030,
    "end": 300010,
    "text": "それ以来、モデルのパラメータサイズは約5億4,000万になった。"
  },
  {
    "start": 300990,
    "end": 306190,
    "text": "トークンは数兆ドルになり、フロップはもっともっと大きくなった。"
  },
  {
    "start": 306610,
    "end": 310350,
    "text": "各社がこの数字を報告しなくなったのはこの頃からだ。"
  },
  {
    "start": 310500,
    "end": 318210,
    "text": "現在の状況を説明することは可能だが、これは私が自信を持ってスライドに載せることができる、最後の厳しい数字だ。"
  },
  {
    "start": 318950,
    "end": 323038,
    "text": "つまり、一般的には、これはこれらの値の関係を表しているに過ぎない。"
  },
  {
    "start": 323134,
    "end": 329750,
    "text": "モデルサイズとトークンの間には一種の乗法関係があり、フロップの数が多くなる。"
  },
  {
    "start": 330970,
    "end": 336486,
    "text": "このシナリオを考えると、2つのハイレベルな質問がある。"
  },
  {
    "start": 336668,
    "end": 343900,
    "text": "例えば、あなたがアンソロピックの重役で、大きなモデルをトレーニングする必要があるとする。"
  },
  {
    "start": 344270,
    "end": 349546,
    "text": "より多くのフロップを使うことの価値は何なのか？"
  },
  {
    "start": 349728,
    "end": 355038,
    "text": "メダルの枚数を増やした場合、実際にどれだけのリターンがあるのだろうか？"
  },
  {
    "start": 355124,
    "end": 356560,
    "text": "その価値はあるのか？"
  },
  {
    "start": 357330,
    "end": 360350,
    "text": "もうひとつは、配分の問題だ。"
  },
  {
    "start": 360690,
    "end": 363118,
    "text": "前回の話とは少し違う。"
  },
  {
    "start": 363284,
    "end": 370878,
    "text": "これは、トークンを増やすこととパラメーターの数を増やすことの間で、どのようなバランスが望ましいかを調べるものだ。"
  },
  {
    "start": 371054,
    "end": 374782,
    "text": "この2つを増やすことで、より良いロットが得られることは分かっている。"
  },
  {
    "start": 374926,
    "end": 379698,
    "text": "この比率を決めることは、実際にはかなり重要だ。"
  },
  {
    "start": 379794,
    "end": 383190,
    "text": "それを間違えると、無駄な失敗を招くことになりかねない。"
  },
  {
    "start": 384970,
    "end": 388442,
    "text": "これらは前回の講演で見たようなグラフだ。"
  },
  {
    "start": 388576,
    "end": 403440,
    "text": "これらは、システムの他の特性によってボトルネックにされることなく、モデルサイズ、トレーニングデータ、および計算量の両方を増加させると、このべき乗則の関係になることを示す、スケーリング則の特性の一種である。"
  },
  {
    "start": 403970,
    "end": 413150,
    "text": "この軸はどちらも対数目盛りで、基礎的な損失がほぼ直線的に出力されるため、べき乗則であることがわかる。"
  },
  {
    "start": 413570,
    "end": 419938,
    "text": "講演を通して、私はテストロスを大規模な言語モデルがどの程度優れているかの代用として使うことにする。"
  },
  {
    "start": 420104,
    "end": 425730,
    "text": "講演の最後には、これらのモデルの実際の精度についてもう少し触れたい。"
  },
  {
    "start": 425890,
    "end": 433378,
    "text": "ここでの主な考え方は、より多くの計算量を投入することが助けになるということであり、この2つの量に役立つということだ。"
  },
  {
    "start": 433474,
    "end": 443258,
    "text": "モデルサイズとトレーニングデータの割り当てについて話すとき、私はほとんどこの論文の用語とグラフを利用するつもりだ。"
  },
  {
    "start": 443344,
    "end": 458830,
    "text": "チンチラ（ChincHilLa Chinchilla）は、パラメーターを増やすこととトークンを増やすことをどのように配分すべきかを予測しようとする、大規模な経験的実験セットから推定された具体的な詳細を提供している。"
  },
  {
    "start": 459170,
    "end": 463210,
    "text": "これが、この講演で使うグラフの主な種類である。"
  },
  {
    "start": 463380,
    "end": 467726,
    "text": "X軸はトレーニングデータとトークン数。"
  },
  {
    "start": 467918,
    "end": 474174,
    "text": "Y軸はモデルサイズ、ラベルは計算量を表す。"
  },
  {
    "start": 474222,
    "end": 478082,
    "text": "これら4つのモデルを表している。"
  },
  {
    "start": 478146,
    "end": 484790,
    "text": "ここに示したのは、4つの異なるモデルと、彼らが今回の割り当てを決定した内容である。"
  },
  {
    "start": 485530,
    "end": 506494,
    "text": "チンチラ論文は、これらのモデルが利用するフロップ数の配分の選択を誤っていると主張し、代わりにこれらのフロップを利用し、モデルサイズと学習データのバランスを等しくすれば、彼らが計算最適言語モデルと呼ぶものが得られることを示した。"
  },
  {
    "start": 506692,
    "end": 516142,
    "text": "フロップのコストを考慮し、パラメーターの数を少なく、トークンの数を多くすることにしました。"
  },
  {
    "start": 516286,
    "end": 528470,
    "text": "ここでは、1兆トークンを超えるモデルが登場し、このようなシステムにますます多くのトークンを使用する将来のモデルにつながることになった。"
  },
  {
    "start": 529930,
    "end": 533494,
    "text": "これは、今回の講演のインスピレーションそのものだ。"
  },
  {
    "start": 533692,
    "end": 540090,
    "text": "このグラフは、私たちがこの問題をどのように考え、どのような質問に答えたいと思っているかを表しています。"
  },
  {
    "start": 541230,
    "end": 544010,
    "text": "これを数式で書くことができる。"
  },
  {
    "start": 545070,
    "end": 554000,
    "text": "この式は、モデルサイズとトレーニングデータに基づいて達成される損失の見積もりを与えてくれる。"
  },
  {
    "start": 554450,
    "end": 565570,
    "text": "他のハイパーパラメーターは固定されているか最適化されていると仮定して、この2つの量を変化させたときに損失がどうなるかを推定することに興味がある。"
  },
  {
    "start": 566150,
    "end": 573534,
    "text": "トーク中、nはモデルの大きさを表し、dはトークンの数を表す。"
  },
  {
    "start": 573662,
    "end": 578760,
    "text": "これにより、次のべき乗則を表す関数形が得られる。"
  },
  {
    "start": 579130,
    "end": 593050,
    "text": "モデルの大きさ、n個のデータ、d、そして経験的実験に当てはめる5つの項を考慮に入れて、lを推定することに興味がある。"
  },
  {
    "start": 593950,
    "end": 600018,
    "text": "チンチラの論文では、この3つの用語が何を表しているのか、大まかな意味を示している。"
  },
  {
    "start": 600214,
    "end": 602506,
    "text": "eは下限のようなものだ。"
  },
  {
    "start": 602618,
    "end": 608138,
    "text": "それが言語の真の喪失であり、言語の真のエントロピーなのだ。"
  },
  {
    "start": 608314,
    "end": 610320,
    "text": "それ以上のことはできない。"
  },
  {
    "start": 611090,
    "end": 619620,
    "text": "最初の項は、明らかに完全にモデル・サイズそのものであり、学習できる可能性のあるモデルのタイプをおおよそ表している。"
  },
  {
    "start": 620310,
    "end": 629960,
    "text": "第2項は、トークンの数には限りがあり、そのため基礎となるシステムについて学べることには限界があることを表している。"
  },
  {
    "start": 630970,
    "end": 640140,
    "text": "そのように考えることもできるし、多くの実験から外挿し、実際にどのようにモデルをトレーニングすべきかを決めようと考えることもできる。"
  },
  {
    "start": 641310,
    "end": 651930,
    "text": "これらの定数を実際の実験に当てはめると、アルファはベータとほぼ等しくなる。"
  },
  {
    "start": 652670,
    "end": 653420,
    "text": "そうだね。"
  },
  {
    "start": 654590,
    "end": 656350,
    "text": "この相互運用性において、いくつかの違いがあった。"
  },
  {
    "start": 656420,
    "end": 659520,
    "text": "今はほぼ同じということですか？"
  },
  {
    "start": 661570,
    "end": 664400,
    "text": "これはチンチラ紙の説明だけですか？"
  },
  {
    "start": 666610,
    "end": 668154,
    "text": "多分、私はただそれを意味しているんだ。"
  },
  {
    "start": 668212,
    "end": 671070,
    "text": "深い意味ではなく、大雑把な意味でね。"
  },
  {
    "start": 671230,
    "end": 671810,
    "text": "その通りだ。"
  },
  {
    "start": 671880,
    "end": 672500,
    "text": "そうだね。"
  },
  {
    "start": 674890,
    "end": 678310,
    "text": "もしかしたら、これまで指摘されていた以上に平等なのかもしれない。"
  },
  {
    "start": 679290,
    "end": 680038,
    "text": "そうだね。"
  },
  {
    "start": 680204,
    "end": 690114,
    "text": "ここでいうeとは、言語そのもののエントロピーではなく、そのアーキテクチャを使って学習できることの限界ということなのだろうか？"
  },
  {
    "start": 690242,
    "end": 696134,
    "text": "アーキテクチャを変え、異なるタイプのネットワークを試せば、もっと低損失になる可能性がある。"
  },
  {
    "start": 696182,
    "end": 703266,
    "text": "資源が無限にある場合でも、ネットワークのエントロピーに達することはないのでしょうか？"
  },
  {
    "start": 703318,
    "end": 706270,
    "text": "私の理解では、最初の用語はそれを表している。"
  },
  {
    "start": 708130,
    "end": 716770,
    "text": "この最初の項は、あなたのモデルがそれを達成できないかもしれないという事実を表しているのに対し、最後の項は言語の真のエントロピーを表している。"
  },
  {
    "start": 717430,
    "end": 718862,
    "text": "いや、解釈だ。"
  },
  {
    "start": 718926,
    "end": 719902,
    "text": "それは解釈だ。"
  },
  {
    "start": 719966,
    "end": 721170,
    "text": "解釈だよ。"
  },
  {
    "start": 721590,
    "end": 722098,
    "text": "そうだね。"
  },
  {
    "start": 722184,
    "end": 727186,
    "text": "彼は、トランスフォーマーはスケーリングに関係なく、その域に達することはないだろうという指摘をしているのだ。"
  },
  {
    "start": 727298,
    "end": 729334,
    "text": "そうだね。"
  },
  {
    "start": 729532,
    "end": 730374,
    "text": "見てみよう。"
  },
  {
    "start": 730492,
    "end": 741750,
    "text": "論文のセクションCで彼らがどのようにこのことについて話しているか、私はほとんど復唱しているが、そう、実際には可能なのだ。"
  },
  {
    "start": 746010,
    "end": 746760,
    "text": "そうだね。"
  },
  {
    "start": 747070,
    "end": 753134,
    "text": "後で話すかもしれないが、実際のところ、データを拡大縮小するような仕事は後でいくつかある。"
  },
  {
    "start": 753252,
    "end": 753774,
    "text": "そうだね。"
  },
  {
    "start": 753892,
    "end": 761322,
    "text": "スケールモデル的な大きさの視点ではない。"
  },
  {
    "start": 761386,
    "end": 763262,
    "text": "それがあなたの見方だ。"
  },
  {
    "start": 763396,
    "end": 765154,
    "text": "ああ、実にいい指摘だ。"
  },
  {
    "start": 765352,
    "end": 782466,
    "text": "これは、この研究の趣旨とはやや直交するのだが、この研究から得られた最も重要な結論であることは間違いないと思う。"
  },
  {
    "start": 782578,
    "end": 786066,
    "text": "大雑把に言えば、このグラフの領域で終わるということだ。"
  },
  {
    "start": 786258,
    "end": 798070,
    "text": "彼らの主張は、実際にモデルを下流で使うときの推論時間の方が、実際に最適性を計算するよりも重要だというものだ。"
  },
  {
    "start": 798230,
    "end": 805390,
    "text": "したがって、下流の制約のために悪いモデルを手に入れ、悪い配分をする価値はある。"
  },
  {
    "start": 805730,
    "end": 815060,
    "text": "しかし、それはある種の異なる最適化問題を解決するための意図的な決断なのだ。"
  },
  {
    "start": 816230,
    "end": 818020,
    "text": "それについてはもう少し詳しく話そう。"
  },
  {
    "start": 821110,
    "end": 832200,
    "text": "規模を実感してもらうために言っておくと、ラマ2世は2兆パラメータだから、だいたいそのあたりだが、パラメータはそのあたりで700億しかない。"
  },
  {
    "start": 835770,
    "end": 857374,
    "text": "このような背景を踏まえて、ある種のユニークで直感に反する問題についてお話ししたいと思います。"
  },
  {
    "start": 857572,
    "end": 869300,
    "text": "これは、利用可能なトークンの量と、実際にデータがなくなる時期の予測をグラフ化した論文からの引用です。"
  },
  {
    "start": 869990,
    "end": 872178,
    "text": "ここには2本の線が描かれている。"
  },
  {
    "start": 872344,
    "end": 879560,
    "text": "一本の線は、人々が実際に作っているモデルに基づいて、スケーリングの見積もりをするようなものだ。"
  },
  {
    "start": 880010,
    "end": 889480,
    "text": "もう1行は、Nvidiaによるコンピュート増加だけに基づいて見積もったもので、一定の価格に対してより多くのフロップ数を生み出している。"
  },
  {
    "start": 890590,
    "end": 894170,
    "text": "このグラフには2つの興味深い点があると思う。"
  },
  {
    "start": 894670,
    "end": 899606,
    "text": "ひとつは、ここではまったくわからないが、非常に大きなばらつきがあることだ。"
  },
  {
    "start": 899718,
    "end": 906160,
    "text": "これがいつ実現するのか、実際にどれだけのデータがあるのか、正確には不明だ。"
  },
  {
    "start": 906690,
    "end": 914682,
    "text": "もうひとつ興味深いのは、このような規模を考えた場合、データ量はそれほど速いペースで増えているわけではないということだ。"
  },
  {
    "start": 914826,
    "end": 919874,
    "text": "ただトークンを生産し続ければ追いつくというものではない。"
  },
  {
    "start": 920072,
    "end": 922098,
    "text": "膨大なデータがある。"
  },
  {
    "start": 922184,
    "end": 933270,
    "text": "この法則を信じ、それに従うのであれば、より多くのデータを必要とするように、実際の計算の種類は倍増している。"
  },
  {
    "start": 934650,
    "end": 938730,
    "text": "これは少し抽象的なので、もう少し現実的な話をしよう。"
  },
  {
    "start": 939230,
    "end": 955950,
    "text": "オープンソースで入手可能な質の高い書籍を例にとると、約1兆トークンが存在する。"
  },
  {
    "start": 956690,
    "end": 957102,
    "text": "何だと？"
  },
  {
    "start": 957156,
    "end": 958714,
    "text": "ああ、1兆6000億だ。"
  },
  {
    "start": 958842,
    "end": 963550,
    "text": "学術論文をすべて合わせると、約1兆トークンになる。"
  },
  {
    "start": 963910,
    "end": 971810,
    "text": "ウィキペディアのような他の情報源は非常に質が高いが、実際には想像しているよりもずっと少ないトークンしかない。"
  },
  {
    "start": 972310,
    "end": 979720,
    "text": "だから、人々が到達しているスケールの観点からこれらを考えるなら、利用可能な桁はそれほど多くはない。"
  },
  {
    "start": 981210,
    "end": 991586,
    "text": "ええ、あなたのトピックとは必ずしも関係ないのですが、要約のためにより良くできています。"
  },
  {
    "start": 991698,
    "end": 992360,
    "text": "そうだね。"
  },
  {
    "start": 992670,
    "end": 995260,
    "text": "シンセティック・データはできることだ。"
  },
  {
    "start": 996030,
    "end": 998806,
    "text": "それは全体の分析に入るのか？"
  },
  {
    "start": 998998,
    "end": 1008638,
    "text": "ジェギンの話をもっと聞いてみたいが、私の理解では、合成データがこの種の問題を解決できるという証拠はどこにもない。"
  },
  {
    "start": 1008804,
    "end": 1013282,
    "text": "特定の仕事のために、特定の方法で物事を組み立てることができる。"
  },
  {
    "start": 1013416,
    "end": 1018660,
    "text": "私たちは確かに、桁違いの合成テキストを作る人を見たことがない。"
  },
  {
    "start": 1019190,
    "end": 1027454,
    "text": "私は、すでに優れたモデルから生成されたテキストが、より小さなモデルのトレーニングに効果的だと思われるのとは区別したい。"
  },
  {
    "start": 1027582,
    "end": 1033266,
    "text": "世代によって桁が増えるようなことは見たことがない。"
  },
  {
    "start": 1033458,
    "end": 1036840,
    "text": "もしイェジンがその場にいたら、同意したかどうか聞いてみたい。"
  },
  {
    "start": 1038590,
    "end": 1044518,
    "text": "ええ、私はタスクに特化したシナリオで成功したケースしか見たことがありません。"
  },
  {
    "start": 1044694,
    "end": 1051382,
    "text": "多くの計算能力を持つ他のクレイジーな人々が何を追求できるかは誰にもわからない。"
  },
  {
    "start": 1051446,
    "end": 1054350,
    "text": "ああ、それはまだわからないと思う。"
  },
  {
    "start": 1054420,
    "end": 1054894,
    "text": "それは素晴らしいことだ。"
  },
  {
    "start": 1054932,
    "end": 1056394,
    "text": "腕にタトゥーを入れるつもりだ。"
  },
  {
    "start": 1056442,
    "end": 1059390,
    "text": "計算能力の高いクレイジーな人たちが何をするかは誰にもわからない。"
  },
  {
    "start": 1059460,
    "end": 1073842,
    "text": "そう、だから、私はデータ制約のあるものに対して少し反論したかったのかもしれない。"
  },
  {
    "start": 1073896,
    "end": 1087670,
    "text": "基本的に、推論時にデフォルトのモデルよりも賢くなるようなことができれば、合成データのソースになる。"
  },
  {
    "start": 1087740,
    "end": 1092362,
    "text": "これは、合成データのかなり準備が整ったソースになるはずだ。"
  },
  {
    "start": 1092416,
    "end": 1097302,
    "text": "実証的な研究では、この方法がモデルのパフォーマンスを向上させるという結果も出ている。"
  },
  {
    "start": 1097366,
    "end": 1100502,
    "text": "ええ、では実際に、この会話をしましょう。"
  },
  {
    "start": 1100566,
    "end": 1100826,
    "text": "私はそう思う。"
  },
  {
    "start": 1100848,
    "end": 1101882,
    "text": "時間全部使うつもりはない。"
  },
  {
    "start": 1101936,
    "end": 1103580,
    "text": "このことについてもっと話したい。"
  },
  {
    "start": 1104590,
    "end": 1108718,
    "text": "今回の話は、この特定のことに関して比較的実証的なものになると思う。"
  },
  {
    "start": 1108804,
    "end": 1116382,
    "text": "しかし、私の制約の中でも、私たちが何を学ぶことができるかを主張したい。"
  },
  {
    "start": 1116436,
    "end": 1119282,
    "text": "ああ、基本的にはこのページに書いてある。"
  },
  {
    "start": 1119336,
    "end": 1130466,
    "text": "というのも、ここで話しているデータはすべてオンラインデータであり、インターネットにアクセスできるのは特定の人口だけだということがわかっているからだ。"
  },
  {
    "start": 1130578,
    "end": 1135746,
    "text": "私たちが言っているのは、オンラインデータが制限されているということだ。"
  },
  {
    "start": 1135858,
    "end": 1152038,
    "text": "次のスライドは、現実の世界や、インターネットにアクセスできない多くの人々、また他のモダリティについて考えてみましょう。"
  },
  {
    "start": 1152134,
    "end": 1153350,
    "text": "先に進もう。"
  },
  {
    "start": 1153440,
    "end": 1154442,
    "text": "この会話をしよう。"
  },
  {
    "start": 1154506,
    "end": 1157040,
    "text": "それについて話す時間は最後まで取っておくつもりだ。"
  },
  {
    "start": 1157490,
    "end": 1162318,
    "text": "物理学の漸近的な考え方のようなものに関わってみたい。"
  },
  {
    "start": 1162404,
    "end": 1168706,
    "text": "スケーリングウォールを購入することを前提に、このアイデアを試してみたい。"
  },
  {
    "start": 1168808,
    "end": 1170450,
    "text": "オーケー、クールだ。"
  },
  {
    "start": 1170600,
    "end": 1173394,
    "text": "他に質問は？"
  },
  {
    "start": 1173432,
    "end": 1174260,
    "text": "まず最初に？"
  },
  {
    "start": 1176230,
    "end": 1176738,
    "text": "素晴らしい。"
  },
  {
    "start": 1176824,
    "end": 1192650,
    "text": "GitHubには約14兆のトークンがあり、そのコードが実際に使えるという証拠がたくさんあります。"
  },
  {
    "start": 1193550,
    "end": 1195098,
    "text": "それはいいことだ。"
  },
  {
    "start": 1195264,
    "end": 1201898,
    "text": "悪い点は、特定の言語用のモデルを訓練したい場合、事態がもっと悪くなることだ。"
  },
  {
    "start": 1202074,
    "end": 1213146,
    "text": "このプロジェクトは、研究者たちがフィンランド語特有の言語モデルをトレーニングしていたところ、約380億のトークンを集めることができたことから始まった。"
  },
  {
    "start": 1213258,
    "end": 1216020,
    "text": "今は数桁少ない。"
  },
  {
    "start": 1216710,
    "end": 1222606,
    "text": "なぜすべての言語について巨大な言語モデルを作る必要があるのか、という反論があるかもしれない。"
  },
  {
    "start": 1222718,
    "end": 1225394,
    "text": "フィンランドの人たちはそれを望んでいると思う。"
  },
  {
    "start": 1225432,
    "end": 1226238,
    "text": "とても重要なことだ。"
  },
  {
    "start": 1226344,
    "end": 1231800,
    "text": "だから、それに賛成するかどうかは別として、みんなが考えていることだと思う。"
  },
  {
    "start": 1232330,
    "end": 1234166,
    "text": "オーケー、素晴らしい。"
  },
  {
    "start": 1234268,
    "end": 1237062,
    "text": "この質問は最後にしよう。"
  },
  {
    "start": 1237116,
    "end": 1238780,
    "text": "私は彼らに戻りたい。"
  },
  {
    "start": 1239310,
    "end": 1240060,
    "text": "クールだ。"
  },
  {
    "start": 1244910,
    "end": 1248746,
    "text": "今日のお話は、とてもシンプルなアイデアについてです。"
  },
  {
    "start": 1248928,
    "end": 1252190,
    "text": "エポックについて話したい。"
  },
  {
    "start": 1253330,
    "end": 1256510,
    "text": "これは機械学習では非常に基本的な考え方のようだ。"
  },
  {
    "start": 1256660,
    "end": 1261920,
    "text": "多くの場合、同じデータで何度もモデルを訓練するだけだ。"
  },
  {
    "start": 1262630,
    "end": 1271170,
    "text": "しかし、これは大規模な言語モデルトレーニングの文献では、ある種の悪口になっているようだ。"
  },
  {
    "start": 1271910,
    "end": 1280760,
    "text": "GP threeの論文を読むと、トレーニング中にデータを置換なしでサンプリングしているとはっきり書いてある。"
  },
  {
    "start": 1281770,
    "end": 1293180,
    "text": "今、彼らはウィキペディアのような特定の情報源を繰り返しているが、何を繰り返すかについて非常に明確な選択をしており、必要がないときはそれを避けている。"
  },
  {
    "start": 1294110,
    "end": 1305930,
    "text": "パーム論文では、3つのモデルすべてを正確に1エポックのデータで訓練し、どの分科会でもデータの繰り返しを避けるために混合部分を選択するとしている。"
  },
  {
    "start": 1306530,
    "end": 1315780,
    "text": "さて、この種の論文は、データを何度も繰り返すと何かが本当におかしくなると人々に考えさせたと思う。"
  },
  {
    "start": 1316390,
    "end": 1321634,
    "text": "ある意味、データが不足しているのであれば、それは当然のことのように思える。"
  },
  {
    "start": 1321832,
    "end": 1332150,
    "text": "この論文では、データを繰り返し、この体制下でのスケーリング則の経験的観察を行った場合に何が起こるかを探りたかった。"
  },
  {
    "start": 1337050,
    "end": 1340442,
    "text": "ああ、でもチンチラ紙では1本のFは使っていなかった。"
  },
  {
    "start": 1340576,
    "end": 1343322,
    "text": "彼らは決してデータを繰り返さない。"
  },
  {
    "start": 1343376,
    "end": 1343706,
    "text": "なるほど。"
  },
  {
    "start": 1343728,
    "end": 1346140,
    "text": "彼らは違っていた。"
  },
  {
    "start": 1346910,
    "end": 1347660,
    "text": "そうだね。"
  },
  {
    "start": 1350050,
    "end": 1354970,
    "text": "オーケー、では次のようにしよう。"
  },
  {
    "start": 1355050,
    "end": 1360106,
    "text": "我々は、非常に大規模でコントロールされた実験を行うつもりだ。"
  },
  {
    "start": 1360298,
    "end": 1368590,
    "text": "私たちは大型スーパーコンピューターを利用することができ、さまざまな大規模言語モデルを実行することができる。"
  },
  {
    "start": 1368750,
    "end": 1376870,
    "text": "本稿では、大規模言語モデルの学習を300回行い、以下の設定で実行した。"
  },
  {
    "start": 1377210,
    "end": 1385110,
    "text": "モデルサイズを28億パラメータから87億パラメータまで変化させる。"
  },
  {
    "start": 1385930,
    "end": 1395046,
    "text": "ここでのトレーニングデータは、これらのモデルサイズに最適なチンチラのトークン数を表している。"
  },
  {
    "start": 1395238,
    "end": 1398230,
    "text": "左側には計算量が表示されている。"
  },
  {
    "start": 1398390,
    "end": 1403086,
    "text": "しかし、私たちが見ているような規模にはなっていない。"
  },
  {
    "start": 1403188,
    "end": 1405760,
    "text": "今のところ、巨大な言語モデルのようなものだ。"
  },
  {
    "start": 1407410,
    "end": 1415358,
    "text": "それぞれについて、8つの異なるモデルをトレーニングし、繰り返されるデータ量を変えていく。"
  },
  {
    "start": 1415534,
    "end": 1422850,
    "text": "基本的には、データをサブサンプリングし、同じトークンに対して複数のエポックを実行する。"
  },
  {
    "start": 1424470,
    "end": 1436360,
    "text": "その他の詳細はすべて論文に記載されているが、注目すべき重要な点は、これらはすべて標準的なアーキテクチャを使用しており、基本的にチンチラ論文から採用された標準的なハイパーパラメータを使用しているということだ。"
  },
  {
    "start": 1436810,
    "end": 1445754,
    "text": "今回は、ウェブクロールデータの比較的クリーンなデータセットであるc fourデータセットでの実験を主に紹介する。"
  },
  {
    "start": 1445952,
    "end": 1449850,
    "text": "また、他のデータセットを使った実験も論文に掲載している。"
  },
  {
    "start": 1451810,
    "end": 1461242,
    "text": "データ設定の感覚をつかんでもらうために言っておくと、私がエポックと言ったのは、同じ量のユニーク・トークンで複数のエポックを意味するのではない。"
  },
  {
    "start": 1461386,
    "end": 1464526,
    "text": "つまり、フロップを固定しておくということだ。"
  },
  {
    "start": 1464718,
    "end": 1470878,
    "text": "複数のエポックで計算するようにデータを分割するには？"
  },
  {
    "start": 1471054,
    "end": 1482440,
    "text": "7エポックというのは、1エポックと同じフロップ数で、ユニークデータ量を早めに切り上げて、それを繰り返すという意味だ。"
  },
  {
    "start": 1483530,
    "end": 1495500,
    "text": "これは混乱しやすい点なので、私がやっていることをはっきりさせておきたいのですが、これらのモデルはすべてまったく同じフロップ数で実行されますが、ユニークデータの量は変えています。"
  },
  {
    "start": 1497950,
    "end": 1499770,
    "text": "これが結論である。"
  },
  {
    "start": 1500370,
    "end": 1510670,
    "text": "これらの列のそれぞれでパラメータを変えて3回実行しても、すべて同じフロップ数である。"
  },
  {
    "start": 1512390,
    "end": 1522130,
    "text": "1つのエポックに異なるパラメータ数で訓練する場合、つまりすべてのユニークなデータで訓練する場合、次のような損失曲線が得られます。"
  },
  {
    "start": 1522470,
    "end": 1530920,
    "text": "左側に検証の損失、下側にトレーニング・トークンの数、そして最終地点に最終的な損失がある。"
  },
  {
    "start": 1531450,
    "end": 1547174,
    "text": "チンチラ計算で予想されるように、最適モデルが28億のパラメーターから87億のパラメーターになると、精度は比較的大きく向上し、損失は減少する。"
  },
  {
    "start": 1547302,
    "end": 1550300,
    "text": "2.6から2.4になる。"
  },
  {
    "start": 1551810,
    "end": 1557018,
    "text": "今、同じサイズのモデルを2つのエポックで走らせている。"
  },
  {
    "start": 1557194,
    "end": 1562590,
    "text": "つまり、ユニーク・トークンの数は半分しかなかったが、2回使い切ったことになる。"
  },
  {
    "start": 1563430,
    "end": 1565970,
    "text": "非常によく似たカーブを描いている。"
  },
  {
    "start": 1568150,
    "end": 1573170,
    "text": "今は3エポックと4エポックをやっている。"
  },
  {
    "start": 1574150,
    "end": 1579810,
    "text": "4エポックの時点で、私はユニークトークンの25％しか持っていなかった。"
  },
  {
    "start": 1579970,
    "end": 1585190,
    "text": "同じ量のフロップを使えば、ほぼ同じロスが出る。"
  },
  {
    "start": 1586730,
    "end": 1588310,
    "text": "損失額は同じ。"
  },
  {
    "start": 1589070,
    "end": 1594220,
    "text": "7時くらいになると、ここに顕著なギャップが生まれるんだ。"
  },
  {
    "start": 1595150,
    "end": 1600250,
    "text": "14歳くらいになると、もっと早くから負けが尾を引くようになるんだ。"
  },
  {
    "start": 1601650,
    "end": 1603982,
    "text": "44歳まで続けている。"
  },
  {
    "start": 1604116,
    "end": 1610560,
    "text": "不安定になったり、オーバーフィッティングになったり、レジームから外れてしまったりするんだ。"
  },
  {
    "start": 1613350,
    "end": 1618610,
    "text": "主な結論は、少ないリピート回数で同じようなパフォーマンスが得られるということだ。"
  },
  {
    "start": 1619830,
    "end": 1622930,
    "text": "スコープを外れるとパフォーマンスが落ちる。"
  },
  {
    "start": 1623590,
    "end": 1624340,
    "text": "そうだね。"
  },
  {
    "start": 1624790,
    "end": 1632018,
    "text": "例えば、2つ、3つの叙事詩を上演する場合、効果的なデータ量という点で、これを定量化する方法はありますか？"
  },
  {
    "start": 1632114,
    "end": 1641702,
    "text": "目で見るのはちょっと難しいんですが、3つのエピックの場合、550億の代わりに420億のトークンを持っているように見えるかもしれません。"
  },
  {
    "start": 1641766,
    "end": 1644554,
    "text": "ジェイコブ、これはいいアイデアだ。"
  },
  {
    "start": 1644592,
    "end": 1645770,
    "text": "スライドを2枚くれ。"
  },
  {
    "start": 1646990,
    "end": 1649018,
    "text": "ええ、私たちが何を目指しているかは理解してもらえたと思います。"
  },
  {
    "start": 1649104,
    "end": 1649740,
    "text": "そうだね。"
  },
  {
    "start": 1650110,
    "end": 1662270,
    "text": "もうひとつの質問は、以前、他の情報源でいくつか見たことがあるのですが、列車対検証という点で、何か一致するものはありますか？"
  },
  {
    "start": 1662950,
    "end": 1664702,
    "text": "そう、これは検証の損失なんだ。"
  },
  {
    "start": 1664766,
    "end": 1666946,
    "text": "間違いなくこれが原因だと思う。"
  },
  {
    "start": 1666968,
    "end": 1668914,
    "text": "トレーニングとのギャップ"
  },
  {
    "start": 1669032,
    "end": 1670180,
    "text": "ああ、いい質問だね。"
  },
  {
    "start": 1673110,
    "end": 1677358,
    "text": "普通、エポックは1つだから、実に興味深い。"
  },
  {
    "start": 1677374,
    "end": 1681910,
    "text": "私は言語モデルについてそのような違いは考えていないが、あなたの言う通り、それは重要なはずだ。"
  },
  {
    "start": 1682060,
    "end": 1687094,
    "text": "その数字が新聞に載っているかどうかは覚えていないが、もっと重要な数字なのだから、それを載せるべきだった。"
  },
  {
    "start": 1687132,
    "end": 1687430,
    "text": "ここだよ。"
  },
  {
    "start": 1687500,
    "end": 1689190,
    "text": "直接は知らない。"
  },
  {
    "start": 1689610,
    "end": 1695690,
    "text": "つまり、44エポックを見ているのだから、列車のデータ、列車のロスは本当に少ないと思うだろう。"
  },
  {
    "start": 1695760,
    "end": 1696380,
    "text": "そうだね。"
  },
  {
    "start": 1696910,
    "end": 1703182,
    "text": "もうひとつ関連した質問として、下流の評価について、もし彼らがそのような態度を取るようなら、それをチェックしたのか？"
  },
  {
    "start": 1703236,
    "end": 1705440,
    "text": "そう、彼らはあなたが期待するような振る舞いをする。"
  },
  {
    "start": 1706770,
    "end": 1707278,
    "text": "そうだね。"
  },
  {
    "start": 1707364,
    "end": 1714930,
    "text": "この44のエピックの結果を見ると、このような典型的なオーバーフィッティング曲線が見られる。"
  },
  {
    "start": 1717350,
    "end": 1728366,
    "text": "というのも、より多くのデータがある場合、同じように壊滅的なオーバーフィッティングが起こるかどうかはわからないからだ。"
  },
  {
    "start": 1728478,
    "end": 1735286,
    "text": "このカーブを何度も何度も繰り返せば、大きなデータセットでも同じようにオーバーフィッティングが起こると思いますか？"
  },
  {
    "start": 1735308,
    "end": 1741742,
    "text": "あるいは、最終的にデータのクリティカルマス（臨界量）が得られれば、異なるモデルサイトでもある程度安定するようになるのでしょうか？"
  },
  {
    "start": 1741826,
    "end": 1744646,
    "text": "ただ、はっきりさせておきたいのは、私たちは本当に慎重でありたいということだ。"
  },
  {
    "start": 1744758,
    "end": 1749770,
    "text": "44の方がユニークなデータは少ない。"
  },
  {
    "start": 1749840,
    "end": 1750218,
    "text": "そうだね。"
  },
  {
    "start": 1750304,
    "end": 1753814,
    "text": "トークンの数は増えたが、ユニークなトークンは減った。"
  },
  {
    "start": 1753942,
    "end": 1757774,
    "text": "これからいくつかの数字を挙げて、質問に対する答えにしたいと思う。"
  },
  {
    "start": 1757892,
    "end": 1765282,
    "text": "この仕事では、オーバーフィッティングのようなケースはあまり心配していない。"
  },
  {
    "start": 1765336,
    "end": 1767646,
    "text": "ほぼ同じハイパーパラメーターを使っただけだ。"
  },
  {
    "start": 1767758,
    "end": 1774580,
    "text": "私が本当に興味があるのは、この体制と、この空間で何が起きているかを定量化することだ。"
  },
  {
    "start": 1775430,
    "end": 1776180,
    "text": "クールだ。"
  },
  {
    "start": 1776710,
    "end": 1780422,
    "text": "この現象は別のデータセットを保持している。"
  },
  {
    "start": 1780476,
    "end": 1782406,
    "text": "C4に限ったことではない。"
  },
  {
    "start": 1782588,
    "end": 1784470,
    "text": "同じような曲線を描いている。"
  },
  {
    "start": 1784810,
    "end": 1788774,
    "text": "さて、繰り返しになるが、これが深い思想のようなものだということを言いたいのではない。"
  },
  {
    "start": 1788892,
    "end": 1789366,
    "text": "そうだね。"
  },
  {
    "start": 1789468,
    "end": 1796762,
    "text": "ここで見ているのは、標準的な機械学習システムのトレーニングから想像されるようなものだ。"
  },
  {
    "start": 1796896,
    "end": 1801680,
    "text": "私たちは大規模な言語モデルでこれを検証し、定量化しようとしているところです。"
  },
  {
    "start": 1803090,
    "end": 1803790,
    "text": "クールだ。"
  },
  {
    "start": 1803940,
    "end": 1804734,
    "text": "オーケー。"
  },
  {
    "start": 1804932,
    "end": 1815730,
    "text": "ジェイコブの質問に答えて、次の自然な考えは、繰り返しの際に有効なデータ量をどのように定量化するかということだと思う。"
  },
  {
    "start": 1816230,
    "end": 1831094,
    "text": "というのも、フロップの割り当てを決めるとき、一意なデータが無限にある場合はどうすればいいかわかるが、使えるデータ量が限られている場合はどうするか決めなければならないからだ。"
  },
  {
    "start": 1831292,
    "end": 1838310,
    "text": "ここでは比較的単純なモデルを使って、指数関数的に減衰するデータについて考えてみる。"
  },
  {
    "start": 1838650,
    "end": 1846186,
    "text": "ここで例えるなら、半減期のようなもので、データを使うたびに指数関数的に減衰していく。"
  },
  {
    "start": 1846368,
    "end": 1853360,
    "text": "私たちは、繰り返し使用されたデータの減衰に対応するいくつかの用語デルタを持っています。"
  },
  {
    "start": 1854290,
    "end": 1862430,
    "text": "だから、これをもう少し具体的にするために、dプライムと呼ぶ新しい用語を設けるというモデルだ。"
  },
  {
    "start": 1862850,
    "end": 1876310,
    "text": "dプライムは、リピートで実際に利用している有効データ量を表しており、実際に収集できたデータであるユニークデータuとは異なることになる。"
  },
  {
    "start": 1877130,
    "end": 1883320,
    "text": "データそのものの繰り返し回数を表すrdという項も用意する。"
  },
  {
    "start": 1884330,
    "end": 1893080,
    "text": "つまり、もう少し具体的に言うと、もしデルタが1に等しければ、この問題を心配する必要はまったくないということになる。"
  },
  {
    "start": 1893690,
    "end": 1894362,
    "text": "ああ、申し訳ない。"
  },
  {
    "start": 1894416,
    "end": 1895014,
    "text": "その反対だ。"
  },
  {
    "start": 1895062,
    "end": 1898730,
    "text": "もしデルタが1に等しければ、データを繰り返すことはまったくできない。"
  },
  {
    "start": 1898800,
    "end": 1902380,
    "text": "最初の使用でその価値をすべて使ってしまう。"
  },
  {
    "start": 1902930,
    "end": 1911360,
    "text": "もしデルタがゼロに等しければ、永遠にデータを繰り返すことができ、事実上、まったく問題のない完璧な世界だ。"
  },
  {
    "start": 1911810,
    "end": 1917780,
    "text": "デルタが0.5であれば、繰り返されたデータはその値の50％を保持する。"
  },
  {
    "start": 1918950,
    "end": 1923458,
    "text": "物事を単純化するために、複利の形で考えることにする。"
  },
  {
    "start": 1923624,
    "end": 1925518,
    "text": "方程式を書き出すと"
  },
  {
    "start": 1925614,
    "end": 1941318,
    "text": "デルタを測定する代わりに、r star dと呼ぶ項を設け、壊滅的な故障が発生するまでにデータを繰り返すことができる回数に対応させる。"
  },
  {
    "start": 1941414,
    "end": 1943660,
    "text": "基本的に、それは非常に悪くなる。"
  },
  {
    "start": 1944030,
    "end": 1949180,
    "text": "もしr starがゼロに等しいなら、それは繰り返されたデータに何の価値もない場合である。"
  },
  {
    "start": 1949630,
    "end": 1954000,
    "text": "rスターが無限大に等しい場合、繰り返されたデータは新しいものと同じである。"
  },
  {
    "start": 1954610,
    "end": 1965380,
    "text": "前の実験で見たように、言語モデリングデータのr starは44より小さい。"
  },
  {
    "start": 1966470,
    "end": 1967220,
    "text": "素晴らしい。"
  },
  {
    "start": 1967910,
    "end": 1971678,
    "text": "では、これらの用語を実際にどのように当てはめるのか、少し見てみよう。"
  },
  {
    "start": 1971774,
    "end": 1972818,
    "text": "ああ、頑張れ。"
  },
  {
    "start": 1972904,
    "end": 1974702,
    "text": "関数式に戻れるか？"
  },
  {
    "start": 1974766,
    "end": 1975090,
    "text": "そうだね。"
  },
  {
    "start": 1975160,
    "end": 1983554,
    "text": "待って、もしr star dが無限大なら、RDが有限でも無限のデータが得られるということではないのか？"
  },
  {
    "start": 1983682,
    "end": 1984070,
    "text": "あるいは"
  },
  {
    "start": 1984140,
    "end": 1985960,
    "text": "たぶん、そうではないと思う。"
  },
  {
    "start": 1986330,
    "end": 1987094,
    "text": "そう、その通りだ。"
  },
  {
    "start": 1987132,
    "end": 1993162,
    "text": "r star dは、デルタがゼロに等しい場合とほぼ同じだと思う。"
  },
  {
    "start": 1993296,
    "end": 1995866,
    "text": "rdは2rdのようなものだ。"
  },
  {
    "start": 1995968,
    "end": 1998102,
    "text": "データは2回しか繰り返していない。"
  },
  {
    "start": 1998246,
    "end": 2003630,
    "text": "それなら、Dプライムは最高のケースでもUの2倍くらいにしかならないような気がする。"
  },
  {
    "start": 2003700,
    "end": 2006400,
    "text": "これはUの無限倍のようなものだと言っているのだ。"
  },
  {
    "start": 2006850,
    "end": 2007840,
    "text": "なるほど。"
  },
  {
    "start": 2009250,
    "end": 2010814,
    "text": "その方がいいのかなと思っているんだ。"
  },
  {
    "start": 2010852,
    "end": 2014270,
    "text": "たぶん、最初のRの星はRDにすべきだろう。"
  },
  {
    "start": 2017410,
    "end": 2018570,
    "text": "いい質問だ。"
  },
  {
    "start": 2018740,
    "end": 2023540,
    "text": "その場では無理だと思うけど、覗き込んでタイプミスとかがないか確認するよ。"
  },
  {
    "start": 2024710,
    "end": 2025554,
    "text": "そうだね。"
  },
  {
    "start": 2025752,
    "end": 2030054,
    "text": "データを繰り返すからといって、学習率を変えることはあるのだろうか？"
  },
  {
    "start": 2030172,
    "end": 2036262,
    "text": "例えば、同じデータを2回繰り返すが、学習率は半分にする。"
  },
  {
    "start": 2036316,
    "end": 2041978,
    "text": "2倍の学習率でデータを1回変更した場合とまったく同じ結果が得られるのですか？"
  },
  {
    "start": 2042064,
    "end": 2043098,
    "text": "ああ、いい質問だね。"
  },
  {
    "start": 2043184,
    "end": 2051310,
    "text": "私たちはほとんど、チンチラの種類やチンチラの配合をアレンジしたものにこだわっています。"
  },
  {
    "start": 2051460,
    "end": 2060158,
    "text": "チンチラの学習率は少し複雑なので、その仕組みの詳細は重要です。"
  },
  {
    "start": 2060324,
    "end": 2066820,
    "text": "この規模では、さまざまな学習率やそのようなものをあまり試すことができなかった。"
  },
  {
    "start": 2067190,
    "end": 2075034,
    "text": "繰り返しデータを扱う場合、モデルをトレーニングするための最適なハイパーパラメータが、通常とは変わってくる可能性がある。"
  },
  {
    "start": 2075182,
    "end": 2078280,
    "text": "その可能性はあると思う。"
  },
  {
    "start": 2082650,
    "end": 2089900,
    "text": "さて、このスライドで最終的なリターンの感覚がつかめると思う。"
  },
  {
    "start": 2090910,
    "end": 2097078,
    "text": "ここでやることは、前回の実験の最終点をプロットすることだ。"
  },
  {
    "start": 2097254,
    "end": 2102574,
    "text": "これは、以前見たのとまったく同じデータで、ただ違う形で表示されているだけだ。"
  },
  {
    "start": 2102772,
    "end": 2110750,
    "text": "これらのポイントは、それぞれ異なるバージョンのモデルを表し、異なる割合のユニークトークンでトレーニングされている。"
  },
  {
    "start": 2111810,
    "end": 2120622,
    "text": "この線は、パラメーターとフロップ数だけで、すべてが同じであるような完璧な世界を表している。"
  },
  {
    "start": 2120766,
    "end": 2124770,
    "text": "上部の線は一種の強い崩壊を表している。"
  },
  {
    "start": 2125270,
    "end": 2129586,
    "text": "ここに示した各ポイントは、私たちの実証実験を表している。"
  },
  {
    "start": 2129778,
    "end": 2137110,
    "text": "この線は、前のスライドでお見せした崩壊方程式のもとで予測されたものです。"
  },
  {
    "start": 2138670,
    "end": 2141466,
    "text": "こちらは大型モデル用。"
  },
  {
    "start": 2141568,
    "end": 2148490,
    "text": "これが42億のパラメータで、これが87億のパラメータだ。"
  },
  {
    "start": 2148990,
    "end": 2157120,
    "text": "これら3つすべてについて、データそのものの指数関数的減衰のようなものを使うことで、比較的うまくフィットすることがわかる。"
  },
  {
    "start": 2158290,
    "end": 2163650,
    "text": "また、実際に異なるエポック数の予測損失を見ることもできる。"
  },
  {
    "start": 2164230,
    "end": 2176390,
    "text": "点線は、28億のパラメータを持つモデルで、ユニークなデータを使って新しいトークンを取得した場合に期待される値です。"
  },
  {
    "start": 2176970,
    "end": 2185590,
    "text": "予測線は、1000億トークン前後から同じデータを繰り返した場合に予想されるものです。"
  },
  {
    "start": 2185930,
    "end": 2203440,
    "text": "エポック数が増えるにつれて、リピートの限界値は相対的に低くなる。"
  },
  {
    "start": 2204850,
    "end": 2214720,
    "text": "これが42億ドルと87億ドルの曲線である。"
  },
  {
    "start": 2215830,
    "end": 2216580,
    "text": "素晴らしい。"
  },
  {
    "start": 2217830,
    "end": 2223380,
    "text": "それでは、チンチラのフォーミュラそのものが何を意味するのか、簡単に説明しよう。"
  },
  {
    "start": 2223830,
    "end": 2230498,
    "text": "この時点で、前のスライドにあった線を予測するという点で、私たちが何をしようとしているか、おそらく想像がつくでしょう。"
  },
  {
    "start": 2230674,
    "end": 2241830,
    "text": "我々はdを取り、その代わりに、リピートを使った減衰の下で起こると思われる有効なデータ量dプライムを入れるつもりだ。"
  },
  {
    "start": 2241990,
    "end": 2249850,
    "text": "前のスライドにあったd primeの公式を代入すれば、方程式のこの部分が得られる。"
  },
  {
    "start": 2250770,
    "end": 2261854,
    "text": "また、パラメータがあまり有効でないデータから学習されているという事実を考慮するために、nを代入する必要があることもわかった。"
  },
  {
    "start": 2262052,
    "end": 2266420,
    "text": "パラメータ自体にも同様の指数関数的減衰が見られる。"
  },
  {
    "start": 2267030,
    "end": 2272638,
    "text": "ここでは、ユニークな数のパラメーターから得られる値を表すためにminを持っている。"
  },
  {
    "start": 2272814,
    "end": 2275910,
    "text": "申し訳ないが、これらのパラメータを学習するユニークなトークンだ。"
  },
  {
    "start": 2276250,
    "end": 2277000,
    "text": "そうだね。"
  },
  {
    "start": 2277370,
    "end": 2282774,
    "text": "これは経験的に導き出されたものなのか、そうでないのか？"
  },
  {
    "start": 2282812,
    "end": 2284790,
    "text": "そうなるべきだと私は思う。"
  },
  {
    "start": 2284860,
    "end": 2287720,
    "text": "多少の効果はあるが、正確な関数形はない。"
  },
  {
    "start": 2288350,
    "end": 2291274,
    "text": "我々は基本的に同じ関数形式をとった。"
  },
  {
    "start": 2291472,
    "end": 2298074,
    "text": "因果関係のようなものを理解しきれていないのは同感だ。"
  },
  {
    "start": 2298112,
    "end": 2306480,
    "text": "たしか、データを繰り返すことで、第1タームで行われる特徴学習が弱くなるというような理論だったと思う。"
  },
  {
    "start": 2307810,
    "end": 2312650,
    "text": "それ以上に、これはまさに経験的な問題である。"
  },
  {
    "start": 2312810,
    "end": 2314094,
    "text": "そうだね。"
  },
  {
    "start": 2314292,
    "end": 2315134,
    "text": "それは興味深いね。"
  },
  {
    "start": 2315252,
    "end": 2315920,
    "text": "そうだね。"
  },
  {
    "start": 2316690,
    "end": 2318254,
    "text": "フリーパラメーターは何ですか？"
  },
  {
    "start": 2318382,
    "end": 2320722,
    "text": "RスターとRスターだけ。"
  },
  {
    "start": 2320776,
    "end": 2321890,
    "text": "ああ、いい質問だね。"
  },
  {
    "start": 2321960,
    "end": 2329810,
    "text": "ab、alpha、beta、eをチンチラと同じ方法でフィットさせた。"
  },
  {
    "start": 2329890,
    "end": 2332262,
    "text": "ただ、1つのエポックの結果を使って、それらに正確にフィットさせるだけだ。"
  },
  {
    "start": 2332316,
    "end": 2332920,
    "text": "そうだね。"
  },
  {
    "start": 2333450,
    "end": 2338438,
    "text": "マルチエポックを加えると、新しいパラメータはr starとr star dだけになる。"
  },
  {
    "start": 2338524,
    "end": 2339400,
    "text": "その通りだ。"
  },
  {
    "start": 2343310,
    "end": 2344060,
    "text": "そうだね。"
  },
  {
    "start": 2346430,
    "end": 2346842,
    "text": "クールだ。"
  },
  {
    "start": 2346896,
    "end": 2347562,
    "text": "他に質問は？"
  },
  {
    "start": 2347616,
    "end": 2348620,
    "text": "素晴らしい質問だ。"
  },
  {
    "start": 2349970,
    "end": 2350720,
    "text": "そうだね。"
  },
  {
    "start": 2351090,
    "end": 2358138,
    "text": "ここでは、10回ほど繰り返しても、それほど損失が出ないかどうかということだ。"
  },
  {
    "start": 2358314,
    "end": 2374340,
    "text": "勾配降下のゲインと呼んでいるのですが、なんという名前なのかちょっとわからないのですが、ハイパーパラメーターを変えて、もう少しフィードバックが増えるようにしたら、低い数値で飽和するでしょうか？"
  },
  {
    "start": 2374950,
    "end": 2376610,
    "text": "この数字は関係あるのですか？"
  },
  {
    "start": 2377110,
    "end": 2379046,
    "text": "みんながこの質問をしてくれるのが嬉しい。"
  },
  {
    "start": 2379148,
    "end": 2381094,
    "text": "いい答えはないんだ。"
  },
  {
    "start": 2381292,
    "end": 2384680,
    "text": "それがこの分野での仕事のモチベーションになれば、最高だ。"
  },
  {
    "start": 2385290,
    "end": 2392774,
    "text": "これらの実験は十分に大規模なものなので、チンチラで言われていることをそのまま私たちのデータに適応させ、スケールアップして実施した。"
  },
  {
    "start": 2392902,
    "end": 2395882,
    "text": "そのハイパーパラメーターを変更して、どうなったか見てくれないか？"
  },
  {
    "start": 2395936,
    "end": 2400042,
    "text": "十分な計算能力があればできる実験なのか？"
  },
  {
    "start": 2400096,
    "end": 2401066,
    "text": "ああ、もちろんだ。"
  },
  {
    "start": 2401168,
    "end": 2407946,
    "text": "チンチラの論文では、ハイパーパラメータの設定が以前の大規模言語モデルから変更されている。"
  },
  {
    "start": 2407978,
    "end": 2409566,
    "text": "私は、それが違いを生むと思う。"
  },
  {
    "start": 2409668,
    "end": 2413386,
    "text": "これが宇宙の普遍的な性質だとは言いたくない。"
  },
  {
    "start": 2413418,
    "end": 2415486,
    "text": "経験的な道具という感じだ。"
  },
  {
    "start": 2415678,
    "end": 2416274,
    "text": "そうだね。"
  },
  {
    "start": 2416392,
    "end": 2417074,
    "text": "関連質問"
  },
  {
    "start": 2417112,
    "end": 2423774,
    "text": "そう、チンチラの論文では、ハイパーパラメーターでモーメント粒子を作り、カーブをプロットして最小値を取っている。"
  },
  {
    "start": 2423902,
    "end": 2425454,
    "text": "最低限にとどまっているだけだ。"
  },
  {
    "start": 2425502,
    "end": 2427250,
    "text": "その検索は二度としないことだ。"
  },
  {
    "start": 2427320,
    "end": 2428200,
    "text": "その通りだ。"
  },
  {
    "start": 2428810,
    "end": 2431142,
    "text": "そこに何か絞り出すものがあるかもしれない。"
  },
  {
    "start": 2431276,
    "end": 2432280,
    "text": "ああ、もちろんだ。"
  },
  {
    "start": 2433290,
    "end": 2434694,
    "text": "本当にいい指摘だ。"
  },
  {
    "start": 2434812,
    "end": 2441154,
    "text": "繰り返しになるが、こうしたことをするのは難しいということだ。経済的に厳しいからだ。"
  },
  {
    "start": 2441282,
    "end": 2451918,
    "text": "このような大規模な言語モデルは、コンピュータによって制限されているが、これらの結果の多くは、小規模なモデルにも適用できそうだ。"
  },
  {
    "start": 2452004,
    "end": 2453550,
    "text": "何か実験をしましたか？"
  },
  {
    "start": 2454450,
    "end": 2455694,
    "text": "ええ、まだ試していません。"
  },
  {
    "start": 2455732,
    "end": 2457102,
    "text": "私が知っている限りでは、それを縮小する。"
  },
  {
    "start": 2457156,
    "end": 2457760,
    "text": "そうだね。"
  },
  {
    "start": 2459090,
    "end": 2459840,
    "text": "クールだ。"
  },
  {
    "start": 2461410,
    "end": 2464830,
    "text": "ただ、nプライムの根拠がわからなかった。"
  },
  {
    "start": 2465250,
    "end": 2465614,
    "text": "そうだね。"
  },
  {
    "start": 2465652,
    "end": 2481000,
    "text": "その理論的根拠は、n個の素項が、このような設定においてモデルがどの程度優れたものになりうるかを表しており、ある種の特徴学習スタイルのモデルでは、データの繰り返しが、ここで学習されうるものをある種制限することになると考えているからである。"
  },
  {
    "start": 2482410,
    "end": 2495660,
    "text": "もし第一項がまったく同じであると考えるなら、rの星の値はそれを反映したものであるべきだ。"
  },
  {
    "start": 2496670,
    "end": 2498538,
    "text": "オーケー、素晴らしい。"
  },
  {
    "start": 2498704,
    "end": 2504590,
    "text": "では、もう2、3興味深い結果をお見せしましょう。それから、データの制約に関するいくつかの疑問についてお話ししましょう。"
  },
  {
    "start": 2505090,
    "end": 2506110,
    "text": "私たちはこれをやった。"
  },
  {
    "start": 2506180,
    "end": 2509806,
    "text": "r star dとr star nの値が得られる。"
  },
  {
    "start": 2509908,
    "end": 2512190,
    "text": "それが前のスライドでお見せしたものです。"
  },
  {
    "start": 2514050,
    "end": 2517010,
    "text": "次に、この配分の問題について話したい。"
  },
  {
    "start": 2517430,
    "end": 2524914,
    "text": "現実的な疑問として、どれくらいの頻度で繰り返せばいいのか、モデルの大きさはどれくらいにすればいいのか、ということがあるかもしれない。"
  },
  {
    "start": 2525112,
    "end": 2538390,
    "text": "では、チンチラ曲線があるとして、データが限られているから繰り返すとして、実際に曲線自体に何が起こるのか、もっと頻繁に繰り返すのか、それとももっとパラメーターを追加するのか。"
  },
  {
    "start": 2539050,
    "end": 2543978,
    "text": "どの選択をするのか、私には直感的に理解できなかった。"
  },
  {
    "start": 2544144,
    "end": 2546742,
    "text": "もし私が1000億トークンしか持っていないとしたら。"
  },
  {
    "start": 2546806,
    "end": 2549930,
    "text": "実際のところ、練習で何をすればいいのか分からなかった。"
  },
  {
    "start": 2551250,
    "end": 2554320,
    "text": "このデータをプロットしてみた。"
  },
  {
    "start": 2555010,
    "end": 2561470,
    "text": "これは、モデルのサイズとデータの繰り返しを変化させた1億ユニークトークンを使用した場合である。"
  },
  {
    "start": 2562130,
    "end": 2570110,
    "text": "このグラフ上のすべての点は、反復回数とモデルパラメーターの異なる選択を表している。"
  },
  {
    "start": 2570270,
    "end": 2582310,
    "text": "1億のユニークなトークンが与えられた場合、点の色は最終的な損失を表し、等高線プロットはこれらのモデルで損失がある種最小であった場所を表す。"
  },
  {
    "start": 2582650,
    "end": 2586690,
    "text": "フロップ数は乗算であることを思い出してほしい。"
  },
  {
    "start": 2586850,
    "end": 2591930,
    "text": "右に行くか、上に行くかで、練習での失敗が増える。"
  },
  {
    "start": 2592830,
    "end": 2604400,
    "text": "この黒い星は、チンチラポイントというユニークなトークンしか使っていないことを表し、この白い星は、モデル自体の損失が大きいことを表している。"
  },
  {
    "start": 2605410,
    "end": 2614980,
    "text": "前のスライドで見た曲線や計算式をこのデータに当てはめると、次のような形の等高線が得られる。"
  },
  {
    "start": 2615350,
    "end": 2620020,
    "text": "これは基本的に、フロップを増やせば増やすほど良くなることを示している。"
  },
  {
    "start": 2620470,
    "end": 2627160,
    "text": "また、より多くのパラメーターを追加した場合と、より多くの回数データを繰り返した場合の比率のようなものも示している。"
  },
  {
    "start": 2627770,
    "end": 2636438,
    "text": "特に、このデータの上にチンチラ曲線を描くと、等倍スケーリングによって次のようになることがわかる。"
  },
  {
    "start": 2636604,
    "end": 2645914,
    "text": "の公式は、基礎となるシステムにパラメーターを追加する前に、データをより頻繁に繰り返すべきだと予測している。"
  },
  {
    "start": 2646112,
    "end": 2652400,
    "text": "チンチララインの下あたりから、より良い損失が出始める。"
  },
  {
    "start": 2653650,
    "end": 2661774,
    "text": "チンチラのように無限にカーブを続けることができるのとは違い、このカーブはある地点でピークアウトする。"
  },
  {
    "start": 2661892,
    "end": 2670420,
    "text": "これは、ハイパーパラメータが間違っていると思われ、レジームから外れてオーバーフィッティングモードに入る、あるいはそれを何と呼んでもいい。"
  },
  {
    "start": 2674150,
    "end": 2684834,
    "text": "仮定のチンチララインを要約すると、私たちの等高線は、パラメーターの数をスケールするよりも、繰り返しをスケールする方が速いことを予測している。"
  },
  {
    "start": 2684962,
    "end": 2690620,
    "text": "これは、r star dの値がr star nよりも高いことに対応している。"
  },
  {
    "start": 2691950,
    "end": 2694010,
    "text": "この結果について質問は？"
  },
  {
    "start": 2699070,
    "end": 2701222,
    "text": "そこでのrはrとは違う。"
  },
  {
    "start": 2701376,
    "end": 2701934,
    "text": "その通りだ。"
  },
  {
    "start": 2701972,
    "end": 2702222,
    "text": "そうだ。"
  },
  {
    "start": 2702276,
    "end": 2703390,
    "text": "彼らは別々に行く。"
  },
  {
    "start": 2704450,
    "end": 2705166,
    "text": "なるほど。"
  },
  {
    "start": 2705268,
    "end": 2705486,
    "text": "そうだね。"
  },
  {
    "start": 2705508,
    "end": 2706080,
    "text": "申し訳ない。"
  },
  {
    "start": 2706930,
    "end": 2710350,
    "text": "ああ、似たような言葉だから似たような名前をつけたんだ。"
  },
  {
    "start": 2711570,
    "end": 2717534,
    "text": "そう、このフィッティング実験による外挿実験のようなものだ。"
  },
  {
    "start": 2717582,
    "end": 2725418,
    "text": "あなたのレジームで小規模にフィットさせ、より大規模なスケールでフィット感を評価する。"
  },
  {
    "start": 2725534,
    "end": 2734546,
    "text": "ええ、小さなスケールで曲線をフィットさせ、より大きなスケールに外挿する実験を試みました。"
  },
  {
    "start": 2734658,
    "end": 2744646,
    "text": "これらについて、データ制約として250億トークンから開始し、より大きな2つのモデルを実行する。"
  },
  {
    "start": 2744838,
    "end": 2761470,
    "text": "この2つのモデルは、フロップ数は同じだが、一方はチンチロの公式を素朴に適応させたもので、もう一方は我々が予測するデータスケーリングの法則を利用して、下方に傾斜している。"
  },
  {
    "start": 2761890,
    "end": 2768430,
    "text": "同じフロップ数であれば、このモデルの方が損失が少ないことがわかる。"
  },
  {
    "start": 2768590,
    "end": 2774850,
    "text": "これは小さく見えるが、この手のモデルの低損失では比較的大きい。"
  },
  {
    "start": 2775590,
    "end": 2786840,
    "text": "データをもう少し頻繁に繰り返し、より少ないパラメーターを使用したほうが、実際にはより良い損失が得られるようだ。"
  },
  {
    "start": 2788250,
    "end": 2788710,
    "text": "クールだ。"
  },
  {
    "start": 2788780,
    "end": 2793594,
    "text": "繰り返しになるが、なぜそうなるのか、メカニズム的な理由を主張したいわけではない。"
  },
  {
    "start": 2793712,
    "end": 2800650,
    "text": "ジョナサンが指摘したように、これはほとんど外挿実験のようなもので、私たちの小規模な実験を利用して、これを実際にスケールアップしようとするものだ。"
  },
  {
    "start": 2802190,
    "end": 2802940,
    "text": "クールだ。"
  },
  {
    "start": 2803470,
    "end": 2810378,
    "text": "この結果、過去の論文にさかのぼって反実仮想的に検証することができるようになった。"
  },
  {
    "start": 2810554,
    "end": 2819678,
    "text": "一見、データを繰り返しているように見える大規模な言語モデルが1つあるが、それはデータの規模がすでに使い果たされていたからだ。"
  },
  {
    "start": 2819854,
    "end": 2829174,
    "text": "このギャラクティカでの仕事は、特に科学論文で訓練されたもので、科学論文で訓練するとなると、その量には限りがある。"
  },
  {
    "start": 2829292,
    "end": 2835000,
    "text": "そこで彼らはこの時点で、データを4.25回繰り返すことにした。"
  },
  {
    "start": 2835450,
    "end": 2841978,
    "text": "彼らが決めたことは、繰り返しを少なくするためにパラメーターを増やすことだった。"
  },
  {
    "start": 2842144,
    "end": 2844870,
    "text": "彼らはチンチラのカーブを超えていった。"
  },
  {
    "start": 2845030,
    "end": 2851450,
    "text": "私たちが主張しているのは、チンチラカーブを下回ることができ、より良い負け方をしたのではないかということだと思う。"
  },
  {
    "start": 2852110,
    "end": 2858126,
    "text": "彼らは最初にもっと大きなモデルを訓練し、4.25回繰り返した。"
  },
  {
    "start": 2858308,
    "end": 2865200,
    "text": "この点については、もっと小さなモデルで12回ほど繰り返せば、もっといい結果が出たと思う。"
  },
  {
    "start": 2865730,
    "end": 2869998,
    "text": "これが実際に正しいかどうかは、まだわからないと思う。"
  },
  {
    "start": 2870094,
    "end": 2876530,
    "text": "私たちは、この実験から予測された値が、より良い大振りの長さを生み出すのに役立ったのではないかと考えている。"
  },
  {
    "start": 2877690,
    "end": 2878440,
    "text": "素晴らしい。"
  },
  {
    "start": 2879850,
    "end": 2882262,
    "text": "では、質問の時間を設けたいと思います。"
  },
  {
    "start": 2882316,
    "end": 2887000,
    "text": "論文にある他の実験について、あと2分ほど説明するつもりだ。"
  },
  {
    "start": 2887530,
    "end": 2898550,
    "text": "ある意味、この結果は、今後2、3年のうちに、桁違いの結果を得るためにできる経験的なことを教えてくれたという点で、素晴らしいと思う。"
  },
  {
    "start": 2898710,
    "end": 2905806,
    "text": "また、「あと1回か2回、この2倍が減るだけかもしれない」という悲観的な見方もある。"
  },
  {
    "start": 2905988,
    "end": 2910830,
    "text": "私たちは、実際に役立つかもしれない他の補完的な戦略にも目を向けたいと思った。"
  },
  {
    "start": 2911170,
    "end": 2913300,
    "text": "私たちは新聞に掲載されたカップルを見た。"
  },
  {
    "start": 2913670,
    "end": 2925150,
    "text": "もうひとつは、大規模な言語モデルのフィルタリングプロセスを見直すことだ。"
  },
  {
    "start": 2925310,
    "end": 2929858,
    "text": "さて、2つ目は、これらのモデルがどのようにトレーニングされるかについて、いくつかの具体的な話に入る。"
  },
  {
    "start": 2929954,
    "end": 2931830,
    "text": "それについてはあまり突っ込みたくない。"
  },
  {
    "start": 2931900,
    "end": 2934146,
    "text": "詳細を説明する時間がないんだ。"
  },
  {
    "start": 2934338,
    "end": 2945500,
    "text": "大まかには、より効果的なトークンを得るために、あるいは、あまり効果的でないトークンを得て、それをより頻繁に繰り返すために、これらのデータセットの構築方法を変更することになる。"
  },
  {
    "start": 2945950,
    "end": 2949610,
    "text": "このコードは、実際にはもう少し面白いものだと思う。"
  },
  {
    "start": 2950190,
    "end": 2954142,
    "text": "ああ、それは一度だけのことで、あるいは途中で調整しているんだ。"
  },
  {
    "start": 2954276,
    "end": 2955600,
    "text": "もう1回言ってくれる？"
  },
  {
    "start": 2956370,
    "end": 2958510,
    "text": "あなたは当惑フィルターを見ている。"
  },
  {
    "start": 2959490,
    "end": 2960382,
    "text": "一度だけだ。"
  },
  {
    "start": 2960436,
    "end": 2961054,
    "text": "1回だけね。"
  },
  {
    "start": 2961092,
    "end": 2968174,
    "text": "これらのデータセットのデフォルトのプレプレキシティ・フィルターを取り除き、より高いプレプレキシティ・フィルターをかける。"
  },
  {
    "start": 2968222,
    "end": 2971220,
    "text": "より質の高いデータを取得し、それをより頻繁に繰り返す。"
  },
  {
    "start": 2971910,
    "end": 2973140,
    "text": "そんな感じだ。"
  },
  {
    "start": 2974230,
    "end": 2977582,
    "text": "弱体化したモデルのようだ。"
  },
  {
    "start": 2977726,
    "end": 2978434,
    "text": "ああ、そうだ。"
  },
  {
    "start": 2978472,
    "end": 2981670,
    "text": "魔法のような当惑モデルを持っていると仮定する。"
  },
  {
    "start": 2981740,
    "end": 2982262,
    "text": "そうだね。"
  },
  {
    "start": 2982396,
    "end": 2984002,
    "text": "どちらが強いか、弱いか？"
  },
  {
    "start": 2984066,
    "end": 2984806,
    "text": "実はよくわからないんだ。"
  },
  {
    "start": 2984828,
    "end": 2991350,
    "text": "実際には、抽象度の多様性を考慮するだけで、より強力だと言える。"
  },
  {
    "start": 2993630,
    "end": 2994860,
    "text": "本当にいい質問だ。"
  },
  {
    "start": 2996830,
    "end": 3001114,
    "text": "いくつかの実験では重複排除を行っているが、これは多様性の非常に弱い形態だと思う。"
  },
  {
    "start": 3001242,
    "end": 3005834,
    "text": "そうでなければ、デフォルトの一般的なクロールプロセスを使うだけだ。"
  },
  {
    "start": 3005962,
    "end": 3009200,
    "text": "私たちは、彼らがどのようにデータセットを構成することに決めたか、その方法を取るようなものだ。"
  },
  {
    "start": 3011490,
    "end": 3016370,
    "text": "さて、これが僕の最後の実験的スライドになると思う。"
  },
  {
    "start": 3016790,
    "end": 3027482,
    "text": "というのも、これらは実際に損失に対してではなく、基本的にベンチマークとなるデータセットに対して実行されているからだ。"
  },
  {
    "start": 3027646,
    "end": 3036950,
    "text": "これらのベンチマーク・データセットは一種のNLPデータセットだが、NLPデータセットの多くは推論タスクのように見えることに注意すべきだ。"
  },
  {
    "start": 3037390,
    "end": 3040250,
    "text": "その中には多くの基礎技術が含まれている。"
  },
  {
    "start": 3041070,
    "end": 3046090,
    "text": "紫色の線は、これまで私たちが大まかに見てきたデータの繰り返し曲線を表している。"
  },
  {
    "start": 3046240,
    "end": 3050502,
    "text": "これは当惑を示すのではなく、下流の正確さを示している。"
  },
  {
    "start": 3050646,
    "end": 3053386,
    "text": "グラフはこれまで見てきたものとよく似ている。"
  },
  {
    "start": 3053488,
    "end": 3055326,
    "text": "これは完全な固有データによるものである。"
  },
  {
    "start": 3055428,
    "end": 3056714,
    "text": "これはリピートと一緒だ。"
  },
  {
    "start": 3056842,
    "end": 3058042,
    "text": "これはリピートと一緒だ。"
  },
  {
    "start": 3058106,
    "end": 3060960,
    "text": "その後、これは何度も繰り返され、やがて尻すぼみになる。"
  },
  {
    "start": 3062210,
    "end": 3073940,
    "text": "ここで言いたかったのは、コードはかなりうまく機能するようだということ、つまり、大量のトークンをコードに置き換えることで、かなり高い精度を維持できるということだ。"
  },
  {
    "start": 3074390,
    "end": 3079438,
    "text": "基本的にはリピートデータと同じようなカーブを保つようだ。"
  },
  {
    "start": 3079544,
    "end": 3082760,
    "text": "実際、いくつかの仕事には大いに役立っているようだ。"
  },
  {
    "start": 3083370,
    "end": 3092490,
    "text": "残りのトークンの半分をコードで埋めることは、このようなシステムを改善するためにかなり有効であるという証拠があるようだ。"
  },
  {
    "start": 3093150,
    "end": 3097226,
    "text": "この論文では、複 雑性、フィルタリング、重複排除に関するいくつかの実験を行っている。"
  },
  {
    "start": 3097408,
    "end": 3116180,
    "text": "しかし、これはかなりデータセットに特有なことであり、当惑度を使ったフィルタリングや、その後の繰り返しが効果的であることがわかった。"
  },
  {
    "start": 3117430,
    "end": 3118180,
    "text": "素晴らしい。"
  },
  {
    "start": 3119510,
    "end": 3121620,
    "text": "コードについての質問です。"
  },
  {
    "start": 3122790,
    "end": 3130914,
    "text": "この下流のパフォーマンスの下流のタスクのどれかが、コードスタイルの理解に関わっているのだろうか？"
  },
  {
    "start": 3131042,
    "end": 3139562,
    "text": "それとも、コメントをたくさん書く人がいるから、たまたまコードに自然言語が含まれているだけなのか、それとも何かあるのか？"
  },
  {
    "start": 3139616,
    "end": 3141290,
    "text": "ああ、いろいろ考えているよ。"
  },
  {
    "start": 3142670,
    "end": 3144380,
    "text": "どれもコードには関係ない。"
  },
  {
    "start": 3144990,
    "end": 3147850,
    "text": "コードにはそれほど自然な言語はないと思う。"
  },
  {
    "start": 3148190,
    "end": 3152698,
    "text": "僕には多くのタスクが、基本的にコードのタスクのように感じられる。"
  },
  {
    "start": 3152874,
    "end": 3165906,
    "text": "タスクの多くは、自然言語で表現された赤ん坊のようなタスクのようなものだが、実際には2つのものをマッチングさせて入れるというものだ。"
  },
  {
    "start": 3165928,
    "end": 3171010,
    "text": "私にとっては、合成作業やコード作業とさほど変わらないように感じる。"
  },
  {
    "start": 3171510,
    "end": 3172660,
    "text": "それは理にかなっている。"
  },
  {
    "start": 3173270,
    "end": 3175646,
    "text": "それは、あなたが改善を見ているタスクですか？"
  },
  {
    "start": 3175758,
    "end": 3177262,
    "text": "ええ、赤ちゃんみたいに？"
  },
  {
    "start": 3177326,
    "end": 3178310,
    "text": "赤ちゃんのようにね。"
  },
  {
    "start": 3178650,
    "end": 3181686,
    "text": "にもかかわらず、である。"
  },
  {
    "start": 3181868,
    "end": 3187640,
    "text": "私はその論文の著者であり、超自慢ではないが、そう、そういう意味では奇妙な仕事のようなものだ。"
  },
  {
    "start": 3188650,
    "end": 3189062,
    "text": "実はね。"
  },
  {
    "start": 3189116,
    "end": 3190226,
    "text": "1ミリ秒くれ。"
  },
  {
    "start": 3190338,
    "end": 3191442,
    "text": "結論から申し上げたい。"
  },
  {
    "start": 3191506,
    "end": 3202178,
    "text": "これが、この講演の主な結論のようなもので、それから最後に、下流のタスクでこれを使用している人たちがいるということを述べておこう。"
  },
  {
    "start": 3202294,
    "end": 3207566,
    "text": "この論文の著者たちは、フィンランド語の言語モデルを学習することができた。"
  },
  {
    "start": 3207668,
    "end": 3212842,
    "text": "彼らはそれを8回繰り返し、実際に使っているようだ。"
  },
  {
    "start": 3212986,
    "end": 3220450,
    "text": "それでも大量のトークンを獲得することはできないが、実際に有効なトークンを獲得することはできる。"
  },
  {
    "start": 3221510,
    "end": 3232550,
    "text": "とはいえ、昨日はこの件について本当に素晴らしい会話がたくさんあったと思う。"
  },
  {
    "start": 3232970,
    "end": 3238386,
    "text": "もっと多くのデータ・トークンを得ることができるかもしれない直交的な方法は他にもたくさんあると思う。"
  },
  {
    "start": 3238498,
    "end": 3246138,
    "text": "ここまでの進歩の多くは、同じトランスフォーマーのモデルを拡大縮小することでもたらされたものであることに留意したい。"
  },
  {
    "start": 3246304,
    "end": 3252650,
    "text": "それがある種の終着点に近づいているような気がするのは興味深いことだと思う。"
  },
  {
    "start": 3253010,
    "end": 3267278,
    "text": "だから、このような作品が、データから価値を引き出したり、他のモダリティから価値を引き出したり、このような単なる漸近的な評価よりも優れた評価を考え出したりするために、人々がもっと創造的な方法を試すようになることを期待している。"
  },
  {
    "start": 3267374,
    "end": 3270020,
    "text": "そこで終わりにして、また質問を受け付けましょう。"
  },
  {
    "start": 3278810,
    "end": 3285526,
    "text": "コードは特別なものなのか、それともフィンランド人に嫌われてもフィンランド語を進歩させる一つの方法なのか？"
  },
  {
    "start": 3285628,
    "end": 3289660,
    "text": "フィンランド語のデータを2倍の英語データで補強する。"
  },
  {
    "start": 3292030,
    "end": 3297900,
    "text": "ああ、確かに試してみる価値はある。"
  },
  {
    "start": 3300370,
    "end": 3311054,
    "text": "言語モデルを1つの言語で訓練しようとする理由が、完全に単純性に基づくものだとは100％確信していない。"
  },
  {
    "start": 3311252,
    "end": 3324420,
    "text": "言語モデルを1つの言語だけで訓練したい理由は他にもあるかもしれませんが、当惑度を気にするだけなら、多言語モデルの方がより効果的な方法だと思います。"
  },
  {
    "start": 3327610,
    "end": 3332598,
    "text": "コードを外してトレーニングするという意味がよくわからない。"
  },
  {
    "start": 3332764,
    "end": 3336582,
    "text": "Pythonのコードをそのまま使っているのか、それとも何らかのフィルターをかけているのか？"
  },
  {
    "start": 3336636,
    "end": 3337590,
    "text": "生のPythonコード。"
  },
  {
    "start": 3337660,
    "end": 3343638,
    "text": "ということは、ifとthenとforキーワードをうまく使いこなさなければならない。"
  },
  {
    "start": 3343734,
    "end": 3347494,
    "text": "そのようなコードから訓練すれば、語彙的なことは些細なことだと思う。"
  },
  {
    "start": 3347542,
    "end": 3348618,
    "text": "それはすぐにわかる。"
  },
  {
    "start": 3348704,
    "end": 3359310,
    "text": "遠距離恋愛のコピーとか、基本的な数学的推理とか、いろんなことを学んでいるんだろうね。"
  },
  {
    "start": 3360290,
    "end": 3363386,
    "text": "それは自然言語と関係があるのか？"
  },
  {
    "start": 3363498,
    "end": 3373426,
    "text": "ええ、先ほど私が言ったのは、私たちは自然言語の当惑度を測っているのではなく、自然言語処理研究者が考え出したタスクの正確さを測っているということです。"
  },
  {
    "start": 3373528,
    "end": 3378850,
    "text": "そのうちのいくつかは、コードと同じようなスキルを必要とする性質を持っている、というのが私の仮説だ。"
  },
  {
    "start": 3384650,
    "end": 3389174,
    "text": "データを繰り返しているとき、これは複数のエポックのような問題なのか？"
  },
  {
    "start": 3389222,
    "end": 3399834,
    "text": "モデルはある意味で収束していないし、データを繰り返すことで、ある意味で早期停止を少なくしている。"
  },
  {
    "start": 3399872,
    "end": 3410270,
    "text": "コンバージェンスという言葉を使わなかったのは、スケーリング法則の話をするときに、スロットの数を積極的に考慮するからで、それは少し違うような気がするからです。"
  },
  {
    "start": 3410340,
    "end": 3414898,
    "text": "我々はコンバージェンスだけでなく、実際の運営コストにも興味がある。"
  },
  {
    "start": 3414984,
    "end": 3415330,
    "text": "そうだね。"
  },
  {
    "start": 3415400,
    "end": 3419774,
    "text": "私たちはリピートすることで、ある意味、より多くの価値を得ている。"
  },
  {
    "start": 3419902,
    "end": 3420290,
    "text": "そうだね。"
  },
  {
    "start": 3420360,
    "end": 3427538,
    "text": "というのも、複数のエポックを実行することで、早期の停止から得られる正則化がデータ量に応じて変化してしまうからです。"
  },
  {
    "start": 3427704,
    "end": 3435426,
    "text": "なぜ、一度サブサンプリングしてから繰り返すのではなく、各エポックでデータをサブサンプリングしないのか？"
  },
  {
    "start": 3435538,
    "end": 3439126,
    "text": "というのも、私たちは文字通り、あなたがより多くのデータを持っていないと仮定しているからだ。"
  },
  {
    "start": 3439228,
    "end": 3439494,
    "text": "オーケー。"
  },
  {
    "start": 3439532,
    "end": 3439782,
    "text": "そうだね。"
  },
  {
    "start": 3439836,
    "end": 3451402,
    "text": "私たちが気にしている設定は、このようなものだ。この部屋にいる人たちはもっと詳しく教えてくれるだろうが、5000億ものパラメーターがあるときに正則化の意味をどう考えればいいのかわからない。"
  },
  {
    "start": 3451466,
    "end": 3452126,
    "text": "そうだろう？"
  },
  {
    "start": 3452308,
    "end": 3454014,
    "text": "私の専門用語は古いかもしれない。"
  },
  {
    "start": 3454052,
    "end": 3461326,
    "text": "その点、コードの質問に戻ると、私は見落としていたかもしれない。"
  },
  {
    "start": 3461348,
    "end": 3465300,
    "text": "あなたは、タスクが何となくコードのようだという事実に注目していた。"
  },
  {
    "start": 3466230,
    "end": 3471454,
    "text": "当惑に目を向けても、同じ結果は得られないと？"
  },
  {
    "start": 3471502,
    "end": 3477602,
    "text": "ああ、確かにハーフコードでトレーニングすれば、言語に対する戸惑いは同じようには減らない。"
  },
  {
    "start": 3477736,
    "end": 3480818,
    "text": "というのも、流通から外れているからだ。"
  },
  {
    "start": 3480994,
    "end": 3482566,
    "text": "下がるのか？"
  },
  {
    "start": 3482748,
    "end": 3483766,
    "text": "いい質問だね。"
  },
  {
    "start": 3483868,
    "end": 3485094,
    "text": "論文に結果があると思う。"
  },
  {
    "start": 3485132,
    "end": 3488166,
    "text": "イプシロンは必要ない。"
  },
  {
    "start": 3488188,
    "end": 3488798,
    "text": "オフェンス"
  },
  {
    "start": 3488914,
    "end": 3489578,
    "text": "そうだね。"
  },
  {
    "start": 3489744,
    "end": 3495930,
    "text": "この結果は、学習アルゴリズムとしてのSGDの非効率性を証明する一種の証明書だと考えていいのだろうか？"
  },
  {
    "start": 3496670,
    "end": 3499500,
    "text": "私たちは結果を得るために、時に目を向けなければならない。"
  },
  {
    "start": 3500910,
    "end": 3503466,
    "text": "でも、僕らが発明したわけじゃない。"
  },
  {
    "start": 3503488,
    "end": 3504182,
    "text": "そうだろ？"
  },
  {
    "start": 3504336,
    "end": 3505898,
    "text": "それは確かに知られている。"
  },
  {
    "start": 3506074,
    "end": 3508458,
    "text": "改善の余地があることを示唆しているのかもしれない。"
  },
  {
    "start": 3508634,
    "end": 3509310,
    "text": "十分フェアだ。"
  },
  {
    "start": 3509380,
    "end": 3515860,
    "text": "とはいえ、SCDは多くの試練を乗り越えてきた。"
  },
  {
    "start": 3516950,
    "end": 3521220,
    "text": "また、CBARテンのモデルは3000のようにトレーニングされていることも特筆に値する。"
  },
  {
    "start": 3524550,
    "end": 3527950,
    "text": "SGDの非効率性という点では、信じられないほど穏やかだ。"
  },
  {
    "start": 3528030,
    "end": 3528660,
    "text": "そうだろう？"
  },
  {
    "start": 3530070,
    "end": 3533974,
    "text": "そうだね。効率について話すときは、フロップの効率についても話すべきだと思う。"
  },
  {
    "start": 3534012,
    "end": 3537960,
    "text": "もっとクレイジーな方法を思いつくかもしれないが、それは何らかの方法でより多くのフロップを必要とするかもしれない。"
  },
  {
    "start": 3538350,
    "end": 3543434,
    "text": "小さなデータセットを得るとき、大きなデータセットから無作為にサンプリングしているのですか？"
  },
  {
    "start": 3543472,
    "end": 3544700,
    "text": "まさにこれだ。"
  },
  {
    "start": 3545790,
    "end": 3551626,
    "text": "7つのエポックについて、1つの7番目のランダムサンプルを取っているだけですか？"
  },
  {
    "start": 3551738,
    "end": 3553566,
    "text": "いや、始まりは同じだ。"
  },
  {
    "start": 3553748,
    "end": 3555440,
    "text": "このようにカットするだけだ。"
  },
  {
    "start": 3557330,
    "end": 3560126,
    "text": "ああ、最初の7番を取って、それからという感じか。"
  },
  {
    "start": 3560148,
    "end": 3562110,
    "text": "ああ、たまにシャッフルするんだ。"
  },
  {
    "start": 3562180,
    "end": 3562414,
    "text": "そうだね。"
  },
  {
    "start": 3562452,
    "end": 3571202,
    "text": "より小さなデータセットの多様性、例えば今回のようにより小さなデータセットがあったとしても、多様性はより大きなデータセットから生まれますよね？"
  },
  {
    "start": 3571256,
    "end": 3572386,
    "text": "そうだね。"
  },
  {
    "start": 3572488,
    "end": 3577202,
    "text": "もしデータセットがもっと少なくて、それがすべて1つのドメインだったらどうなるかというようなことは主張しない。"
  },
  {
    "start": 3577256,
    "end": 3578900,
    "text": "ええ、本当に訴えられる例です。"
  },
  {
    "start": 3579210,
    "end": 3580054,
    "text": "何か質問は？"
  },
  {
    "start": 3580092,
    "end": 3581880,
    "text": "もっと大きなことについて話したい？"
  },
  {
    "start": 3582570,
    "end": 3583362,
    "text": "大きくはない。"
  },
  {
    "start": 3583426,
    "end": 3594774,
    "text": "というのも、これは大規模な言語モデルをトレーニングする際の戦略的な選択のように思えるからだ。"
  },
  {
    "start": 3594822,
    "end": 3603774,
    "text": "昨日のGBDの4人のトークのように、非常に微妙な場合は、何とかトップ20のパーセンテージしか使わないと話していた。"
  },
  {
    "start": 3603812,
    "end": 3605630,
    "text": "このプロセスでどのような役割を果たしているのか？"
  },
  {
    "start": 3605700,
    "end": 3607502,
    "text": "何がフィルターにかけられたのか？"
  },
  {
    "start": 3607636,
    "end": 3614586,
    "text": "フィルターが当惑度を最適化したのなら、私たちの突出もまた当惑度を最適化したことになる。"
  },
  {
    "start": 3614698,
    "end": 3616450,
    "text": "それは問題ですか？"
  },
  {
    "start": 3616520,
    "end": 3617186,
    "text": "あるいは問題だ。"
  },
  {
    "start": 3617288,
    "end": 3618194,
    "text": "それは本当にいい指摘だ。"
  },
  {
    "start": 3618232,
    "end": 3626580,
    "text": "昨日、暗黙の蒸留について話したが、より質の高いデータを持つことで利益を得ることができるかもしれないということと関連していると思う。"
  },
  {
    "start": 3626970,
    "end": 3638794,
    "text": "一方、ディーと私は、このような錯綜フィルターが、方言や、よくわからないが、あなたが本当に気にしている分布の一部をフィルターしてしまうかもしれないという話をしていた。"
  },
  {
    "start": 3638912,
    "end": 3640620,
    "text": "トレーニング前の優先事項。"
  },
  {
    "start": 3641070,
    "end": 3643770,
    "text": "とはいえ、コリン、君ならもう少しうまく答えられると思うよ。"
  },
  {
    "start": 3643840,
    "end": 3651020,
    "text": "戸惑いのフィルターについてどう考えるべきかという感覚はありますか？"
  },
  {
    "start": 3651710,
    "end": 3658302,
    "text": "つまり、あまり拡大解釈したくはないのですが、質の高いデータとは何かという問題と結びついていると思います。"
  },
  {
    "start": 3658356,
    "end": 3665598,
    "text": "というのも、どれだけのデータが入手可能かという議論は、暗黙のうちに良いデータもあれば悪いデータもあることを前提としているからだ。"
  },
  {
    "start": 3665684,
    "end": 3665994,
    "text": "そうだね。"
  },
  {
    "start": 3666052,
    "end": 3672302,
    "text": "というのも、データセットを作成するときに、ランダムな文字と同じようにフィルタリングされてしまうウェブページがあるからだ。"
  },
  {
    "start": 3672366,
    "end": 3675122,
    "text": "私たちは皆、それは悪いことだ、質の低いデータだと言うだろう。"
  },
  {
    "start": 3675176,
    "end": 3677670,
    "text": "そうしないと、データセットが大きくなる。"
  },
  {
    "start": 3678810,
    "end": 3683926,
    "text": "質の高いデータとは何かを定義することなしに、どれだけのデータを持っているかという議論はできない。"
  },
  {
    "start": 3684028,
    "end": 3686840,
    "text": "高品質とは何かを定義することはできない。"
  },
  {
    "start": 3687530,
    "end": 3689270,
    "text": "信じられないほど漠然としている。"
  },
  {
    "start": 3689770,
    "end": 3694700,
    "text": "そう、そして、確かに当惑フィルタリングは高品質の代用品ではない。"
  },
  {
    "start": 3696110,
    "end": 3696714,
    "text": "その通りだ。"
  },
  {
    "start": 3696832,
    "end": 3699370,
    "text": "わからないけど、妥当なのかもしれない。"
  },
  {
    "start": 3701470,
    "end": 3704470,
    "text": "また、収穫逓増の問題でもあると思う。"
  },
  {
    "start": 3704550,
    "end": 3708746,
    "text": "繰り返しになるが、スクレイピングしたデータだけを見ても、ゴミのようなものもある。"
  },
  {
    "start": 3708778,
    "end": 3714786,
    "text": "あなたは、ゴミのようなデータを取り除くために、いくつかのヒューリスティックを考え出した。"
  },
  {
    "start": 3714888,
    "end": 3721486,
    "text": "そしてヒューリスティックを増やしていくと、実際にはゴミではないものまでフィルターにかけるようになる。"
  },
  {
    "start": 3721518,
    "end": 3729266,
    "text": "惑わしのフィルタリングは、フィルタリングの連鎖のかなり下流にあり、私たちが望んでいることをやっているのかどうか、もう確信が持てないと思う。"
  },
  {
    "start": 3729448,
    "end": 3731358,
    "text": "すみません、ニコラス・カルリーニはいますか？"
  },
  {
    "start": 3731384,
    "end": 3732550,
    "text": "昨日、彼を見たと思う。"
  },
  {
    "start": 3733050,
    "end": 3735046,
    "text": "これについても、彼は別の見解を持っていると思う。"
  },
  {
    "start": 3735068,
    "end": 3737880,
    "text": "まあ、このトピックに興味があるなら、彼も多くのことを書いている。"
  },
  {
    "start": 3742250,
    "end": 3751466,
    "text": "実際のナチュラル・イングリッシュには取り組んでいないということですが、その動機についてもっと質問してもいいですか？"
  },
  {
    "start": 3751578,
    "end": 3768930,
    "text": "これは本当に、置換なしのサンプリングについてのチャットdb 3のコメントの癖からきているのだと思います。"
  },
  {
    "start": 3772150,
    "end": 3779618,
    "text": "半減期減衰のようなものを使っているとほざいているが、これはむしろピストニウム分布のようなものだ。"
  },
  {
    "start": 3779794,
    "end": 3790970,
    "text": "それなら、超幾何分布の特徴ではなく、あなたがやっているようなプロスタニアン分布の特徴をもっと期待すべきなのでしょうか？"
  },
  {
    "start": 3794350,
    "end": 3796540,
    "text": "ああ、そうだな。"
  },
  {
    "start": 3799390,
    "end": 3800442,
    "text": "言いたいことは分かるよ。"
  },
  {
    "start": 3800496,
    "end": 3802498,
    "text": "そんなことは言っていない。"
  },
  {
    "start": 3802614,
    "end": 3809182,
    "text": "私は、彼らが文字どおり繰り返さないようにしているというだけで、それとは対照的に、もっと弱い主張をしていたように思う。"
  },
  {
    "start": 3809236,
    "end": 3814110,
    "text": "面白いことに、ハイパージオメトリックのようなことはしていないと思うからだ。"
  },
  {
    "start": 3814450,
    "end": 3820270,
    "text": "これだけデータがあるのに、なぜ全部使わないんだ？"
  },
  {
    "start": 3820340,
    "end": 3824286,
    "text": "それが最も効果的な方法だ。"
  },
  {
    "start": 3824468,
    "end": 3826982,
    "text": "ここでブレークすべきだろう。"
  },
  {
    "start": 3827036,
    "end": 3828840,
    "text": "サーシャにもう一度お礼を言おう。"
  },
  {
    "start": 3833370,
    "end": 3836454,
    "text": "外に軽食があるはずだ。"
  },
  {
    "start": 3836572,
    "end": 3838660,
    "text": "20分後に再開する。"
  }
]