[
  {
    "start": 64490,
    "end": 71546,
    "text": "では、次のスピーカー、ワシントン大学のチェ・エージェントをご紹介しましょう。"
  },
  {
    "start": 71738,
    "end": 83910,
    "text": "彼女はNLPを研究しており、インサットの火花についての話に続く完璧なものだと思う。"
  },
  {
    "start": 84330,
    "end": 89180,
    "text": "イェジンはマッカーサーフェローでもあり、素晴らしいスピーカーなので、あなたの講演を楽しみにしています。"
  },
  {
    "start": 89710,
    "end": 91514,
    "text": "オーケー、ありがとう。"
  },
  {
    "start": 91632,
    "end": 105690,
    "text": "だから、阿木の火花で可能なこと、不可能なこと、特に一見不可能に見えるけど可能なこと、その逆について話すのが楽しみなんだ。"
  },
  {
    "start": 105850,
    "end": 114462,
    "text": "その方向性に沿って、起こるかもしれないし、起こらないかもしれない未来を想像することから始めよう。"
  },
  {
    "start": 114596,
    "end": 122574,
    "text": "すべてを解決して、ビーチに行く以外にすることがなくなったら、新しい2050年も実現しないかもしれない。"
  },
  {
    "start": 122702,
    "end": 128600,
    "text": "もしまだあるとしたら、それはどんなものだろう？"
  },
  {
    "start": 129130,
    "end": 134882,
    "text": "GPUを使いすぎて、メタバースに移行しなければならないのかもしれない。"
  },
  {
    "start": 134946,
    "end": 141370,
    "text": "気候変動のせいで、誰も旅行しなくなった。"
  },
  {
    "start": 141790,
    "end": 144926,
    "text": "会場は2050年に初めて火星となる。"
  },
  {
    "start": 144928,
    "end": 151930,
    "text": "その会議の中で、フーゾット・エンジニアリングをもっと見たいと思いますか？"
  },
  {
    "start": 152830,
    "end": 158894,
    "text": "その時のスケーリングロスはどうなるのか、どんなアプリケーションに取り組むのか。"
  },
  {
    "start": 159092,
    "end": 160186,
    "text": "私は本当にそう願っている。"
  },
  {
    "start": 160298,
    "end": 165440,
    "text": "配管工事はAGIにとって最も難しい課題のひとつらしい。"
  },
  {
    "start": 165970,
    "end": 176950,
    "text": "もしかしたら、AIが論文を書き始め、論文の筆頭著者である人間に与えられる最優秀論文賞とは別に、最優秀人間論文賞が設けられるかもしれない。"
  },
  {
    "start": 177020,
    "end": 181270,
    "text": "もしかしたら、チャチPTと人間のパネルがあるかもしれない。"
  },
  {
    "start": 182090,
    "end": 194860,
    "text": "おそらくその会議では、実際、チャチPTが論文を書き、また論文のレビューもする。"
  },
  {
    "start": 195230,
    "end": 200374,
    "text": "ディフュージョンがあなたのスライドデッキを作成し、ネルフがあなたの講演を発表します。"
  },
  {
    "start": 200512,
    "end": 203134,
    "text": "それなら、わざわざ講演に行く必要はないだろう？"
  },
  {
    "start": 203172,
    "end": 205790,
    "text": "チャチPTがその話を要約してくれるからだ。"
  },
  {
    "start": 205940,
    "end": 209630,
    "text": "もしかしたら、これはすべて全くのナンセンスかもしれない。"
  },
  {
    "start": 210130,
    "end": 220894,
    "text": "もし量子コンピューターが実現し、QPTが1に近いパープレキシティを持つようになったら、その時点で、私たちはビーチに行くことになるかもしれない。"
  },
  {
    "start": 221022,
    "end": 236630,
    "text": "AGIがすぐそこまで来ているのかどうか、ドク・レベルの具現化されたAIが完成したのかどうか、コンポジションが本当に存在するのかどうか、まだ議論している可能性もある。"
  },
  {
    "start": 236780,
    "end": 245878,
    "text": "というわけで、このくだらない話をしたところで、この話はまず不可能な可能性について、そしてその逆について話すことになる。"
  },
  {
    "start": 245974,
    "end": 249420,
    "text": "それから、私たちが考えることのできるいくつかのパラドックスについて説明しよう。"
  },
  {
    "start": 250290,
    "end": 257082,
    "text": "しかし、免責事項として、私は良い答えを提供することができるよりも、より多くの質問をしてしまいそうだ。"
  },
  {
    "start": 257146,
    "end": 259406,
    "text": "あまり期待しないでください。"
  },
  {
    "start": 259588,
    "end": 271746,
    "text": "不可能を可能にするために、アギはすぐそこにあるように思える。"
  },
  {
    "start": 271928,
    "end": 275322,
    "text": "その中で、本当に不可能なことはあるのだろうか？"
  },
  {
    "start": 275406,
    "end": 280754,
    "text": "必要なのはスケーリングだけかもしれない。"
  },
  {
    "start": 280882,
    "end": 292474,
    "text": "一方では、私たちは阿木にとても近づいたと感じるが、他方では、インタッドの講演でも言及されたように、まだ理解していないことがたくさんある。"
  },
  {
    "start": 292672,
    "end": 296150,
    "text": "物理学で起こったことと少し似ているかもしれない。"
  },
  {
    "start": 296230,
    "end": 318174,
    "text": "1878年、物理学を専攻しようと考えていたマックス・プランクという学生がいたのだが、フィリップ・フォン・ジョリー教授はそうしないように勧めた。"
  },
  {
    "start": 318302,
    "end": 326050,
    "text": "私は新しいことを発見したいのではなく、既知の基本を理解したいだけなのです。"
  },
  {
    "start": 326130,
    "end": 330610,
    "text": "そして、量子物理学という新しい分野を創り上げたのである。"
  },
  {
    "start": 330770,
    "end": 348774,
    "text": "それ以来、ダークマター（暗黒物質）、生きていると同時に死んでいるかもしれないシュレディンガーの猫、波動粒子の二重性など、少なくとも物理学の専門外の人々にとっては、ほとんど直感に反するような奇妙なことがたくさんある。"
  },
  {
    "start": 348822,
    "end": 354062,
    "text": "これらはすべて、概念的に非常に奇妙なことなのだ。"
  },
  {
    "start": 354196,
    "end": 365566,
    "text": "私たちが変圧器などを理解したりしなかったりするのと同じように、知性を理解したりしなかったりする方法にも、そのようなことが起こっているのではないかと思う。"
  },
  {
    "start": 365678,
    "end": 371090,
    "text": "もっと調べてみると、直感に反する面白いことがあるかもしれない。"
  },
  {
    "start": 371430,
    "end": 374418,
    "text": "多くの疑問があるだろう。"
  },
  {
    "start": 374584,
    "end": 385622,
    "text": "私は、具現化の有無にかかわらずアジは起こりうるのか、RLHfはllmsを事実と完全に一致させることができるのか、といった疑問を投げかけることができる。"
  },
  {
    "start": 385756,
    "end": 387986,
    "text": "これに対する答えはない。"
  },
  {
    "start": 388028,
    "end": 392986,
    "text": "今回の講演では、構成性についてぼんやりと触れただけだ。"
  },
  {
    "start": 393168,
    "end": 397878,
    "text": "今回の講演は、このアーカイブに関する論文に基づいて行われる。"
  },
  {
    "start": 398054,
    "end": 401950,
    "text": "偶然にも、最初の5人の著者、共同著者はすべて女性である。"
  },
  {
    "start": 402020,
    "end": 406522,
    "text": "こんなことになるとは思っていなかったが、これも運命だったのかもしれない。"
  },
  {
    "start": 406666,
    "end": 417218,
    "text": "ともあれ、問題はAGIの火花、別名GPT4、あるいは私の好きな言語モデルは掛け算ができるのかできないのか、ということだ。"
  },
  {
    "start": 417304,
    "end": 423918,
    "text": "と聞くと、実は正しいように見えるが、少し間違っている。"
  },
  {
    "start": 424094,
    "end": 435558,
    "text": "少なくとも最初の一桁は正しいし、最後の一桁も、ちなみに普通は最初の一桁、最後の一桁は正しいんだけど、一歩一歩考えるように言わなきゃいけないらしいね？"
  },
  {
    "start": 435644,
    "end": 439930,
    "text": "今は違う数字になっているが、それも少し間違っている。"
  },
  {
    "start": 440510,
    "end": 451278,
    "text": "これを体系的に尋ねると、GPT4は3桁と3桁の掛け算ができる。"
  },
  {
    "start": 451444,
    "end": 456394,
    "text": "見方によっては、これは度肝を抜かれるようなことだ。"
  },
  {
    "start": 456442,
    "end": 456798,
    "text": "いいね。"
  },
  {
    "start": 456884,
    "end": 461630,
    "text": "それとも、予想より少し悪いのだろうか？"
  },
  {
    "start": 462690,
    "end": 466130,
    "text": "4桁×4桁の掛け算は何だと思う？"
  },
  {
    "start": 469030,
    "end": 476450,
    "text": "これより高いと、もっと練習量が増える可能性がある。"
  },
  {
    "start": 478970,
    "end": 480390,
    "text": "どうやったんだろう？"
  },
  {
    "start": 480540,
    "end": 482262,
    "text": "実際には、もっと増やすことができる。"
  },
  {
    "start": 482316,
    "end": 492794,
    "text": "もっとスクラッチパッドをすれば、声に出して考えることができるようになり、思考の推論経路が格段に良くなることをお見せしよう。"
  },
  {
    "start": 492832,
    "end": 506154,
    "text": "数字をお見せしますが、そうしなくても、直接尋ねるのと同じように、4×4の正確さは4％、5×5の掛け算の正確さは0％であることがわかります。"
  },
  {
    "start": 506202,
    "end": 507546,
    "text": "またしても人間によるものだ。"
  },
  {
    "start": 507658,
    "end": 508734,
    "text": "これもできない。"
  },
  {
    "start": 508772,
    "end": 510510,
    "text": "それを正当化できるかもしれない。"
  },
  {
    "start": 510580,
    "end": 511440,
    "text": "大丈夫だよ。"
  },
  {
    "start": 511810,
    "end": 513360,
    "text": "私たちは多くを求めすぎている。"
  },
  {
    "start": 513970,
    "end": 518750,
    "text": "なぜ掛け算にこだわるのか？"
  },
  {
    "start": 518910,
    "end": 524850,
    "text": "この論文では、動的計画問題とアインシュタイン正論理問題にも注目している。"
  },
  {
    "start": 525000,
    "end": 535720,
    "text": "というのも、掛け算は構成的な多段階推論を必要とするからです。"
  },
  {
    "start": 536570,
    "end": 549898,
    "text": "ある意味、ただ興味があるだけなんだ。特に、GPT4位の言語能力のすごさや、自分の能力よりもはるかに優れた方法で文章を作成できるのを見てね。"
  },
  {
    "start": 549984,
    "end": 553818,
    "text": "境界線がどこにあるのかを理解するのは、とても魅力的なことだ。"
  },
  {
    "start": 553904,
    "end": 557358,
    "text": "要するに、掛け算は面白いかもしれない。"
  },
  {
    "start": 557444,
    "end": 569442,
    "text": "とはいえ、まさか自分が掛け算に焦点を当てた論文を書く日が来るなんて、想像もしていなかった。"
  },
  {
    "start": 569496,
    "end": 576278,
    "text": "ケビン・マーフィーは、この論文は掛け算の話であるにもかかわらず、非常に気に入っていると語った。"
  },
  {
    "start": 576444,
    "end": 583906,
    "text": "もう1つ質問だが、GPTの4人は掛け算の例を十分に見ていないのかもしれない。"
  },
  {
    "start": 584018,
    "end": 586886,
    "text": "監督付きトレーニングをしたらどうなるのか？"
  },
  {
    "start": 587068,
    "end": 595206,
    "text": "このAPIはOpenAIによって提供されていないため、GPT 4スケールの教師あり学習はできません。"
  },
  {
    "start": 595318,
    "end": 603530,
    "text": "GPT-3ダヴィンチモデルにはAPIがあり、お金を払って監視付きトレーニングを行うことができる。"
  },
  {
    "start": 603600,
    "end": 610958,
    "text": "GPT-3ダヴィンチは、この例の80％にあたる180万でトレーニングした。"
  },
  {
    "start": 611044,
    "end": 624146,
    "text": "180万例には、1桁×1、2桁×1、2桁×2、3桁×1、3桁×2、4×2までのすべての組み合わせが含まれる。"
  },
  {
    "start": 624328,
    "end": 630440,
    "text": "そして、それをランダムに80％のトレーニングと10％の開発用とテスト用に分けた。"
  },
  {
    "start": 630810,
    "end": 631862,
    "text": "どう思う？"
  },
  {
    "start": 631916,
    "end": 633622,
    "text": "これがどれほどのものなのか？"
  },
  {
    "start": 633756,
    "end": 636786,
    "text": "これは、本当に多くの例に対する教師ありのトレーニングである。"
  },
  {
    "start": 636898,
    "end": 642780,
    "text": "手を挙げてみてください。トランスフォーマーならできるはずだと思う人は何人いますか？"
  },
  {
    "start": 644430,
    "end": 646102,
    "text": "例はたくさんある。"
  },
  {
    "start": 646166,
    "end": 650846,
    "text": "つまり、テストがいかに優れているかという意味で、できるはずなんだ。"
  },
  {
    "start": 650868,
    "end": 651454,
    "text": "4×4だ。"
  },
  {
    "start": 651492,
    "end": 658480,
    "text": "ここでも、4×1や4×2といった数字がテストに含まれるかもしれない。"
  },
  {
    "start": 659330,
    "end": 661390,
    "text": "4×1は些細なことだろう。"
  },
  {
    "start": 661970,
    "end": 666626,
    "text": "今は流通のテストケースだ。"
  },
  {
    "start": 666808,
    "end": 670370,
    "text": "実際、私たちはどう考えていいのかわからなかった。"
  },
  {
    "start": 670520,
    "end": 676438,
    "text": "少なくとも、これはできるに違いないと信じられる一方で、よくわからないという気持ちもあった。"
  },
  {
    "start": 676524,
    "end": 685670,
    "text": "いずれにせよ、これを訓練してドメイン内のケースをうまくこなすのは、実はあまり簡単ではない。"
  },
  {
    "start": 686010,
    "end": 689050,
    "text": "もちろん、ドメインケースから外れると、さらに悪くなる。"
  },
  {
    "start": 690430,
    "end": 698694,
    "text": "トークン化に問題があるのかもしれない。"
  },
  {
    "start": 698742,
    "end": 702810,
    "text": "GPT-2をゼロからトレーニングするとしたら？"
  },
  {
    "start": 702890,
    "end": 703774,
    "text": "私たちにはできない。"
  },
  {
    "start": 703812,
    "end": 712666,
    "text": "ところで、GPT-3スケールのモデルをゼロからトレーニングするのに十分なGpusをアカデミックは持っていない。"
  },
  {
    "start": 712858,
    "end": 720740,
    "text": "英単語やそれ以上のものは気にせず、数字であるトークンだけを気にすればいい、とだけ言っておこう。"
  },
  {
    "start": 721110,
    "end": 723358,
    "text": "トークンは10枚しかない。"
  },
  {
    "start": 723454,
    "end": 724562,
    "text": "そうしたらどうなる？"
  },
  {
    "start": 724616,
    "end": 726566,
    "text": "これはトレーニングになると思いますか、それともならないと思いますか？"
  },
  {
    "start": 726668,
    "end": 729090,
    "text": "ただの掛け算？"
  },
  {
    "start": 729170,
    "end": 731842,
    "text": "個人的には何を期待していいのかわからなかった。"
  },
  {
    "start": 731906,
    "end": 735714,
    "text": "もしかしたら、握手をする人もいるかもしれないが、そんなことはあり得ないはずだ。"
  },
  {
    "start": 735842,
    "end": 742534,
    "text": "GBTを10個のトークンだけで優秀に訓練するのは非常に難しいことがわかった。"
  },
  {
    "start": 742662,
    "end": 744198,
    "text": "精度は10％程度だ。"
  },
  {
    "start": 744214,
    "end": 749002,
    "text": "言語モデルがどのように事前訓練されているかについては、興味深いことがある。"
  },
  {
    "start": 749056,
    "end": 756390,
    "text": "トークン化が最適でなかった可能性があるにもかかわらず、言語はより良いパフォーマンスを見せているようだ。"
  },
  {
    "start": 756560,
    "end": 760922,
    "text": "これらの例で、あなたはプロンプトの直後に答えを得ようとしていますか？"
  },
  {
    "start": 760986,
    "end": 762062,
    "text": "当面はそうだ。"
  },
  {
    "start": 762116,
    "end": 770482,
    "text": "後ほど、もうひとつのケースもお見せしよう。ご存知のように、思考の連鎖は重要だからだ。"
  },
  {
    "start": 770616,
    "end": 774180,
    "text": "いずれにせよ、トークン化自体は問題ではないかもしれない。"
  },
  {
    "start": 775190,
    "end": 778398,
    "text": "よし、このスクラッチパッドを見たかったんだね。"
  },
  {
    "start": 778494,
    "end": 789082,
    "text": "できる限りいろいろなスクラッチパッドを試してみたが、GPT4はスクラッチパッドの方がはるかに良いことがわかった。"
  },
  {
    "start": 789216,
    "end": 793114,
    "text": "3桁×3桁で、92％の出来だ。"
  },
  {
    "start": 793152,
    "end": 798140,
    "text": "まだ完璧ではないが、確実に良くなっている。"
  },
  {
    "start": 799230,
    "end": 805546,
    "text": "問題は、それが少し長い桁にも当てはまるかどうかだ。"
  },
  {
    "start": 805578,
    "end": 807934,
    "text": "しかし、実際はそうではない。"
  },
  {
    "start": 808132,
    "end": 812010,
    "text": "精度は長い桁で4％まで低下する。"
  },
  {
    "start": 812090,
    "end": 816082,
    "text": "つまり、もっと悪くなることは想像できるだろうが、そんなことはない。"
  },
  {
    "start": 816136,
    "end": 816740,
    "text": "悪いことだ。"
  },
  {
    "start": 817190,
    "end": 821060,
    "text": "この結果は、推測するまでもないことだと思う。"
  },
  {
    "start": 822390,
    "end": 829974,
    "text": "もしあなたが懐疑的なら、それについて監督付きのトレーニングを受けなければならない。"
  },
  {
    "start": 830172,
    "end": 854814,
    "text": "というのも、トランスフォーマーのようなメッセージは、マルチステップアルゴリズムを通して文脈の中で学習することができ、どんな問題サイズにも一般化できるからだ。"
  },
  {
    "start": 854852,
    "end": 864766,
    "text": "これは、プロンプト・エンジニアリングの文脈を利用したアルゴリズム推論の教え方を示す、そのような例のひとつである。"
  },
  {
    "start": 864878,
    "end": 881350,
    "text": "私たちはその論文を開いたが、彼らの結果はある意味で私たちの結果と似ている。"
  },
  {
    "start": 881930,
    "end": 883846,
    "text": "まず第一に、100％ではない。"
  },
  {
    "start": 883948,
    "end": 889000,
    "text": "私が掛け算を8割上手にできると言っても、あまり感心してもらえないだろう。"
  },
  {
    "start": 889690,
    "end": 893434,
    "text": "しかも、この掛け算、紙一重なんだ。"
  },
  {
    "start": 893552,
    "end": 898518,
    "text": "また、GPT4は3桁を超えるとあまり良くないこともわかった。"
  },
  {
    "start": 898614,
    "end": 906062,
    "text": "つまり、長い桁を3つずつ、あるいはそれ以下に分解し、それらを合計するのだ。"
  },
  {
    "start": 906116,
    "end": 911022,
    "text": "実際に4×4を一度にできるのとはちょっと違う。"
  },
  {
    "start": 911156,
    "end": 917522,
    "text": "解剖は必要だが、結局はそれが正しいやり方なのかもしれない。"
  },
  {
    "start": 917576,
    "end": 936630,
    "text": "何が可能で何が不可能かを理解する目的で、私たちは、少なくとも私は、言語ベースの一般化がいかにすごいかを考えると、その数字が私の予想よりも低いことに戸惑い、あるいは驚いた。"
  },
  {
    "start": 937050,
    "end": 947580,
    "text": "それで、GPT-3のスクラッチパッドで監視付きトレーニングをすることにしたんだ。"
  },
  {
    "start": 949150,
    "end": 956142,
    "text": "5人×5人のデューデットをやろうとすると、とても高くつく。"
  },
  {
    "start": 956276,
    "end": 958720,
    "text": "それぐらいしか使えないよ。"
  },
  {
    "start": 961490,
    "end": 964086,
    "text": "だから、興味本位でやってみたんだ。"
  },
  {
    "start": 964138,
    "end": 966290,
    "text": "本当によく動くんだ。"
  },
  {
    "start": 966360,
    "end": 975474,
    "text": "ご想像の通り、無分布では96％の精度ですが、ドメイン外ではあまり一般化されません。"
  },
  {
    "start": 975522,
    "end": 978306,
    "text": "つまり、ここでは桁数の長い掛け算という意味だ。"
  },
  {
    "start": 978418,
    "end": 979954,
    "text": "スクラッチパッドとはどういう意味ですか？"
  },
  {
    "start": 980082,
    "end": 1001742,
    "text": "ああ、スクラッチパッドは思考の連鎖に似ているが、より精巧なバージョンで、この種のものは、私たちが提供するいくつかのショット例として、ステップバイステップの推論を行う方法を正確に示している。"
  },
  {
    "start": 1001876,
    "end": 1007950,
    "text": "問題は、この例は桁数が長くなると少し長くなりすぎるということだ。"
  },
  {
    "start": 1008290,
    "end": 1015906,
    "text": "はっきりさせておきたいのですが、このスライドでは、文脈内の例はクエリーと同じですが、数字が1つずれています。"
  },
  {
    "start": 1015928,
    "end": 1019010,
    "text": "ずっとそうだったのか、それともスライドに書いてあったことなのか？"
  },
  {
    "start": 1019350,
    "end": 1020562,
    "text": "私が何を言いたいか分かる？"
  },
  {
    "start": 1020696,
    "end": 1021700,
    "text": "私が何をしたのか。"
  },
  {
    "start": 1023510,
    "end": 1028742,
    "text": "あなたが文脈で提供している例は999-5866で、それから9を要求している。"
  },
  {
    "start": 1028796,
    "end": 1032598,
    "text": "いやいや、これはたまたまだよ。"
  },
  {
    "start": 1032764,
    "end": 1034230,
    "text": "ランダムにやるんだ。"
  },
  {
    "start": 1038570,
    "end": 1041580,
    "text": "さて、私はどこにいたっけ？"
  },
  {
    "start": 1045790,
    "end": 1046202,
    "text": "そうだね。"
  },
  {
    "start": 1046256,
    "end": 1047802,
    "text": "なぜこのようなことが起こるのか？"
  },
  {
    "start": 1047856,
    "end": 1048890,
    "text": "不思議なことだ。"
  },
  {
    "start": 1050590,
    "end": 1057658,
    "text": "いずれにせよ、この論文をアーカイブ化した後、私たちはグロッキングについても考えた。"
  },
  {
    "start": 1057754,
    "end": 1058890,
    "text": "グロッキングは？"
  },
  {
    "start": 1059050,
    "end": 1063860,
    "text": "まず第一に、OpenAIにもっとお金を払い続けたらどうなるかは誰にもわからない。"
  },
  {
    "start": 1065270,
    "end": 1070594,
    "text": "私たちが喜んで支払う範囲では、まだそのようなことは起こっていない。"
  },
  {
    "start": 1070712,
    "end": 1081286,
    "text": "グループ分けが起こるはずだったが起こらなかったのか、それともまったく起こらなかったのかをどうやって知るのか？"
  },
  {
    "start": 1081388,
    "end": 1083160,
    "text": "もしそうなら、なぜですか？"
  },
  {
    "start": 1084010,
    "end": 1087930,
    "text": "掛け算は国語の問題よりも根本的に難しい。"
  },
  {
    "start": 1088000,
    "end": 1091690,
    "text": "つまり、それについては憶測の域を出ないが、それはさておき。"
  },
  {
    "start": 1092350,
    "end": 1108240,
    "text": "一般的に、少なくとも掛け算や、我々が実験したような動的計画法のアルゴリズムには、根本的な限界があるようだ。"
  },
  {
    "start": 1108690,
    "end": 1116034,
    "text": "ところで、これらのモデル、つまりトランスフォーマーがどのようなエラーを起こすかについて、もう少し詳しく説明しよう。"
  },
  {
    "start": 1116152,
    "end": 1121054,
    "text": "計算グラフを通して乗算を考えることができる。"
  },
  {
    "start": 1121102,
    "end": 1133810,
    "text": "2桁と1桁の掛け算のような単純なものでも、2つの数字の掛け算をして、それをどこかに保存する。"
  },
  {
    "start": 1133890,
    "end": 1142666,
    "text": "そして、何かを運ぶ必要があれば、桁を正しく運ぶことを確認し、最終的にこうなる。"
  },
  {
    "start": 1142848,
    "end": 1154142,
    "text": "この計算グラフを描くことの利点は、トレーニングデータとテストデータで何が正確にトレーニングされたかを実際にトレースできることだ。"
  },
  {
    "start": 1154196,
    "end": 1163394,
    "text": "どの言語モデルを解剖するかによって、私がその学習データに直接アクセスできるかどうかが決まる言語問題とは違う。"
  },
  {
    "start": 1163432,
    "end": 1164894,
    "text": "推測するしかない。"
  },
  {
    "start": 1165022,
    "end": 1172290,
    "text": "この場合、少なくともゼロからトレーニングするのであれば、トレーニングデータにフルアクセスすることができる。"
  },
  {
    "start": 1172710,
    "end": 1174686,
    "text": "それが助けになる。"
  },
  {
    "start": 1174718,
    "end": 1187474,
    "text": "一般的に、計算グラフの幅と深さはもちろん異なり、性能はそのグラフの複雑さに正確に比例することがわかった。"
  },
  {
    "start": 1187522,
    "end": 1199986,
    "text": "例えば、1000と2000を掛け算する場合、実際には多段階の推論をしているわけではなく、暗記したショートカットパターンに基づいて数字のコピー＆ペーストをしているだけだから簡単だ。"
  },
  {
    "start": 1200118,
    "end": 1225318,
    "text": "私たちは、掛け算が起こるとき、トランスフォーマーは本当の意味での多段階推論をしているのではなく、トレーニングデータで何度も繰り返された部分グラフを認識し、それを最終的な答えとして呼び出すことができる、ということを発見した。"
  },
  {
    "start": 1225404,
    "end": 1236120,
    "text": "ある部分グラフが何回起こるかを示すグラフがたくさんあるのは、その部分の計算が実際に正しいかどうかということと関係がある。"
  },
  {
    "start": 1237050,
    "end": 1240954,
    "text": "つまり、人間も掛け算の一部を実際にこの方法で行っているのかもしれない。"
  },
  {
    "start": 1240992,
    "end": 1241798,
    "text": "誰が知っている？"
  },
  {
    "start": 1241894,
    "end": 1243846,
    "text": "このエラーはスキップしようと思う。"
  },
  {
    "start": 1243878,
    "end": 1247466,
    "text": "あなたが見たい以上の詳細を入力する。"
  },
  {
    "start": 1247568,
    "end": 1249722,
    "text": "新聞にそのことを書いただけだ。"
  },
  {
    "start": 1249776,
    "end": 1264610,
    "text": "また、この付録では、設計上、組成の深さを増すと急速に減衰するという理論や証明をスケッチしようと試みている。"
  },
  {
    "start": 1265270,
    "end": 1273138,
    "text": "実際、このような多くの過去の論文が、さまざまな形でそれを実証している。"
  },
  {
    "start": 1273224,
    "end": 1297702,
    "text": "大体において、彼らの先行研究はGPT-2のような弱い言語モデルに基づいていた。そのため、GPT4のようなスケールの大きなモデルには何か魔法のような違いがあるのだろうかと、人々は考えている。"
  },
  {
    "start": 1297766,
    "end": 1307658,
    "text": "そのため、少なくとも経験的には、gpuコンピュート数を増やす余裕はないものの、境界をさらに押し広げられるかどうかを確かめたかった。"
  },
  {
    "start": 1307674,
    "end": 1312110,
    "text": "もし誰かがこの問題をさらに先送りにしようとしたら、どうなるかは誰にもわからない。"
  },
  {
    "start": 1312260,
    "end": 1322180,
    "text": "とにかく、そうだ、代わりにこう質問したらどうなるか、おおよその答えを教えてくれ。"
  },
  {
    "start": 1322790,
    "end": 1326658,
    "text": "4桁×4桁または5桁、これが新しい計算方法だ。"
  },
  {
    "start": 1326754,
    "end": 1339002,
    "text": "ええ、すでにすべての桁で近似値を計算していますし、最後の桁はより良くなっています。"
  },
  {
    "start": 1339056,
    "end": 1343210,
    "text": "また、正解に必要な実際の桁数を知っている。"
  },
  {
    "start": 1343280,
    "end": 1346874,
    "text": "近似値というものは、それほどうまくいくものなのだ。"
  },
  {
    "start": 1346912,
    "end": 1350178,
    "text": "包括的な分析はしていない。"
  },
  {
    "start": 1350294,
    "end": 1361614,
    "text": "例えば、間違った答えを出したとき、それは本当に、よし、ゼロにして近似値を出して、どんな答えが出るか見てみよう、ということだったのだろうか？"
  },
  {
    "start": 1361732,
    "end": 1364980,
    "text": "ああ、おそらくもっとうまくいくだろう。"
  },
  {
    "start": 1367350,
    "end": 1369106,
    "text": "もっとうまくいくだろう。"
  },
  {
    "start": 1369128,
    "end": 1372306,
    "text": "もちろん、計算機を与えれば、もっとうまくいく。"
  },
  {
    "start": 1372408,
    "end": 1373714,
    "text": "それはみんな知っている。"
  },
  {
    "start": 1373912,
    "end": 1376022,
    "text": "それこそがポイントなのかもしれないが。"
  },
  {
    "start": 1376076,
    "end": 1387798,
    "text": "となると、不思議なのは、掛け算はただ単に奇妙なケースのようなものなのか、それとも言語理解や常識の他の側面があるのか、ということだ。"
  },
  {
    "start": 1387964,
    "end": 1392060,
    "text": "この常識的な問題については、また後で触れる。"
  },
  {
    "start": 1393630,
    "end": 1406730,
    "text": "後で常識に奇妙な例えをするつもりだが、今はちょっと、もしかしたら不思議に思うかもしれないが、これは単なる奇妙なエッジケースなのだろうか？"
  },
  {
    "start": 1406810,
    "end": 1411902,
    "text": "それ以外の言語ベースの問題については、どうでもよいことだ。"
  },
  {
    "start": 1411956,
    "end": 1414658,
    "text": "私はただ、その質問を投げかけたかっただけだ。"
  },
  {
    "start": 1414744,
    "end": 1423854,
    "text": "確かな答えは分からないが、なぜ掛け算の方が難しいのか、また他の構成的な課題があるのかが気になる。"
  },
  {
    "start": 1423902,
    "end": 1425060,
    "text": "それがわかった。"
  },
  {
    "start": 1425530,
    "end": 1438454,
    "text": "AI twoの私の同僚であるアシーシュと、ニューヨーク大学の博士課程の学生であるウィル・モレルは、理論的な観点からこのことについて考えてきた。"
  },
  {
    "start": 1438582,
    "end": 1444966,
    "text": "つまり、トランスフォーマーにはせいぜい見た目と正確さくらいしかないという前提だ。"
  },
  {
    "start": 1444998,
    "end": 1454090,
    "text": "ところで、もしあなたの変圧器が無限の精度を持つなら、それはチューリング・コンプリートだが、現実には無限の精度はない。"
  },
  {
    "start": 1454170,
    "end": 1459258,
    "text": "有限精度と可変長精度のどちらかである。"
  },
  {
    "start": 1459274,
    "end": 1468770,
    "text": "いずれにせよ、定理証明の目的のために、最大でもlog n（nはコンテキストの長さ）であると仮定している。"
  },
  {
    "start": 1469590,
    "end": 1505722,
    "text": "そこで、重みが完全であるなどと仮定して、異なる複雑さのクラスを見て、推論なしにチャンネルなしでやるなら、エンコーダのみかデコーダのみかのどちらかであるが、推論なしにチャンネルなしでやることを許可しなければ、このTCゼロクラスの中に、グラフの連結性、線形不等式などの一見単純な滑車時間の問題の多くが含まれていないようなものであるかもしれないという結論に達した。"
  },
  {
    "start": 1505866,
    "end": 1516814,
    "text": "彼らのフォローアップ作業では、それはまだかもしれないし、すぐに出てくるかもしれない。"
  },
  {
    "start": 1516862,
    "end": 1523042,
    "text": "驚くなかれ、それは能力を大きく向上させる。"
  },
  {
    "start": 1523096,
    "end": 1536802,
    "text": "しかし、先ほどインテルが述べたように、プランニングは実現可能な範囲外である。"
  },
  {
    "start": 1536866,
    "end": 1540506,
    "text": "これは、私が理解しているように、上限の結果のようなものだ。"
  },
  {
    "start": 1540608,
    "end": 1543020,
    "text": "もしかしたら、もっと考えることがあるかもしれない。"
  },
  {
    "start": 1543630,
    "end": 1554638,
    "text": "彼らの推測に基づくと、掛け算が可能であったはずかどうかはあまりはっきりしない。"
  },
  {
    "start": 1554804,
    "end": 1558894,
    "text": "あるいは、もっと理論的な縛りがあるのかもしれない。"
  },
  {
    "start": 1559012,
    "end": 1563514,
    "text": "もう一度言うが、私は答えを出すことができるよりも多くの質問をしているだけだ。"
  },
  {
    "start": 1563642,
    "end": 1566222,
    "text": "少しネガティブだったかもしれない。"
  },
  {
    "start": 1566286,
    "end": 1567934,
    "text": "もっと前向きな話をしよう。"
  },
  {
    "start": 1567982,
    "end": 1569726,
    "text": "待って、ごめん、混乱してる。"
  },
  {
    "start": 1569758,
    "end": 1575134,
    "text": "掛け算はNC1にもあるし、TCゼロにもあるよね？"
  },
  {
    "start": 1575272,
    "end": 1579986,
    "text": "特に掛け算に興味があるのであれば、このような非常に低いクラスになるだろう。"
  },
  {
    "start": 1580098,
    "end": 1583510,
    "text": "いや、彼女はビット数に少し制限があったんだ。"
  },
  {
    "start": 1584330,
    "end": 1585880,
    "text": "ああ、でも入っている。"
  },
  {
    "start": 1587610,
    "end": 1593658,
    "text": "さて、では具体的な数字を突っ込み、それでは不十分だと言うことにしよう。"
  },
  {
    "start": 1593824,
    "end": 1596220,
    "text": "つまり、モデルのサイズがそうなんだ。"
  },
  {
    "start": 1596590,
    "end": 1602406,
    "text": "大規模な言語モデルについて話すなら、そこには何十億ものニューロンがある。"
  },
  {
    "start": 1602448,
    "end": 1609514,
    "text": "nはトークンの数、コンテキストのサイズ、アーキテクチャのサイズである。"
  },
  {
    "start": 1609562,
    "end": 1622018,
    "text": "というのも、私たちの実験では、明らかに大きなモデルの方が能力が高い。"
  },
  {
    "start": 1622104,
    "end": 1623634,
    "text": "そう、それがもうひとつのポイントだ。"
  },
  {
    "start": 1623672,
    "end": 1627430,
    "text": "実際には、私たちは絆以上に多くのレイヤーを持っている。"
  },
  {
    "start": 1627930,
    "end": 1634610,
    "text": "私たちはもっと多くの層を持っているし、ログの精度が弱すぎるように思える。"
  },
  {
    "start": 1634770,
    "end": 1640854,
    "text": "ああ、いろいろな説がある。"
  },
  {
    "start": 1640982,
    "end": 1653360,
    "text": "私たちが直面している経験的な現象に対して、ある仮定がすぐに関連するかどうかわからないと感じることもあるが、頭のいい人たちがここで何かを見つけ出してくれることを期待している。"
  },
  {
    "start": 1654770,
    "end": 1663730,
    "text": "明らかに、LLMの失敗を、文字どおりそのサイズ内でこのアルゴリズムを表現できないと言うことで説明できる問題がある。"
  },
  {
    "start": 1663800,
    "end": 1666594,
    "text": "ただ、整数の掛け算はそのうちのひとつではないということだ。"
  },
  {
    "start": 1666712,
    "end": 1667380,
    "text": "そうだね。"
  },
  {
    "start": 1668710,
    "end": 1672382,
    "text": "とにかく、もっと前向きな話をしよう。"
  },
  {
    "start": 1672526,
    "end": 1680120,
    "text": "次のトークは、不可能に見えるかもしれないが、可能かもしれないことについてだ。"
  },
  {
    "start": 1680650,
    "end": 1696650,
    "text": "サム・アルトマンは今年初め、ラジャン・アナンダッドにインドの新興企業がインドのための財団モデルを作るにはどうしたらいいかと聞かれ、それは絶望的だと答えた。"
  },
  {
    "start": 1698670,
    "end": 1699820,
    "text": "まず第一に。"
  },
  {
    "start": 1700190,
    "end": 1704526,
    "text": "ああ、それは真実だし、学問の世界でも同じことだ。"
  },
  {
    "start": 1704548,
    "end": 1706986,
    "text": "計算量が少ないと絶望的だ。"
  },
  {
    "start": 1707098,
    "end": 1715140,
    "text": "インドの新興企業の意欲を削がないことを願うし、学術界が他に何ができるかを考える意欲を削がないことを願う。"
  },
  {
    "start": 1715910,
    "end": 1720382,
    "text": "その意味で、次の質問は不可能か絶望的に見えるだろう。"
  },
  {
    "start": 1720446,
    "end": 1724542,
    "text": "絶望的な蒸留、あるいは不可能な蒸留とでも言おうか。"
  },
  {
    "start": 1724606,
    "end": 1730290,
    "text": "ここで私たちがやろうとしているのは、ちょっとした思考実験のようなものだ。"
  },
  {
    "start": 1730450,
    "end": 1737906,
    "text": "今、私たち全員が知っているように、勝利のレシピは極端なスケールのプレトレーニングであり、それに続くスケールのRLHFである。"
  },
  {
    "start": 1738018,
    "end": 1740730,
    "text": "そうすれば、素晴らしい結果が得られる。"
  },
  {
    "start": 1741070,
    "end": 1743322,
    "text": "次のような質問をしたらどうだろう？"
  },
  {
    "start": 1743376,
    "end": 1752430,
    "text": "GPT-2という低品質の小型モデルから始めて、どうにかして高品質の小型モデルを焼き上げるとしたら？"
  },
  {
    "start": 1752580,
    "end": 1764430,
    "text": "そんなことが可能なのかわからないが、とりあえず仮にそう仮定して、桁違いの大きさのGPT-3と競争してみよう。"
  },
  {
    "start": 1766230,
    "end": 1768290,
    "text": "これが不可能に見えることに同意しますか？"
  },
  {
    "start": 1769830,
    "end": 1777766,
    "text": "プロプライエタリなllmからの模倣も絶望的だと言われているのに、どうしてそんなことが可能なんだ？"
  },
  {
    "start": 1777788,
    "end": 1786178,
    "text": "このバークレー校の論文によれば、独自のLLMSを模倣することの可能性は非常に大きいという。"
  },
  {
    "start": 1786274,
    "end": 1800220,
    "text": "その要点は、一般的な目的のために、一般的な目的のモデルを構築するためには、より大きなモデルから知識を抽出するだけでは十分ではないかもしれないということだ。"
  },
  {
    "start": 1800590,
    "end": 1803242,
    "text": "大型モデルの方がいいに決まっている。"
  },
  {
    "start": 1803386,
    "end": 1815646,
    "text": "また、この論文で検討された特定のタスクの選択について、このような小さなモデルが完全にリーグから外れてしまうような影響を受けやすい何かがあるのかもしれない。"
  },
  {
    "start": 1815758,
    "end": 1828742,
    "text": "さっきのインタットの話も含めて、もっと良いデータや何かでもっと小さなモデルが作れるという論文もある。"
  },
  {
    "start": 1828876,
    "end": 1835746,
    "text": "少なくとも、タスクに特化したモデルであれば、大型の相手に対して競争力を持つことができる。"
  },
  {
    "start": 1835858,
    "end": 1841260,
    "text": "それが、今回の講演で掘り下げる部分だ。"
  },
  {
    "start": 1841630,
    "end": 1849786,
    "text": "特に、文章要約の意味での抽象化学習をやってみるつもりだ。"
  },
  {
    "start": 1849888,
    "end": 1866660,
    "text": "個人的にそうであるように、人間は学んだり読んだりしたことを抽象化するのがとても上手なのに対して、GBD4は膨大な数のトークンを一度に追加することができるという事実に魅了されている。"
  },
  {
    "start": 1867110,
    "end": 1875854,
    "text": "でも、人間の知性というのは、純粋にすぐに抽象化してしまうものなんだ。"
  },
  {
    "start": 1875992,
    "end": 1882406,
    "text": "要するに、私たちはこの要約問題を特に考えることに興奮したのだ。"
  },
  {
    "start": 1882508,
    "end": 1886150,
    "text": "私たちの挑戦の枠組みは次のようなものだ。"
  },
  {
    "start": 1886810,
    "end": 1889382,
    "text": "極端なスケールの事前トレーニングに頼るつもりはない。"
  },
  {
    "start": 1889436,
    "end": 1892454,
    "text": "規模に頼るつもりはない。"
  },
  {
    "start": 1892502,
    "end": 1897354,
    "text": "我々は、要約のために監視されたデータセットに頼るつもりもない。"
  },
  {
    "start": 1897472,
    "end": 1906714,
    "text": "ところで、もしあなたがNLPの分野にいるのなら、要約には大規模な教師あり学習データかRLHFが必要だ。"
  },
  {
    "start": 1906762,
    "end": 1908026,
    "text": "実際、それがPPOだ。"
  },
  {
    "start": 1908058,
    "end": 1914322,
    "text": "オリジナルのアルゴリズムは、そもそも要約タスクのために試されたものだと思う。"
  },
  {
    "start": 1914376,
    "end": 1921170,
    "text": "要約は、大規模なrlhfsまたは教師ありデータセットによってうまく機能する。"
  },
  {
    "start": 1921320,
    "end": 1925780,
    "text": "GPT4は本当に要約がうまい。"
  },
  {
    "start": 1926390,
    "end": 1931426,
    "text": "スケールアップのための事前トレーニングだけでなく、この2つがミックスされているのだろう。"
  },
  {
    "start": 1931458,
    "end": 1934390,
    "text": "それはまた別の問題だ。"
  },
  {
    "start": 1934460,
    "end": 1947446,
    "text": "このようなことを一切行わず、極端なスケールの事前トレーニングだけを行った場合、要約のような言語能力は実際に現れるのでしょうか、それとも他のステップの副産物でしかないのでしょうか？"
  },
  {
    "start": 1947478,
    "end": 1954074,
    "text": "いずれにせよ、今のところ、それを抜きにして、どうやってどこへ行こうというのか？"
  },
  {
    "start": 1954202,
    "end": 1958414,
    "text": "重要なのは、AIは訓練されたデータと同じくらい優れているということだ。"
  },
  {
    "start": 1958452,
    "end": 1968174,
    "text": "実際、これはインタが披露した「ブギーズ・オール・ユー・ニード」の実験の背景にあるレシピのようなものだ。"
  },
  {
    "start": 1968222,
    "end": 1971506,
    "text": "ただし、大きなモデルから蒸留するつもりはまったくない。"
  },
  {
    "start": 1971528,
    "end": 1973874,
    "text": "我々は、人間が書いたデータに頼るつもりはない。"
  },
  {
    "start": 1973912,
    "end": 1977254,
    "text": "GPT-3の蒸留に頼るつもりもない。"
  },
  {
    "start": 1977292,
    "end": 1983240,
    "text": "GPT-2にこだわるだけだ。十分不可能に見えると納得してもらえたかな？"
  },
  {
    "start": 1985050,
    "end": 1987282,
    "text": "不可能だと思う人がどれだけいるだろうか？"
  },
  {
    "start": 1987346,
    "end": 1988870,
    "text": "実は、ちょっと興味があるんだ。"
  },
  {
    "start": 1989930,
    "end": 1997978,
    "text": "これは合理的に不可能に思えるし、いや、おそらく私は可能な話をしているのだろう。"
  },
  {
    "start": 1998064,
    "end": 2000940,
    "text": "では、どのサイズのモデルを使うつもりですか？"
  },
  {
    "start": 2001650,
    "end": 2003034,
    "text": "億未満？"
  },
  {
    "start": 2003082,
    "end": 2005390,
    "text": "ああ、15億だ。"
  },
  {
    "start": 2005730,
    "end": 2010858,
    "text": "最大のものは、パイプラインのどの時点でもGPT-3やGBDを使用していない。"
  },
  {
    "start": 2010874,
    "end": 2011440,
    "text": "4人だ。"
  },
  {
    "start": 2011810,
    "end": 2015360,
    "text": "いや、でも他のパイプラインモデルを使うつもりだ。"
  },
  {
    "start": 2015890,
    "end": 2017646,
    "text": "ただ、一回り小さいモデルだ。"
  },
  {
    "start": 2017828,
    "end": 2019770,
    "text": "他のパイプラインが絡んでいる"
  },
  {
    "start": 2019850,
    "end": 2020646,
    "text": "アメリカだ。"
  },
  {
    "start": 2020778,
    "end": 2022146,
    "text": "まるで小さい。"
  },
  {
    "start": 2022248,
    "end": 2022658,
    "text": "そうだね。"
  },
  {
    "start": 2022744,
    "end": 2052038,
    "text": "GPT-3は、一般的な言語モデルから出発し、常識モデルを構築するためにGPT-3から抽出したものです。"
  },
  {
    "start": 2052214,
    "end": 2058142,
    "text": "そうすれば、オリジナルの教師用モデルに対してかなり競争力のある小型モデルが手に入る。"
  },
  {
    "start": 2058196,
    "end": 2062094,
    "text": "実際、私たちの場合、最終的にはGPT-3よりもうまくいった。"
  },
  {
    "start": 2062292,
    "end": 2075650,
    "text": "私は、教師が大きなモデルに依存しているという事実が好きではない。"
  },
  {
    "start": 2075720,
    "end": 2084630,
    "text": "GPT-2の出力を見ると、絶望的に見える。"
  },
  {
    "start": 2085130,
    "end": 2096838,
    "text": "この話にはまた少し戻るが、我々のゴールは質の高いタスク固有のデータセットをどうにか絞り出すことであり、そうすればあとは簡単なことだ。"
  },
  {
    "start": 2096934,
    "end": 2101774,
    "text": "ストレス・ワードの監督付きトレーニングができるから、簡単だろう。"
  },
  {
    "start": 2101972,
    "end": 2111614,
    "text": "GPT-2を使ってまとめようとすると、多くのことを試した結果、良いものは何もなかったということになる。"
  },
  {
    "start": 2111652,
    "end": 2113282,
    "text": "0％でもいいような。"
  },
  {
    "start": 2113416,
    "end": 2117250,
    "text": "要約してくださいと頼むと、適当なことをたくさん言うんだ。"
  },
  {
    "start": 2117320,
    "end": 2122642,
    "text": "何百回とやっても、ゼロがいい。"
  },
  {
    "start": 2122696,
    "end": 2123300,
    "text": "ゼロだ。"
  },
  {
    "start": 2125830,
    "end": 2130278,
    "text": "これは驚くようなことではないはずだ。"
  },
  {
    "start": 2130444,
    "end": 2138490,
    "text": "そこで私たちが行うのは、まず神経学的デコーディングと呼ばれる先行研究を用いて辞書的制約デコーディングを試みることだ。"
  },
  {
    "start": 2140750,
    "end": 2152666,
    "text": "そこで、いくつかのキーワードを特定し、GPT-2が元の入力からそれらのキーワードを含む出力を確実に生成できるかどうかを確認する。"
  },
  {
    "start": 2152778,
    "end": 2158522,
    "text": "GPT-2があまりにひどいので、一般的にはまだかなり絶望的だ。"
  },
  {
    "start": 2158666,
    "end": 2163746,
    "text": "この時点で可能性はゼロから1％程度まで高まるだろう。"
  },
  {
    "start": 2163928,
    "end": 2170340,
    "text": "正しい要約が生成される確率は、以前より少し良くなっている。"
  },
  {
    "start": 2170950,
    "end": 2171362,
    "text": "そうだね。"
  },
  {
    "start": 2171416,
    "end": 2173550,
    "text": "神経学的なデコーディングについて教えてください。"
  },
  {
    "start": 2173630,
    "end": 2174530,
    "text": "もう少し詳しく？"
  },
  {
    "start": 2174600,
    "end": 2176034,
    "text": "具体的に何をしているのか？"
  },
  {
    "start": 2176072,
    "end": 2187350,
    "text": "そのうちのひとつは、筆頭著者として学部生が書いたもので、細かいことだが、少なくとも当時はそうだった。"
  },
  {
    "start": 2187420,
    "end": 2202458,
    "text": "多様なパターンをカバーできるようにしないと、非常に狭いビームから抜け出せなくなるからだ。"
  },
  {
    "start": 2202554,
    "end": 2205278,
    "text": "それが要点だ。"
  },
  {
    "start": 2205444,
    "end": 2214798,
    "text": "この論文では、実際には完全なCNF形式、すなわち接続正規形に基づく論理制約をサポートしている。"
  },
  {
    "start": 2214974,
    "end": 2220194,
    "text": "ここでは、論理的制約そのものはそれほど必要ない。"
  },
  {
    "start": 2220232,
    "end": 2228550,
    "text": "実際、このメソッドは、私たちがここで実際に行っていることよりも多くのユースケースに対応している。"
  },
  {
    "start": 2228620,
    "end": 2239980,
    "text": "一方、私たちがここで実際に行っているのは、この超高速生成とフィルタリングの目的のために、元の論文をより高速に再実装したようなものだ。"
  },
  {
    "start": 2241230,
    "end": 2241980,
    "text": "オーケー。"
  },
  {
    "start": 2242350,
    "end": 2247098,
    "text": "すると、生徒たちは実に興味深いアイデアを思いついた。"
  },
  {
    "start": 2247184,
    "end": 2256560,
    "text": "GPT-2は、人間のテキストではなく、GPT-2自身の出力を要約するように指示すれば、よりうまく要約してくれるという直感があったのだ。"
  },
  {
    "start": 2257330,
    "end": 2264538,
    "text": "彼らは、GPT-2に大量の文章を生成させるために、いくつかの左接点を設計した。"
  },
  {
    "start": 2264634,
    "end": 2292186,
    "text": "ところで、この部分をメタセンスで要約すると、実はまだ他にもあるのですが、私たちはいろいろなことを試しています。実際にたくさんの続きを生成してみて、すでに生成されたテキスト同士のペアリングを見つけると、正しい要約のペアを発見する確率がほぼ10％まで高まることがわかりました。"
  },
  {
    "start": 2292288,
    "end": 2300306,
    "text": "絶望的なことをやろうとしているだけに、これはちょっと予想外の発見だ。"
  },
  {
    "start": 2300438,
    "end": 2303246,
    "text": "我々は偶然それを発見した。"
  },
  {
    "start": 2303348,
    "end": 2307102,
    "text": "とにかく、全体的な枠組みはこんな感じだ。"
  },
  {
    "start": 2307236,
    "end": 2319202,
    "text": "GPT-2から始めて、たくさんの要約候補を生成し、それをフィルタリングして、この要約データセットを作る。"
  },
  {
    "start": 2319256,
    "end": 2323758,
    "text": "この要約データセットが手に入れば、十分な品質と規模を持つことになる。"
  },
  {
    "start": 2323854,
    "end": 2329222,
    "text": "教師あり学習はほとんどの場合うまくいくので、あとはとても簡単だ。"
  },
  {
    "start": 2329356,
    "end": 2342666,
    "text": "というのも、要約があまり要約されていないことがあり、1つの単語を削除するだけでは十分ではないからだ。"
  },
  {
    "start": 2342848,
    "end": 2347894,
    "text": "また、より多様な言語パターンを持つための多様性フィルターもある。"
  },
  {
    "start": 2347942,
    "end": 2351642,
    "text": "最も重要なのは、このインテルマン・フィルターだ。"
  },
  {
    "start": 2351706,
    "end": 2364790,
    "text": "これは自然言語推論であり、ある文が他の文から含意されているかどうか、あるいは矛盾しているかどうか、あるいはそのいずれでもないかを教えてくれる。"
  },
  {
    "start": 2364890,
    "end": 2368734,
    "text": "要約は原文から引用されるべきである。"
  },
  {
    "start": 2368862,
    "end": 2370580,
    "text": "だから、これを使う。"
  },
  {
    "start": 2371030,
    "end": 2383074,
    "text": "この分析物フィルターは完璧ではないので、完璧とは言えないが、結果の要約が良い要約になる可能性が高いことを保証するために、積極的に使うことはできるだろう。"
  },
  {
    "start": 2383202,
    "end": 2394134,
    "text": "なぜなら、言い換えには双方向の連言が必要だからだ。"
  },
  {
    "start": 2394262,
    "end": 2400300,
    "text": "そのため、この方法でも多くの言い換えが可能で、質の高いデータセットを生成することができる。"
  },
  {
    "start": 2401330,
    "end": 2407678,
    "text": "わかったよ、矛盾とかそういうのはないんだね。"
  },
  {
    "start": 2407764,
    "end": 2416990,
    "text": "これは人間が行うか、あるいは自動で、既存のNLIモデルに基づいて行われる。"
  },
  {
    "start": 2417140,
    "end": 2419070,
    "text": "それがスーパーバイズド・モデルだ。"
  },
  {
    "start": 2419220,
    "end": 2429010,
    "text": "何万もの例を監修しているようなもので、インテルモントと言いながらインテルモントではない。"
  },
  {
    "start": 2429090,
    "end": 2431926,
    "text": "そのためのデータセットはない。"
  },
  {
    "start": 2432028,
    "end": 2436662,
    "text": "そう、そのためのデータセットがあるんだ。"
  },
  {
    "start": 2436716,
    "end": 2444602,
    "text": "SNLI、スタンフォード、NLI、マルチモーダルという有名なデータセットがある。"
  },
  {
    "start": 2444736,
    "end": 2445482,
    "text": "そうだ。"
  },
  {
    "start": 2445536,
    "end": 2458270,
    "text": "GPT-3を使って例を生成し、それを人間が検証する。"
  },
  {
    "start": 2458850,
    "end": 2460942,
    "text": "それはまた別の話だ。"
  },
  {
    "start": 2460996,
    "end": 2462634,
    "text": "上記のどれを使ってもいい。"
  },
  {
    "start": 2462682,
    "end": 2470290,
    "text": "文字を使っているが、その場合、GPT-3では正しいNLiの例を生成することができない。"
  },
  {
    "start": 2472470,
    "end": 2477640,
    "text": "GPTの4番なら可能かもしれないが、当時はあまり試していなかった。"
  },
  {
    "start": 2478250,
    "end": 2483160,
    "text": "人間が実際に座ってすべての注釈をつけなければ、ノイズが多すぎて役に立たなかった。"
  },
  {
    "start": 2483770,
    "end": 2488262,
    "text": "ここでは5つのラージサイズをトレーニングすることになる。"
  },
  {
    "start": 2488316,
    "end": 2491834,
    "text": "ちなみに、これはGPTよりもさらに小さい。"
  },
  {
    "start": 2491872,
    "end": 2495066,
    "text": "億のパラメーターモデルですらない。"
  },
  {
    "start": 2495248,
    "end": 2498086,
    "text": "実際、これを何度も繰り返すことができる。"
  },
  {
    "start": 2498198,
    "end": 2499594,
    "text": "立ち寄る理由はない。"
  },
  {
    "start": 2499632,
    "end": 2507200,
    "text": "一回繰り返せば、その生徒は次の世代の生徒の先生となり、人間の人生と同じようにこれを繰り返すことができる。"
  },
  {
    "start": 2508210,
    "end": 2512030,
    "text": "そこで、このデータセットをディムサムと名付けた。"
  },
  {
    "start": 2512630,
    "end": 2518450,
    "text": "340万ものサンプルがあり、得られたモデルをスティールに入力する。"
  },
  {
    "start": 2520790,
    "end": 2522894,
    "text": "ああ、このレッスンを売る必要がある。"
  },
  {
    "start": 2523022,
    "end": 2525146,
    "text": "修理用のスカラーフィルターはありますか？"
  },
  {
    "start": 2525198,
    "end": 2533990,
    "text": "人間が出力を見て、自分たちのデータセットでテストしているのだろうか？"
  },
  {
    "start": 2534060,
    "end": 2535990,
    "text": "ループの中に人間はいない。"
  },
  {
    "start": 2536330,
    "end": 2545290,
    "text": "つまり、人間がこのパイプラインを設計したという意味では、ループの中に人間がいるのかもしれないが、フィルターに関しては、すべて自動なんだ。"
  },
  {
    "start": 2546110,
    "end": 2552510,
    "text": "ああ、紙固有のネオノテーションは一切やっていない。"
  },
  {
    "start": 2553330,
    "end": 2555950,
    "text": "写真の右側にフィルターはありますか？"
  },
  {
    "start": 2558050,
    "end": 2563146,
    "text": "ああ、ごめん、フィルターはここにあるんだけど、小さすぎて気づかなかったんだ。"
  },
  {
    "start": 2563178,
    "end": 2571460,
    "text": "フィルターがあったことは理解しているが、これを何度も繰り返すことができると言いたいのだ。"
  },
  {
    "start": 2573110,
    "end": 2574002,
    "text": "技術的な詳細"
  },
  {
    "start": 2574056,
    "end": 2577666,
    "text": "5倍以上の大きさのモデルなら、すでに要約で初期化されているのでは？"
  },
  {
    "start": 2577778,
    "end": 2579880,
    "text": "トレーニング前のプロセスでトレーニングを受けているか？"
  },
  {
    "start": 2580730,
    "end": 2582982,
    "text": "ああ、おそらくそうだろう。"
  },
  {
    "start": 2583116,
    "end": 2586674,
    "text": "私たちは対照的に、病みつきになるようなスティックが好きだった。"
  },
  {
    "start": 2586722,
    "end": 2593658,
    "text": "このままgpt-2にこだわることもできたが、そのおかげでさらに小さなモデルを持つことができた。"
  },
  {
    "start": 2593744,
    "end": 2594954,
    "text": "それも理由の一つだ。"
  },
  {
    "start": 2595072,
    "end": 2602494,
    "text": "もし、市販のT5スチルだけを使うのであれば、私たちが実現したような優れたパフォーマンスは必ずしも得られないだろう。"
  },
  {
    "start": 2602532,
    "end": 2611642,
    "text": "実際、gpt-3 t fiveと比べると、要約は苦手だと思う。"
  },
  {
    "start": 2611786,
    "end": 2620820,
    "text": "フィルターそのものが間違っていたら訂正してほしい。"
  },
  {
    "start": 2622310,
    "end": 2640730,
    "text": "SOTAモデルは、独自のトレーニング、データスタイルの例では、90％を超えるようなものですが、ドメイン外のケースでは、かなり悪化する可能性があります。"
  },
  {
    "start": 2641390,
    "end": 2643866,
    "text": "たぶん、わからない。"
  },
  {
    "start": 2643888,
    "end": 2644406,
    "text": "ボードパーク？"
  },
  {
    "start": 2644438,
    "end": 2645578,
    "text": "70歳くらいかな。"
  },
  {
    "start": 2645744,
    "end": 2647978,
    "text": "サーシャ、この中のいくつかを積極的にやってみるかい？"
  },
  {
    "start": 2648064,
    "end": 2649034,
    "text": "5人じゃないと思う。"
  },
  {
    "start": 2649072,
    "end": 2649850,
    "text": "私はそう呼んでいる。"
  },
  {
    "start": 2649920,
    "end": 2651606,
    "text": "要約の訓練はしていないと思う。"
  },
  {
    "start": 2651718,
    "end": 2654298,
    "text": "ああ、明確にしてくれてありがとう。"
  },
  {
    "start": 2654384,
    "end": 2658014,
    "text": "というのも、人々が使うT5には2つのバリエーションがあるからだ。"
  },
  {
    "start": 2658052,
    "end": 2668622,
    "text": "1つはt 5で、これは要約を含む事前学習混合であり、もう1つはt 5.1.1で、これは教師なし学習のみである。"
  },
  {
    "start": 2668686,
    "end": 2676340,
    "text": "使用したベースモデルが要約について学習されたものであるかどうかは、t 5を使用するかt 5.1.1を使用するかによって異なります。"
  },
  {
    "start": 2677450,
    "end": 2679240,
    "text": "それはよかった。"
  },
  {
    "start": 2682410,
    "end": 2683238,
    "text": "ちょっとクレイジーだ。"
  },
  {
    "start": 2683324,
    "end": 2683718,
    "text": "そうだね。"
  },
  {
    "start": 2683804,
    "end": 2684630,
    "text": "ありがとう。"
  },
  {
    "start": 2684780,
    "end": 2689990,
    "text": "とにかく、これは強化学習を少し彷彿とさせる。"
  },
  {
    "start": 2690140,
    "end": 2696394,
    "text": "どのアデルのグループと話すかによって、自己模倣や専門家の反復と呼ばれる特別な種類がある。"
  },
  {
    "start": 2696432,
    "end": 2719998,
    "text": "彼らは違う用語を使っていますが、基本的には同じことです。ある意味、教師あり学習を美化して名前を変えたもので、モデル自身に要約の探索をさせ、報酬ネットワーク（私たちの場合は、それが良い要約かどうかを教えてくれるフィルター）を持たせて、良い経験に対して教師あり学習を行います。"
  },
  {
    "start": 2720094,
    "end": 2724260,
    "text": "要するに、自己模倣学習だ。"
  },
  {
    "start": 2726010,
    "end": 2727720,
    "text": "これがその例だ。"
  },
  {
    "start": 2728410,
    "end": 2750842,
    "text": "この文章を要約すると、冗長化プログラムの終了時には、ニュースルームのあらゆる部分を見渡すことになるが、編集管理、ビデオ・プレゼンテーション、セクション・ライターの役割は大幅に減少すると予想される、というものである。"
  },
  {
    "start": 2750986,
    "end": 2760090,
    "text": "私たちのモデルは、ニュースルームでのステップ数を減らすことを検討しているため、それを要約したものです、と出版社は社内ノートで述べた。"
  },
  {
    "start": 2760250,
    "end": 2767002,
    "text": "一方、GPT-3はまず、元のラインと比べるとあまり要約されていないように見える。"
  },
  {
    "start": 2767076,
    "end": 2778226,
    "text": "これはGPT-3が典型的なもので、あまり要約されていないこともあれば、まったく正しいかどうかわからない情報が少し追加されていることもある。"
  },
  {
    "start": 2778338,
    "end": 2782406,
    "text": "原文をどう精査するかにもよるが、私は知っているはずだ。"
  },
  {
    "start": 2782428,
    "end": 2783830,
    "text": "GPT4は本当にいい。"
  },
  {
    "start": 2783900,
    "end": 2794070,
    "text": "ところで、彼らのチューニングがどうなったのか正確には分からないので、どう比較すればいいのかさえ分からない。"
  },
  {
    "start": 2794150,
    "end": 2805326,
    "text": "いずれにせよ、現実的な質問として、私たちは人間のテキストとは対照的に、シントードのテキストを要約したと言った。"
  },
  {
    "start": 2805508,
    "end": 2816130,
    "text": "というわけで、下流の評価のために実際に人間が書いたデータセットでも、実際によく一般化されることがわかった。"
  },
  {
    "start": 2816710,
    "end": 2825166,
    "text": "実際、他ドメインの人間監視データでは、モデルの傾向よりも良い結果を出している。"
  },
  {
    "start": 2825358,
    "end": 2830210,
    "text": "論文での実験が増え、本当に多様になった。"
  },
  {
    "start": 2831450,
    "end": 2841290,
    "text": "私たちがフィルタリングの多様性を強制しているのは、有益な要約よりも妨害的な要約が多くなるようにするためでもある。"
  },
  {
    "start": 2842030,
    "end": 2845526,
    "text": "さて、最後のビデオはパラドックスについてです。"
  },
  {
    "start": 2845638,
    "end": 2849580,
    "text": "先ほど、常識的な話に戻ると約束した。"
  },
  {
    "start": 2850590,
    "end": 2859358,
    "text": "GPTが司法試験を突破し、与謝野晶子が「存亡の危機がある」と言い、AIやんくんが「AIは犬ほど賢くない」と言う。"
  },
  {
    "start": 2859444,
    "end": 2863200,
    "text": "どこにでもあるようなことが、いっぺんに起こっているような状況だ。"
  },
  {
    "start": 2864690,
    "end": 2869730,
    "text": "その中で、私たちは常識的なパラドックスを持っているのかもしれない、とだけ言っておこうと思う。"
  },
  {
    "start": 2870150,
    "end": 2878310,
    "text": "ダークマター（暗黒物質）に例えるなら、私たちが予想しているよりもダークマター（暗黒物質）はたくさんあるのかもしれない。"
  },
  {
    "start": 2879050,
    "end": 2890118,
    "text": "そのため、GBD4の能力に興奮し、すべての心の理論が自然発生的に生まれたのではないかと報告する人もいる。"
  },
  {
    "start": 2890214,
    "end": 2901914,
    "text": "このようなものには、常に、いや、もしかしたら1が期待したほど頑強ではないかもしれないという反論があるものだ。"
  },
  {
    "start": 2901962,
    "end": 2912442,
    "text": "ところで、この論文では、一見複雑に見える20の例は、事前学習データに含まれていたものと非常に類似していた可能性がある。"
  },
  {
    "start": 2912506,
    "end": 2921550,
    "text": "ハーバードのトマール・アルマンが、少し例を乱したところ、モデルがそれほど強固でないことに気づいた。"
  },
  {
    "start": 2921710,
    "end": 2923634,
    "text": "私なりのバージョンがある。"
  },
  {
    "start": 2923672,
    "end": 2929670,
    "text": "昨年のGPT-3では、古典的な心の理論ができなかった。"
  },
  {
    "start": 2929740,
    "end": 2932466,
    "text": "アリスとボブはキッチンのテーブルにリンゴがあるのを見た。"
  },
  {
    "start": 2932498,
    "end": 2933990,
    "text": "アリスはキッチンに向かった。"
  },
  {
    "start": 2934410,
    "end": 2937714,
    "text": "その間にボブはリンゴをキャビネットに移した。"
  },
  {
    "start": 2937762,
    "end": 2939270,
    "text": "2つの場所、2人の人間がいる。"
  },
  {
    "start": 2939340,
    "end": 2940606,
    "text": "これは非常に古典的だ。"
  },
  {
    "start": 2940738,
    "end": 2947500,
    "text": "この質問に対してGPT-3はキャビネットの中と答えていたが、それは後の場所であり、間違った答えだ。"
  },
  {
    "start": 2948350,
    "end": 2950220,
    "text": "去年みたいだ。"
  },
  {
    "start": 2950830,
    "end": 2956650,
    "text": "今年はGPT4人が正解している。"
  },
  {
    "start": 2957250,
    "end": 2968926,
    "text": "最初の場所が正解だが、通常は正解が最初の場所になりがちな、ちょっとしたひっかけ問題であることに少し頼りすぎていたかもしれない。"
  },
  {
    "start": 2969038,
    "end": 2976802,
    "text": "好奇心から、物語の最後の2つの文章の順番を入れ替え、異なるバージョンを試してみる。"
  },
  {
    "start": 2976866,
    "end": 2980598,
    "text": "GPT4は、最初の1つに固執するようなものだ。"
  },
  {
    "start": 2980764,
    "end": 2992742,
    "text": "エリスが見ていない間に、もしかしたら正しく答えられるかもしれない。"
  },
  {
    "start": 2992806,
    "end": 2999260,
    "text": "GPTの4人には、もっと丁寧に質問しなければならない。"
  },
  {
    "start": 3000350,
    "end": 3011102,
    "text": "いずれにせよ、人間というものは、たとえ礼儀正しくなくとも、GPT4よりもこの仕事をうまくこなせるものなのだ。"
  },
  {
    "start": 3011236,
    "end": 3023822,
    "text": "この論文では、アラン・サーがもうすぐバークレーに加わるので、3人、3つのコンテナ、あるいは4つのコンテナなど、さまざまなバリエーションをデザインした。"
  },
  {
    "start": 3023886,
    "end": 3027438,
    "text": "これらはチャットGPTまたはGPTを4回行う前にデザインされた。"
  },
  {
    "start": 3027464,
    "end": 3033442,
    "text": "この論文はもともとGPT-3のために書かれたもので、後にGPT-4の性能だけを追加した。"
  },
  {
    "start": 3033506,
    "end": 3038230,
    "text": "すごくいいときもあれば、もう少しバラエティに富んでいるときもある。"
  },
  {
    "start": 3039790,
    "end": 3046010,
    "text": "それから、TEDの講演で私がやった衣類の乾燥の例があります。"
  },
  {
    "start": 3046350,
    "end": 3048246,
    "text": "オーケー、これは物理学のようなものだ。"
  },
  {
    "start": 3048438,
    "end": 3052640,
    "text": "以前、我々は、この選手が物理的な知識をどれだけ持っているのかについて議論した。"
  },
  {
    "start": 3053490,
    "end": 3056186,
    "text": "私は4月にこの質問をした。"
  },
  {
    "start": 3056298,
    "end": 3059422,
    "text": "今年は5枚の洗濯物を天日干しにした。"
  },
  {
    "start": 3059476,
    "end": 3061758,
    "text": "完全に乾くまで5時間かかった。"
  },
  {
    "start": 3061924,
    "end": 3065618,
    "text": "洗濯物30枚とGPT4枚を乾かすのに何時間かかりますか？"
  },
  {
    "start": 3065704,
    "end": 3067442,
    "text": "当時は30時間と言われていた。"
  },
  {
    "start": 3067496,
    "end": 3071460,
    "text": "もちろん、このような例はすぐに消えてしまう。"
  },
  {
    "start": 3071830,
    "end": 3080546,
    "text": "6月18日現在、これは私のCVPR基調講演のためのもので、前の質問は解決したので別の質問をする。"
  },
  {
    "start": 3080738,
    "end": 3084562,
    "text": "ほとんど同じ質問だが、少し言い回しが違う。"
  },
  {
    "start": 3084626,
    "end": 3085702,
    "text": "どれくらい？"
  },
  {
    "start": 3085836,
    "end": 3090822,
    "text": "5枚の洗濯物を乾かすのに10時間かかるとしたら、20枚の洗濯物を乾かすのに何時間かかるでしょうか？"
  },
  {
    "start": 3090966,
    "end": 3095610,
    "text": "私のオリジナル・バージョンを違う数字で試してみると、GPT4は正しくなる。"
  },
  {
    "start": 3095760,
    "end": 3102190,
    "text": "こう聞くと、6月18日の時点では間違っていた。"
  },
  {
    "start": 3102690,
    "end": 3108560,
    "text": "その基調講演で私は、これはrlhfのワコモゲームの状況に似ていると言った。"
  },
  {
    "start": 3109010,
    "end": 3111102,
    "text": "もちろん、この質問もなくなった。"
  },
  {
    "start": 3111156,
    "end": 3114814,
    "text": "今日これを試せば、正解が出るはずだ。"
  },
  {
    "start": 3114932,
    "end": 3117866,
    "text": "昨日、私はそれを知っていた。"
  },
  {
    "start": 3117908,
    "end": 3118354,
    "text": "それは分かっていた。"
  },
  {
    "start": 3118392,
    "end": 3122260,
    "text": "昨夜まで、他の質問も試してみた。"
  },
  {
    "start": 3122870,
    "end": 3124594,
    "text": "別のものを試してみようと思ったんだ。"
  },
  {
    "start": 3124632,
    "end": 3125682,
    "text": "これはどうだ？"
  },
  {
    "start": 3125816,
    "end": 3131574,
    "text": "短パンを乾かすのに3時間、ズボンを天日干しするのに5時間かかるとする。"
  },
  {
    "start": 3131692,
    "end": 3138742,
    "text": "そして、彼はこう言った。"
  },
  {
    "start": 3138796,
    "end": 3148538,
    "text": "ところで、数日前、ジェレミー・ハワードが、ああ、習慣を聞かなければならないと言っているのを見た。"
  },
  {
    "start": 3148704,
    "end": 3152022,
    "text": "この習慣から始めなければならない。"
  },
  {
    "start": 3152086,
    "end": 3165314,
    "text": "だから、ウルーラ、逆行言語モデル、ファインチューニング、何でもいいけど、推論に優れていて、もう少しやる気のあるモデルをプライムするべきだ。"
  },
  {
    "start": 3165512,
    "end": 3171380,
    "text": "試してみたところ、確かに動作は変わったが、最終的な答えはまだ間違っている。"
  },
  {
    "start": 3173190,
    "end": 3178550,
    "text": "数週間も経たないうちに、おそらくまた直るだろう。"
  },
  {
    "start": 3178700,
    "end": 3180520,
    "text": "それだと、本当に遠くまでしか行けない。"
  },
  {
    "start": 3181050,
    "end": 3186534,
    "text": "つまり、この件に関して、あなたは自分自身に他のLHFをする必要はなかったということだ。"
  },
  {
    "start": 3186652,
    "end": 3187750,
    "text": "答えはわかっているはずだ。"
  },
  {
    "start": 3187820,
    "end": 3189882,
    "text": "こんな例は見たことがないだろう。"
  },
  {
    "start": 3190016,
    "end": 3191034,
    "text": "答えはわかっているはずだ。"
  },
  {
    "start": 3191072,
    "end": 3193578,
    "text": "これがどう機能するのかが私の疑問である。"
  },
  {
    "start": 3193664,
    "end": 3194886,
    "text": "とても気になるんだ。"
  },
  {
    "start": 3194998,
    "end": 3196570,
    "text": "どうやったんだ？"
  },
  {
    "start": 3196720,
    "end": 3204334,
    "text": "あのトランスフォーマー、あの素晴らしいGPT4人、私ができない司法試験さえパスできる。"
  },
  {
    "start": 3204372,
    "end": 3207790,
    "text": "何か不思議な感じがする。"
  },
  {
    "start": 3207860,
    "end": 3214038,
    "text": "つまり、パラドックスとは、あるバージョンではこれらすべてが同時に真実である可能性があるということだ。"
  },
  {
    "start": 3214154,
    "end": 3223934,
    "text": "人間にとって常識は些細なことであり、機械にとっては難しいことである。"
  },
  {
    "start": 3224062,
    "end": 3227314,
    "text": "ということは、llmsは膨大な常識的知識を身につけていることになる。"
  },
  {
    "start": 3227362,
    "end": 3233410,
    "text": "GPT4ほど常識的な予備知識を与えてくれるものはない。"
  },
  {
    "start": 3233570,
    "end": 3237414,
    "text": "しかし、ヤムルックンの言うことにも一理あるのかもしれない。"
  },
  {
    "start": 3237612,
    "end": 3242760,
    "text": "モラヴァックのパラドックス、これは早い段階で出てきたと思う。"
  },
  {
    "start": 3243470,
    "end": 3250910,
    "text": "たぶん、アリョーシャはこのことについて、基本的に伝統的な前提に反していると先に述べたのだろう。"
  },
  {
    "start": 3251250,
    "end": 3256110,
    "text": "コンピューターにとっては、低レベルの知覚的なものよりも高次の推論の方が簡単なのかもしれない。"
  },
  {
    "start": 3256260,
    "end": 3268110,
    "text": "もしかしたら、視覚よりも言語のほうが簡単で、具現化しなくてもドープレベルの能力を持つAgiができるかもしれない。"
  },
  {
    "start": 3268190,
    "end": 3268926,
    "text": "誰が知っている？"
  },
  {
    "start": 3269038,
    "end": 3278242,
    "text": "しかし、私たちはllmsに比べ、本当に良いデータセット、あるいは事前のトレーニングの目的を持っていないということが課題のひとつだと思う。"
  },
  {
    "start": 3278386,
    "end": 3291078,
    "text": "何かすごいことが起こるたびに、このようなデータ革新が少しずつ起こってくるんですね。基本的には教科書があればいいというケースでしたし、先ほど見た不可能蒸留もそうです。"
  },
  {
    "start": 3291174,
    "end": 3295914,
    "text": "データセットの見つけ方について、さらに多くのアクションがある。"
  },
  {
    "start": 3296032,
    "end": 3299334,
    "text": "最後に、超簡単に、一般的なAIのパラドックスについて。"
  },
  {
    "start": 3299382,
    "end": 3303758,
    "text": "これは、僕らが取り組んでいるグループでの作り話みたいなものなんだ。"
  },
  {
    "start": 3303844,
    "end": 3323330,
    "text": "AIにとって生成は、人間にとってどうであるかに比べて理解するよりも簡単なことなのかもしれない。ダリ2人は多くのものを生成することができるが、最先端の視覚モデルは、AIが生成できるモデルなど見ることができない。"
  },
  {
    "start": 3324950,
    "end": 3328614,
    "text": "なぜそう思うのか、実はもう少し具体的な理由がある。"
  },
  {
    "start": 3328652,
    "end": 3333880,
    "text": "もう時間がないので、これだけ持っていて、ありがとうと言いたい。"
  },
  {
    "start": 3346510,
    "end": 3354140,
    "text": "もし、あなたの背景の仮定が、シャツを干す場所は物干し竿の上にひとつしかない、というものだとしたら、その推論は完全に正しいだろう。"
  },
  {
    "start": 3355870,
    "end": 3358510,
    "text": "欠けているのは、この空間的な知識だ。"
  },
  {
    "start": 3358930,
    "end": 3362458,
    "text": "さて、物干し竿には通常、複数のシャツを収納するスペースがある。"
  },
  {
    "start": 3362554,
    "end": 3363054,
    "text": "そうだね。"
  },
  {
    "start": 3363172,
    "end": 3370050,
    "text": "GPT4の質問の仕方によっては、あるいは同じ質問であっても、GPT4はそのような理由付けをすることがある。"
  },
  {
    "start": 3370470,
    "end": 3373538,
    "text": "スペースの広さによる。"
  },
  {
    "start": 3373624,
    "end": 3378658,
    "text": "正しい答えを出すとき、そのような推論が働いていることがよくある。"
  },
  {
    "start": 3378754,
    "end": 3381782,
    "text": "私にとっては、なぜその後なのかが少し驚きだ。"
  },
  {
    "start": 3381916,
    "end": 3384600,
    "text": "それで解決したと思った。"
  },
  {
    "start": 3385050,
    "end": 3386054,
    "text": "問題はなくなった。"
  },
  {
    "start": 3386092,
    "end": 3386534,
    "text": "行ってしまった。"
  },
  {
    "start": 3386652,
    "end": 3390682,
    "text": "という質問をした。"
  },
  {
    "start": 3390736,
    "end": 3394454,
    "text": "まるで元のミスモードに戻ったようだ。"
  },
  {
    "start": 3394502,
    "end": 3398506,
    "text": "なぜこんなことが起こるのか、本当に不思議だ。"
  },
  {
    "start": 3398608,
    "end": 3399514,
    "text": "これはどのような仕組みなのか？"
  },
  {
    "start": 3399552,
    "end": 3401214,
    "text": "解決策はあるのだろうか？"
  },
  {
    "start": 3401252,
    "end": 3402014,
    "text": "分からないよ。"
  },
  {
    "start": 3402132,
    "end": 3404254,
    "text": "ただただ辛く感じる。"
  },
  {
    "start": 3404452,
    "end": 3405742,
    "text": "ええ、やらせてください。"
  },
  {
    "start": 3405796,
    "end": 3416010,
    "text": "スコットの質問に従えば、多くの場合、モデルには推測や決断に基づく仮定があると思う。"
  },
  {
    "start": 3416100,
    "end": 3418702,
    "text": "人間にも、そういう前提がある。"
  },
  {
    "start": 3418846,
    "end": 3423470,
    "text": "人間とマシンの共通基盤は、今のところ一致していないようだ。"
  },
  {
    "start": 3423550,
    "end": 3433794,
    "text": "もし方法があるのなら、別の方法でプロンプトを出すのではなく、頭の中にある仮定を明示的に伝えれば、パラドックスは解決するのだろうか？"
  },
  {
    "start": 3433842,
    "end": 3435720,
    "text": "プロンプトを通してということですか？"
  },
  {
    "start": 3436410,
    "end": 3442802,
    "text": "もっと言えば、そうだが、まず物理学で乾燥がどのように機能するかを説明してくれ。"
  },
  {
    "start": 3442946,
    "end": 3449900,
    "text": "もしかしたら、この機械は一度に1枚のシャツしか取れないから、2枚作るのに6時間かかるとか思い込んでいるのかもしれない。"
  },
  {
    "start": 3450690,
    "end": 3461754,
    "text": "革新的で迅速なエンジニアリングのような、よりスマートな方法でこの根本的な課題を解決しようということだ。"
  },
  {
    "start": 3461882,
    "end": 3465570,
    "text": "私はまた、まったく違う何かがまだあるのではないかと考えている。"
  },
  {
    "start": 3465640,
    "end": 3471780,
    "text": "答えはわからないが、まったく別の解決策があるのではないかと思っている。"
  },
  {
    "start": 3476550,
    "end": 3482898,
    "text": "常識に関する教科書はないのだろうか？"
  },
  {
    "start": 3483074,
    "end": 3496346,
    "text": "心理学というのは、間違いなくあまりうまく書き表せなかった常識的なルールをすべて書き記そうとする一つのバージョンだと思う。"
  },
  {
    "start": 3496368,
    "end": 3502154,
    "text": "そこで、私もそのようなバージョンを作ろうとした。"
  },
  {
    "start": 3502202,
    "end": 3507950,
    "text": "私はそれを、AIの常識を学ぶための教科書のようなものと捉えがちだ。"
  },
  {
    "start": 3508020,
    "end": 3512720,
    "text": "十分に広くないし、十分に自然ではない。"
  },
  {
    "start": 3513590,
    "end": 3519614,
    "text": "僕は、認知科学からインスピレーションを得るのが好きなんだ。"
  },
  {
    "start": 3519662,
    "end": 3523394,
    "text": "人間は自分でより多くのものを獲得することができる。"
  },
  {
    "start": 3523432,
    "end": 3530630,
    "text": "つまり、子供たちは教科書や保育者から多くの知識を得るが、自分たちでもかなりのことを理解する。"
  },
  {
    "start": 3530700,
    "end": 3543114,
    "text": "私は、AIが大規模な事前訓練に依存するよりも、もっと自力で習得するべきかどうか疑問に思っている。"
  },
  {
    "start": 3543312,
    "end": 3547462,
    "text": "ジェイコブ、最後のパラドックスに戻ってくれないか？"
  },
  {
    "start": 3547526,
    "end": 3553066,
    "text": "生成はこうだった。"
  },
  {
    "start": 3553088,
    "end": 3557422,
    "text": "私はただ、そうなんだ、という声明を見たかっただけなんだ。"
  },
  {
    "start": 3557476,
    "end": 3561530,
    "text": "なぜジェネレーションの方が簡単なのか、直感的にわかる？"
  },
  {
    "start": 3561610,
    "end": 3566066,
    "text": "というのも、私の直感はまだその逆だからだ。"
  },
  {
    "start": 3566248,
    "end": 3566980,
    "text": "そうだね。"
  },
  {
    "start": 3570710,
    "end": 3575650,
    "text": "積極的に取り組んでいる私のグループの中でさえ、私たちは理解を深めている。"
  },
  {
    "start": 3575730,
    "end": 3578854,
    "text": "私たちはこのことを別の角度から見なければならない。"
  },
  {
    "start": 3578892,
    "end": 3584070,
    "text": "ある意味で、私は次のように言いたい。"
  },
  {
    "start": 3584570,
    "end": 3597354,
    "text": "非ネイティブ・スピーカーが、ライティングの仕事の一部をチャットJPTに頼ったと私に言うことがよくあるが、その意味で、チャットJPTは彼らよりも良い仕事を生み出すことができた。"
  },
  {
    "start": 3597472,
    "end": 3603562,
    "text": "となると、それはまったく正しくなく、プロンプトのリクエストのいくつかは無視された。"
  },
  {
    "start": 3603626,
    "end": 3606734,
    "text": "彼らは自分たちのニーズに合うように少し修正する必要がある。"
  },
  {
    "start": 3606772,
    "end": 3613918,
    "text": "そういう意味では、理解という点では世代交代に少し遅れをとっていたし、コンピュータ・ビジョンの面でも遅れをとっていた。"
  },
  {
    "start": 3614004,
    "end": 3625534,
    "text": "ところで、言語の場合、私たちは1つのモデルを両方の目的に使う傾向があり、それはちょうど変圧器のようなものだ。"
  },
  {
    "start": 3625662,
    "end": 3644220,
    "text": "少なくとも私の他の実験では、低レベルのコンピュータービジョンは十分に信頼できない。"
  },
  {
    "start": 3646350,
    "end": 3655760,
    "text": "暗号に例えて言うなら、生成とは一方通行の関数を計算することであり、理解とは反転させることかもしれない。"
  },
  {
    "start": 3657410,
    "end": 3658400,
    "text": "ありがとう。"
  },
  {
    "start": 3660450,
    "end": 3670818,
    "text": "最初に戻りますが、メルトンの掛け算について話しているとき、大規模な言語モデルを使用しているときは、直感的な推測をしているように思えます。"
  },
  {
    "start": 3670984,
    "end": 3676166,
    "text": "そういうふうに考えているんですか？"
  },
  {
    "start": 3676348,
    "end": 3693290,
    "text": "ええ、確かに掛け算については学習したと思います。事前学習データにはたくさんの例がありますから、あるいは、スクラッチパッドで与える指示自体も理解できるように学習しているからでしょう。"
  },
  {
    "start": 3693360,
    "end": 3706974,
    "text": "しかし、それを本当に正確に実行できるかというと、意外にも1が期待したほどの信頼性はない。"
  },
  {
    "start": 3707172,
    "end": 3708826,
    "text": "一方では、知的好奇心をそそられる。"
  },
  {
    "start": 3708858,
    "end": 3711566,
    "text": "その一方で、AIの安全性にも少し問題がある。"
  },
  {
    "start": 3711748,
    "end": 3727010,
    "text": "一方、数字の正確な掛け算を学びたいなら、ずっと前に誰かがそれを見つけていて、一般的にはそれを教わる。"
  },
  {
    "start": 3727160,
    "end": 3732360,
    "text": "それは、あなたが期待するような学習モデルとはまったく違う種類のものだ。"
  },
  {
    "start": 3732730,
    "end": 3738810,
    "text": "スクラッチパッドは役に立つが、必ずしもそこにたどり着くとは限らない。"
  },
  {
    "start": 3738880,
    "end": 3740682,
    "text": "そうだね。"
  },
  {
    "start": 3740816,
    "end": 3741210,
    "text": "オーケー。"
  },
  {
    "start": 3741280,
    "end": 3747260,
    "text": "スクラッチパッドを、もっとしっかりした学習モデルのようにする考えはないのか？"
  },
  {
    "start": 3747870,
    "end": 3762160,
    "text": "数日前、私は、より良いスクラッチパッドで数字を上げることができる新しいアーカイブ用紙に気づいた。"
  },
  {
    "start": 3762870,
    "end": 3765454,
    "text": "それは人間の知性だ。"
  },
  {
    "start": 3765582,
    "end": 3773854,
    "text": "プロンプトの出し方を理解することは良くなったが、それでも領域外の驚くべきパフォーマンスを達成することはできなかった。"
  },
  {
    "start": 3773902,
    "end": 3776614,
    "text": "僕らがどれだけできたかよりずっといい。"
  },
  {
    "start": 3776652,
    "end": 3782854,
    "text": "私たちのスクラッチパッドは、以前グーグル・ペーパーが使っていたものより優れている。"
  },
  {
    "start": 3782972,
    "end": 3791180,
    "text": "しかし、そのプロンプトの空間には、根本的にもっといいものがあるのかもしれない。"
  },
  {
    "start": 3792430,
    "end": 3793980,
    "text": "では、最後の2つの質問です。"
  },
  {
    "start": 3795470,
    "end": 3796220,
    "text": "そうだね。"
  },
  {
    "start": 3797470,
    "end": 3806090,
    "text": "初期の研究では、このようなニューラル・アーキテクチャーの場合、特に領域外では汎化されないことが多いことを示した。"
  },
  {
    "start": 3806170,
    "end": 3813790,
    "text": "そして、例えば再帰をニューラル・プログラミングの合成アーキテクチャに加えると、実際に一般化できる。"
  },
  {
    "start": 3813950,
    "end": 3816558,
    "text": "特定のケースでは、私は実際に完璧に一般化することができる。"
  },
  {
    "start": 3816654,
    "end": 3838282,
    "text": "プロンプトやこのスペースに再帰をどのように組み込めば、掛け算などの問題を解くのに役立つのか、実際に考えてみたことがあるのかどうか、興味がある。"
  },
  {
    "start": 3838416,
    "end": 3850522,
    "text": "ああ、スクラッチパッドのデザインを通して再帰性を伝えるもっといい方法があるかもしれない。"
  },
  {
    "start": 3850586,
    "end": 3853870,
    "text": "これがアーカイブ作成時のベストだった。"
  },
  {
    "start": 3855170,
    "end": 3862106,
    "text": "私は、あなたが考えていたのは、建築が含まれるのとは対照的に、迅速なエンジニアリングのようなものだけだと思っています。"
  },
  {
    "start": 3862218,
    "end": 3870020,
    "text": "もしかしたら、プロンプト・エンジニアリングを通じて、再帰性などを学ぶのに役立つかもしれない。"
  },
  {
    "start": 3870470,
    "end": 3872100,
    "text": "分からないような気がする。"
  },
  {
    "start": 3875130,
    "end": 3877650,
    "text": "この計算グラフのスライドがありましたね。"
  },
  {
    "start": 3877730,
    "end": 3878360,
    "text": "そうだね。"
  },
  {
    "start": 3879210,
    "end": 3889366,
    "text": "最も素朴なスクラッチパッドで、文字通りトークン1つずつグラフを表示するようなことをやってみたことはある？"
  },
  {
    "start": 3889388,
    "end": 3892102,
    "text": "各ノードがトークンの列を持つように。"
  },
  {
    "start": 3892246,
    "end": 3893674,
    "text": "それ程ではないかもしれない。"
  },
  {
    "start": 3893712,
    "end": 3901760,
    "text": "私たちは自然言語が少なく記号的なものを試したが、思ったほどうまくいかなかった。"
  },
  {
    "start": 3903410,
    "end": 3906430,
    "text": "トークンの位置は固定されている。"
  },
  {
    "start": 3907170,
    "end": 3908794,
    "text": "常にグラフの同じ音符に対応する。"
  },
  {
    "start": 3908842,
    "end": 3911114,
    "text": "いや、あなたが思い描いているようなことはやっていない。"
  },
  {
    "start": 3911162,
    "end": 3911854,
    "text": "いや。"
  },
  {
    "start": 3912052,
    "end": 3914750,
    "text": "試してみたらどうなるのか気になる。"
  },
  {
    "start": 3916290,
    "end": 3919520,
    "text": "素晴らしい話をありがとう。"
  },
  {
    "start": 3925370,
    "end": 3925782,
    "text": "それだ。"
  },
  {
    "start": 3925836,
    "end": 3929894,
    "text": "昼食は130分までで、その後は各自で。"
  },
  {
    "start": 3929932,
    "end": 3935480,
    "text": "知り合いがいなくても、近所にいい店がたくさんある。"
  },
  {
    "start": 3939770,
    "end": 3940520,
    "text": "もちろんだ。"
  },
  {
    "start": 3944090,
    "end": 3944450,
    "text": "それはそうだ。"
  }
]