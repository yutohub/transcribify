[
  {
    "start": 250,
    "end": 4382,
    "text": "大規模な言語モデルをトレーニングし、それを微調整することの何が問題なのか？"
  },
  {
    "start": 4436,
    "end": 8298,
    "text": "ここで重要なのは、最終的に本当に大きなウェイトになるということだ。"
  },
  {
    "start": 8394,
    "end": 10686,
    "text": "このことは、ここでさまざまな問題を提起している。"
  },
  {
    "start": 10788,
    "end": 12798,
    "text": "これらの問題は主に2つある。"
  },
  {
    "start": 12964,
    "end": 16062,
    "text": "ひとつは、そのためのトレーニングにはもっと多くの計算機が必要だということだ。"
  },
  {
    "start": 16116,
    "end": 26194,
    "text": "モデルがどんどん大きくなるにつれて、これらのモデルを微調整するためには、より大きなGPUや複数のGPUが必要になってくる。"
  },
  {
    "start": 26322,
    "end": 34422,
    "text": "つ目の問題は、基本的に計算が必要なことに加えて、ファイルサイズが巨大になることだ。"
  },
  {
    "start": 34556,
    "end": 41082,
    "text": "T five XXLのチェックポイントのサイズは約40GB。"
  },
  {
    "start": 41216,
    "end": 48330,
    "text": "言うまでもなく、現在発表されている200億のパラメーターを持つモデルは、どんどん大きくなっている。"
  },
  {
    "start": 48480,
    "end": 54478,
    "text": "そこで、パラメーターの効率的な微調整という考え方が生まれる。"
  },
  {
    "start": 54564,
    "end": 57534,
    "text": "これからは、このことについて話すだけだ。"
  },
  {
    "start": 57652,
    "end": 60938,
    "text": "peftはさまざまなテクニックを駆使する。"
  },
  {
    "start": 61034,
    "end": 65502,
    "text": "今日取り上げるのはローラで、ローランクアダプテーションの略だ。"
  },
  {
    "start": 65566,
    "end": 69166,
    "text": "これは、大規模な言語モデルに対してこれを行うことについての論文に由来する。"
  },
  {
    "start": 69278,
    "end": 80142,
    "text": "peftには他にも、プレフィックス・チューニング、ptuning、プロンプト・チューニングといったクールなテクニックがあります。"
  },
  {
    "start": 80206,
    "end": 86774,
    "text": "これらのモデルをクラウド上で微調整できるようにするために、Nvidiaのような企業が実際に使用している技術もある。"
  },
  {
    "start": 86892,
    "end": 88938,
    "text": "それは本当に興味深いことだ。"
  },
  {
    "start": 89024,
    "end": 104586,
    "text": "PEFがすること、特にローラがすることは、学習済みのネットワークのパラメーターのほとんどをフリーズさせたまま、モデル内の少数の重みだけを微調整できるようにすることだ。"
  },
  {
    "start": 104698,
    "end": 111742,
    "text": "つまり、元のウェイトをトレーニングするのではなく、ウェイトを追加して微調整するのだ。"
  },
  {
    "start": 111876,
    "end": 116382,
    "text": "この利点のひとつは、オリジナルのウェイトが残っていることだ。"
  },
  {
    "start": 116446,
    "end": 121314,
    "text": "これはまた、あなたが知らなければ、壊滅的な忘却を止めるのに役立つ傾向がある。"
  },
  {
    "start": 121352,
    "end": 127058,
    "text": "壊滅的な忘却とは、モデルが最初に訓練した内容を忘れてしまうことである。"
  },
  {
    "start": 127144,
    "end": 136678,
    "text": "微調整をする場合、微調整をしすぎると、学習させた元のデータのいくつかを忘れてしまうことになる。"
  },
  {
    "start": 136764,
    "end": 144906,
    "text": "PEfは、ウェイトを追加するだけで、元のウェイトを凍らせながらチューニングしているから、そのような問題はない。"
  },
  {
    "start": 145088,
    "end": 150906,
    "text": "peftはまた、データ量が少ない場合に、非常に優れた微調整を可能にする。"
  },
  {
    "start": 151008,
    "end": 155518,
    "text": "また、他のシナリオにも応用できる。"
  },
  {
    "start": 155604,
    "end": 164346,
    "text": "このようなことは、大規模な言語モデルや、安定した拡散のようなモデルを微調整する上で大きな利点となる。"
  },
  {
    "start": 164458,
    "end": 168786,
    "text": "現在私たちが目にしているAIモデルの多くも、これを使い始めている。"
  },
  {
    "start": 168888,
    "end": 173662,
    "text": "最も良いことのひとつは、最後にほんの小さなチェックポイントで終わることだ。"
  },
  {
    "start": 173726,
    "end": 179014,
    "text": "最近のビデオでは、ラマのモデルを微調整してアルパカのモデルを作った。"
  },
  {
    "start": 179132,
    "end": 184626,
    "text": "アドオン部分だけの最終チェックポイントは12メガバイトくらいだったと思う。"
  },
  {
    "start": 184658,
    "end": 193318,
    "text": "今は小さくなったが、元のウェイトが必要なのは変わりない。"
  },
  {
    "start": 193414,
    "end": 199962,
    "text": "一般的に、ペフトアプローチは、基本的にフルモデルをファインチューニングするのと同じようなパフォーマンスを得ることができる。"
  },
  {
    "start": 200096,
    "end": 205162,
    "text": "ただ、微調整をすることで、追加するウェイトを調整することができる。"
  },
  {
    "start": 205216,
    "end": 208542,
    "text": "ハギング・フェイスは、これをテーマにしたライブラリーをリリースしている。"
  },
  {
    "start": 208596,
    "end": 210750,
    "text": "そこでこのペフトの出番だ。"
  },
  {
    "start": 210820,
    "end": 219054,
    "text": "彼らは多くの論文を取り上げ、トランスフォーマー・ライブラリーやアクセラレート・ライブラリーと連動するように実装した。"
  },
  {
    "start": 219182,
    "end": 235366,
    "text": "これは基本的に、Googleやmeta、さまざまな企業によって作成された、あらかじめ訓練された顔を抱きしめるモデルを、既製品から取り出して、これを使って微調整できるようにするものだ。"
  },
  {
    "start": 235468,
    "end": 243820,
    "text": "これからコードに飛び込んで、基本的にPEFを使ってモデルのローラ微調整を行う方法を見ていこう。"
  },
  {
    "start": 245550,
    "end": 258574,
    "text": "このノートでは、ペフトビットアンドバイトを使ったモデルのトレーニングやファインチューニング、そしてローラ・チェックポイントについて説明する。"
  },
  {
    "start": 258612,
    "end": 260458,
    "text": "これはローラの微調整だ。"
  },
  {
    "start": 260554,
    "end": 265886,
    "text": "覚えているかな、ローラとのアイデアは、僕らが順応者を育てるということなんだ。"
  },
  {
    "start": 265908,
    "end": 274658,
    "text": "実際のウェイトをトレーニングしているのではなく、モデルのさまざまなポイントでウェイトを追加し、そのウェイトを微調整することで、望みの結果が得られるようにしているのだ。"
  },
  {
    "start": 274824,
    "end": 278018,
    "text": "ここにライブラリをインストールする。"
  },
  {
    "start": 278104,
    "end": 294442,
    "text": "なぜなら、もしこのままトレーニングが終わりに近づいたら、基本的にモデルやウェイトをできるだけ早くハギングフェースハブに保存しておきたいからだ。"
  },
  {
    "start": 294496,
    "end": 295802,
    "text": "私はこれを前線に置くことが多い。"
  },
  {
    "start": 295856,
    "end": 298950,
    "text": "これは基本的に、ハグして、ここをクリックして、ハグする顔のトークンを手に入れるだけだ。"
  },
  {
    "start": 299030,
    "end": 301786,
    "text": "そのためには、当然ながら右トークンが必要だ。"
  },
  {
    "start": 301888,
    "end": 304478,
    "text": "このコラボは100点満点だ。"
  },
  {
    "start": 304564,
    "end": 312078,
    "text": "ブルームのモデルを小さくすれば、T4でもできるはずだ。"
  },
  {
    "start": 312164,
    "end": 315374,
    "text": "私がここでトレーニングしているモデル、あるいはここで微調整しているモデルだ。"
  },
  {
    "start": 315412,
    "end": 318578,
    "text": "ブルーム70億パラメーターモデルである。"
  },
  {
    "start": 318664,
    "end": 325794,
    "text": "760億のバージョンもあるし、13億のバージョンもあると思う。"
  },
  {
    "start": 325912,
    "end": 327762,
    "text": "モデルにロードしている。"
  },
  {
    "start": 327816,
    "end": 328306,
    "text": "そうだろう。"
  },
  {
    "start": 328328,
    "end": 329042,
    "text": "注文を受けたばかりだ。"
  },
  {
    "start": 329096,
    "end": 332950,
    "text": "トランスフォーマーからビット・アンド・バイトを導入し、8ビットを処理する。"
  },
  {
    "start": 333020,
    "end": 337634,
    "text": "我々のモデルを8ビットにすることで、gpuラムをそれほど消費しない。"
  },
  {
    "start": 337762,
    "end": 341482,
    "text": "より簡単に、より素早く、そして後々の収納も簡単になる。"
  },
  {
    "start": 341536,
    "end": 343542,
    "text": "オート・トークナイザーができました。"
  },
  {
    "start": 343606,
    "end": 347430,
    "text": "私たちは、この因果言語モデリングの自動モデルを手に入れた。"
  },
  {
    "start": 347510,
    "end": 353666,
    "text": "プレトレーニングを受けた選手だけを連れてくれば、ブルーム70億の名目でパスすることができる。"
  },
  {
    "start": 353798,
    "end": 357614,
    "text": "ここでしなければならないのは、8ビットイコールtrueでロードを渡すことだ。"
  },
  {
    "start": 357732,
    "end": 361306,
    "text": "トランスが8ビット変換を行う。"
  },
  {
    "start": 361418,
    "end": 364206,
    "text": "そのためにbits and bytesライブラリを使う。"
  },
  {
    "start": 364308,
    "end": 366222,
    "text": "自宅でGPUを使っている場合。"
  },
  {
    "start": 366276,
    "end": 368626,
    "text": "おそらく30 90とかそんなところだろう。"
  },
  {
    "start": 368648,
    "end": 373454,
    "text": "複数のGPUを使用している場合は、デバイスマップを作成することができます。"
  },
  {
    "start": 373502,
    "end": 375746,
    "text": "基本的にモデルの一部を横方向にマッピングする。"
  },
  {
    "start": 375848,
    "end": 377934,
    "text": "この場合はautoを使うだけだ。"
  },
  {
    "start": 377982,
    "end": 381046,
    "text": "とにかく最初はオートで試してみることをお勧めする。"
  },
  {
    "start": 381148,
    "end": 382374,
    "text": "モデルを入れた。"
  },
  {
    "start": 382412,
    "end": 384438,
    "text": "ここにトークナイザーがある。"
  },
  {
    "start": 384524,
    "end": 389426,
    "text": "次にやりたいことは、基本的にオリジナルのウエイトを凍結することだ。"
  },
  {
    "start": 389538,
    "end": 394022,
    "text": "ここでは、基本的にこれらのウェイトを凍結させているのがわかるだろう。"
  },
  {
    "start": 394086,
    "end": 396854,
    "text": "いくつかの例外を除いて、レイヤーはレイヤーを規範とする。"
  },
  {
    "start": 396902,
    "end": 397882,
    "text": "私たちは彼らをキープしたい。"
  },
  {
    "start": 397936,
    "end": 400618,
    "text": "実際にフロート32に留めておきたい。"
  },
  {
    "start": 400704,
    "end": 404382,
    "text": "また、フロート32として保持したい出力もある。"
  },
  {
    "start": 404436,
    "end": 405870,
    "text": "これは何をやっているのか。"
  },
  {
    "start": 405940,
    "end": 408302,
    "text": "これはそのための標準的なコードです。"
  },
  {
    "start": 408356,
    "end": 411882,
    "text": "次は実際のローラ・アダプターのセットアップだ。"
  },
  {
    "start": 412026,
    "end": 415150,
    "text": "すべてはこのコンフィグに集約される。"
  },
  {
    "start": 415220,
    "end": 417114,
    "text": "基本的にはコンフィグを取得する。"
  },
  {
    "start": 417162,
    "end": 419282,
    "text": "ここにモデルがある。"
  },
  {
    "start": 419336,
    "end": 420978,
    "text": "これはフルサイズモデルだ。"
  },
  {
    "start": 421064,
    "end": 423330,
    "text": "まだローラは加わっていない。"
  },
  {
    "start": 423400,
    "end": 425374,
    "text": "ここで、このコンフィグを作成する。"
  },
  {
    "start": 425502,
    "end": 434966,
    "text": "そして、基本的に私たちが持っていたモデルを渡して、peftモデルを取得します。peftモデルには、オリジナルのモデルとローラ・アダプターがあります。"
  },
  {
    "start": 435068,
    "end": 436950,
    "text": "ここでのコンフィギュレーションが鍵となる。"
  },
  {
    "start": 437020,
    "end": 445142,
    "text": "モデルに特定のターゲット・モジュールがあることが分かっている場合、基本的にはアテンション・ヘッドの数、アルファ・スケーリングを設定することになる。"
  },
  {
    "start": 445206,
    "end": 457902,
    "text": "今のところ、ライブラリにはこの件に関するドキュメントはあまり見当たらないが、私の推測では、今後、大きなモジュールの中で、基本的にローラ・アダプタを置くには、これらのモジュールが最適であることを、人々が理解するようになるだろう。"
  },
  {
    "start": 457956,
    "end": 462574,
    "text": "ロラのドロップアウトの設定と、もうひとつ重要なのはタスクタイプの設定だ。"
  },
  {
    "start": 462692,
    "end": 468498,
    "text": "デコーダのみのモデル、GPTスタイルのモデルということですか？"
  },
  {
    "start": 468584,
    "end": 474334,
    "text": "それとも、T5モデルやフランモデルのようなシーク・トゥ・シークモデルになるのだろうか？"
  },
  {
    "start": 474462,
    "end": 479014,
    "text": "シーク・トゥ・シークモデルの微調整の様子を撮影したビデオをまた作ろうと思う。"
  },
  {
    "start": 479052,
    "end": 481062,
    "text": "ここにその違いがある。"
  },
  {
    "start": 481196,
    "end": 489206,
    "text": "この2つの設定を弄ることで、トレーニング可能な量の大きさがかなり変わってくる。"
  },
  {
    "start": 489308,
    "end": 497030,
    "text": "ここでいろいろなアイデアを試してみてほしい。"
  },
  {
    "start": 497110,
    "end": 501098,
    "text": "訓練可能なパラメーターは、ここではほんのわずかだ。"
  },
  {
    "start": 501184,
    "end": 505838,
    "text": "これで、そこで起こっていることを確認できるトレーニング可能なパラメーターの合計がわかる。"
  },
  {
    "start": 505924,
    "end": 507566,
    "text": "よし、この場合はデータだ。"
  },
  {
    "start": 507668,
    "end": 510718,
    "text": "ここでは本当にシンプルな小さな仕事を選んだだけだ。"
  },
  {
    "start": 510804,
    "end": 520754,
    "text": "英語の引用のデータセットがあり、ほとんどの人がそれを使って引用を終えるのではなく、誰かが引用を始めたら、それを終えることができる。"
  },
  {
    "start": 520792,
    "end": 524990,
    "text": "データセットを見てみると、実は名言に関するタグがたくさんあることがわかった。"
  },
  {
    "start": 525070,
    "end": 534098,
    "text": "私がクールだと思ったのは、自分の引用文を入力すると、その引用文のタグが生成されるモデルを作ってみることです。"
  },
  {
    "start": 534194,
    "end": 538726,
    "text": "ここで私がしたことは、基本的にいくつかの列を統合して作っただけであることがお分かりいただけるだろう。"
  },
  {
    "start": 538748,
    "end": 542730,
    "text": "この引用があり、そしてここに3人の登場人物がいる。"
  },
  {
    "start": 542800,
    "end": 550554,
    "text": "今、この3人のキャラクターが選ばれているのは、おそらくプレトレーニングなどでこの順番で登場することはあまりないだろうからだ。"
  },
  {
    "start": 550592,
    "end": 561086,
    "text": "この3つの文字が表示されたら、それ以前の入力を条件とし、それ以降のタグを生成することをモデルに教えようとしているのだ。"
  },
  {
    "start": 561188,
    "end": 567266,
    "text": "我々が作成したデータセットを見てもらえばわかると思うが、これは他のみんなが取ったものだ。"
  },
  {
    "start": 567368,
    "end": 568334,
    "text": "タグ"
  },
  {
    "start": 568382,
    "end": 569522,
    "text": "それはそこにある。"
  },
  {
    "start": 569576,
    "end": 571474,
    "text": "となると、タグはこうなる。"
  },
  {
    "start": 571512,
    "end": 576622,
    "text": "自分自身であれ、正直であれ。"
  },
  {
    "start": 576766,
    "end": 577682,
    "text": "こういうことだ。"
  },
  {
    "start": 577736,
    "end": 587442,
    "text": "引用が誰かのものであるかどうかを予測することは、モデルにとって容易なことではないだろう。"
  },
  {
    "start": 587506,
    "end": 596918,
    "text": "ここにあるように、引用のキーワードのようなものがここに現れるべき要素であることは確かだ。"
  },
  {
    "start": 597014,
    "end": 597322,
    "text": "そうだね。"
  },
  {
    "start": 597376,
    "end": 598714,
    "text": "それは試してみるにはいい。"
  },
  {
    "start": 598752,
    "end": 600090,
    "text": "それ、私にちょうだい。"
  },
  {
    "start": 600160,
    "end": 602138,
    "text": "後で試してみよう。"
  },
  {
    "start": 602224,
    "end": 604026,
    "text": "そこにデータがある。"
  },
  {
    "start": 604128,
    "end": 609790,
    "text": "入力IDやアテンション・マスクを取得するために、基本的にそれを実行しているだけだ。"
  },
  {
    "start": 609860,
    "end": 611326,
    "text": "では、トレーニングをセットアップしよう。"
  },
  {
    "start": 611428,
    "end": 617642,
    "text": "このトレーニングでは、ハグする顔、つまりこのモデルでパスするトランスフォーマー・トレーナーのようなものを使うだけだ。"
  },
  {
    "start": 617716,
    "end": 619602,
    "text": "次に訓練データセットを渡す。"
  },
  {
    "start": 619656,
    "end": 624398,
    "text": "ここに訓練データセットがあり、引数を渡す必要があるのがわかるだろう。"
  },
  {
    "start": 624494,
    "end": 626254,
    "text": "いくつかの議論を見てみよう。"
  },
  {
    "start": 626302,
    "end": 628166,
    "text": "最初のものはこれだ。"
  },
  {
    "start": 628268,
    "end": 630914,
    "text": "つまり、グラデーションの蓄積ステップを持つことになる。"
  },
  {
    "start": 630962,
    "end": 635842,
    "text": "これらは、より小さなGPUで動作させようとする場合に変更するものだ。"
  },
  {
    "start": 635986,
    "end": 637622,
    "text": "これだ"
  },
  {
    "start": 637676,
    "end": 646294,
    "text": "4つの例、4つのフォワードパスを行い、勾配を計算する前に4つのパスを行う。"
  },
  {
    "start": 646422,
    "end": 653838,
    "text": "通常、バッチを考えるなら、多くのGpusでこれをトレーニングする場合、バッチサイズは128か、それ以上だろう。"
  },
  {
    "start": 653924,
    "end": 657278,
    "text": "リャマの論文では、バッチサイズを400万としている。"
  },
  {
    "start": 657364,
    "end": 657614,
    "text": "そうだね。"
  },
  {
    "start": 657652,
    "end": 661674,
    "text": "彼らは非常に多くのGPUを使っているが、残念ながら我々にはその予算がない。"
  },
  {
    "start": 661802,
    "end": 667410,
    "text": "ここで、私がお見せしたいのは、100点満点で使うことができるということです。"
  },
  {
    "start": 667480,
    "end": 669202,
    "text": "ここでもっと大きなバッチを作ることもできる。"
  },
  {
    "start": 669256,
    "end": 674002,
    "text": "ここにあるように、基本的には一度に4つの例をやるということだ。"
  },
  {
    "start": 674136,
    "end": 680610,
    "text": "そのグラデーションを集めて、4ステップ分蓄積して、それが1つのバッチになる。"
  },
  {
    "start": 680690,
    "end": 683638,
    "text": "ここで16本まとめてやるのと同じことだ。"
  },
  {
    "start": 683804,
    "end": 686258,
    "text": "次は、ウォームアップのステップを設定したい。"
  },
  {
    "start": 686354,
    "end": 693610,
    "text": "ただそこに行って、学習率をフルに使ってすべてを振り回すようなことはしたくない。"
  },
  {
    "start": 693760,
    "end": 698874,
    "text": "学習率を極端に下げることから始めて、設定した学習率まで高めていく。"
  },
  {
    "start": 698912,
    "end": 700618,
    "text": "それには一定の時間がかかる。"
  },
  {
    "start": 700704,
    "end": 702698,
    "text": "であれば、ここで最大ステップ数を設定することができる。"
  },
  {
    "start": 702784,
    "end": 705038,
    "text": "私が設定した最大ステップは非常に小さい。"
  },
  {
    "start": 705124,
    "end": 708874,
    "text": "これは単なるおもちゃのようなもので、何かをロードしているところを見せるためのプロジェクトなんだ。"
  },
  {
    "start": 708922,
    "end": 710746,
    "text": "我々は浮動小数点数16を使っている。"
  },
  {
    "start": 710858,
    "end": 713054,
    "text": "ここにセットしている。"
  },
  {
    "start": 713252,
    "end": 715886,
    "text": "アウトプットを出して、いろいろチェックするんだ。"
  },
  {
    "start": 715988,
    "end": 717554,
    "text": "それからトレーニングを開始するんだ。"
  },
  {
    "start": 717672,
    "end": 721234,
    "text": "トレーニングの長さが表示される。"
  },
  {
    "start": 721272,
    "end": 727554,
    "text": "この場合、トレーニングは非常に短時間で終わる。"
  },
  {
    "start": 727672,
    "end": 731894,
    "text": "そうすれば、時間の経過とともに、そうだ、確かに損失は減っている。"
  },
  {
    "start": 732012,
    "end": 734678,
    "text": "モデルは何かを学び始めている。"
  },
  {
    "start": 734764,
    "end": 739350,
    "text": "私がここでやったことよりも、もっと多くのトレーニングを積んで実験してみるといい。"
  },
  {
    "start": 739420,
    "end": 743462,
    "text": "そして次のパートは、これをハグする顔のハブにシェアすることだ。"
  },
  {
    "start": 743526,
    "end": 750490,
    "text": "ここでは、基本的にハグする顔のハブのユーザー名と、これから呼ぶモデル名を入れている。"
  },
  {
    "start": 750560,
    "end": 754746,
    "text": "これがブルーム70億のローラタガーだ。"
  },
  {
    "start": 754768,
    "end": 757102,
    "text": "コミットメッセージに情報を入れようかな。"
  },
  {
    "start": 757236,
    "end": 759758,
    "text": "非公開にも公開にも設定できる。"
  },
  {
    "start": 759844,
    "end": 763918,
    "text": "このチェックポイントは後で公開するので、それで遊んでほしい。"
  },
  {
    "start": 764004,
    "end": 766178,
    "text": "そうすれば、基本的にアップロードされる。"
  },
  {
    "start": 766264,
    "end": 769026,
    "text": "ロラのウェイトもアップされるだけだ。"
  },
  {
    "start": 769048,
    "end": 772750,
    "text": "フルブルームモデルとローラのウェイトがアップロードされないんだ。"
  },
  {
    "start": 772830,
    "end": 776766,
    "text": "ハングリーフェイス・ハブで見られるように、これは小さな小さなファイルになる。"
  },
  {
    "start": 776798,
    "end": 780322,
    "text": "ここではメガバイトの話をしているのであって、ギガバイトの話をしているのではない。"
  },
  {
    "start": 780376,
    "end": 785298,
    "text": "実際、完全にアップロードされると31メガくらいになることがおわかりいただけるだろう。"
  },
  {
    "start": 785394,
    "end": 790874,
    "text": "次に、もし推論だけをしたいのであれば、基本的にはこのように持ち込めばいい。"
  },
  {
    "start": 790912,
    "end": 800458,
    "text": "を読み込むと、基本的にトレーニングしたものをまとめるだけでなく、実際のフルモデルも取り込むことができる。"
  },
  {
    "start": 800544,
    "end": 803370,
    "text": "これは基本的に持ち込みであることがわかるだろう。"
  },
  {
    "start": 803440,
    "end": 805150,
    "text": "これからうまくいくだろう。"
  },
  {
    "start": 805220,
    "end": 807614,
    "text": "よし、ブルーム70億のモデルが必要だ。"
  },
  {
    "start": 807732,
    "end": 808606,
    "text": "それを持ってくるよ。"
  },
  {
    "start": 808628,
    "end": 809982,
    "text": "そのためにトークナイザーが必要なんだ。"
  },
  {
    "start": 810036,
    "end": 813514,
    "text": "それを持ち込むと、ダウンロードが始まるんだ。"
  },
  {
    "start": 813652,
    "end": 815906,
    "text": "そして最後にこうなる。"
  },
  {
    "start": 816008,
    "end": 818334,
    "text": "基本的に推論はできる。"
  },
  {
    "start": 818462,
    "end": 827874,
    "text": "ここでは基本的に引用符を渡し、魔法のような3つの文字を入力する。"
  },
  {
    "start": 827912,
    "end": 832002,
    "text": "まだそんなに長くトレーニングしていないから、ループしているように見えるんだ。"
  },
  {
    "start": 832066,
    "end": 837590,
    "text": "データの中に文末タグのようなものを入れることも検討できるだろう。"
  },
  {
    "start": 837660,
    "end": 839266,
    "text": "世界は君のものだ。"
  },
  {
    "start": 839298,
    "end": 842038,
    "text": "世界と牡蠣というキーワードをうまく使っている。"
  },
  {
    "start": 842134,
    "end": 842826,
    "text": "見てみよう。"
  },
  {
    "start": 842928,
    "end": 846650,
    "text": "本が多すぎて時間がない。"
  },
  {
    "start": 846800,
    "end": 850870,
    "text": "トークンの最大数を変更することもできる。"
  },
  {
    "start": 850950,
    "end": 852534,
    "text": "さて、本が多すぎて時間がない。"
  },
  {
    "start": 852592,
    "end": 857870,
    "text": "本が生まれ、読書の時間、読書、執筆の時間、執筆、また消える。"
  },
  {
    "start": 857940,
    "end": 858654,
    "text": "それはわかるだろう。"
  },
  {
    "start": 858692,
    "end": 861194,
    "text": "よし、リピートモードに入るぞ。"
  },
  {
    "start": 861242,
    "end": 863890,
    "text": "これは、おそらく、もっとたくさんやれば助けになるだろう。"
  },
  {
    "start": 863960,
    "end": 865122,
    "text": "何か入れよう。"
  },
  {
    "start": 865176,
    "end": 871698,
    "text": "なるほど、ペップとローラを使ったトレーニングモデルはクールだ。"
  },
  {
    "start": 871784,
    "end": 872322,
    "text": "見てみよう。"
  },
  {
    "start": 872376,
    "end": 875860,
    "text": "それで何を選ぶんだ？"
  },
  {
    "start": 876550,
    "end": 882694,
    "text": "そのうちのいくつかは、明らかにキーワードを選び出すことができるが、そのうちのいくつかは、他のものも選び出すことができる。"
  },
  {
    "start": 882732,
    "end": 883302,
    "text": "今は面白い。"
  },
  {
    "start": 883356,
    "end": 888882,
    "text": "トレーニングもあるし、ここで教えてもペフトとローラはうまくいかない。"
  },
  {
    "start": 888946,
    "end": 892682,
    "text": "まだ以前のトレーニングが残っているのがわかるだろう。"
  },
  {
    "start": 892736,
    "end": 899418,
    "text": "トレーニングモデルに関連したものがいくつかあるようだが、それは跳ね返されているようだ。"
  },
  {
    "start": 899504,
    "end": 904574,
    "text": "これを本当にモデルとして使いたいのであれば、もっと長い時間トレーニングする必要があるだろう。"
  },
  {
    "start": 904692,
    "end": 915274,
    "text": "これは、peftを使って因果言語モデルを作成し、ローラを使ってより大きな因果言語モデルを微調整する方法の良い例を示している。"
  },
  {
    "start": 915322,
    "end": 918078,
    "text": "そうすれば、その分を特に欲しいものに使うことができる。"
  },
  {
    "start": 918164,
    "end": 920510,
    "text": "データセットで遊ぶのはとても簡単だ。"
  },
  {
    "start": 920580,
    "end": 922606,
    "text": "全体をここにまとめる。"
  },
  {
    "start": 922788,
    "end": 925858,
    "text": "いつものように、質問があればコメント欄に書いてください。"
  },
  {
    "start": 925954,
    "end": 932886,
    "text": "もしこのビデオが役に立ったと思われたら、「いいね！」と「購読」をクリックしてください。"
  },
  {
    "start": 933068,
    "end": 934278,
    "text": "とりあえず、さようなら。"
  },
  {
    "start": 934444,
    "end": 934660,
    "text": "近い。"
  }
]