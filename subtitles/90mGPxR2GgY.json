[
  {
    "start": 170,
    "end": 622,
    "text": "やあ、みんな。"
  },
  {
    "start": 676,
    "end": 2410,
    "text": "バートの新しいビデオへようこそ。"
  },
  {
    "start": 2490,
    "end": 5658,
    "text": "このビデオでは、バートについてゼロから説明する。"
  },
  {
    "start": 5754,
    "end": 7294,
    "text": "これはどういう意味だろう？"
  },
  {
    "start": 7412,
    "end": 15566,
    "text": "私は、バートを構成するすべての構成要素を、これを理解するために必要なすべての背景知識とともに実際に説明する。"
  },
  {
    "start": 15668,
    "end": 22218,
    "text": "言語モデルとは何か、言語モデルはどのように学習されるのか、言語モデルはどのように推論されるのか、といった文学的な見方から始める。"
  },
  {
    "start": 22314,
    "end": 26098,
    "text": "次に、少なくとも言語モデルによって使用されていないトランスフォーマーアーキテクチャを見てみよう。"
  },
  {
    "start": 26114,
    "end": 32338,
    "text": "エンコーダーのパートでは、埋め込みベクトル、位置エンコーディング、自己注意、因果マスクについて復習する。"
  },
  {
    "start": 32434,
    "end": 34498,
    "text": "では、バートを紹介しよう。"
  },
  {
    "start": 34594,
    "end": 41850,
    "text": "バートを学び始める前に、まず大きな予備知識を与えよう。"
  },
  {
    "start": 42430,
    "end": 45302,
    "text": "あなたはバートの各構成要素を理解している。"
  },
  {
    "start": 45446,
    "end": 51654,
    "text": "そして、左の文脈とは何か、右の文脈とは何か、バートでどのように使われるのかといった概念を見ていくことになる。"
  },
  {
    "start": 51782,
    "end": 54538,
    "text": "バートが事前にトレーニングを受けている2つのタスクを見てみよう。"
  },
  {
    "start": 54554,
    "end": 58282,
    "text": "マスケット言語モデルと次の文の予測タスク。"
  },
  {
    "start": 58426,
    "end": 68766,
    "text": "最後に、テキスト分類タスクと質問応答タスクを使って、ファインチューニングとは何か、どのようにバートをファインチューニングするかについても見ていく。"
  },
  {
    "start": 68958,
    "end": 72274,
    "text": "このビデオを見る前に知っておいてほしいことは？"
  },
  {
    "start": 72392,
    "end": 76420,
    "text": "まあ、確かに、トランスフォーマーというモデルを知っていることを願うよ。"
  },
  {
    "start": 76950,
    "end": 88534,
    "text": "例えば、トランスフォーマーのコンセプトのいくつかをレビューするとしても、そのすべてをレビューするわけではないからだ。"
  },
  {
    "start": 88572,
    "end": 101174,
    "text": "例えば、クロス・アテンションやマルチヘッド・アテンション、ノーマライゼーション、フィードフォワード層などの概念には触れない。"
  },
  {
    "start": 101222,
    "end": 103834,
    "text": "トランスフォーマーモデルのビデオではありません。"
  },
  {
    "start": 103952,
    "end": 113094,
    "text": "トランスフォーマーについてよく知らない人は、私のトランスフォーマーモデルに関する以前のビデオを見てください。"
  },
  {
    "start": 113242,
    "end": 117010,
    "text": "言語モデルの旅に出発しよう。"
  },
  {
    "start": 117350,
    "end": 119090,
    "text": "言語モデルとは何か？"
  },
  {
    "start": 119240,
    "end": 125214,
    "text": "言語モデルとは、単語列に確率を割り当てる確率モデルである。"
  },
  {
    "start": 125352,
    "end": 138470,
    "text": "実際には、言語モデルによって次のような確率を計算することができる。これは、例えば、Shanghai is a city inという文の後にChinaという単語が続く確率である。"
  },
  {
    "start": 138620,
    "end": 145354,
    "text": "この文の次にChinaという単語が来る確率は？"
  },
  {
    "start": 145392,
    "end": 153726,
    "text": "このような確率をモデル化するのが言語モデルであり、これは非常に大量のテキストを対象に学習させたニューラルネットワークである。"
  },
  {
    "start": 153908,
    "end": 159578,
    "text": "この大企業のテキストが非常に非常に大きい場合、我々は大言語モデルとも呼ぶ。"
  },
  {
    "start": 159674,
    "end": 161914,
    "text": "大規模な言語モデルの例はたくさんある。"
  },
  {
    "start": 161962,
    "end": 164862,
    "text": "例えば、リャマにはGPTがある。"
  },
  {
    "start": 165006,
    "end": 176190,
    "text": "これらは、例えばウィキペディア全体やインターネット上の何十億ものページなど、非常に大きなテキストコーパスに対して事前に学習されているため、基礎モデルとも呼ばれる。"
  },
  {
    "start": 176350,
    "end": 180866,
    "text": "そうすれば、プロンプトを出したり、微調整しながら使うことができる。"
  },
  {
    "start": 181058,
    "end": 183560,
    "text": "どうすればいいのか、後で考えてみよう。"
  },
  {
    "start": 184890,
    "end": 190922,
    "text": "では、大規模な言語モデル、あるいは一般的な言語モデルをどのようにトレーニングするかを復習しよう。"
  },
  {
    "start": 191056,
    "end": 196358,
    "text": "大規模な言語モデルの学習には、いくつかのコーパスが必要だ。"
  },
  {
    "start": 196374,
    "end": 204318,
    "text": "ウィキペディア全体でもいいし、ウェブページでもいいし、一冊の本でもいいし、何でもいい。"
  },
  {
    "start": 204484,
    "end": 210602,
    "text": "私の例では、中国語の詩について大規模な言語モデルや大規模な言語モデルを学習させたいとします。"
  },
  {
    "start": 210746,
    "end": 214702,
    "text": "詩はひとつしかない。"
  },
  {
    "start": 214756,
    "end": 216950,
    "text": "これはリバイのとても有名な詩だ。"
  },
  {
    "start": 216970,
    "end": 223054,
    "text": "中国文学や中国語を勉強しようと思ったら、最初に習う詩のひとつだ。"
  },
  {
    "start": 223182,
    "end": 226974,
    "text": "次の行だけに集中する。"
  },
  {
    "start": 227032,
    "end": 230626,
    "text": "私のベッドの前には、明るい月が浮かんでいる。"
  },
  {
    "start": 230818,
    "end": 233414,
    "text": "大規模言語モデルの学習方法を見てみよう。"
  },
  {
    "start": 233612,
    "end": 239930,
    "text": "さて、最初にすることは、モデルに教えたいラインのシーケンスを作成することだ。"
  },
  {
    "start": 240000,
    "end": 241980,
    "text": "まずは1行目から。"
  },
  {
    "start": 242510,
    "end": 253750,
    "text": "文の前に文頭と呼ばれるトークンを1つ付加して文を作成し、この文または入力はトークンで構成される。"
  },
  {
    "start": 253910,
    "end": 258474,
    "text": "この場合、非常に単純なケースでは、それぞれの単語がトークンであると考えることができる。"
  },
  {
    "start": 258522,
    "end": 265170,
    "text": "トークナイザーによっては、各単語が複数のトークンに分割されることがあるからだ。"
  },
  {
    "start": 265510,
    "end": 271118,
    "text": "簡単のために、トークナイザーは常に1つの単語を1つのトークンとして扱うとします。"
  },
  {
    "start": 271294,
    "end": 276598,
    "text": "この入力シーケンスを、言語モデルをモデリングしているニューラルネットワークに送り込む。"
  },
  {
    "start": 276684,
    "end": 279826,
    "text": "通常はトランスのエンコーダー部分だ。"
  },
  {
    "start": 279858,
    "end": 291494,
    "text": "この部分だけ、リニアとソフトマックスと一緒に、たとえばllamaのように、このトランスフォーマー・エンコーダーはトークンのシーケンスを出力する。"
  },
  {
    "start": 291542,
    "end": 300010,
    "text": "つまり、トランスフォーマーはシーケンス to シーケンスのモデルであり、10個のトークンのシーケンスを与えると、10個のトークンのシーケンスを出力する。"
  },
  {
    "start": 301950,
    "end": 306590,
    "text": "モデルを訓練するとき、入力があり、ターゲットもある。"
  },
  {
    "start": 307570,
    "end": 310970,
    "text": "これが、私たちがモデルに出力させたいものだ。"
  },
  {
    "start": 311130,
    "end": 319902,
    "text": "モデルには同じ文を出力させたいが、前置詞はつけずに、文末という最後のトークンを追加する。"
  },
  {
    "start": 319966,
    "end": 322718,
    "text": "の場合、全長は10トークンのままである。"
  },
  {
    "start": 322814,
    "end": 328918,
    "text": "は前と同じ文だが、トークンが最初にある代わりに、トークンが最後にある。"
  },
  {
    "start": 329004,
    "end": 331750,
    "text": "このトークンは文末トークンと呼ばれる。"
  },
  {
    "start": 332170,
    "end": 333222,
    "text": "さて、復習しよう。"
  },
  {
    "start": 333276,
    "end": 337090,
    "text": "なぜ文頭トークンと文末トークンが必要なのか。"
  },
  {
    "start": 337170,
    "end": 343162,
    "text": "というのも、先ほども言ったように、私たちが使っているニューラルネットワークは変換であり、シーケンスからシーケンスへのモデルだからだ。"
  },
  {
    "start": 343216,
    "end": 349910,
    "text": "つまり、入力としてn個のトークンがあれば、出力としてn個のトークンを生成するということだ。"
  },
  {
    "start": 350070,
    "end": 358394,
    "text": "例えば、このニューラルネットワークに最初のトークン、つまり文頭のトークンだけを与えると、出力として1つのトークンしか出力しない。"
  },
  {
    "start": 358442,
    "end": 363022,
    "text": "すでに学習済みの場合は、ターゲットの最初のトークンを出力する。"
  },
  {
    "start": 363166,
    "end": 368834,
    "text": "ペンに持ち替える前に"
  },
  {
    "start": 368952,
    "end": 379638,
    "text": "よし、最初の2つのトークンを入力すれば、例えば、before文頭と入力すれば、myの前にターゲットの最初の2つのトークンが出力されるはずだ。"
  },
  {
    "start": 379804,
    "end": 388546,
    "text": "もしmyの前に文頭の3つのトークンを入力すれば、出力の最初の3つのトークンを出力するはずだ。"
  },
  {
    "start": 388578,
    "end": 402002,
    "text": "だから、私のベッドの前に、ご覧のように、入力を与えるたびに、出力の最後のトークンが、すでに学習済みでターゲットにマッチしている場合は、文を完成させるために必要な次のトークンとなる。"
  },
  {
    "start": 402086,
    "end": 412430,
    "text": "例えば、文頭の最初の2つのトークンのみをbeforeに与えると、モデルはbefore myを出力し、beforeの次のトークンを出力する。"
  },
  {
    "start": 412580,
    "end": 418050,
    "text": "文頭をmyの前にすると、my bedの前に出力される。"
  },
  {
    "start": 418120,
    "end": 422526,
    "text": "myの次のトークンは、bed、et cetera、et cetera。"
  },
  {
    "start": 422558,
    "end": 430274,
    "text": "つまり、モデルにトークンを与えるたびに、モデルは次のトークンを出力シーケンスの最後のトークンとして返す。"
  },
  {
    "start": 430322,
    "end": 433062,
    "text": "これが言語モデルのトレーニング方法だ。"
  },
  {
    "start": 433196,
    "end": 441130,
    "text": "ターゲットトークンと出力が得られたら、クロスエントロピー損失である損失を計算し、逆伝播を実行する。"
  },
  {
    "start": 442590,
    "end": 446730,
    "text": "ここで、言語モデルからどのように推論するかを復習しておこう。"
  },
  {
    "start": 446880,
    "end": 453870,
    "text": "では、あなたがリヴァイの詩を暗記しなければならなかった学生で、最初の2語しか覚えていないとしよう。"
  },
  {
    "start": 454020,
    "end": 455998,
    "text": "試験を乗り切るには？"
  },
  {
    "start": 456164,
    "end": 461310,
    "text": "あなたは詩の最初の行の最初の2語しか覚えていない。"
  },
  {
    "start": 461730,
    "end": 462574,
    "text": "何ができる？"
  },
  {
    "start": 462612,
    "end": 466362,
    "text": "中国語の詩を学習した言語モデルがすでにあるとする。"
  },
  {
    "start": 466426,
    "end": 469874,
    "text": "言語モデルに詩の続きを書いてもらうこともできる。"
  },
  {
    "start": 469992,
    "end": 473746,
    "text": "もちろん、あなたが何を望んでいるか言語モデルにインプットする必要がある。"
  },
  {
    "start": 473848,
    "end": 475630,
    "text": "この入力をプロンプトと呼ぶ。"
  },
  {
    "start": 475710,
    "end": 483990,
    "text": "言語モデルに最初の2つのトークンを伝えると、モデルは詩を構成する次のトークンを考え出す。"
  },
  {
    "start": 485210,
    "end": 489718,
    "text": "この詩をもう一度復習してみよう。私のベッドの前には月明かりの池がある。"
  },
  {
    "start": 489814,
    "end": 491770,
    "text": "さあ、推論を始めよう。"
  },
  {
    "start": 492350,
    "end": 503178,
    "text": "文頭トークンを前置することで、最初の2つのトークンをモデルに与え、すでに事前学習済みのニューラルネットワークに入力する。"
  },
  {
    "start": 503354,
    "end": 506878,
    "text": "そうすれば、寝る前にモデルが出力されるはずだ。"
  },
  {
    "start": 506964,
    "end": 513006,
    "text": "この最後のトークンを入力に追加し、モデルに返す。"
  },
  {
    "start": 513108,
    "end": 520974,
    "text": "そしてモデルは次のトークンを出力する。"
  },
  {
    "start": 521102,
    "end": 532038,
    "text": "この最後のトークン（嘘）を再び入力に加え、それを再びトランスフォーマーモデルに送り、モデルは次の行のトークンを出力する。"
  },
  {
    "start": 532124,
    "end": 544570,
    "text": "そして、行の終わりか詩の終わりに到達するまで、この作業を続ける。これは、モデルの訓練方法にもよるが、文末トークンで示される。"
  },
  {
    "start": 544640,
    "end": 551990,
    "text": "私たちの場合、この詩の1行目だけを対象に学習させたとすると、言語モデルからの推論はこのようになる。"
  },
  {
    "start": 552080,
    "end": 554480,
    "text": "例えば、llamaのような言語モデルでは。"
  },
  {
    "start": 555010,
    "end": 566334,
    "text": "さて、トランスフォーマーモデルのアーキテクチャを理解しよう。少なくとも、我々が興味を持っている部分、つまりエンコーダーを理解しよう。"
  },
  {
    "start": 566382,
    "end": 570578,
    "text": "トランスフォーマーのモデルをすでに見ていたとしても、構成要素を理解する必要がある。"
  },
  {
    "start": 570664,
    "end": 572178,
    "text": "トランスフォーマーモデルに関する私のビデオ。"
  },
  {
    "start": 572264,
    "end": 574420,
    "text": "このコンセプトをもう一度おさらいしてみよう。"
  },
  {
    "start": 574950,
    "end": 579510,
    "text": "前にも言ったように、これはバニラ・トランスだ。"
  },
  {
    "start": 580410,
    "end": 587062,
    "text": "これはバニラ・トランスであり、元の論文で発表されたトランス・モデルである。"
  },
  {
    "start": 587196,
    "end": 599850,
    "text": "ほとんどの言語モデルは、実際にはエンコーダー側かデコーダー側を使ってモデル化される。"
  },
  {
    "start": 600590,
    "end": 603930,
    "text": "このモデルの構成要素をすべて確認してみよう。"
  },
  {
    "start": 604000,
    "end": 605614,
    "text": "そのすべてをレビューするつもりはない。"
  },
  {
    "start": 605652,
    "end": 612046,
    "text": "ノーマライゼーション、リニアレイヤー、フィードフォワードについては、すでにご存知のことと思うので、ここでは説明しない。"
  },
  {
    "start": 612068,
    "end": 623022,
    "text": "私が実際に興味を持っているのは、入力の埋め込み、位置エンコーディング、そして多頭の注意である。"
  },
  {
    "start": 623166,
    "end": 627400,
    "text": "より詳しい情報をお知りになりたい方は、トランスフォーマーモデルに関する私の以前のビデオをご覧ください。"
  },
  {
    "start": 628090,
    "end": 632758,
    "text": "ここで、埋め込みベクトルとは何か、どのように使うのかを復習しておこう。"
  },
  {
    "start": 632924,
    "end": 641146,
    "text": "通常、言語モデルを訓練したり推論したりするときには、プロンプトや何らかの入力を使って訓練する。"
  },
  {
    "start": 641328,
    "end": 642922,
    "text": "この入力はテキストである。"
  },
  {
    "start": 643056,
    "end": 643354,
    "text": "え？"
  },
  {
    "start": 643392,
    "end": 647078,
    "text": "まず最初にすることは、このテキストをトークンに分割することだ。"
  },
  {
    "start": 647254,
    "end": 653270,
    "text": "この単純なケースでは、各単語を分割するという非常に単純なトークン化を行う。"
  },
  {
    "start": 653440,
    "end": 655242,
    "text": "それぞれの単語がトークンになる。"
  },
  {
    "start": 655386,
    "end": 658394,
    "text": "これは言語モデルにおいては必ずしもそうではない。"
  },
  {
    "start": 658522,
    "end": 663770,
    "text": "例えば、llamaや他の言語モデルでは、BPEトークナイザーを使います。"
  },
  {
    "start": 663850,
    "end": 665026,
    "text": "バートでは、そうなるだろう。"
  },
  {
    "start": 665048,
    "end": 669726,
    "text": "私たちはワード・ピース・トークナイザーを使っているので、各単語は複数のトークンになる。"
  },
  {
    "start": 669838,
    "end": 677522,
    "text": "この単純なケースでは、各単語は実際にはトークンであり、いくつかのトークンは実際には単語にマッピングされないということにする。"
  },
  {
    "start": 677576,
    "end": 681922,
    "text": "例えば、文頭トークンは仮想的にしか存在しない特別なトークンである。"
  },
  {
    "start": 681986,
    "end": 684520,
    "text": "実際にはトレーニングテキストの一部ではない。"
  },
  {
    "start": 685450,
    "end": 690086,
    "text": "さて、まず最初にすることは、それぞれの単語がトークンになるように、トークン化を解除することだ。"
  },
  {
    "start": 690198,
    "end": 694086,
    "text": "各トークンを語彙内の位置にマッピングする。"
  },
  {
    "start": 694118,
    "end": 696842,
    "text": "非常に大きなテキスト・コーパスがあるとする。"
  },
  {
    "start": 696976,
    "end": 699162,
    "text": "このテキストは言葉で構成されている。"
  },
  {
    "start": 699296,
    "end": 702778,
    "text": "それぞれの単語はボキャブラリーのポジションを占める。"
  },
  {
    "start": 702874,
    "end": 710110,
    "text": "この場合、各単語を語彙内の位置に対応付ける。"
  },
  {
    "start": 710690,
    "end": 721058,
    "text": "次に、語彙におけるトークンの位置であるこれらの数値のそれぞれを、サイズ512の埋め込みベクトルにマッピングする。"
  },
  {
    "start": 721144,
    "end": 727242,
    "text": "さて、このサイズ512はバニラ・トランスで使われているものだ。"
  },
  {
    "start": 727326,
    "end": 732578,
    "text": "バートでは、私の記憶が間違っていなければ、サイズは768だ。"
  },
  {
    "start": 732754,
    "end": 738386,
    "text": "今のところ、私は常にバニラ・トランスのコンフィギュレーションだけを参照することにする。"
  },
  {
    "start": 738418,
    "end": 742314,
    "text": "注意書きにあるトランスが必要なものすべてです。"
  },
  {
    "start": 742512,
    "end": 749414,
    "text": "各単語のトークンの位置であるこれらの入力Idsは、それぞれ埋め込みに投影される。"
  },
  {
    "start": 749462,
    "end": 757840,
    "text": "この埋め込みは、各トークン、この場合は各単語の意味をとらえる512サイズのベクトルである。"
  },
  {
    "start": 758290,
    "end": 764110,
    "text": "各トークンの意味を理解するために、なぜ埋め込みベクトルを使うのか？"
  },
  {
    "start": 765090,
    "end": 771220,
    "text": "例えば、チェリー、デジタル、インフォメーションという言葉がある。"
  },
  {
    "start": 771910,
    "end": 773234,
    "text": "そのアイデアはこうだ。"
  },
  {
    "start": 773272,
    "end": 785590,
    "text": "埋め込みベクトルが512次元ではなく、2次元で構成され、xy平面上にこれらのベクトルを投影することができる、非常に単純な世界に住んでいると想像してください。"
  },
  {
    "start": 785930,
    "end": 800342,
    "text": "それらを投影し、埋め込みベクトルが正しく訓練されていれば、同じような意味を持つ単語は空間上で同じ方向を指し、異なる意味を持つ単語は空間上で異なる方向を指すことがわかる。"
  },
  {
    "start": 800496,
    "end": 815970,
    "text": "例えば、デジタルという言葉とインフォメーションという言葉は、同じような意味情報を捉えているので、同じような方向を指すことになり、その角度を測ることで類似性を測ることができる。"
  },
  {
    "start": 816040,
    "end": 819522,
    "text": "例えば、デジタルと情報の間の角度は非常に小さい。"
  },
  {
    "start": 819576,
    "end": 828010,
    "text": "チェリーとデジタルの間の角度はかなり大きいが、これは両者が異なる意味グループを表しているためで、意味も異なることがお分かりいただけるだろう。"
  },
  {
    "start": 828190,
    "end": 830706,
    "text": "トマトという別の単語があることを想像してほしい。"
  },
  {
    "start": 830738,
    "end": 835670,
    "text": "私たちは、トマトという言葉が、例えばチェリーと同じように、垂直方向を指していると考えている。"
  },
  {
    "start": 835740,
    "end": 841580,
    "text": "ここでは、例えばチェリーとトマトの角度が非常に小さくなるようにすることができる。"
  },
  {
    "start": 841950,
    "end": 849782,
    "text": "このベクトル間の角度は、2つのベクトル間のドット積に基づく正弦類似度を用いて測定する。"
  },
  {
    "start": 849846,
    "end": 856400,
    "text": "というのも、このドット積は、後で説明するアテンション・メカニズムで使うからだ。"
  },
  {
    "start": 858210,
    "end": 865780,
    "text": "さて、ここでトランスフォーマーモデルの原著論文で紹介されている位置エンコーディングをおさらいしておこう。"
  },
  {
    "start": 866710,
    "end": 876018,
    "text": "今は単語の意味を表すベクトルしか与えていないので、モデルに位置情報を与える必要がある。"
  },
  {
    "start": 876184,
    "end": 884994,
    "text": "さらに、この単語は文の1番目の位置にあり、この単語は文の2番目の位置にある、などとモデルに伝える必要がある。"
  },
  {
    "start": 885042,
    "end": 887138,
    "text": "これは位置エンコーディングの仕事である。"
  },
  {
    "start": 887234,
    "end": 888934,
    "text": "どのように機能するか見てみよう。"
  },
  {
    "start": 889132,
    "end": 898746,
    "text": "まず、512サイズの埋め込みベクトルに変換する前に見た漢詩の最初の行から始める。"
  },
  {
    "start": 898928,
    "end": 902858,
    "text": "そして、この埋め込みベクトルそれぞれに別のベクトルを加える。"
  },
  {
    "start": 903034,
    "end": 906394,
    "text": "これは位置エンコーディングまたは位置エンベッディングと呼ばれる。"
  },
  {
    "start": 906442,
    "end": 916174,
    "text": "私はこの2つの名前が使われているのを見たが、この位置埋め込みは、実際に文中のこの特定のトークンの位置を示している。"
  },
  {
    "start": 916222,
    "end": 924130,
    "text": "このベクトルはこのトークンのポジション1を示し、これはポジション2、ポジション3、ポジション4を示す。"
  },
  {
    "start": 924280,
    "end": 933954,
    "text": "現在、この位置埋め込みは、少なくともバニラ変換器では、一度計算され、訓練と推論の間、すべての文に再利用される。"
  },
  {
    "start": 934002,
    "end": 951260,
    "text": "つまり、ポジションゼロのトークンやポジション1のトークンには、ポジション番号1を表す特定のベクトルが付加されることになる。"
  },
  {
    "start": 951730,
    "end": 960320,
    "text": "この足し算の結果がこのベクトルで、これがエンコーダーの入力になる。"
  },
  {
    "start": 961330,
    "end": 966786,
    "text": "少なくとも元のトランスフォーマーの論文で提示されているような位置エンコーディングは、どうやって計算するのだろうか？"
  },
  {
    "start": 966888,
    "end": 972210,
    "text": "さて、3つの単語、あるいは3つのトークンで構成される文章があるとしよう。"
  },
  {
    "start": 972630,
    "end": 975682,
    "text": "これらの公式は以前にも論文で見たことがある。"
  },
  {
    "start": 975736,
    "end": 977074,
    "text": "注意を払うだけでいい。"
  },
  {
    "start": 977192,
    "end": 989914,
    "text": "サイズ512のベクトルを作成し、このベクトルの偶数次元には1つ目の式を、偶数次元には2つ目の式を使用する。"
  },
  {
    "start": 989952,
    "end": 995462,
    "text": "最初のものはポーズで、文中の単語の位置を示す。"
  },
  {
    "start": 995526,
    "end": 1007850,
    "text": "最初のトークンはゼロで、2つのIは適用するベクトルの次元を示し、2番目のベクトル、3番目の位置などでも計算できる。"
  },
  {
    "start": 1007930,
    "end": 1018062,
    "text": "例えば、I love youのように3つのトークンで構成される別の文があれば、他の文と同じベクトルを再利用する。"
  },
  {
    "start": 1018126,
    "end": 1024226,
    "text": "この特定のベクトルは、前のトークンではなく、ゼロの位置に関連している。"
  },
  {
    "start": 1024328,
    "end": 1030600,
    "text": "別のトークンがあれば、同じベクトルをゼロの位置に再利用する。"
  },
  {
    "start": 1032650,
    "end": 1035238,
    "text": "さて、復習しよう。"
  },
  {
    "start": 1035324,
    "end": 1039222,
    "text": "私たちが言語モデルで使っている自己注目とは何か？"
  },
  {
    "start": 1039286,
    "end": 1048854,
    "text": "なぜなら言語モデルは、トークン間の相互作用を計算できるように、トークン同士を関連付ける方法を見つける必要があるからだ。"
  },
  {
    "start": 1048982,
    "end": 1060714,
    "text": "例えば、トークンはそれ自体で意味を持つのではなく、文中での存在方法や文中の他のトークンとの関係によって意味を持つ。"
  },
  {
    "start": 1060762,
    "end": 1064298,
    "text": "これは、ここで行われる自己注目の仕事である。"
  },
  {
    "start": 1064324,
    "end": 1068958,
    "text": "マルチヘッドアテンションでは、マルチヘッドアテンションは見られず、シングルヘッドアテンションが見られる。"
  },
  {
    "start": 1069054,
    "end": 1078994,
    "text": "さあ、この自己注目のためのインプットを作ろう。今までは独立したトークンを使っていたのだから。"
  },
  {
    "start": 1079042,
    "end": 1082706,
    "text": "このトークンをベクトルに変換した。"
  },
  {
    "start": 1082818,
    "end": 1085270,
    "text": "そして、位置エンコーディングを追加した。"
  },
  {
    "start": 1086650,
    "end": 1092060,
    "text": "まずエンベッディングに変換し、次に位置情報をキャプチャするために位置エンコーディングを追加した。"
  },
  {
    "start": 1092670,
    "end": 1097062,
    "text": "私たちは独立して仕事をしている、と言ったのは嘘だったんだ。"
  },
  {
    "start": 1097206,
    "end": 1101642,
    "text": "実際、トランスフォーマーをコーディングするときは、常に行列形式で作業する。"
  },
  {
    "start": 1101696,
    "end": 1105962,
    "text": "これらすべてのトークン、すべてのベクトルは決して単独ではない。"
  },
  {
    "start": 1106026,
    "end": 1109246,
    "text": "彼らは常に、それらすべてを含む大きなマトリックスの中にいる。"
  },
  {
    "start": 1109348,
    "end": 1111194,
    "text": "ここで、この大きなマトリックスを作成する。"
  },
  {
    "start": 1111242,
    "end": 1116814,
    "text": "以前は、大きなマトリックスを直接扱うのは簡単ではなかった。"
  },
  {
    "start": 1116862,
    "end": 1118270,
    "text": "これで大きな行列ができた。"
  },
  {
    "start": 1118350,
    "end": 1126906,
    "text": "ここでは、これらのベクトルを1つの大きな行列にまとめ、それぞれの行をこれらのベクトルの1つとする。"
  },
  {
    "start": 1126958,
    "end": 1138354,
    "text": "例えば、1番目のベクトルは1行目になり、2番目のベクトルは2行目になり、3番目のベクトルは3行目になる。"
  },
  {
    "start": 1138482,
    "end": 1148650,
    "text": "トークンが10個あり、各トークンは512次元のベクトルで表されるため、この行列の形は10×512となる。"
  },
  {
    "start": 1149310,
    "end": 1152858,
    "text": "このマトリックスを3つコピーする。"
  },
  {
    "start": 1152944,
    "end": 1155754,
    "text": "このマトリックスの3つの同じコピー。"
  },
  {
    "start": 1155882,
    "end": 1161918,
    "text": "最初のものを \"query\"、2番目のものを \"key\"、3番目のものを \"value \"と呼ぶことにする。"
  },
  {
    "start": 1162004,
    "end": 1166850,
    "text": "おわかりのように、このマトリックスの値はすべて同じである。"
  },
  {
    "start": 1167510,
    "end": 1171662,
    "text": "なぜ3つの同じコピーを使うかというと、これが自己注意メカニズムだからだ。"
  },
  {
    "start": 1171726,
    "end": 1184050,
    "text": "つまり、2つの異なる文や2つの異なる言語のトークンを関連付ける場合、同じ文に属するトークン同士を関連付けるのである。"
  },
  {
    "start": 1184130,
    "end": 1187026,
    "text": "例えば、言語翻訳をするとき。"
  },
  {
    "start": 1187138,
    "end": 1189670,
    "text": "その場合は、クロス・アテンションについて話すことになる。"
  },
  {
    "start": 1189750,
    "end": 1191734,
    "text": "この場合、私たちは自己注意について話す。"
  },
  {
    "start": 1191782,
    "end": 1195894,
    "text": "それは言語モデルで使われるような注目度だ。"
  },
  {
    "start": 1196022,
    "end": 1196554,
    "text": "オーケー。"
  },
  {
    "start": 1196672,
    "end": 1213866,
    "text": "つまり、アテンションは、クエリのソフトマックスにキーの転置率を掛け、DKの平方根で割ったものにBを掛けたものとして計算される。"
  },
  {
    "start": 1213988,
    "end": 1215794,
    "text": "この腐敗は何なんだ？"
  },
  {
    "start": 1215912,
    "end": 1227814,
    "text": "ここでの減衰は、実際にはシナリオを単純化し、1つのヘッドだけで作業しているため、マルチヘッド注目の場合の各ヘッドのベクトルの次元を表している。"
  },
  {
    "start": 1227932,
    "end": 1230710,
    "text": "我々の場合、DKはdモデルに相当する。"
  },
  {
    "start": 1230780,
    "end": 1236280,
    "text": "これは、前に作成した埋め込みベクトルのサイズで、512である。"
  },
  {
    "start": 1237930,
    "end": 1240502,
    "text": "以前作ったマトリックスを使う。"
  },
  {
    "start": 1240556,
    "end": 1248666,
    "text": "というクエリでは、10×512となり、これにキーの移調をかけると、512×10となる。"
  },
  {
    "start": 1248768,
    "end": 1254000,
    "text": "これらは基本的に同じ行列だが、一方は転置され、一方は転置されていない。"
  },
  {
    "start": 1254450,
    "end": 1257918,
    "text": "それを512の平方根で割る。"
  },
  {
    "start": 1258084,
    "end": 1275330,
    "text": "ソフトマックスを適用すると、次のような行列ができる。この行列の各値は、あるトークンと別のトークンとのドット積のソフトマックスであり、あるトークンと別のトークンとの埋め込みである。"
  },
  {
    "start": 1275410,
    "end": 1280082,
    "text": "例えば、ベクトルを使って視覚化してみよう。"
  },
  {
    "start": 1280146,
    "end": 1294326,
    "text": "各行がトークンであり、512次元のベクトルであるため、各行には512個の数字が含まれる。"
  },
  {
    "start": 1294358,
    "end": 1303854,
    "text": "次元から512次元まで、そして1次元から512次元まで、10個の次元がある。"
  },
  {
    "start": 1304052,
    "end": 1306106,
    "text": "これはこの行列の転置行列である。"
  },
  {
    "start": 1306138,
    "end": 1315266,
    "text": "10行ではなく、1次元から512次元までの10列のベクトルを持つことになる。"
  },
  {
    "start": 1315448,
    "end": 1325754,
    "text": "次元の列ベクトルがもう1つあり、512、エトセトラ、エトセトラ、エトセトラ、エトセトラ、エトセトラ、エトセトラ、エトセトラ、エトセトラ、エトセトラ、エトセトラ、エトセトラ、エトセトラ、エトセトラ、エトセトラ。"
  },
  {
    "start": 1325902,
    "end": 1347866,
    "text": "この値は、最初の行列の最初の行と最初の行列の最初の列の内積で、これは最初のトークンの埋め込みで、覚えていれば、文頭トークンの埋め込みと文頭トークンの埋め込みです。"
  },
  {
    "start": 1348048,
    "end": 1358606,
    "text": "そしてこの値は、文頭と2番目のトークン、つまりここにあるトークン、そしてその前のトークンとの埋め込みの内積である。"
  },
  {
    "start": 1358788,
    "end": 1361850,
    "text": "これはその前だ。"
  },
  {
    "start": 1362010,
    "end": 1363834,
    "text": "次にソフトマックスを適用する。"
  },
  {
    "start": 1363882,
    "end": 1369250,
    "text": "ソフトマックスは基本的に、この行列の値を合計が1になるように変更する。"
  },
  {
    "start": 1369320,
    "end": 1380178,
    "text": "この行列の各行、たとえばこの行は合計が1、この行も合計が1、などなど。"
  },
  {
    "start": 1380354,
    "end": 1389660,
    "text": "ここでおわかりのように、この文頭の単語は、前の単語と関連づけることができる。"
  },
  {
    "start": 1390670,
    "end": 1399962,
    "text": "というのも、先に述べたように、私たちの目的は言語モデルをモデリングすることだからだ。"
  },
  {
    "start": 1400096,
    "end": 1405454,
    "text": "つまり、言語モデルとは、単語列に確率を割り当てる確率モデルである。"
  },
  {
    "start": 1405572,
    "end": 1411930,
    "text": "つまり、chinaという単語が文中の次の単語になる確率を計算したいのである。"
  },
  {
    "start": 1412010,
    "end": 1414398,
    "text": "上海はその都市である。"
  },
  {
    "start": 1414564,
    "end": 1420530,
    "text": "つまり、チャイナという言葉を、その前に来る言葉だけを条件としたいのだ。"
  },
  {
    "start": 1420600,
    "end": 1423154,
    "text": "つまり、上海はその中の都市である。"
  },
  {
    "start": 1423192,
    "end": 1429490,
    "text": "このモデルは、次のトークンを予測するために、文のこの部分だけを見ることができるはずである。"
  },
  {
    "start": 1429650,
    "end": 1441434,
    "text": "これは左コンテクストとも呼ばれるが、ここで起こっていることはそうではない。なぜなら、未来に来るトークンも過去に来るトークンと関連づけることができるからだ。"
  },
  {
    "start": 1441472,
    "end": 1452400,
    "text": "例えば、SOSという単語は前のトークンと関連しており、SOSというトークンは、たとえmyというトークンが後に来たとしても、myというトークンとも関連している。"
  },
  {
    "start": 1452850,
    "end": 1457962,
    "text": "私たちは何をするかというと、基本的には因果関係のあるマスクを導入する必要がある。"
  },
  {
    "start": 1458026,
    "end": 1459360,
    "text": "どう動くか見てみよう。"
  },
  {
    "start": 1460690,
    "end": 1463040,
    "text": "因果関係のあるマスクはこうだ。"
  },
  {
    "start": 1463490,
    "end": 1470622,
    "text": "前に見たような、すべての注目度や因果関係のない相互作用のある指標を使う。"
  },
  {
    "start": 1470686,
    "end": 1477854,
    "text": "その右に来る単語と単語の相互作用はすべてマイナス無限大に置き換えられる。"
  },
  {
    "start": 1477902,
    "end": 1483974,
    "text": "例えば、文頭は前の単語と関連付けられないようにする。"
  },
  {
    "start": 1484092,
    "end": 1489666,
    "text": "ソフトマックスを適用する前に、両者の相互作用をマイナス無限大に置き換える。"
  },
  {
    "start": 1489858,
    "end": 1496166,
    "text": "ということは、例えば、beforeという単語はmy bad lies poolという単語を見ることはできないはずだ。"
  },
  {
    "start": 1496198,
    "end": 1498902,
    "text": "これらの相互作用もすべてマイナス無限大に置き換えられる。"
  },
  {
    "start": 1498966,
    "end": 1505950,
    "text": "基本的に、ここに見える主対角線より上の値はすべてマイナス無限大に置き換えられる。"
  },
  {
    "start": 1506290,
    "end": 1508394,
    "text": "次にソフトマックスを適用する。"
  },
  {
    "start": 1508522,
    "end": 1523010,
    "text": "ソフトマックスの公式を覚えているなら、ここにあるように、分子はeのzi乗であり、これはソフトマックスを適用する項目であり、eのマイナス無限大乗はゼロになる。"
  },
  {
    "start": 1523080,
    "end": 1531698,
    "text": "ソフトマックスを適用すると、ソフトマックスによってゼロに置き換えられる。"
  },
  {
    "start": 1531874,
    "end": 1542762,
    "text": "こうすることで、文頭の単語とその後に続くすべてのトークンとの間の相互作用の情報に、モデルがアクセスすることがなくなる。"
  },
  {
    "start": 1542896,
    "end": 1544890,
    "text": "ゼロに置き換えるからだ。"
  },
  {
    "start": 1544960,
    "end": 1552554,
    "text": "このトークンの間に何らかのつながりがあったとしても、モデルがそれを学習することはできない。"
  },
  {
    "start": 1552752,
    "end": 1554970,
    "text": "こうしてモデルは因果関係を持つようになる。"
  },
  {
    "start": 1555050,
    "end": 1559566,
    "text": "前のトークンをすべて見ることができる唯一のトークンが、この明るいトークンだ。"
  },
  {
    "start": 1559588,
    "end": 1564002,
    "text": "なぜなら、このトークンはその前にあるすべてのトークンを見ることができるからである。"
  },
  {
    "start": 1564056,
    "end": 1566820,
    "text": "だからこのラインにはゼロがない。"
  },
  {
    "start": 1569670,
    "end": 1574162,
    "text": "さて、注目の公式では、vとの掛け算もある。"
  },
  {
    "start": 1574216,
    "end": 1581462,
    "text": "ソフトマックスの出力、つまり因果マスクを適用する前に見た行列を取り出し、次にvを掛ける。"
  },
  {
    "start": 1581596,
    "end": 1585922,
    "text": "そうすれば、なぜ私たちが因果律MAXを適用するのかが理解できるだろう。"
  },
  {
    "start": 1586066,
    "end": 1587970,
    "text": "形状を見直そう。"
  },
  {
    "start": 1588050,
    "end": 1596614,
    "text": "この行列は10×10の行列で、この行列Vはベクトルで作った初期行列である。"
  },
  {
    "start": 1596662,
    "end": 1602640,
    "text": "これは10×512の値行列で、前に作った3つの同じコピーのうちの1つだ。"
  },
  {
    "start": 1603010,
    "end": 1608880,
    "text": "この2つの行列の掛け算は、10×512の出力行列を生成する。"
  },
  {
    "start": 1609250,
    "end": 1611600,
    "text": "どのように出力されるか見てみよう。"
  },
  {
    "start": 1613250,
    "end": 1618254,
    "text": "さて、これはベクトルの行からなる行列だ。"
  },
  {
    "start": 1618302,
    "end": 1621282,
    "text": "最初の行はサイズ512のベクトルである。"
  },
  {
    "start": 1621416,
    "end": 1624594,
    "text": "2行目はサイズ512のベクトルである。"
  },
  {
    "start": 1624712,
    "end": 1627670,
    "text": "3番目も512サイズのベクトルである。"
  },
  {
    "start": 1627740,
    "end": 1641154,
    "text": "ここに512の次元があり、これもまた1次元、2次元、3512次元、1次元、2次元、3512次元、エトセトラ、エトセトラ。"
  },
  {
    "start": 1641282,
    "end": 1649766,
    "text": "出力トークンも同じ形をしているので、10×512、つまり512次元のベクトルが10個あることになる。"
  },
  {
    "start": 1649798,
    "end": 1662030,
    "text": "次元、2次元、3次元が512まで、1次元、2次元、3次元が512まで、などなど。"
  },
  {
    "start": 1662370,
    "end": 1668126,
    "text": "では、この行列の最初の値を得るために、この積を手でやってみよう。"
  },
  {
    "start": 1668158,
    "end": 1678290,
    "text": "注目出力マトリックスの最初のベクトルの次元1、この値は最初の行のドット積である。"
  },
  {
    "start": 1678370,
    "end": 1691718,
    "text": "この行は、この行列の最初の列と一緒になっているので、入力の各トークンの埋め込みの最初の次元となる。"
  },
  {
    "start": 1691894,
    "end": 1718722,
    "text": "ご覧のように、因果関係のマスクのために、このマトリックスの値のほとんどはゼロです。これは、ここでの出力値が、最初のトークンの最初の次元しか見ることができないことを意味します。"
  },
  {
    "start": 1718856,
    "end": 1722018,
    "text": "例えば、2番目の出力を見てみよう。"
  },
  {
    "start": 1722104,
    "end": 1733042,
    "text": "この行列の2行目の1次元目、注目、ここに出力、つまりこの値は初期行列の2行目に由来する。"
  },
  {
    "start": 1733106,
    "end": 1738178,
    "text": "この行列の1列目と掛け合わせたもの。"
  },
  {
    "start": 1738354,
    "end": 1742046,
    "text": "これで、ゼロでない2つの値ができた。"
  },
  {
    "start": 1742098,
    "end": 1750438,
    "text": "つまり、この出力は最初の2つのトークンのみに依存することになる。"
  },
  {
    "start": 1750624,
    "end": 1753870,
    "text": "こうして因果関係のあるモデルにするのだ。"
  },
  {
    "start": 1754850,
    "end": 1760218,
    "text": "さて、バートとバートの背後にある建築を探求する準備が整った。"
  },
  {
    "start": 1760394,
    "end": 1765954,
    "text": "バートのアーキテクチャーもトランスフォーマーモデルのエンコーダーを使用している。"
  },
  {
    "start": 1766072,
    "end": 1769134,
    "text": "入力エンベッディングもあれば、位置エンコーディングもある。"
  },
  {
    "start": 1769182,
    "end": 1772066,
    "text": "位置エンコーディングは実際には異なることがわかるだろう。"
  },
  {
    "start": 1772248,
    "end": 1779734,
    "text": "そして、自己注意があり、正常化があり、フィードフォワードがある。"
  },
  {
    "start": 1779852,
    "end": 1786178,
    "text": "しかし、バートを使用する特定のタスクに応じて変化することは後で説明する。"
  },
  {
    "start": 1786354,
    "end": 1789314,
    "text": "バートは2つの事前学習済みモデルで導入された。"
  },
  {
    "start": 1789362,
    "end": 1791978,
    "text": "ひとつはバート・ベース、もうひとつはバート・ラージだ。"
  },
  {
    "start": 1792144,
    "end": 1801738,
    "text": "例えば、バート・ベースには12層のエンコーダー・レイヤーがあり、このブロック、つまりここに見えるグレーのブロックが12回、次々に繰り返されることになる。"
  },
  {
    "start": 1801824,
    "end": 1807738,
    "text": "最後のレイヤーの出力は、この線形レイヤーに送られ、さらにソフトマックスに送られる。"
  },
  {
    "start": 1807914,
    "end": 1812206,
    "text": "フィードフォワード層の隠れサイズは3072である。"
  },
  {
    "start": 1812228,
    "end": 1819586,
    "text": "ここにあるフィードフォワード層は、基本的に2つの線形層で、特徴量のサイズは3072である。"
  },
  {
    "start": 1819608,
    "end": 1821442,
    "text": "なら12人だ。"
  },
  {
    "start": 1821496,
    "end": 1823122,
    "text": "マルチヘッドに注目。"
  },
  {
    "start": 1823176,
    "end": 1823918,
    "text": "注目してほしい。"
  },
  {
    "start": 1824094,
    "end": 1826082,
    "text": "そして、バート・ラージにはこの数字がある。"
  },
  {
    "start": 1826216,
    "end": 1830034,
    "text": "さて、バートとバニラ・トランスの違いは何だろう？"
  },
  {
    "start": 1830162,
    "end": 1841210,
    "text": "最初の違いは、埋め込みベクトルが512ではなくなり、Bert baseでは768、Bert largeでは1024になったことだ。"
  },
  {
    "start": 1841360,
    "end": 1848940,
    "text": "今後、私は常に768という数字を参照し、バート基底にベクトルを埋め込むことにする。"
  },
  {
    "start": 1850030,
    "end": 1858046,
    "text": "もうひとつの違いは、バニラ・トランスフォームの位置エンコーディングは、前に見たサイン関数とコサイン関数を使って計算されていることだ。"
  },
  {
    "start": 1858228,
    "end": 1868660,
    "text": "Bertでは、実はこれらの位置埋め込みは、固定された関数を用いてあらかじめ計算されたものではなく、実際には学習中に学習される埋め込みなのである。"
  },
  {
    "start": 1869110,
    "end": 1874290,
    "text": "もちろん、埋め込みベクトルの大きさは同じである。"
  },
  {
    "start": 1874360,
    "end": 1880840,
    "text": "バート・ベースでは768、バート・ラージでは1024の寸法がある。"
  },
  {
    "start": 1881290,
    "end": 1900250,
    "text": "これらの埋め込みベクトルは、申し訳ないが、これらの位置埋め込みは512位置に制限されている。つまり、位置を表すベクトルが512個しかないため、birdは512トークンより長い文章を扱うことができない。"
  },
  {
    "start": 1900590,
    "end": 1903966,
    "text": "リニアレイヤーヘッドは用途によって変わる。"
  },
  {
    "start": 1904068,
    "end": 1921998,
    "text": "このリニア・レイヤーでも、バートはこれまで使ってきたトークナイザー、つまり単純なトークナイザーではなく、ワード・ピース・トークナイザーと呼ばれるものを使っている。"
  },
  {
    "start": 1922174,
    "end": 1927320,
    "text": "語彙のサイズは、バート・ベースでおよそ33万トークンで、バートは大きい。"
  },
  {
    "start": 1928330,
    "end": 1934482,
    "text": "では、バートとGPTやllamaのような言語モデルの違いを見てみよう。"
  },
  {
    "start": 1934626,
    "end": 1940726,
    "text": "私のスライドでは、このようなモデルを一般的な言語モデルと呼んでいます。"
  },
  {
    "start": 1940758,
    "end": 1942070,
    "text": "GPTやラマみたいにね。"
  },
  {
    "start": 1942150,
    "end": 1951782,
    "text": "Bertはそれらとは異なり、特別なタスクをプロンプトで処理するのではなく、微調整によって特定のタスクに特化させることができる。"
  },
  {
    "start": 1951846,
    "end": 1954910,
    "text": "これが何を意味するかは、次のスライドをご覧いただきたい。"
  },
  {
    "start": 1955650,
    "end": 1960954,
    "text": "つ目の違いは、バートが左の文脈と右の文脈を使って訓練されていることだ。"
  },
  {
    "start": 1961082,
    "end": 1963840,
    "text": "これが何を意味するかは、また後ほど。"
  },
  {
    "start": 1964690,
    "end": 1974542,
    "text": "Bertはテキスト生成のために特別に作られたわけではないので、例えば、プロンプトが与えられたらllamaを使って大きな記事を生成することはできるが、この目的のためにBertを使うことはできない。"
  },
  {
    "start": 1974686,
    "end": 1978440,
    "text": "バートは他の種類の仕事にも役立つ。"
  },
  {
    "start": 1978810,
    "end": 1983042,
    "text": "Bertは次のトークン予測タスクのトレーニングを受けていない。"
  },
  {
    "start": 1983186,
    "end": 1992390,
    "text": "最初に漢詩で訓練したモデルは、次のトークン予測タスクで訓練されたが、バートはこの特定のタスクでは訓練されていない。"
  },
  {
    "start": 1992470,
    "end": 1996950,
    "text": "Bertはマスケット言語モデルと次文予測タスクで学習された。"
  },
  {
    "start": 1997030,
    "end": 1998860,
    "text": "二人とも見ることになるだろう。"
  },
  {
    "start": 1999630,
    "end": 2007290,
    "text": "では、GPTやllamaで異なるタスクをどのように処理するか、Bertでどのように処理するか見てみよう。"
  },
  {
    "start": 2007450,
    "end": 2012842,
    "text": "GPTを使うか、llamaを使うか。"
  },
  {
    "start": 2012906,
    "end": 2031094,
    "text": "これはフーゾット・プロンプトと呼ばれるもので、プロンプトの中でタスクを処理する方法をモデルに教え、最後にプロンプトなしでモデルに答えを言わせる。"
  },
  {
    "start": 2031132,
    "end": 2035474,
    "text": "プロンプトの最後の部分では、モデルに次のトークンを使って答えさせる。"
  },
  {
    "start": 2035602,
    "end": 2050060,
    "text": "つまり、例えば、文脈と質問が与えられたときに、どのように答えを構築するかをモデルに指示すれば、文脈と質問が与えられたときに、モデルは前の例から考えて意味のある答えを導き出すことができるはずだ。"
  },
  {
    "start": 2050990,
    "end": 2066098,
    "text": "バートでは、チャットGPTやラマ、GPTのようにプロンプトで作業するのではなく、バートを特定のタスクに微調整する。"
  },
  {
    "start": 2066264,
    "end": 2079634,
    "text": "前にも言ったように、言語モデルとは、各単語の左側の文脈、つまり各単語の左側に来るトークンだけを用いて次のトークンを予測するモデルである。"
  },
  {
    "start": 2079762,
    "end": 2081698,
    "text": "バートの場合はそうではない。"
  },
  {
    "start": 2081794,
    "end": 2085042,
    "text": "バートは左コンテクストと右コンテクストを使う。"
  },
  {
    "start": 2085186,
    "end": 2087750,
    "text": "その理由を直感的に理解してもらいたい。"
  },
  {
    "start": 2087820,
    "end": 2091670,
    "text": "また、私たち人間は左右の文脈を使い分けているのかもしれない。"
  },
  {
    "start": 2091750,
    "end": 2103390,
    "text": "私は言語学者ではないので、私の直感は技術的に正しいとは言えないかもしれないが、人間の会話における左右の文脈の重要性を理解するのに役立つだろう。"
  },
  {
    "start": 2103730,
    "end": 2105626,
    "text": "まずは左の文脈から。"
  },
  {
    "start": 2105738,
    "end": 2109738,
    "text": "人間の会話における左の文脈は、電話で会話をするたびに使われる。"
  },
  {
    "start": 2109834,
    "end": 2115626,
    "text": "例えば、オペレーターの回答はユーザーの入力に基づいている。"
  },
  {
    "start": 2115658,
    "end": 2118690,
    "text": "ユーザーが何かを言えば、オペレーターも何かを言う。"
  },
  {
    "start": 2118760,
    "end": 2127922,
    "text": "そして、ユーザーはオペレーターが言ったことに基づいて何か返事をし、オペレーターは前の会話で与えられた文脈に基づいて話を続ける。"
  },
  {
    "start": 2128066,
    "end": 2130390,
    "text": "これは左のコンテキストを使って呼び出される。"
  },
  {
    "start": 2130810,
    "end": 2133970,
    "text": "正しい文脈をイメージするのは難しい。"
  },
  {
    "start": 2134130,
    "end": 2138498,
    "text": "例えば、ある子供が母親のお気に入りのネックレスを壊してしまったとする。"
  },
  {
    "start": 2138594,
    "end": 2143078,
    "text": "子供は母親に本当のことを言いたくないので、嘘をでっち上げることにした。"
  },
  {
    "start": 2143174,
    "end": 2148198,
    "text": "お気に入りのネックレスが壊れてしまったんだ。"
  },
  {
    "start": 2148294,
    "end": 2154346,
    "text": "子供はこう言うかもしれない、「ママ、今部屋で猫が遊んでいるのを見たんだけど、お気に入りのネックレスが壊れてしまったよ」。"
  },
  {
    "start": 2154458,
    "end": 2161770,
    "text": "あるいは、お母さん、エイリアンがレーザー銃を持って窓から入ってきて、お気に入りのネックレスが壊れてしまった、と言うかもしれない。"
  },
  {
    "start": 2161850,
    "end": 2168066,
    "text": "おわかりのように、この子は次に何を言いたいかを考えて嘘をついたのだ。"
  },
  {
    "start": 2168248,
    "end": 2180690,
    "text": "というのも、彼は結論を出す前に、文脈を作りたいからだ。"
  },
  {
    "start": 2180770,
    "end": 2186742,
    "text": "彼は次に何を言いたいかによって、最初に選ぶ言葉を決めている。"
  },
  {
    "start": 2186876,
    "end": 2189558,
    "text": "それが嘘をでっち上げることの定義だ。"
  },
  {
    "start": 2189654,
    "end": 2195370,
    "text": "これは、人間が適切な文脈をどのように使うかについての直感かもしれない。"
  },
  {
    "start": 2195710,
    "end": 2197820,
    "text": "あなたにも見てほしい。"
  },
  {
    "start": 2198510,
    "end": 2211162,
    "text": "例えば、llamaやGPTは、大規模なテキスト・コーパスで事前学習されたモデルです。"
  },
  {
    "start": 2211306,
    "end": 2224366,
    "text": "Bertは、llamaやGPTのように次のトークン予測タスクを使った事前学習ではなく、マスケット言語モデルと次の文予測タスクという2つの特定のタスクを使った事前学習が行われている。"
  },
  {
    "start": 2224478,
    "end": 2225700,
    "text": "おさらいしておこう。"
  },
  {
    "start": 2227350,
    "end": 2236210,
    "text": "マスケット銃の言語モデルは別名「洋服タスク」とも呼ばれ、大学時代の論文やテストで習ったことがあるかもしれない。"
  },
  {
    "start": 2236290,
    "end": 2244138,
    "text": "例えば、教師が単語が1つ欠けている文章を与え、空いたスペースを欠けている単語で埋めなければならない。"
  },
  {
    "start": 2244224,
    "end": 2247594,
    "text": "このようにバートはマスケット言語モデルでトレーニングされている。"
  },
  {
    "start": 2247712,
    "end": 2254458,
    "text": "基本的に、彼らはバートを訓練するためにコーパスからいくつかの文章を取り出した。"
  },
  {
    "start": 2254634,
    "end": 2264974,
    "text": "いくつかのトークンをランダムに選び、このランダムなトークンをマスクと呼ばれる特別なトークンに置き換える。"
  },
  {
    "start": 2265102,
    "end": 2275410,
    "text": "そして、このマスクされた入力をバートに与え、バートは最初に取り除かれた単語を考え出さなければならない。"
  },
  {
    "start": 2276170,
    "end": 2277670,
    "text": "1つ以上の単語。"
  },
  {
    "start": 2277820,
    "end": 2280626,
    "text": "技術的にどうだったかを見てみよう。"
  },
  {
    "start": 2280738,
    "end": 2286950,
    "text": "そこでまず、バートが左右の文脈をどのように使い分けているかを理解する必要がある。"
  },
  {
    "start": 2287610,
    "end": 2297594,
    "text": "だから、前に見たように、アテンションを計算するときは、クエリーのソフトマックスにキーの転置をdkの平方根で割ったものをかけ、それにvをかけたものを使う。"
  },
  {
    "start": 2297792,
    "end": 2303818,
    "text": "つまり、クエリー行列を取り出し、それにキーの転置行列を乗じる。"
  },
  {
    "start": 2303914,
    "end": 2308238,
    "text": "ソフトマックスを行い、先ほど見たこの行列を作る。"
  },
  {
    "start": 2308404,
    "end": 2321618,
    "text": "以前と違って、言語モデルでやったように、この場合、単語とその後に続く単語の相互作用を打ち消すマスクは使わない。"
  },
  {
    "start": 2321784,
    "end": 2334834,
    "text": "例えば、以前はこの値をマイナス無限大に置き換え、この値もマイナス無限大に置き換えた。"
  },
  {
    "start": 2334962,
    "end": 2345318,
    "text": "つまり、各トークンは文の中で左のトークンと右のトークンに付随する。"
  },
  {
    "start": 2345494,
    "end": 2350074,
    "text": "さて、マスク言語モデルの詳細をおさらいしよう。"
  },
  {
    "start": 2350192,
    "end": 2352842,
    "text": "では、この文章をマスクしたいとしよう。"
  },
  {
    "start": 2352906,
    "end": 2358506,
    "text": "ローマはイタリアの首都であり、そのため多くの政府機関がある。"
  },
  {
    "start": 2358698,
    "end": 2364614,
    "text": "事前学習手順では、この文から15％のトークンを選んでマスクする。"
  },
  {
    "start": 2364762,
    "end": 2369006,
    "text": "トークンが選択された場合、たとえばcapitalという単語が選択される。"
  },
  {
    "start": 2369118,
    "end": 2375150,
    "text": "そして80％の確率でマスク・トークンに置き換えられ、バートの入力となる。"
  },
  {
    "start": 2375310,
    "end": 2384466,
    "text": "10％の確率でランダムなトークンのゼブラと入れ替わり、10％の確率でまったく入れ替わらないので、元のゼブラのままである。"
  },
  {
    "start": 2384578,
    "end": 2389354,
    "text": "これら3つのケースのいずれにおいても、バートはマスクされた単語を予測しなければならない。"
  },
  {
    "start": 2389392,
    "end": 2397420,
    "text": "バートは、これら3つのインプットのそれぞれについて、それらが起こる確率に基づいて資本を出力すべきである。"
  },
  {
    "start": 2398670,
    "end": 2400794,
    "text": "だからトレーニング中、私たちは何をするのか。"
  },
  {
    "start": 2400832,
    "end": 2407870,
    "text": "例えば、これが最初の文章で、先ほどのようにcapitalという単語をマスクしたとしよう。"
  },
  {
    "start": 2408020,
    "end": 2412026,
    "text": "この結果、バートには14個のトークンが入力されることになる。"
  },
  {
    "start": 2412058,
    "end": 2416238,
    "text": "ここでメダルを数えたら、バートに渡す。"
  },
  {
    "start": 2416414,
    "end": 2419970,
    "text": "バートは変圧器モデルなので、バートは出力を生成する。"
  },
  {
    "start": 2420040,
    "end": 2425050,
    "text": "14トークンの入力は14トークンの出力になる。"
  },
  {
    "start": 2425230,
    "end": 2429798,
    "text": "マスクされたポジションだけをチェックするのだ。"
  },
  {
    "start": 2429884,
    "end": 2439622,
    "text": "このトークンを資本金や目標と比較し、損失を計算して逆伝播を実行する。"
  },
  {
    "start": 2439686,
    "end": 2445210,
    "text": "基本的には、このトークンを資本金にしたい。"
  },
  {
    "start": 2447390,
    "end": 2451150,
    "text": "さて、次の文章予測タスクの復習をしよう。"
  },
  {
    "start": 2451490,
    "end": 2456110,
    "text": "次の文の予測タスクは、テキストがあることを意味する。"
  },
  {
    "start": 2456260,
    "end": 2459310,
    "text": "ランダムに2つの文章を選ぶ。"
  },
  {
    "start": 2459890,
    "end": 2468654,
    "text": "実際には、ランダムに1文、つまりAという文を選び、50％の確率でそのすぐ次の文を選ぶ。"
  },
  {
    "start": 2468702,
    "end": 2474882,
    "text": "行目、つまり50％の確率で、残りのテキストから無作為に文章を選ぶ。"
  },
  {
    "start": 2475016,
    "end": 2477238,
    "text": "この場合は3行目だ。"
  },
  {
    "start": 2477324,
    "end": 2483414,
    "text": "この場合、1行目を文A、3行目を文Bとした。"
  },
  {
    "start": 2483532,
    "end": 2486562,
    "text": "直後の文章ではない。"
  },
  {
    "start": 2486706,
    "end": 2495034,
    "text": "この2つの文をバートに送り、バートは文Aの直後に文Bが来るかどうかを予測しなければならない。"
  },
  {
    "start": 2495152,
    "end": 2503402,
    "text": "この場合、Bの文は3行目、Aの文は1行目なので、直後ではない。"
  },
  {
    "start": 2503466,
    "end": 2506158,
    "text": "バートは次はないと答えるべきだ。"
  },
  {
    "start": 2506324,
    "end": 2512240,
    "text": "2行目をセンテンスBとした場合、バートは次のように答えるはずだ。"
  },
  {
    "start": 2513750,
    "end": 2515362,
    "text": "ここには2つの問題がある。"
  },
  {
    "start": 2515496,
    "end": 2520174,
    "text": "バートの入力になるように2つの文章をエンコードするにはどうすればいいのか？"
  },
  {
    "start": 2520222,
    "end": 2529330,
    "text": "2つ目の問題は、バートがどうやって次の文章だと言えるのか、あるいは次の文章ではないと言えるのか、ということだ。"
  },
  {
    "start": 2529490,
    "end": 2531160,
    "text": "まあ、考え方はこうだ。"
  },
  {
    "start": 2531530,
    "end": 2538678,
    "text": "2つのセンテンスを1つのセンテンスとしてエンコードし、1つの入力とする。"
  },
  {
    "start": 2538774,
    "end": 2547574,
    "text": "二つの文のトークンが連結された一つの入力になる。"
  },
  {
    "start": 2547702,
    "end": 2550022,
    "text": "次に、最初の文のトークン。"
  },
  {
    "start": 2550086,
    "end": 2552618,
    "text": "という文章があったとする。"
  },
  {
    "start": 2552714,
    "end": 2559306,
    "text": "次に、セパレーターと呼ばれるトークンを追加し、次に第2文のトークンを追加する。"
  },
  {
    "start": 2559418,
    "end": 2561790,
    "text": "次に、もう1つのトークンのセップ。"
  },
  {
    "start": 2563410,
    "end": 2573682,
    "text": "問題は、この入力だけをバートに与えても、バートはこのmyが文aに属し、このlikesが文bに属することを理解できないことだ。"
  },
  {
    "start": 2573816,
    "end": 2583702,
    "text": "前にもお話ししたように、トランスフォーマーを使う言語モデルに入力を与えるときは、まずそれをトークン化する。"
  },
  {
    "start": 2583756,
    "end": 2592214,
    "text": "そして、バニラ変換の場合は512、Bertの場合は768のサイズの埋め込みベクトルに変換する。"
  },
  {
    "start": 2592342,
    "end": 2596422,
    "text": "次に、このトークンの位置を表す別のベクトルを追加する。"
  },
  {
    "start": 2596486,
    "end": 2601670,
    "text": "Bertの位置埋め込みに続いて、セグメント埋め込みと呼ばれる埋め込みがある。"
  },
  {
    "start": 2601750,
    "end": 2612398,
    "text": "これは、位置埋め込みに追加し、トークン埋め込みに追加する別のベクトルで、このトークンが文aに属するか、文bに属するかを表します。"
  },
  {
    "start": 2612484,
    "end": 2616342,
    "text": "これがバートの入力をエンコードする方法だ。"
  },
  {
    "start": 2616506,
    "end": 2619298,
    "text": "どのようにトレーニングするのか見てみよう。"
  },
  {
    "start": 2619384,
    "end": 2632854,
    "text": "この詩の1行目と3行目、そしてここに見えるセパレーター・トークン、そしてClsという特別なトークンを入力として作成する。"
  },
  {
    "start": 2633052,
    "end": 2641274,
    "text": "また、これらのトークンはすべて文Aに属し、これらのトークンはすべて文Bに属するという情報もエンコードする。"
  },
  {
    "start": 2641312,
    "end": 2646780,
    "text": "ここで紹介した特殊なセグメント埋め込みを使用する。"
  },
  {
    "start": 2647310,
    "end": 2650530,
    "text": "この入力をバートに送る。"
  },
  {
    "start": 2650710,
    "end": 2656190,
    "text": "バートはトランスフォーマーモデルなので出力が出る。"
  },
  {
    "start": 2656260,
    "end": 2660990,
    "text": "20トークンの入力は20トークンの出力に対応する。"
  },
  {
    "start": 2661330,
    "end": 2669810,
    "text": "出力の最初のトークン、トークンCLS（classifierの略）に対応するものだけを取り出す。"
  },
  {
    "start": 2670630,
    "end": 2679446,
    "text": "次を示す特徴量と次でないことを示す特徴量の2つだけを出力する線形層に送る。"
  },
  {
    "start": 2679548,
    "end": 2681218,
    "text": "ソフト・マックスを適用する。"
  },
  {
    "start": 2681314,
    "end": 2691480,
    "text": "バートには、2行目ではなく、3行目をB文として与えたので、次はないと言うだろうと予想する。"
  },
  {
    "start": 2692090,
    "end": 2697446,
    "text": "そして、クロスエントロピー損失である損失を計算し、重みを更新するために逆伝播を実行した。"
  },
  {
    "start": 2697478,
    "end": 2701370,
    "text": "こうして、次の文の予測タスクでバートを訓練する。"
  },
  {
    "start": 2702430,
    "end": 2706894,
    "text": "さて、このCLSトークンだが、どのような仕組みになっているのかおさらいしておこう。"
  },
  {
    "start": 2707012,
    "end": 2711354,
    "text": "つまり、前に見たように、注目度の計算式はクエリーとキーの掛け算である。"
  },
  {
    "start": 2711402,
    "end": 2715170,
    "text": "キーの移調を768の平方根で割ったもの。"
  },
  {
    "start": 2715320,
    "end": 2720494,
    "text": "ソフトマックスを適用することで、このようなスコア指標が生まれる。"
  },
  {
    "start": 2720542,
    "end": 2722866,
    "text": "注目のスコアマトリックス"
  },
  {
    "start": 2723048,
    "end": 2732070,
    "text": "これでわかるように、CLSトークンは常に他のすべてのトークンと相互作用する。"
  },
  {
    "start": 2732410,
    "end": 2747766,
    "text": "というのも、ここでのアテンションメトリクスでは、ソフトマックスを適用する前にマスクを適用していないからである。"
  },
  {
    "start": 2747878,
    "end": 2752240,
    "text": "これらの注目値はすべて、モデルによって実際に学習される。"
  },
  {
    "start": 2752850,
    "end": 2756270,
    "text": "これがこのCLSトークンの背景にある考え方だ。"
  },
  {
    "start": 2756610,
    "end": 2760894,
    "text": "例えば、前にやった行列の掛け算をする場合。"
  },
  {
    "start": 2761012,
    "end": 2765518,
    "text": "出力行列の最初の行を計算してみよう。"
  },
  {
    "start": 2765614,
    "end": 2766500,
    "text": "見てみよう。"
  },
  {
    "start": 2767190,
    "end": 2775154,
    "text": "この行列は入力行列で、10×768である。"
  },
  {
    "start": 2775192,
    "end": 2777746,
    "text": "私のベッドの前には、明るい月が浮かんでいる。"
  },
  {
    "start": 2777858,
    "end": 2783426,
    "text": "10トークンの入力、1、2、3、などなど。"
  },
  {
    "start": 2783458,
    "end": 2793050,
    "text": "つまり、10個のトークンは、それぞれ768個の埋め込みを持つことになる。"
  },
  {
    "start": 2793120,
    "end": 2795050,
    "text": "最初の次元"
  },
  {
    "start": 2795470,
    "end": 2796806,
    "text": "最初の次元だ。"
  },
  {
    "start": 2796918,
    "end": 2800138,
    "text": "最初の次元は768まで。"
  },
  {
    "start": 2800304,
    "end": 2805006,
    "text": "この結果、出力マトリックスは10×768となる。"
  },
  {
    "start": 2805028,
    "end": 2812806,
    "text": "最初の次元は768まで、1次元は768まで、など。"
  },
  {
    "start": 2812938,
    "end": 2819490,
    "text": "ここでは、CLSトークンの位置に対応するこの出力にのみ興味がある。"
  },
  {
    "start": 2820310,
    "end": 2822926,
    "text": "最初の次元を見てみよう。"
  },
  {
    "start": 2822958,
    "end": 2843290,
    "text": "この出力トークンの次元数1は、10次元で構成されるこのベクトルと、同じく10次元で構成されるこの行列の最初の列との内積になる。"
  },
  {
    "start": 2843360,
    "end": 2845882,
    "text": "オーケー、実はここにあるのはゼロなんだ。"
  },
  {
    "start": 2845936,
    "end": 2863454,
    "text": "仮にこれが0.3と0.4だとすると、この行列の値はどれも0ではないので、出力されるCLSはすべてのトークンのアテンション・スコアにアクセスできることになる。"
  },
  {
    "start": 2863502,
    "end": 2868542,
    "text": "基本的に、このトークンはアテンション・スコアを集約する。"
  },
  {
    "start": 2868606,
    "end": 2871330,
    "text": "すべてのトークンとの関係。"
  },
  {
    "start": 2872310,
    "end": 2877534,
    "text": "CLSは会社のCEOで、あなたは株主と考えることもできる。"
  },
  {
    "start": 2877582,
    "end": 2883186,
    "text": "あなたが株主である場合、従業員に情報を尋ねるのではなく、CEOに尋ねる。"
  },
  {
    "start": 2883378,
    "end": 2893126,
    "text": "CEOの仕事は、社内のあらゆる人と話し、ゴールに到達するために必要な情報を得ることだ。"
  },
  {
    "start": 2893318,
    "end": 2895014,
    "text": "これがCLSの目標である。"
  },
  {
    "start": 2895062,
    "end": 2903930,
    "text": "CLSは、文の内部に存在するすべての情報を集約したものと考えることができ、私たちはそれを使って分類を行う。"
  },
  {
    "start": 2904010,
    "end": 2906190,
    "text": "だからCLSトークンと呼ばれている。"
  },
  {
    "start": 2907730,
    "end": 2910138,
    "text": "では、微調整について話そう。"
  },
  {
    "start": 2910234,
    "end": 2916606,
    "text": "前に見たように、バートはllamaやGPTのようなプロンプトでは機能しない。"
  },
  {
    "start": 2916718,
    "end": 2924530,
    "text": "ゼロ発プロンプティング、数発プロンプティング、思考の連鎖、その他のプロンプティング・テクニックは使えない。"
  },
  {
    "start": 2924870,
    "end": 2928694,
    "text": "バートでは、微調整をしながら作業するので、事前に訓練されたモデルを使う。"
  },
  {
    "start": 2928812,
    "end": 2935666,
    "text": "テキスト分類を行いたい場合は、テキスト分類や質問応答用のデータセットでBertを微調整する。"
  },
  {
    "start": 2935698,
    "end": 2937880,
    "text": "この2つのタスクがどのように機能するか見てみよう。"
  },
  {
    "start": 2939050,
    "end": 2941850,
    "text": "テキストの分類を行いたいとする。"
  },
  {
    "start": 2942190,
    "end": 2947018,
    "text": "テキストの分類は、テキストの一部にラベルを割り当てる作業である。"
  },
  {
    "start": 2947104,
    "end": 2952000,
    "text": "たとえば、あなたがインターネット・プロバイダーを経営していて、顧客から苦情を受けたとする。"
  },
  {
    "start": 2952690,
    "end": 2959054,
    "text": "ユーザーからのリクエストを、ハードウェアの問題、ソフトウェアの問題、課金の問題として分類したい場合があります。"
  },
  {
    "start": 2959172,
    "end": 2964770,
    "text": "例えば、この不満は間違いなくハードウェアの問題だ。"
  },
  {
    "start": 2964840,
    "end": 2970114,
    "text": "この苦情は間違いなくソフトウェアの問題で、これは間違いなく課金の問題だ。"
  },
  {
    "start": 2970312,
    "end": 2975490,
    "text": "私たちは、お客様から寄せられるこの要望を自動的に分類したいと考えています。"
  },
  {
    "start": 2975640,
    "end": 2976966,
    "text": "どうすればいいんだ？"
  },
  {
    "start": 2977148,
    "end": 2980594,
    "text": "私たちのリクエストを受けて、バートに食べさせるんだ。"
  },
  {
    "start": 2980642,
    "end": 2987510,
    "text": "バートは、3つの選択肢のうちどれがこの特定の要求を最もよく表しているかを教えてくれるはずだ。"
  },
  {
    "start": 2989070,
    "end": 2992954,
    "text": "バートはこの3つの選択肢のうちの1つをどうやって教えてくれるのだろうか？"
  },
  {
    "start": 2993072,
    "end": 2995382,
    "text": "どうすればバートにリクエストを伝えることができるのか？"
  },
  {
    "start": 2995446,
    "end": 2996300,
    "text": "見てみよう。"
  },
  {
    "start": 2996670,
    "end": 3001434,
    "text": "テスト分類のためにバートを訓練するとき、私たちは入力を作成する。"
  },
  {
    "start": 3001562,
    "end": 3009658,
    "text": "リクエストテキストの前に分類器トークン、つまりCLSトークンを付加し、それをBertに送る。"
  },
  {
    "start": 3009754,
    "end": 3011594,
    "text": "バートが出力してくる。"
  },
  {
    "start": 3011642,
    "end": 3014894,
    "text": "16の入力トークンは16の出力トークンに対応する。"
  },
  {
    "start": 3014942,
    "end": 3019330,
    "text": "CLSトークンに対応するものである。"
  },
  {
    "start": 3020870,
    "end": 3024686,
    "text": "出力を3つの出力特徴を持つ線形層に送る。"
  },
  {
    "start": 3024718,
    "end": 3027182,
    "text": "3つのクラスが考えられるからだ。"
  },
  {
    "start": 3027246,
    "end": 3029842,
    "text": "ひとつはソフトウェア、ひとつはハードウェア、ひとつは課金だ。"
  },
  {
    "start": 3029906,
    "end": 3031746,
    "text": "であれば、ソフトマックスを適用する。"
  },
  {
    "start": 3031858,
    "end": 3036786,
    "text": "私たちはそれを、バートがこの特別なリクエストについて知ることを期待していることと比較する。"
  },
  {
    "start": 3036898,
    "end": 3039670,
    "text": "つまり、この要求はハードウエアだということだ。"
  },
  {
    "start": 3040090,
    "end": 3045686,
    "text": "そして、クロスエントロピー損失である損失を計算し、最後に逆伝播を実行して重みを更新する。"
  },
  {
    "start": 3045718,
    "end": 3049770,
    "text": "こうしてバートはテキスト分類を微調整する。"
  },
  {
    "start": 3050770,
    "end": 3054330,
    "text": "次のタスクは質問に答えることだ。"
  },
  {
    "start": 3054490,
    "end": 3057118,
    "text": "質問に答えるということは、基本的にこういうことだ。"
  },
  {
    "start": 3057204,
    "end": 3062960,
    "text": "私たちは、特定の質問に対する答えを抽出する必要がある文脈を持っている。"
  },
  {
    "start": 3063330,
    "end": 3067390,
    "text": "例えば、上海は中国の都市である。"
  },
  {
    "start": 3067470,
    "end": 3069042,
    "text": "金融の中心地でもある。"
  },
  {
    "start": 3069096,
    "end": 3071554,
    "text": "ファッションの中心地であり、工業都市でもある。"
  },
  {
    "start": 3071672,
    "end": 3074926,
    "text": "問題は、中国のファッションの中心地はどこか、ということだ。"
  },
  {
    "start": 3075038,
    "end": 3082358,
    "text": "模範解答は、その単語を強調するか、文脈のどの部分に答えがあるかを教えてくれるはずだ。"
  },
  {
    "start": 3082444,
    "end": 3089750,
    "text": "バートは、文脈のどこで答えが始まり、どこで答えが終わるのかを教えてくれるはずだ。"
  },
  {
    "start": 3091310,
    "end": 3092634,
    "text": "我々には2つの問題がある。"
  },
  {
    "start": 3092752,
    "end": 3099546,
    "text": "まず、鳥が入力のどの部分が文脈で、どの部分が質問なのかを理解する方法を見つける必要がある。"
  },
  {
    "start": 3099728,
    "end": 3106122,
    "text": "第二に、鳥が答えの始まりと終わりを教えてくれる方法を見つける必要がある。"
  },
  {
    "start": 3106266,
    "end": 3108174,
    "text": "この2つの問題を見てみよう。"
  },
  {
    "start": 3108372,
    "end": 3113314,
    "text": "最初の問題は、前に見たセグメント埋め込みを使えば簡単に解決できる。"
  },
  {
    "start": 3113432,
    "end": 3121810,
    "text": "質問と文脈を一つの入力として連結し、前に見たようにセパレータトークンを真ん中に置きます。"
  },
  {
    "start": 3121880,
    "end": 3123730,
    "text": "次の文章を。"
  },
  {
    "start": 3124470,
    "end": 3137030,
    "text": "次の文予測タスクでは、質問は文aとしてエンコードされ、文脈は文bとしてエンコードされる。"
  },
  {
    "start": 3137100,
    "end": 3138970,
    "text": "この問題は解決した。"
  },
  {
    "start": 3139870,
    "end": 3142566,
    "text": "バートからどうやって答えを引き出すのか？"
  },
  {
    "start": 3142678,
    "end": 3143786,
    "text": "まあ、見てみよう。"
  },
  {
    "start": 3143888,
    "end": 3147686,
    "text": "まず、バートの入力を準備する。"
  },
  {
    "start": 3147798,
    "end": 3150102,
    "text": "の前にclsトークンを置く。"
  },
  {
    "start": 3150166,
    "end": 3152838,
    "text": "中国のファッションの中心地とは？"
  },
  {
    "start": 3152934,
    "end": 3153542,
    "text": "セパレーター。"
  },
  {
    "start": 3153606,
    "end": 3155306,
    "text": "上海は中国の都市である。"
  },
  {
    "start": 3155338,
    "end": 3158366,
    "text": "これは文bとしてエンコードされた文脈である。"
  },
  {
    "start": 3158468,
    "end": 3161518,
    "text": "最初の部分は文aとしてエンコードされている。"
  },
  {
    "start": 3161684,
    "end": 3163562,
    "text": "バートに食べさせるんだ。"
  },
  {
    "start": 3163706,
    "end": 3167486,
    "text": "バートは25個の7トークンを出力する。"
  },
  {
    "start": 3167518,
    "end": 3178934,
    "text": "入力は27個のトークンで構成されているので、どのトークンがどの文に対応するかもわかっている。"
  },
  {
    "start": 3178972,
    "end": 3185442,
    "text": "入力として与えるので、2つの出力特徴を持つ線形レイヤーを適用する。"
  },
  {
    "start": 3185506,
    "end": 3195770,
    "text": "ある特定のトークンが開始トークンであるかどうかを示す機能と、そのトークンが終了トークンであるかどうかを示す機能がある。"
  },
  {
    "start": 3196990,
    "end": 3206158,
    "text": "答えがどこにあるかはわかっている。なぜなら、答えは上海という単語であり、始まりはトークン10であり、終わりはトークン10であるべきだからだ。"
  },
  {
    "start": 3206324,
    "end": 3211562,
    "text": "そして、目標とこの線形層の出力に基づいて損失を計算する。"
  },
  {
    "start": 3211626,
    "end": 3216530,
    "text": "私たちは逆伝播を実行し、こうして質問応答のために鳥を微調整する。"
  },
  {
    "start": 3217830,
    "end": 3221380,
    "text": "みんな、僕のビデオを気に入ってくれたかな？"
  },
  {
    "start": 3222150,
    "end": 3225294,
    "text": "私はバートをとても型破りな方法で表現した。"
  },
  {
    "start": 3225342,
    "end": 3227554,
    "text": "つまり、言語モデルから始めた。"
  },
  {
    "start": 3227682,
    "end": 3243020,
    "text": "言語モデルがどのように機能するかというコンセプトを紹介し、次にワードを紹介したのは、バートがどのように機能するかということと、他の言語モデルがどのように機能するかということを比較することで、両者の長所と短所を理解してもらいたかったからだ。"
  },
  {
    "start": 3243470,
    "end": 3245482,
    "text": "バートは実はそれほど最近のモデルではない。"
  },
  {
    "start": 3245536,
    "end": 3248838,
    "text": "私の記憶が正しければ、2018年に導入された。"
  },
  {
    "start": 3249014,
    "end": 3253046,
    "text": "かなり古くなったが、今でも多くの仕事に使える。"
  },
  {
    "start": 3253238,
    "end": 3258190,
    "text": "また僕のチャンネルに遊びに来てくれると嬉しい。"
  },
  {
    "start": 3258260,
    "end": 3261102,
    "text": "このビデオが気に入ったら、購読とシェアをお願いします。"
  },
  {
    "start": 3261156,
    "end": 3264202,
    "text": "質問があれば、コメントに書いてください。"
  },
  {
    "start": 3264266,
    "end": 3267600,
    "text": "LinkedInにも積極的に参加しているので、よかったら登録してください。"
  },
  {
    "start": 3268370,
    "end": 3275826,
    "text": "特定のビデオレビューモデルをレビューしてもらいたい場合は、コメントに書いてください。"
  },
  {
    "start": 3276018,
    "end": 3280754,
    "text": "次回のビデオでは、バートをゼロからコーディングできるようにしたい。"
  },
  {
    "start": 3280802,
    "end": 3287542,
    "text": "PyTorchを使うことで、今日のビデオで学んだ知識を実践的に学ぶこともできる。"
  },
  {
    "start": 3287676,
    "end": 3290980,
    "text": "僕のチャンネルに来てくれてありがとう。"
  }
]