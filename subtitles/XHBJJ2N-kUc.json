[
  {
    "start": 330,
    "end": 5870,
    "text": "さて、全員が昼食から戻ったわけではないが、これから始めよう。"
  },
  {
    "start": 5940,
    "end": 14430,
    "text": "残りの時間は惑星モデルか何かに取り組んでいるマイルズを紹介できることを嬉しく思う。"
  },
  {
    "start": 14580,
    "end": 22490,
    "text": "今日の講演では、ニューラルネットワークの記号的蒸留について話すつもりだ。"
  },
  {
    "start": 22650,
    "end": 23680,
    "text": "頑張れ。"
  },
  {
    "start": 24970,
    "end": 26694,
    "text": "私の声が聞こえる？"
  },
  {
    "start": 26732,
    "end": 27510,
    "text": "オーケー、クールだ。"
  },
  {
    "start": 27660,
    "end": 40586,
    "text": "では、ディープ・ニューラル・ネットワークが学習した洞察を、私たちが理解できる言語に変換するための一つの戦略についてお話しします。"
  },
  {
    "start": 40688,
    "end": 46198,
    "text": "私が欲しいのはAI科学者だ。"
  },
  {
    "start": 46374,
    "end": 52800,
    "text": "私は、実際に私のために科学をしてくれて、私の科学研究を助けてくれる機械学習アルゴリズムが欲しい。"
  },
  {
    "start": 53970,
    "end": 61310,
    "text": "既存の機械学習研究の多くは、コンピューター・ビジョンや自然言語のベンチマークによって推進されてきたと思う。"
  },
  {
    "start": 62290,
    "end": 70110,
    "text": "これは、産業界がロボット工学に関心を持ち、さまざまな作業で人間レベルの性能を達成しようとする試みが動機となっている。"
  },
  {
    "start": 70190,
    "end": 75640,
    "text": "例えば、画像分類のようなものだ。"
  },
  {
    "start": 76490,
    "end": 91014,
    "text": "なぜなら、機械学習の科学研究の多くは、コンピュータ・ビジョンやNLPのベンチマークを利用し、そのデータを科学的なデータセットに置き換えただけのように感じるからです。"
  },
  {
    "start": 91062,
    "end": 95418,
    "text": "これは的外れだと思う。"
  },
  {
    "start": 95504,
    "end": 107054,
    "text": "機械学習と科学は、新しい科学を発見することから逆算して、それをサポートする問題を構築していく必要があると思う。"
  },
  {
    "start": 107252,
    "end": 113360,
    "text": "私は、AIが自然科学の研究において人間レベルに達することを望んでいる。"
  },
  {
    "start": 113810,
    "end": 114782,
    "text": "何が必要なのか？"
  },
  {
    "start": 114836,
    "end": 118722,
    "text": "つまり自然科学では、回帰の問題でも分類の問題でもない。"
  },
  {
    "start": 118776,
    "end": 122894,
    "text": "モデルが学習する事柄を理解する必要がある。"
  },
  {
    "start": 122942,
    "end": 126520,
    "text": "ここが重要な違いだ。"
  },
  {
    "start": 127850,
    "end": 136722,
    "text": "機械学習を使って普遍的な概念を発見し、それを人間の言葉で表現できるようにしたい。"
  },
  {
    "start": 136786,
    "end": 138620,
    "text": "これが決定的な違いだ。"
  },
  {
    "start": 139550,
    "end": 142454,
    "text": "では、伝統的な科学のやり方は？"
  },
  {
    "start": 142582,
    "end": 151094,
    "text": "あるデータセットがあり、それは低次元の要約統計量かもしれない。"
  },
  {
    "start": 151142,
    "end": 154640,
    "text": "私たちは、そのデータセットを説明しようとするさまざまな理論を考え出す。"
  },
  {
    "start": 155330,
    "end": 161994,
    "text": "例えば、ケプラーの第三法則は、ニュートンの重力の法則より80年前だったと思う。"
  },
  {
    "start": 162042,
    "end": 166274,
    "text": "ニュートンの重力の法則は、最終的にこれを説明するために作られた。"
  },
  {
    "start": 166472,
    "end": 172430,
    "text": "プランクの法則が実は経験的な表現であることをご存じない方のために補足しておくと、それはリンゴではなかった。"
  },
  {
    "start": 172510,
    "end": 177666,
    "text": "この関係は、データで観察されたものであり、少し理論的なものでもある。"
  },
  {
    "start": 177698,
    "end": 181960,
    "text": "量子力学は、このようなことを説明するために部分的に作られた。"
  },
  {
    "start": 182490,
    "end": 195594,
    "text": "このニューラルネットワーク、つまり多様なデータで訓練された大規模なモデルがあるとして、すべてがこのニューラルネットワークの空間に留まっているとしたら、それを説明する理論を構築することはできない。"
  },
  {
    "start": 195632,
    "end": 196122,
    "text": "そうだね。"
  },
  {
    "start": 196256,
    "end": 198380,
    "text": "これは私の悩みのようなものだ。"
  },
  {
    "start": 199230,
    "end": 215470,
    "text": "高次元のデータセットからニューラルネットに移行するときに、この違いを解決する一つのアプローチがあると思います。"
  },
  {
    "start": 215810,
    "end": 217490,
    "text": "フィードバックは得られているか？"
  },
  {
    "start": 217910,
    "end": 218754,
    "text": "分からないよ。"
  },
  {
    "start": 218872,
    "end": 220606,
    "text": "これは圧縮のようなものだ。"
  },
  {
    "start": 220638,
    "end": 222180,
    "text": "私たちはニューラルネット・スペースに行く。"
  },
  {
    "start": 223110,
    "end": 227266,
    "text": "最終段階を蒸留として行う方法をお伝えしよう。"
  },
  {
    "start": 227378,
    "end": 243238,
    "text": "つまり、ニューラルネットが学習した圧縮を理論に変換するということだ。"
  },
  {
    "start": 243414,
    "end": 252122,
    "text": "さて、このプレゼンテーションで私が皆さんに納得していただこうと思う重要なポイントは、膨大なデータセットで訓練されたニューラルネットである。"
  },
  {
    "start": 252266,
    "end": 263322,
    "text": "そのため、まったく関係のないトレーニングから学んだウェイトの中に、新しい洞察が埋もれていることもある。"
  },
  {
    "start": 263386,
    "end": 264990,
    "text": "あなたはデータセットを考えている。"
  },
  {
    "start": 265670,
    "end": 269246,
    "text": "本当の挑戦は、その洞察力を引き出し、抽出することだ。"
  },
  {
    "start": 269278,
    "end": 274130,
    "text": "では、実際に何を学び、それを科学理論の改善にどう役立てることができるのか？"
  },
  {
    "start": 274950,
    "end": 276882,
    "text": "さて、それでは解釈可能性について話そうと思う。"
  },
  {
    "start": 277026,
    "end": 280546,
    "text": "シンボリック回帰という考え方についてお話ししましょう。"
  },
  {
    "start": 280738,
    "end": 282194,
    "text": "神経の蒸留について話そう。"
  },
  {
    "start": 282242,
    "end": 284002,
    "text": "いくつか例を挙げてみよう。"
  },
  {
    "start": 284146,
    "end": 292246,
    "text": "さて、ニューラルネットワークの伝統的な解釈の仕方は、特徴の重要性に関わるものだ。"
  },
  {
    "start": 292358,
    "end": 300262,
    "text": "例えば、この画像分類器を持っていて、画像の最も敏感な部分はどこだろう？"
  },
  {
    "start": 300406,
    "end": 302094,
    "text": "私の分類器は何に敏感なのか？"
  },
  {
    "start": 302132,
    "end": 306560,
    "text": "ライオンの顔や犬の顔のようなものかもしれない。"
  },
  {
    "start": 307170,
    "end": 312350,
    "text": "これは伝統的に、コンピュータビジョンや言語における特徴の重要性を高める方法である。"
  },
  {
    "start": 313910,
    "end": 316974,
    "text": "これは2016年の論文からの楽しい例である。"
  },
  {
    "start": 317022,
    "end": 320990,
    "text": "これはライムと呼ばれる解釈可能性のテクニックを使ったものだ。"
  },
  {
    "start": 321150,
    "end": 327494,
    "text": "基本的には、モデルを調査し、何が重要かを確認することができる。"
  },
  {
    "start": 327532,
    "end": 330162,
    "text": "犬がギターを弾いている。"
  },
  {
    "start": 330226,
    "end": 343500,
    "text": "エレキギターの確率はその部分を強調し、アコースティックギターの確率はその部分を強調し、ラブラドールの確率はその部分を特定する。"
  },
  {
    "start": 343870,
    "end": 348406,
    "text": "これは、コンピュータ・ビジョンにおける解釈可能性を実現する方法のようなものだ。"
  },
  {
    "start": 348438,
    "end": 349206,
    "text": "NLP。"
  },
  {
    "start": 349318,
    "end": 355374,
    "text": "科学の世界では、すでにモデリング言語があり、それを活用することが本当に重要だと思う。"
  },
  {
    "start": 355492,
    "end": 358846,
    "text": "コンピュータ・ビジョンは、犬や猫のモデルを持っていないんだ。"
  },
  {
    "start": 358948,
    "end": 361198,
    "text": "猫に数学的モデルはない。"
  },
  {
    "start": 361284,
    "end": 362574,
    "text": "科学の世界ではそうだ。"
  },
  {
    "start": 362612,
    "end": 366526,
    "text": "私たちは宇宙を記述するための実に強力な枠組みを持っている。"
  },
  {
    "start": 366558,
    "end": 366946,
    "text": "そうだね。"
  },
  {
    "start": 367048,
    "end": 368690,
    "text": "数学的方程式。"
  },
  {
    "start": 370150,
    "end": 379110,
    "text": "物理学のカンニングペーパーを見れば、非常に多くの異なる解析式があり、それらは我々の宇宙を非常に正確にモデル化していることがわかる。"
  },
  {
    "start": 379450,
    "end": 390810,
    "text": "機械学習やサイエンスのためのインタープリタビリティ（解釈可能性）を行う際には、単なる特徴のインポータビリティ（重要性）ではなく、このような言語による説明を得るようにすべきだと思う。"
  },
  {
    "start": 391630,
    "end": 409978,
    "text": "物理的な問題では、記号的な表現でモデルを解釈すると、元のニューラルネットワークよりも優れた汎化性能が得られることがよくある。"
  },
  {
    "start": 410074,
    "end": 416478,
    "text": "もう少し例を挙げるが、これは別の帰納的バイアスのようなものだ。"
  },
  {
    "start": 416654,
    "end": 425954,
    "text": "ニューラルネットワークを訓練するときについて考えてみると、ニューラルネットワークが取りうる関数の空間について、暗黙の事前準備のようなものがある。"
  },
  {
    "start": 425992,
    "end": 426626,
    "text": "そうだね。"
  },
  {
    "start": 426808,
    "end": 433746,
    "text": "解析式の空間を探索する場合、それは関数の空間に対する事前準備とは異なる。"
  },
  {
    "start": 433778,
    "end": 436146,
    "text": "宇宙をモデル化するのにとても適しているようだ。"
  },
  {
    "start": 436258,
    "end": 443030,
    "text": "これは、私たちが慣れ親しんでいる解析的な演算子には、幾何学的な性質があるからだと思う。"
  },
  {
    "start": 443190,
    "end": 446198,
    "text": "プラスアルファは空間変換のようなものだ。"
  },
  {
    "start": 446374,
    "end": 449530,
    "text": "掛け算は、長さ対面積対体積のようなものだ。"
  },
  {
    "start": 449950,
    "end": 454986,
    "text": "となると、さまざまな微分方程式の解のような演算子があることになる。"
  },
  {
    "start": 455098,
    "end": 459466,
    "text": "これは一種のクールな帰納的バイアスだと思う。"
  },
  {
    "start": 459498,
    "end": 462270,
    "text": "機能的帰納バイアスというか。"
  },
  {
    "start": 462950,
    "end": 467998,
    "text": "ということで、象徴回帰というテクニックに行き着いた。"
  },
  {
    "start": 468174,
    "end": 471220,
    "text": "これが私がここ数年取り組んできたことだ。"
  },
  {
    "start": 472710,
    "end": 483186,
    "text": "基本的には、ある目的を最適化する分析式を探す機械学習タスクだ。"
  },
  {
    "start": 483298,
    "end": 496698,
    "text": "yはzに2の正弦zを足したものに等しいという方程式があるとすると、これはこのツリーで表される。"
  },
  {
    "start": 496864,
    "end": 506030,
    "text": "つまり、固定されたモデルのパラメーターをフィッティングするのではなく、パラメーターをフィッティングしながら樹木の空間を探索するわけだ。"
  },
  {
    "start": 507330,
    "end": 511146,
    "text": "これに対する最先端のアプローチは、実は遺伝的アルゴリズムである。"
  },
  {
    "start": 511178,
    "end": 516114,
    "text": "トランスフォーマーを使ったアプローチのようなものはあるが、まだベンチマークでは勝てない。"
  },
  {
    "start": 516152,
    "end": 520530,
    "text": "遺伝的アルゴリズムは、このタスクのための最先端技術である。"
  },
  {
    "start": 521510,
    "end": 527394,
    "text": "例えば、ツリーで突然変異を起こすかもしれない。"
  },
  {
    "start": 527522,
    "end": 530754,
    "text": "これらは、この表現空間を探求する方法にすぎない。"
  },
  {
    "start": 530802,
    "end": 533570,
    "text": "なるほど、表現を繁殖させることもできる。"
  },
  {
    "start": 533650,
    "end": 541674,
    "text": "例えば、データセットのこの部分に合う記号式があり、データセットのこの部分に合う別の記号式がある。"
  },
  {
    "start": 541712,
    "end": 544700,
    "text": "一緒に繁殖させて、交配させることもできるかもしれない。"
  },
  {
    "start": 546190,
    "end": 548714,
    "text": "これらはすべて、この空間を探求する方法にすぎない。"
  },
  {
    "start": 548832,
    "end": 557370,
    "text": "記号回帰でやろうとしていることは、フィットの精度を最適化し、複雑さを最小化することである。"
  },
  {
    "start": 557530,
    "end": 561370,
    "text": "複雑さは非常に複雑だ。"
  },
  {
    "start": 561530,
    "end": 565230,
    "text": "非常に複雑な問題だが、基本的にはユーザー定義だ。"
  },
  {
    "start": 565310,
    "end": 569890,
    "text": "ノードの数を式の複雑さと考えることもできる。"
  },
  {
    "start": 570710,
    "end": 575438,
    "text": "記号的回帰は、正確さと複雑さを最適化しようとしている。"
  },
  {
    "start": 575534,
    "end": 581762,
    "text": "私が取り組んできたオープンソースのフレームワークには、picerとsymbolic regressionがある。"
  },
  {
    "start": 581826,
    "end": 585160,
    "text": "Jl、もし興味があるなら、ぜひチェックしてみてほしい。"
  },
  {
    "start": 586090,
    "end": 590566,
    "text": "ニューラルネットワークを解釈する上で、とても役に立つテクニックだと思う。"
  },
  {
    "start": 590598,
    "end": 592380,
    "text": "その例をいくつかお見せしよう。"
  },
  {
    "start": 593710,
    "end": 602210,
    "text": "これらのコードが機能する方法は、多人数進化と呼ばれるテクニックを使うことだ。"
  },
  {
    "start": 602390,
    "end": 605674,
    "text": "本質的に、もし私がこのような表現集団を持っているとしたら。"
  },
  {
    "start": 605802,
    "end": 610560,
    "text": "これらは、それぞれが異なる方法でデータに適合する異なる候補式のようなものだ。"
  },
  {
    "start": 611490,
    "end": 613922,
    "text": "僕がやっているのは、無作為にサンプリングすることだ。"
  },
  {
    "start": 613976,
    "end": 619970,
    "text": "その中からランダムに2つくらいの表現をサンプリングし、フィットネスを計算する。"
  },
  {
    "start": 620310,
    "end": 624366,
    "text": "フィットネスとは、正確さと複雑さの組み合わせのようなものなのかもしれない。"
  },
  {
    "start": 624558,
    "end": 626978,
    "text": "適性者を選び、摂動をかける。"
  },
  {
    "start": 627074,
    "end": 633538,
    "text": "変異させるか、代数的に単純化するか、一定の最適化を行うかだ。"
  },
  {
    "start": 633714,
    "end": 635960,
    "text": "これらは空間を探索する方法に過ぎない。"
  },
  {
    "start": 636330,
    "end": 641130,
    "text": "私はその動揺の表情を人口に戻す。"
  },
  {
    "start": 641950,
    "end": 645370,
    "text": "基本的には、このループを何度も繰り返すだけだ。"
  },
  {
    "start": 645520,
    "end": 648090,
    "text": "ああ、邪魔したくなかったんだ。"
  },
  {
    "start": 649150,
    "end": 656942,
    "text": "このループを何度も繰り返せば、最終的には表現空間の一点に収束する。"
  },
  {
    "start": 656996,
    "end": 657406,
    "text": "そうだね。"
  },
  {
    "start": 657508,
    "end": 660382,
    "text": "この進化は言語モデルを使用していない。"
  },
  {
    "start": 660436,
    "end": 661882,
    "text": "これは単純なパイソンと同じだ。"
  },
  {
    "start": 661946,
    "end": 662560,
    "text": "そうだね。"
  },
  {
    "start": 663250,
    "end": 666130,
    "text": "言語モデルのアプローチもある。"
  },
  {
    "start": 668550,
    "end": 680680,
    "text": "しかし、一般的な言語モデルによるアプローチは難しい。"
  },
  {
    "start": 681770,
    "end": 691654,
    "text": "この場合、元のデータにフィットさせようとしているのか、それともニューラルネットワークをトレーニングして、その表現にニューラルネットワークをフィットさせようとしているのか、それはまた別の問題だ。"
  },
  {
    "start": 691782,
    "end": 695740,
    "text": "ああ、第二の方法、その例を示そう。"
  },
  {
    "start": 697710,
    "end": 705050,
    "text": "さて、ではこのスケールをどうするかだが、私はニューラルネットワークに戻るつもりだ。"
  },
  {
    "start": 705130,
    "end": 710046,
    "text": "これは解釈戦略なので、規模を拡大することができる。"
  },
  {
    "start": 710228,
    "end": 719410,
    "text": "これを多くのコアに拡張する方法は、異なるコアに異なる表現の集団が住んでいるようなものだ。"
  },
  {
    "start": 719750,
    "end": 724450,
    "text": "表現の島のようなもので、それぞれが進化を遂げていると考えればいい。"
  },
  {
    "start": 724810,
    "end": 727778,
    "text": "島によって特化している部分がある。"
  },
  {
    "start": 727874,
    "end": 732146,
    "text": "一つの島は多項式のようなもので、一つの島は指数なのかもしれない。"
  },
  {
    "start": 732258,
    "end": 733922,
    "text": "違う種族のようなものだ。"
  },
  {
    "start": 734066,
    "end": 735826,
    "text": "それらは独立して進化する。"
  },
  {
    "start": 736018,
    "end": 740118,
    "text": "その理由は、平行移動がうまくいくからだ。"
  },
  {
    "start": 740214,
    "end": 740522,
    "text": "そうだろう？"
  },
  {
    "start": 740576,
    "end": 751898,
    "text": "異なるコアで進化させ、何度も繰り返した後、島々の間を移動させることができる。"
  },
  {
    "start": 752074,
    "end": 760714,
    "text": "はい、すみません、あなたの手順はニューラルネットの内部をまったく見ていません。"
  },
  {
    "start": 760762,
    "end": 761230,
    "text": "そうなのか？"
  },
  {
    "start": 761300,
    "end": 763518,
    "text": "ああ、それはまた今度。"
  },
  {
    "start": 763684,
    "end": 767780,
    "text": "では、最後の質問も。"
  },
  {
    "start": 768950,
    "end": 779558,
    "text": "足し算のようなものなら大丈夫かもしれないが、これでは検索手順としてあまり進歩しないような気がする。"
  },
  {
    "start": 779644,
    "end": 791290,
    "text": "2つの式を掛け合わせることを考えた場合、出てくる式は前の式のどちらとも似ていないような気がする。"
  },
  {
    "start": 791440,
    "end": 793258,
    "text": "ああ、交配そのもののことですか？"
  },
  {
    "start": 793424,
    "end": 798182,
    "text": "ああ、あまりゴールに向かう感じはしないね。"
  },
  {
    "start": 798246,
    "end": 798522,
    "text": "そうだ。"
  },
  {
    "start": 798576,
    "end": 803786,
    "text": "育種がどのように役立つかを考える最も簡単な方法は、機能の重要性について考えることだ。"
  },
  {
    "start": 803898,
    "end": 806666,
    "text": "100個の変数があるとする。"
  },
  {
    "start": 806858,
    "end": 810378,
    "text": "最も重要な変数は、より多くの表現で登場することになる。"
  },
  {
    "start": 810474,
    "end": 817170,
    "text": "そのため、交配を行う際には、その変数がエクスプレッションツリーに入れられる可能性が高くなる。"
  },
  {
    "start": 817670,
    "end": 824062,
    "text": "もちろん、アトミックな部分式もあるので、それよりもはるかに複雑だ。"
  },
  {
    "start": 824206,
    "end": 830680,
    "text": "もしかしたら、同じサブ表現が多くの異なるツリーに現れるかもしれないし、同じ表現に複数回現れるかもしれない。"
  },
  {
    "start": 831450,
    "end": 837350,
    "text": "特徴の重要性、交配についての考え方のようなものだ。"
  },
  {
    "start": 838650,
    "end": 851520,
    "text": "遺伝的アルゴリズムというのは、本当に何もないような、間抜けなアイデアのような気がするけど、基本的には僕らが持っている最高のアイデアなんだ。"
  },
  {
    "start": 852290,
    "end": 858270,
    "text": "ヤモリでは毎年、記号回帰ベンチというコンペティションがある。"
  },
  {
    "start": 859170,
    "end": 865390,
    "text": "残念ながら、勝者は常に遺伝的アルゴリズムなのだ。"
  },
  {
    "start": 865550,
    "end": 866260,
    "text": "そうだね。"
  },
  {
    "start": 867190,
    "end": 869138,
    "text": "何の競争ですか？"
  },
  {
    "start": 869304,
    "end": 870690,
    "text": "シニアベンチ"
  },
  {
    "start": 872310,
    "end": 873758,
    "text": "シンボリック回帰ベンチ。"
  },
  {
    "start": 873854,
    "end": 874500,
    "text": "そうだね。"
  },
  {
    "start": 875110,
    "end": 880514,
    "text": "では、このスライドをもう2枚ほど見て、ニューラルネットワークの話に戻ろう。"
  },
  {
    "start": 880562,
    "end": 882962,
    "text": "検索しているときはこんな感じ。"
  },
  {
    "start": 883106,
    "end": 893782,
    "text": "この線にフィットさせようとする場合、ノードの変異、正弦への惰行、定数の最適化、より多くの候補など、さまざまな摂動を行う。"
  },
  {
    "start": 893846,
    "end": 896700,
    "text": "表現空間をMCMCしているようなものだ。"
  },
  {
    "start": 897710,
    "end": 899066,
    "text": "これに時間をかけすぎた。"
  },
  {
    "start": 899088,
    "end": 900380,
    "text": "再プレーするつもりだ。"
  },
  {
    "start": 900830,
    "end": 904570,
    "text": "よし、ノードを追加しよう。"
  },
  {
    "start": 904910,
    "end": 905974,
    "text": "これらはすべて違う。"
  },
  {
    "start": 906112,
    "end": 923170,
    "text": "私はただ、進化が変異する表現の空間を探っているだけです。ある種のMCTsアプローチがあるので、現在の表現に基づいて確率を重み付けすることができます。"
  },
  {
    "start": 924230,
    "end": 927330,
    "text": "私のコードでは、Pycerは固定確率を使っているだけだ。"
  },
  {
    "start": 927750,
    "end": 934310,
    "text": "なるほど、パイソンのAPIがあるんですね。"
  },
  {
    "start": 935610,
    "end": 937282,
    "text": "さて、ニューラルネットワークの話に戻ろう。"
  },
  {
    "start": 937346,
    "end": 948166,
    "text": "このプレゼンテーションで私が伝えたいことは、この記号的回帰を使ってニューラルネットワークを方程式に落とし込むことができるということだ。"
  },
  {
    "start": 948278,
    "end": 948650,
    "text": "オーケー。"
  },
  {
    "start": 948720,
    "end": 950880,
    "text": "そうすれば、そこから洞察を得ることができる。"
  },
  {
    "start": 953490,
    "end": 957550,
    "text": "基本的な考え方は、ニューラルネットワークを普通に訓練するというものだ。"
  },
  {
    "start": 957970,
    "end": 960590,
    "text": "ニューラルネットワークが何であれ、それを普通にトレーニングする。"
  },
  {
    "start": 961090,
    "end": 963438,
    "text": "トレーニング後、パラメーターを凍結する。"
  },
  {
    "start": 963614,
    "end": 969678,
    "text": "トレーニングを中断し、パラメータを凍結し、潜在的な空間を観察するだけだ。"
  },
  {
    "start": 969854,
    "end": 979270,
    "text": "この潜在的な空間を見て、そこに何が入るかを記録し、アウトプットを見て、この別の潜在的な空間を見て、アウトプットを記録する。"
  },
  {
    "start": 979610,
    "end": 988840,
    "text": "トレーニングセットで十分な数が得られたら、それをシンボリック回帰コードに入力し、さまざまな式を当てはめる。"
  },
  {
    "start": 989450,
    "end": 990200,
    "text": "オーケー。"
  },
  {
    "start": 992810,
    "end": 995378,
    "text": "ニューラルネットを使用しない元のデータセット。"
  },
  {
    "start": 995404,
    "end": 996170,
    "text": "ああ、いい質問だね。"
  },
  {
    "start": 996240,
    "end": 998682,
    "text": "それはまた今度。"
  },
  {
    "start": 998816,
    "end": 1000442,
    "text": "でも、いい質問だね。"
  },
  {
    "start": 1000576,
    "end": 1010730,
    "text": "つまり、ネットワークの複数の部分でこれを行い、記号表現の階層を構築するということだ。"
  },
  {
    "start": 1010890,
    "end": 1015982,
    "text": "もしかしたら、私は2つのネットワークを持っていて、これはグラフネットワークのようなものなのかもしれない。"
  },
  {
    "start": 1016126,
    "end": 1018370,
    "text": "これが私のノード更新モデルだ。"
  },
  {
    "start": 1018440,
    "end": 1020290,
    "text": "これは私のエッジ・アップデート・モデルのようなものだ。"
  },
  {
    "start": 1020440,
    "end": 1027190,
    "text": "私なら、まずこのネットワークに適合させ、それを表現に置き換える。"
  },
  {
    "start": 1028570,
    "end": 1033190,
    "text": "だから、ニューラルネットワークを私の記号表現に置き換えたんだ。"
  },
  {
    "start": 1033850,
    "end": 1043126,
    "text": "そして2つ目のネットワークを再トレーニングし、1つ目のネットワークのトレーニングで発生したエラーを拾い上げ、2つ目のネットワークを置き換える。"
  },
  {
    "start": 1043318,
    "end": 1050910,
    "text": "だから今、僕はネットワークのあらゆる部分を完全に表現、記号的表現に置き換えているんだ。"
  },
  {
    "start": 1053010,
    "end": 1058782,
    "text": "これは結局のところ、この巨大なニューラルネットワークモデルよりも解釈しやすい。"
  },
  {
    "start": 1058836,
    "end": 1059198,
    "text": "そうだね。"
  },
  {
    "start": 1059284,
    "end": 1066078,
    "text": "直接記号的回帰を行わない理由は、組み合わせが爆発的に増えるからだ。"
  },
  {
    "start": 1066174,
    "end": 1077702,
    "text": "もしこの表現を直接探すとしたら、これとこれとを分けて考えるよりも多くの記号表現を考えなければならない。"
  },
  {
    "start": 1077836,
    "end": 1082790,
    "text": "スケーリングはn乗式から2n式になる。"
  },
  {
    "start": 1088170,
    "end": 1093458,
    "text": "基本的には、問題を部分式に因数分解する方法だ。"
  },
  {
    "start": 1093554,
    "end": 1104554,
    "text": "私が疑問に思っているのは、1つの隠れ層やニューラルネットワークが表現できる関数の空間と、ディープネットワークが表現できる関数の空間は同じなのかということです。"
  },
  {
    "start": 1104602,
    "end": 1113762,
    "text": "それは単に入力と出力の次元のせいなのか、それとも実際にはもっと単純だからなのか、それとも別の理由なのか？"
  },
  {
    "start": 1113896,
    "end": 1121246,
    "text": "通常であれば、機能的縮退を除去するためにある種の誘導バイアスをかける。"
  },
  {
    "start": 1121438,
    "end": 1133590,
    "text": "fは何らかの複雑さを表す式で、2番目のネットワークであるgも同じ複雑さを表す式だと仮定しているのだろう。"
  },
  {
    "start": 1134330,
    "end": 1141100,
    "text": "そのため、両方のネットワークを組み合わせた可能性のあるすべての表現を検索するのではなく、一度に1つずつ検索することになる。"
  },
  {
    "start": 1141950,
    "end": 1150854,
    "text": "生徒指導のようなものだが、ネットワークの一部を好きになることはできないし、このモジュールにはこれだけの複雑さがあると証明することもできない。"
  },
  {
    "start": 1150902,
    "end": 1153754,
    "text": "層が少ないからそうだと思い込んでいるだけだ。"
  },
  {
    "start": 1153802,
    "end": 1159434,
    "text": "自分の表現以上に複雑な場合は、神に祈りなさい。"
  },
  {
    "start": 1159482,
    "end": 1167970,
    "text": "このアプローチで問題になるのは、どのオペレーターを使ってネットワークを抽出するかという選択を迫られることだ。"
  },
  {
    "start": 1169190,
    "end": 1174210,
    "text": "通常、最高の解釈が得られるかどうかでオペレーターを選ぶだろう。"
  },
  {
    "start": 1177050,
    "end": 1182850,
    "text": "宇宙物理学の問題なら、私ならプラスマイナスの掛け算や指数対数を選ぶだろう。"
  },
  {
    "start": 1183010,
    "end": 1190380,
    "text": "もし素粒子物理学に携わっていたら、その分野でよく使われるディ対数や演算子を選ぶだろうね。"
  },
  {
    "start": 1193150,
    "end": 1193802,
    "text": "そうだ。"
  },
  {
    "start": 1193936,
    "end": 1194234,
    "text": "そうだね。"
  },
  {
    "start": 1194272,
    "end": 1198998,
    "text": "内挿のように見える前に、あなたはいくつかの図式的な例を示した。"
  },
  {
    "start": 1199094,
    "end": 1203390,
    "text": "これは補間を実現するためのタイプなのでしょうか、方法なのでしょうか？"
  },
  {
    "start": 1206370,
    "end": 1206782,
    "text": "オーケー。"
  },
  {
    "start": 1206836,
    "end": 1215118,
    "text": "基本的な考え方は、ニューラルネットワークを訓練するとき、暗黙のうちに関数空間に対する事前分布を仮定しているということだ。"
  },
  {
    "start": 1215294,
    "end": 1228550,
    "text": "この記号的蒸留を行うとき、あなたはそれを別の事前分布に移すようなもので、分布から外れると、2番目の事前分布、つまり記号的事前分布を得ることになる。"
  },
  {
    "start": 1232890,
    "end": 1234022,
    "text": "後で開けるよ。"
  },
  {
    "start": 1234076,
    "end": 1234920,
    "text": "ああ、わかったよ。"
  },
  {
    "start": 1238510,
    "end": 1250410,
    "text": "ニューラルネットワークの構造やトポロジーに何らかの制約があり、それが解に適合しなければならないのでしょうか？"
  },
  {
    "start": 1252190,
    "end": 1253034,
    "text": "いい質問だね。"
  },
  {
    "start": 1253072,
    "end": 1258250,
    "text": "だから、普通は普通の多層パーセプトロンを選ぶだろう。"
  },
  {
    "start": 1258330,
    "end": 1267010,
    "text": "グラフネットワークのように、関数的な正則化は行わない。"
  },
  {
    "start": 1267990,
    "end": 1270802,
    "text": "つまり、近似誤差があるんだろ？"
  },
  {
    "start": 1270936,
    "end": 1276866,
    "text": "多層パーセプトロンの完全な近似値は得られないように、それは無限に複雑な式になるからだ。"
  },
  {
    "start": 1276978,
    "end": 1283282,
    "text": "あなたは、多層パーセプトロンの振る舞いの単純な近似を見つけようとしている。"
  },
  {
    "start": 1283426,
    "end": 1304990,
    "text": "以前のスライドでは、この反復プロセスを示しているときに、ネットワークのどの部分に何を当てはめればいいかなんとなくわかっているように見えましたが、これは近似値になります。"
  },
  {
    "start": 1305810,
    "end": 1308314,
    "text": "オペレーターを選ぶ、その選択をしなければならない。"
  },
  {
    "start": 1308362,
    "end": 1311722,
    "text": "演算子を選ばなければならないし、複雑さをどう定義するかも選ばなければならない。"
  },
  {
    "start": 1311786,
    "end": 1315730,
    "text": "それは、自分の制約に基づく恣意的な判断だ。"
  },
  {
    "start": 1320230,
    "end": 1323650,
    "text": "ひとつの事業者を選ぶ根本的な理由はない。"
  },
  {
    "start": 1324150,
    "end": 1329734,
    "text": "つまり、問題によって違うんだ。"
  },
  {
    "start": 1329772,
    "end": 1338466,
    "text": "例えば、何かを再発見して人間レベルのパフォーマンスを示そうとすることもあれば、実際に新しい科学方程式を発見することもある。"
  },
  {
    "start": 1338658,
    "end": 1345226,
    "text": "そのような場合、あなたはそれを選ぶだろう。"
  },
  {
    "start": 1345328,
    "end": 1345690,
    "text": "オーケー。"
  },
  {
    "start": 1345760,
    "end": 1350422,
    "text": "この分野は通常、これらの演算子でうまく表現される。"
  },
  {
    "start": 1350486,
    "end": 1358010,
    "text": "私なら、そのような演算子を選んで検索するが、演算子を知らないだけかもしれない。"
  },
  {
    "start": 1358170,
    "end": 1358974,
    "text": "もちろんだ。"
  },
  {
    "start": 1359172,
    "end": 1360110,
    "text": "それがベストだ。"
  },
  {
    "start": 1360180,
    "end": 1364930,
    "text": "それは例えば、演算子を学習するような別の最適化問題かもしれない。"
  },
  {
    "start": 1365590,
    "end": 1366340,
    "text": "オーケー。"
  },
  {
    "start": 1371030,
    "end": 1371394,
    "text": "そうだね。"
  },
  {
    "start": 1371432,
    "end": 1375662,
    "text": "伝統的なアプローチでは、データがあって、それを理論で説明しようとする。"
  },
  {
    "start": 1375806,
    "end": 1381554,
    "text": "この種の提案では、データからニューラルネットワークに移行する。"
  },
  {
    "start": 1381602,
    "end": 1385810,
    "text": "ニューラルネットワークでデータセットのパターンを圧縮するのだ。"
  },
  {
    "start": 1385890,
    "end": 1389260,
    "text": "そして、それを象徴的な形に蒸留する。"
  },
  {
    "start": 1389870,
    "end": 1393238,
    "text": "じゃあ、いくつか例を挙げよう。"
  },
  {
    "start": 1393334,
    "end": 1402010,
    "text": "そこで、この2020年のヨーロッパの論文で私たちが最初に試したことのひとつが、グラフ・ニューラル・ネットワークだった。"
  },
  {
    "start": 1402090,
    "end": 1412602,
    "text": "グラフ・ニューラル・ネットワークは、古典物理学の力法則に似た関数形をしているので、この戦略で解釈するのは非常に簡単だ。"
  },
  {
    "start": 1412746,
    "end": 1415614,
    "text": "グラフ・ニューラル・ネットワークを解釈するときに何をするか。"
  },
  {
    "start": 1415662,
    "end": 1420750,
    "text": "つまり、ある相互作用する粒子のダイナミクスを予測しようとしているとしよう。"
  },
  {
    "start": 1420910,
    "end": 1422110,
    "text": "グラフネットワークで。"
  },
  {
    "start": 1422190,
    "end": 1428680,
    "text": "つまり、メッセージの受け渡し空間における次元の低さを奨励するのだ。"
  },
  {
    "start": 1429770,
    "end": 1436310,
    "text": "これは基本的に、グラフネットワークに潜伏が少ないことを意味する。"
  },
  {
    "start": 1437390,
    "end": 1439210,
    "text": "基本的に方程式は少ない。"
  },
  {
    "start": 1441390,
    "end": 1443322,
    "text": "このビデオを見せよう。"
  },
  {
    "start": 1443376,
    "end": 1446406,
    "text": "まずは粒子のシミュレーションを行う。"
  },
  {
    "start": 1446598,
    "end": 1448460,
    "text": "だから、これは春の法則なんだ。"
  },
  {
    "start": 1449070,
    "end": 1452170,
    "text": "私たちはこれをグラフネットワークでモデル化しようと試みている。"
  },
  {
    "start": 1452250,
    "end": 1453470,
    "text": "グラフ・ニューラル・ネットワーク。"
  },
  {
    "start": 1454050,
    "end": 1457230,
    "text": "私たちは、グラフ・ニューラル・ネットワークを使ってこれらのダイナミクスを予測しようと試みている。"
  },
  {
    "start": 1458050,
    "end": 1463950,
    "text": "これらはグレーで表示されているのが予測値で、着色されているのがグランドトゥルースである。"
  },
  {
    "start": 1465090,
    "end": 1467390,
    "text": "これをグラフ・ニューラル・ネットワークで行う。"
  },
  {
    "start": 1467550,
    "end": 1469506,
    "text": "l 1で正則化する。"
  },
  {
    "start": 1469608,
    "end": 1472606,
    "text": "これは要するに、グラフ・ネットワークに伝えているのだ。"
  },
  {
    "start": 1472638,
    "end": 1474818,
    "text": "シンプルなモデルを探してほしい。"
  },
  {
    "start": 1474904,
    "end": 1476530,
    "text": "方程式はほとんどいらない。"
  },
  {
    "start": 1476690,
    "end": 1479746,
    "text": "ここでわかるように、2つの方程式しか見つからない。"
  },
  {
    "start": 1479858,
    "end": 1484950,
    "text": "使用する潜在次元は2つで、それ以外はすべてスパースだ。"
  },
  {
    "start": 1486250,
    "end": 1500862,
    "text": "これは本質的に、象徴的な回帰を当てはめようとすると、エッジモデルを置き換える2つの方程式が得られるということであり、それらはたまたまバネの法則と同じである。"
  },
  {
    "start": 1500916,
    "end": 1508000,
    "text": "このグラフ・ネットワークにおけるメッセージ・パッシング機能がバネの法則であることは、なんとなくおわかりいただけただろう。"
  },
  {
    "start": 1508930,
    "end": 1512910,
    "text": "これは再発見だが、このアプローチのテストでもある。"
  },
  {
    "start": 1513990,
    "end": 1514740,
    "text": "オーケー。"
  },
  {
    "start": 1518310,
    "end": 1520926,
    "text": "次にやったのは知識の発見だ。"
  },
  {
    "start": 1521118,
    "end": 1529880,
    "text": "このアプローチの全体的な考え方は、ディープラーニングがデータから見出している科学的な洞察を見つけたいということだ。"
  },
  {
    "start": 1533850,
    "end": 1537582,
    "text": "私たちは宇宙物理学のバックグラウンドを持っているので、ダークマターのシミュレーションができる。"
  },
  {
    "start": 1537746,
    "end": 1545162,
    "text": "ダークマターのクラスター化を記述する新しい法則を見つけたい。"
  },
  {
    "start": 1545296,
    "end": 1548534,
    "text": "さて、私たちはこのグラフ・ニューラル・ネットワークを持っている。"
  },
  {
    "start": 1548662,
    "end": 1559200,
    "text": "私たちはこれを訓練して、周囲の粒子の種類に基づいて暗黒物質の性質を予測し、暗黒物質のクラスタリングについて教えています。"
  },
  {
    "start": 1559890,
    "end": 1565874,
    "text": "この点では、ニューラルネットワークは従来の指標に勝る。"
  },
  {
    "start": 1565992,
    "end": 1572610,
    "text": "従来の指標を打ち負かすことができるのは、どのような洞察に基づいているのか？"
  },
  {
    "start": 1575930,
    "end": 1583378,
    "text": "でも、連続的な事柄をグラフで表すにはどうしたらいいんだろう？"
  },
  {
    "start": 1583554,
    "end": 1584134,
    "text": "いい質問だね。"
  },
  {
    "start": 1584172,
    "end": 1589190,
    "text": "ハロー・ファインダーと呼ばれる方法がある。"
  },
  {
    "start": 1589270,
    "end": 1594300,
    "text": "基本的には、ダークマターの塊の周りにボールを描く。"
  },
  {
    "start": 1594910,
    "end": 1597050,
    "text": "それがグラフのノードになる。"
  },
  {
    "start": 1597870,
    "end": 1599770,
    "text": "ああ、ダークマターはバラバラだ。"
  },
  {
    "start": 1603090,
    "end": 1605514,
    "text": "シミュレーションはそれを連続的に表現しているようなものだ。"
  },
  {
    "start": 1605562,
    "end": 1609854,
    "text": "ダークマターのクラスターを探すようなものだ。"
  },
  {
    "start": 1610052,
    "end": 1622178,
    "text": "そのクラスターがグラフのノードとなり、ノードは、このクラスターの質量はこれ、半径はこれといったプロパティを持つことになる。"
  },
  {
    "start": 1622344,
    "end": 1625454,
    "text": "これがシミュレーションをグラフにする方法だ。"
  },
  {
    "start": 1625582,
    "end": 1627778,
    "text": "エッジは局所性に基づいているだけだ。"
  },
  {
    "start": 1627874,
    "end": 1630280,
    "text": "ラジアスカットをするだけだ。"
  },
  {
    "start": 1630810,
    "end": 1637240,
    "text": "もしシミュレーションできるのなら、重力の法則のような物理法則を使ってすでにシミュレーションしているのではないですか？"
  },
  {
    "start": 1637690,
    "end": 1641206,
    "text": "シミュレーションに打ち込んでいる以上のことを学んでいるのか？"
  },
  {
    "start": 1641318,
    "end": 1645690,
    "text": "どのようにクラスタリングしているのか、要約統計を学んでいるところだ。"
  },
  {
    "start": 1646110,
    "end": 1649050,
    "text": "我々はそれをシミュレートしているのだから、真実の物理学はわかっている。"
  },
  {
    "start": 1649120,
    "end": 1650454,
    "text": "ただの重力だ。"
  },
  {
    "start": 1650502,
    "end": 1651050,
    "text": "そうだね。"
  },
  {
    "start": 1651200,
    "end": 1655210,
    "text": "質量がどのように集まっているかを学んでいるところだ。"
  },
  {
    "start": 1656190,
    "end": 1658142,
    "text": "クラスタリングに適した法則は何かとかね。"
  },
  {
    "start": 1658196,
    "end": 1658526,
    "text": "了解した。"
  },
  {
    "start": 1658548,
    "end": 1659070,
    "text": "我々はそうではない。"
  },
  {
    "start": 1659140,
    "end": 1662974,
    "text": "効果的な物理学であり、基本的には効果的な法則だ。"
  },
  {
    "start": 1663012,
    "end": 1672526,
    "text": "つまり、基本的には、重力の法則から生まれる創発的な特性は、重力の法則から生まれる創発的な特性に対するより良い表現方法を学んでいるのだ。"
  },
  {
    "start": 1672558,
    "end": 1672706,
    "text": "そうだね。"
  },
  {
    "start": 1672728,
    "end": 1676530,
    "text": "基本的には、シミュレーションにおける創発的な特性を記述する法則を学ぶことだ。"
  },
  {
    "start": 1679670,
    "end": 1680420,
    "text": "オーケー。"
  },
  {
    "start": 1681190,
    "end": 1681602,
    "text": "そうだ。"
  },
  {
    "start": 1681656,
    "end": 1688326,
    "text": "バネの法則を求めるのと同じアプローチを適用すると、このダークマター・オーバー・ダンス方程式が得られる。"
  },
  {
    "start": 1688358,
    "end": 1695050,
    "text": "ダークマターの性質は、基本的に隣接する物質の和に基づいて予測される。"
  },
  {
    "start": 1695950,
    "end": 1703142,
    "text": "分野が違うので説明はしないが、基本的には新しい方程式のようなものだ。"
  },
  {
    "start": 1703206,
    "end": 1709626,
    "text": "グラフ・ニューラル・ネットワークがこの方程式を見つけるまでは、この方程式は知られていなかった。"
  },
  {
    "start": 1709818,
    "end": 1713440,
    "text": "ダークマターのクラスターを表現しているようなものだ。"
  },
  {
    "start": 1714770,
    "end": 1717650,
    "text": "では、2つ目の例をお見せしましょう。"
  },
  {
    "start": 1717720,
    "end": 1719214,
    "text": "だから、こっちの方がクラシックだね。"
  },
  {
    "start": 1719342,
    "end": 1726162,
    "text": "私たちは、太陽系の力学からニュートンの重力の法則を見つけることができるかどうかを確かめたかった。"
  },
  {
    "start": 1726306,
    "end": 1730486,
    "text": "なるほど、未知の質量、未知の力学的モデル。"
  },
  {
    "start": 1730588,
    "end": 1736950,
    "text": "私たちは文字通り、太陽系をグラフ・ニューラル・ネットワークに当てはめ、すべての惑星の次のステップを予測しようとしている。"
  },
  {
    "start": 1738030,
    "end": 1741290,
    "text": "これはミラのパブロ・レモスが率いたものだ。"
  },
  {
    "start": 1742750,
    "end": 1744426,
    "text": "基本的にはこんな感じだ。"
  },
  {
    "start": 1744528,
    "end": 1746518,
    "text": "ワイヤーフレームは予測である。"
  },
  {
    "start": 1746694,
    "end": 1750070,
    "text": "色のついた惑星が真実だ。"
  },
  {
    "start": 1750150,
    "end": 1755020,
    "text": "このビデオも作るのに時間がかかりすぎて、後悔している。"
  },
  {
    "start": 1756830,
    "end": 1757526,
    "text": "見えるだろう。"
  },
  {
    "start": 1757568,
    "end": 1762154,
    "text": "元のグラフネットワークでは、すべてのダイナミクスを拾うことはできない。"
  },
  {
    "start": 1762282,
    "end": 1764402,
    "text": "なんとなくそう見えるけど、すべてを拾うわけじゃない。"
  },
  {
    "start": 1764456,
    "end": 1765106,
    "text": "そうだね。"
  },
  {
    "start": 1765288,
    "end": 1777862,
    "text": "それを否定するわけではないが、当時知られていたダイナミクスの観測を参考にするのはどうだろう？"
  },
  {
    "start": 1777996,
    "end": 1780214,
    "text": "ああ、わかったよ。"
  },
  {
    "start": 1780252,
    "end": 1785350,
    "text": "2Dのスカイマップのようにすればいいじゃないか、と言うだろう。"
  },
  {
    "start": 1786010,
    "end": 1787240,
    "text": "そう聞いているのか？"
  },
  {
    "start": 1788010,
    "end": 1788806,
    "text": "いや。"
  },
  {
    "start": 1788988,
    "end": 1789334,
    "text": "オーケー。"
  },
  {
    "start": 1789372,
    "end": 1792390,
    "text": "私が聞きたいのは、何がインプットなのかということだ。"
  },
  {
    "start": 1792470,
    "end": 1795114,
    "text": "今、この軌道について何を知っているか？"
  },
  {
    "start": 1795152,
    "end": 1795450,
    "text": "いや。"
  },
  {
    "start": 1795520,
    "end": 1804046,
    "text": "各惑星の3D位置を入力し、20個の月を時間経過とともに表示するんだ。"
  },
  {
    "start": 1804228,
    "end": 1805006,
    "text": "そうだね。"
  },
  {
    "start": 1805188,
    "end": 1807760,
    "text": "正確な3Dポジションを与える。"
  },
  {
    "start": 1808130,
    "end": 1808782,
    "text": "その通りだ。"
  },
  {
    "start": 1808916,
    "end": 1810122,
    "text": "大衆には与えない。"
  },
  {
    "start": 1810186,
    "end": 1812438,
    "text": "軌道パラメータは与えていない。"
  },
  {
    "start": 1812634,
    "end": 1815250,
    "text": "ただ、生の物理座標を与えるだけだ。"
  },
  {
    "start": 1817430,
    "end": 1818226,
    "text": "いいね。"
  },
  {
    "start": 1818408,
    "end": 1819394,
    "text": "今のところはね。"
  },
  {
    "start": 1819432,
    "end": 1819586,
    "text": "オーケー。"
  },
  {
    "start": 1819608,
    "end": 1820340,
    "text": "今のところはね。"
  },
  {
    "start": 1821030,
    "end": 1823380,
    "text": "GPTフォースを使用しましたか？"
  },
  {
    "start": 1825430,
    "end": 1827390,
    "text": "では、このネットワークを訓練しよう。"
  },
  {
    "start": 1827550,
    "end": 1828866,
    "text": "完璧ではない。"
  },
  {
    "start": 1829048,
    "end": 1829358,
    "text": "オーケー。"
  },
  {
    "start": 1829384,
    "end": 1834934,
    "text": "この新しい解釈可能性のテクニックを使って、潜在空間が実際に何をしているのかを理解したい。"
  },
  {
    "start": 1834972,
    "end": 1836674,
    "text": "実際に何を計算しているのか？"
  },
  {
    "start": 1836802,
    "end": 1840202,
    "text": "それをpicerに突っ込んで、方程式を出すんだ。"
  },
  {
    "start": 1840256,
    "end": 1847414,
    "text": "この軸は複雑さであり、これは本質的に、つまり精度を上げるべきだったということだ。"
  },
  {
    "start": 1847462,
    "end": 1849258,
    "text": "正確さと複雑さのトレードオフだ。"
  },
  {
    "start": 1849344,
    "end": 1852730,
    "text": "基本的には、これはお得ですよ、と言っているのだ。"
  },
  {
    "start": 1853470,
    "end": 1853882,
    "text": "オーケー。"
  },
  {
    "start": 1853936,
    "end": 1856394,
    "text": "最も単純な式は、r上のxである。"
  },
  {
    "start": 1856512,
    "end": 1858282,
    "text": "rに定数を加えたものがxとなる。"
  },
  {
    "start": 1858346,
    "end": 1863346,
    "text": "私たちは複雑さを加え続け、ニュートンの法則であるこの式にたどり着いた。"
  },
  {
    "start": 1863448,
    "end": 1867598,
    "text": "質量×質量×Xの3乗。"
  },
  {
    "start": 1867694,
    "end": 1870290,
    "text": "基本的には力のX成分のようなものだ。"
  },
  {
    "start": 1871350,
    "end": 1873198,
    "text": "実はもう一つ別の表現があった。"
  },
  {
    "start": 1873294,
    "end": 1876818,
    "text": "この方が潜在的な空間によりフィットする。"
  },
  {
    "start": 1876904,
    "end": 1879730,
    "text": "基本的にはニュートンの重力の法則に定数を加えたものだ。"
  },
  {
    "start": 1880790,
    "end": 1883650,
    "text": "それがネットワークの縮退なんだ。"
  },
  {
    "start": 1883730,
    "end": 1889654,
    "text": "基本的には、重力を反転させる方法を知っているからこそ機能する、独自の修正重力の法則を学んでいるのだ。"
  },
  {
    "start": 1889692,
    "end": 1895100,
    "text": "複雑さのトレードオフに見合う価値がないことがわかるだろう。"
  },
  {
    "start": 1895550,
    "end": 1901930,
    "text": "もちろん、ニュートンの法則は真実である。"
  },
  {
    "start": 1902080,
    "end": 1912400,
    "text": "では、この法則を記号化し、ニューラルネットワークに置き換えて、それをまた展開する。"
  },
  {
    "start": 1914130,
    "end": 1916640,
    "text": "だから、そう、良くなったように見える。"
  },
  {
    "start": 1917570,
    "end": 1918800,
    "text": "完璧ではない。"
  },
  {
    "start": 1919910,
    "end": 1921358,
    "text": "これは水星だ。"
  },
  {
    "start": 1921454,
    "end": 1923540,
    "text": "マーキュリーの予想はまだ外れている。"
  },
  {
    "start": 1925430,
    "end": 1928550,
    "text": "では、なぜ完璧ではないのか？"
  },
  {
    "start": 1928620,
    "end": 1930258,
    "text": "これがニュートンの重力の法則である。"
  },
  {
    "start": 1930354,
    "end": 1931480,
    "text": "完璧なはずだ。"
  },
  {
    "start": 1932410,
    "end": 1937602,
    "text": "先ほど、ネットワークに各惑星の質量値を学習させていると言った。"
  },
  {
    "start": 1937666,
    "end": 1939290,
    "text": "私たちはそれを事前情報として伝えない。"
  },
  {
    "start": 1939360,
    "end": 1941050,
    "text": "大衆を学ばなければならない。"
  },
  {
    "start": 1942030,
    "end": 1944700,
    "text": "大衆の多くは、完全にズレている。"
  },
  {
    "start": 1945550,
    "end": 1951150,
    "text": "予測は赤で、複数の点は基本的に異なるトライアルである。"
  },
  {
    "start": 1952530,
    "end": 1958080,
    "text": "地球は100倍も予測を下回っている。"
  },
  {
    "start": 1958690,
    "end": 1960506,
    "text": "火星の予測はかなり甘かった。"
  },
  {
    "start": 1960618,
    "end": 1966350,
    "text": "最も大きな惑星のいくつかは、予想しやすいと思うからだ。"
  },
  {
    "start": 1967090,
    "end": 1980200,
    "text": "つまり、このネットワークは本質的に、このネットワークに有効な修正重力の法則を学習しているのだ。"
  },
  {
    "start": 1981210,
    "end": 2005658,
    "text": "つまり、学習した元の質量は、記号表現ではなく、完全なニューラルネットワークに最適化されていたのです。"
  },
  {
    "start": 2005834,
    "end": 2008586,
    "text": "なるほど、これはネットワークの完全な近似ではない。"
  },
  {
    "start": 2008618,
    "end": 2011866,
    "text": "この修正重力理論を学んだようなものだ。"
  },
  {
    "start": 2012058,
    "end": 2019010,
    "text": "質量のパラメーターを最適化し直すと、うまくいく。"
  },
  {
    "start": 2019430,
    "end": 2022786,
    "text": "すべての予想が当たっているのがわかるだろう。"
  },
  {
    "start": 2022808,
    "end": 2023282,
    "text": "もう一度言う。"
  },
  {
    "start": 2023416,
    "end": 2028390,
    "text": "よし、予想はすべて的中だ。"
  },
  {
    "start": 2028540,
    "end": 2029506,
    "text": "奇妙なカップルもいる。"
  },
  {
    "start": 2029538,
    "end": 2031074,
    "text": "これが月なんだ。"
  },
  {
    "start": 2031122,
    "end": 2033746,
    "text": "ハイペリオンは大勢を予測しすぎている。"
  },
  {
    "start": 2033858,
    "end": 2039980,
    "text": "データセットに欠けている月があったからだと思う。"
  },
  {
    "start": 2040350,
    "end": 2043340,
    "text": "さて、3つ目の例、これが最後の例だ。"
  },
  {
    "start": 2043950,
    "end": 2048438,
    "text": "私が話したことの多くは、解釈可能性についてです。"
  },
  {
    "start": 2048614,
    "end": 2060378,
    "text": "解釈のために分析的な表現に変換するとき、もうひとつクールなことができる。"
  },
  {
    "start": 2060554,
    "end": 2070910,
    "text": "これはウィスコンシン大学の大学院生が率いた論文で、ニューラルネットワークの速度を最適化するためにシンボリック回帰を使っている。"
  },
  {
    "start": 2070990,
    "end": 2085202,
    "text": "ニューラルネットワークの入力と出力を取り出し、そのニューラルネットワークを近似するFPGA上で可能な限り最速の式を見つけようとする。"
  },
  {
    "start": 2085346,
    "end": 2093062,
    "text": "つまり、複雑さを求めるのではなく、FPGAのクロック・サイクル数を求めているのだ。"
  },
  {
    "start": 2093126,
    "end": 2097850,
    "text": "目的はまったく違うが、同じような考えだ。"
  },
  {
    "start": 2097920,
    "end": 2100954,
    "text": "あなたはそれを象徴的な表現に集約しようとしている。"
  },
  {
    "start": 2101082,
    "end": 2105146,
    "text": "90％の精度でニューラルネットワークを近似する。"
  },
  {
    "start": 2105178,
    "end": 2110334,
    "text": "かなり良い予測ができるし、推論時間も5ナノ秒だ。"
  },
  {
    "start": 2110532,
    "end": 2117490,
    "text": "その応用が、異常粒子を検出するための大型ハドロン衝突型加速器である。"
  },
  {
    "start": 2117830,
    "end": 2127586,
    "text": "だから、巨大なニューラルネットワークを持つ必要はなく、このアイデアで超高速FPGAコードに集約することができる。"
  },
  {
    "start": 2127768,
    "end": 2129970,
    "text": "本当に速い予測を得る。"
  },
  {
    "start": 2131610,
    "end": 2133542,
    "text": "そうだね。"
  },
  {
    "start": 2133676,
    "end": 2137320,
    "text": "私はこのウェブページで他の例を紹介している。"
  },
  {
    "start": 2141450,
    "end": 2149990,
    "text": "私が議論したいと思っているのは、科学のためのAIに対する純粋なニューラルネットワークのアプローチだ。"
  },
  {
    "start": 2150370,
    "end": 2160030,
    "text": "例えば、より大きなモデルをどんどんトレーニングしていけば、理論と同じレベルの汎化が期待できるのだろうか？"
  },
  {
    "start": 2161890,
    "end": 2169150,
    "text": "アインシュタインの一般相対性理論は、わずかなアイデアから導き出されたものだが、ブラックホールのようなものを予測することができる。"
  },
  {
    "start": 2169310,
    "end": 2169922,
    "text": "そうだろう？"
  },
  {
    "start": 2170056,
    "end": 2175300,
    "text": "彼はブラックホールを観測したのではなく、ただ導き出しただけなのだ。"
  },
  {
    "start": 2175850,
    "end": 2187830,
    "text": "基礎モデルからそのようなレベルの一般化を期待するのは絶望的なのか、それとも理論にまで落とし込む必要があるのか。"
  },
  {
    "start": 2189070,
    "end": 2189962,
    "text": "では"
  },
  {
    "start": 2190096,
    "end": 2197142,
    "text": "もうひとつ興味があるのは、言語モデルのような非常に大きなモデルをどうやって科学的な洞察に落とし込むかということだ。"
  },
  {
    "start": 2197206,
    "end": 2202830,
    "text": "このアイデアを使えるのか、それとも他のアプローチを取らなければならないのか。"
  },
  {
    "start": 2204290,
    "end": 2206542,
    "text": "ああ、わかった。"
  },
  {
    "start": 2206676,
    "end": 2207806,
    "text": "来てくれてありがとう。"
  },
  {
    "start": 2207988,
    "end": 2209360,
    "text": "呼んでくれてありがとう。"
  },
  {
    "start": 2212130,
    "end": 2213200,
    "text": "もっと質問を"
  },
  {
    "start": 2214770,
    "end": 2217650,
    "text": "前回の質問の続きをしようかな。"
  },
  {
    "start": 2217720,
    "end": 2222574,
    "text": "一般相対性理論などについても言及されていますね。"
  },
  {
    "start": 2222622,
    "end": 2238986,
    "text": "私は重力に関して、法則が策定されたときに、すべての惑星の3次元座標が時間経過とともに存在しなかったと仮定しているという意味で、同じ点を突いていたんだ。"
  },
  {
    "start": 2239168,
    "end": 2244700,
    "text": "予測は極めて少量のデータで行われる。"
  },
  {
    "start": 2245790,
    "end": 2253786,
    "text": "たくさんのデータがあり、ニューラルネットを使い、そして導き出す。"
  },
  {
    "start": 2253898,
    "end": 2257790,
    "text": "あるいは、どのような場合に適用できるのか、もう少し詳しく教えてください。"
  },
  {
    "start": 2258770,
    "end": 2260814,
    "text": "質問の意味がよくわからない。"
  },
  {
    "start": 2260852,
    "end": 2263758,
    "text": "具体的にはニュートンの重力の法則についての質問ですか？"
  },
  {
    "start": 2263854,
    "end": 2264162,
    "text": "いや。"
  },
  {
    "start": 2264216,
    "end": 2282690,
    "text": "ニュートンの法則の導出に対する私の最初の反応は、ああ、これは本当に驚くべきことだ、というものだった。"
  },
  {
    "start": 2282770,
    "end": 2283400,
    "text": "そうだね。"
  },
  {
    "start": 2283930,
    "end": 2292150,
    "text": "でも、歴史的に使われてきた情報よりもはるかに多くの情報を使っている。"
  },
  {
    "start": 2292310,
    "end": 2300502,
    "text": "だから、もしあなたが、これは科学を理解するための方法だと言うのであれば、それはおそらく別の領域の話だ。"
  },
  {
    "start": 2300646,
    "end": 2302382,
    "text": "それについて少し話していただけますか？"
  },
  {
    "start": 2302436,
    "end": 2302606,
    "text": "そうだね。"
  },
  {
    "start": 2302628,
    "end": 2305466,
    "text": "ネットワークはニュートンのレベルではない。"
  },
  {
    "start": 2305578,
    "end": 2308762,
    "text": "単純なニューラルネットワークだ。"
  },
  {
    "start": 2308826,
    "end": 2315662,
    "text": "もっと多くの仕事があると思う。"
  },
  {
    "start": 2315716,
    "end": 2322946,
    "text": "言いそびれてしまったが、このデータセットでこのパフォーマンスを得るためには、ネットワークにもういくつか誘導バイアスをかける必要があった。"
  },
  {
    "start": 2323048,
    "end": 2325102,
    "text": "膨大なデータだ。"
  },
  {
    "start": 2325176,
    "end": 2338120,
    "text": "私たちは30年間の力学データを与えたが、実際には惑星や月、太陽など30天体しかなかったと思う。"
  },
  {
    "start": 2338810,
    "end": 2341186,
    "text": "ただ、ニューラルネットワークのデータ量は多くない。"
  },
  {
    "start": 2341218,
    "end": 2342854,
    "text": "30例しかないようなものだ。"
  },
  {
    "start": 2342982,
    "end": 2348810,
    "text": "ニューラルネットワークが一般的な原理を理解するのは本当に難しいことで、そのためにこのような修正理論を学んだのだと思う。"
  },
  {
    "start": 2349630,
    "end": 2360106,
    "text": "結局のところ、ニューラルネットワークの潜在空間に対して単純性プリオールを課すより良い方法が必要なのだと思う。"
  },
  {
    "start": 2360218,
    "end": 2363402,
    "text": "象徴的なシンプルさのようなものだ。"
  },
  {
    "start": 2363546,
    "end": 2374740,
    "text": "例えば、ニューラルネットワークの一部に記号的に単純なモデルを学習させるような方法があればいいのだが、それは難しいようだ。"
  },
  {
    "start": 2375350,
    "end": 2378338,
    "text": "でも、そうだね。"
  },
  {
    "start": 2378354,
    "end": 2385378,
    "text": "回転の等変量も帰納的なバイアスの1つだが、必ずしもその必要はなかった。"
  },
  {
    "start": 2385554,
    "end": 2390360,
    "text": "他にもいくつか取り除きたいプリオールがあるが、それでもこの結果は回復する。"
  },
  {
    "start": 2394030,
    "end": 2394394,
    "text": "そうだね。"
  },
  {
    "start": 2394432,
    "end": 2403818,
    "text": "内挿法とアインシュタインの特権と一般相対性理論についての私の以前の質問に、あなたは実際に答えてくれたかもしれない。"
  },
  {
    "start": 2403914,
    "end": 2409786,
    "text": "水星の近日点は、その追求の歴史的動機の一部だった。"
  },
  {
    "start": 2409978,
    "end": 2412350,
    "text": "そのことは分かっていたはずだ。"
  },
  {
    "start": 2412420,
    "end": 2442940,
    "text": "つまり、水星の軌道が楕円軌道であることで、より良いマッチングが得られるということは、基本的に、水星の公転の過程に関して楕円曲線補間を実証しているということなのだろうか。このニューラルネットワークを訓練しているのは31種類の質量だけなので、ニュートンの重力の法則をほとんど学ばなかったということなのだろうか？"
  },
  {
    "start": 2443870,
    "end": 2448806,
    "text": "ニューラルネットワークに適合させるには、データが足りないと思うんだ。"
  },
  {
    "start": 2448918,
    "end": 2451366,
    "text": "そして、ニュートンの愛の重力をかろうじて見つけたようなものだ。"
  },
  {
    "start": 2451398,
    "end": 2455886,
    "text": "もっと多くのデータを与えない限り、一般相対性理論は見つからなかったと思う。"
  },
  {
    "start": 2455988,
    "end": 2458554,
    "text": "まあ、あるいは、違う原則を与えたんだ。"
  },
  {
    "start": 2458602,
    "end": 2466286,
    "text": "アインシュタインは、水星の軌道から一般相対性理論の概念のすべてを裏付けているとは言っていない。"
  },
  {
    "start": 2466318,
    "end": 2469154,
    "text": "それがモチベーションとなり、さらなる仕事への補強となった。"
  },
  {
    "start": 2469192,
    "end": 2471666,
    "text": "QとDとは言っていない。"
  },
  {
    "start": 2471688,
    "end": 2472674,
    "text": "それを期待してほしい。"
  },
  {
    "start": 2472712,
    "end": 2473860,
    "text": "そうだね。"
  },
  {
    "start": 2474230,
    "end": 2484674,
    "text": "データから水星の軌道に非常に近づいたことで、楕円曲線補間を効果的に実証したのですか？"
  },
  {
    "start": 2484802,
    "end": 2486486,
    "text": "ああ、間違いない。"
  },
  {
    "start": 2486668,
    "end": 2491750,
    "text": "一般的に、他のプリオールをどのように課すかを考えるのは面白いと思う。"
  },
  {
    "start": 2491910,
    "end": 2496620,
    "text": "つまり、アインシュタインには動力学データだけでなく、他のものもあったということだ。"
  },
  {
    "start": 2497310,
    "end": 2506160,
    "text": "私が言語モデルにとても期待しているのは、モデルにとても柔軟なデータ表現を提供できることです。"
  },
  {
    "start": 2507730,
    "end": 2508094,
    "text": "そうだね。"
  },
  {
    "start": 2508132,
    "end": 2509870,
    "text": "について質問があります。"
  },
  {
    "start": 2510020,
    "end": 2515026,
    "text": "科学的原理が単純であることを期待するのはなぜか？"
  },
  {
    "start": 2515208,
    "end": 2528898,
    "text": "科学には還元主義的な思い込みがあるようだが、複雑なシステムはたくさんあり、そのような硬直した記号表現ではうまく説明できないこともある。"
  },
  {
    "start": 2528994,
    "end": 2531640,
    "text": "科学が単純だとは思わない。"
  },
  {
    "start": 2533050,
    "end": 2542330,
    "text": "シンプルさについて語るとき、私たちは自分の既存のアイデアのベースにおけるシンプルさという観点から語るのだと思う。"
  },
  {
    "start": 2542750,
    "end": 2553438,
    "text": "足し算について考えてみると、それは非常に抽象的なアイデアであり、2つの数字を足すことを単純なことだと考えるようなものだ。"
  },
  {
    "start": 2553604,
    "end": 2557246,
    "text": "それは、私たちがそれを学び、慣れ親しんできたからに他ならない。"
  },
  {
    "start": 2557348,
    "end": 2562270,
    "text": "私たちの脳には、数字を足すためのレジスタがあるわけではない。"
  },
  {
    "start": 2564470,
    "end": 2570622,
    "text": "これらは非常に抽象的な概念であり、シンプルさについて語るときは、明確な目的を持つべきだと思う。"
  },
  {
    "start": 2570686,
    "end": 2578600,
    "text": "この洞察は私にどのようなものを与えてくれるのだろうか？"
  },
  {
    "start": 2579290,
    "end": 2583160,
    "text": "どう解釈するのが一番わかりやすいんだろう？"
  },
  {
    "start": 2583770,
    "end": 2587526,
    "text": "そうやって複雑さを測る方法を選ぶんだ。"
  },
  {
    "start": 2587628,
    "end": 2590300,
    "text": "あるいは、FPGAの例も考えられる。"
  },
  {
    "start": 2590670,
    "end": 2595066,
    "text": "複雑さとは、これを計算するためのクロック・サイクルの数である。"
  },
  {
    "start": 2595248,
    "end": 2598278,
    "text": "だから、それはまったく別の目的のようなものだ。"
  },
  {
    "start": 2598454,
    "end": 2604190,
    "text": "複雑さの普遍的な定義はない。"
  },
  {
    "start": 2607090,
    "end": 2608490,
    "text": "定義するのは難しい。"
  },
  {
    "start": 2608650,
    "end": 2612254,
    "text": "このことを考えるのは、究極的には哲学的ブラックホールのようなものだ。"
  },
  {
    "start": 2612292,
    "end": 2612880,
    "text": "そうだね。"
  },
  {
    "start": 2613810,
    "end": 2617300,
    "text": "ミーシャが提起していると思うんだけど、ちょっと質問があるんだ。"
  },
  {
    "start": 2617910,
    "end": 2631080,
    "text": "30年間、30個の天体のデータはほとんどないとおっしゃいましたが、当時は10個とか、惑星型の天体を実際に追跡できたんですよ。"
  },
  {
    "start": 2632330,
    "end": 2637606,
    "text": "それを10年とか続けていれば、ニューラルネットはまだ訓練するだろう。"
  },
  {
    "start": 2637708,
    "end": 2638594,
    "text": "まだ訓練中だった。"
  },
  {
    "start": 2638642,
    "end": 2639830,
    "text": "非常に低い損失だ。"
  },
  {
    "start": 2640650,
    "end": 2641062,
    "text": "そうだね。"
  },
  {
    "start": 2641116,
    "end": 2649578,
    "text": "あなたが言っているのは、何か間違ったことを学習してしまうということだが、それでもあなたがやったことはできるし、そこから単純な記号表現を抽出することもできる。"
  },
  {
    "start": 2649744,
    "end": 2655040,
    "text": "おそらく、それでもシンプルでうまくフィットするだろうが、間違っている。"
  },
  {
    "start": 2655650,
    "end": 2659614,
    "text": "それがどのようなものであったかを見るのは本当にクールだろう。"
  },
  {
    "start": 2659652,
    "end": 2660558,
    "text": "ええ、私もそう思います。"
  },
  {
    "start": 2660644,
    "end": 2670258,
    "text": "実際にそうなったのは、ケプラーが自分の法則に当てはめたからで、ニュートンの重力は、生のデータではなく、それを説明するためのものだった。"
  },
  {
    "start": 2670344,
    "end": 2677460,
    "text": "ここと同じように、ニュートンの重力の法則を生のデータに直接フィッティングしている点が少し違う。"
  },
  {
    "start": 2681190,
    "end": 2694060,
    "text": "AIサイエンティストのアイデアに話を戻すと、私にとって興味深かったのは、この方法論があれば、おそらく単純に見えるものが間違っている可能性があるということだ。"
  },
  {
    "start": 2694510,
    "end": 2698266,
    "text": "ああ、間違っていた。"
  },
  {
    "start": 2698448,
    "end": 2700262,
    "text": "ニュートンの重力の法則は間違っている。"
  },
  {
    "start": 2700416,
    "end": 2704174,
    "text": "それはほとんど正しい。"
  },
  {
    "start": 2704212,
    "end": 2706590,
    "text": "ああ、これは間違っていることだ。"
  },
  {
    "start": 2706740,
    "end": 2707726,
    "text": "完全に間違っている。"
  },
  {
    "start": 2707828,
    "end": 2708334,
    "text": "そうだね。"
  },
  {
    "start": 2708452,
    "end": 2709950,
    "text": "いい近似値だ。"
  },
  {
    "start": 2713590,
    "end": 2714434,
    "text": "ありがとう。"
  },
  {
    "start": 2714552,
    "end": 2716210,
    "text": "シンクについての簡単な質問だ。"
  },
  {
    "start": 2716280,
    "end": 2720674,
    "text": "平均台で試してみた？"
  },
  {
    "start": 2720712,
    "end": 2722286,
    "text": "ローレンツ・アトラクターを学ぶのだろうか？"
  },
  {
    "start": 2722318,
    "end": 2732658,
    "text": "非常にシンプルなシステムだが、その多くはネットワークの設定や特定の帰納的バイアスのかけ方に左右される。"
  },
  {
    "start": 2732834,
    "end": 2741660,
    "text": "力学的モデルを学習するのであれば、ニューラル微分方程式のように、更新ルールを学習し、それに記号方程式を当てはめるようなことをしたいだろう。"
  },
  {
    "start": 2742030,
    "end": 2749446,
    "text": "言語モデルにダイナミクスのJSONファイルを送り込むだけなら、それを解釈するのはもっと難しいだろう。"
  },
  {
    "start": 2749558,
    "end": 2753660,
    "text": "惑星系と大差ないのでは？"
  },
  {
    "start": 2754050,
    "end": 2758634,
    "text": "ポジションがあり、ダイナミックさがあり、フィジカルがある。"
  },
  {
    "start": 2758762,
    "end": 2763780,
    "text": "誘導バイアスを正しく設定すれば、その法則を見つけるのは簡単だと思う。"
  },
  {
    "start": 2765430,
    "end": 2765938,
    "text": "オーケー。"
  },
  {
    "start": 2766024,
    "end": 2776866,
    "text": "ああ、それは素晴らしいことだが、意外なことに、ひとつ質問を挟むとすれば、申し訳ないが、ニューラルネットがどれほど必要かというこの問題に戻ってきてもいいだろうか？"
  },
  {
    "start": 2776968,
    "end": 2784162,
    "text": "つまり、私の理解が正しければ、あなたの出した答えは、2つのパーツからなるニューラルネットがある、というものだった。"
  },
  {
    "start": 2784226,
    "end": 2785218,
    "text": "FAQ、スライド"
  },
  {
    "start": 2785314,
    "end": 2785622,
    "text": "オーケー。"
  },
  {
    "start": 2785676,
    "end": 2789800,
    "text": "だから、それぞれのハーフを象徴的に表現することができるだろう。"
  },
  {
    "start": 2790410,
    "end": 2804510,
    "text": "というのも、ニューラルネットの構造誘導を別にやっているのでなければ、おそらく人間がそのようにネットを設計したのはそのためだからだ。"
  },
  {
    "start": 2805090,
    "end": 2805790,
    "text": "なるほど。"
  },
  {
    "start": 2805860,
    "end": 2818174,
    "text": "また、そのようなパーツから構成する方法は、限られた用途にしか使えないような気がする。なぜなら、そのようなことをやりすぎると、目的を失ってしまうからだ。"
  },
  {
    "start": 2818222,
    "end": 2818866,
    "text": "そうだね。"
  },
  {
    "start": 2819048,
    "end": 2823390,
    "text": "ニューラルネットが2層のトランスフォーマーのようなものだったとしたら。"
  },
  {
    "start": 2823470,
    "end": 2833506,
    "text": "私は、中層の各細胞の記号表現を学び、さらに最上層のコンピュータの記号表現を学ぼうと考えた。"
  },
  {
    "start": 2833618,
    "end": 2839626,
    "text": "まあ、シンボリック回帰から何かが生まれるだろうが、それはあまりに複雑で、もう誰も見たがらないだろう？"
  },
  {
    "start": 2839808,
    "end": 2840940,
    "text": "そうだね。"
  },
  {
    "start": 2842110,
    "end": 2842860,
    "text": "オーケー。"
  },
  {
    "start": 2847470,
    "end": 2849290,
    "text": "なぜ直接は合わないのですか？"
  },
  {
    "start": 2849970,
    "end": 2854960,
    "text": "最終的には、ニューラルネットワークかもしれない。"
  },
  {
    "start": 2855730,
    "end": 2862074,
    "text": "私たちの問題では、遺伝的アルゴリズムによる勾配降下ステップを100万ステップ必要とします。"
  },
  {
    "start": 2862122,
    "end": 2867730,
    "text": "遺伝的アルゴリズムは効率が悪いからだ。"
  },
  {
    "start": 2868710,
    "end": 2869870,
    "text": "グラデーションがない。"
  },
  {
    "start": 2869950,
    "end": 2870530,
    "text": "そうだね。"
  },
  {
    "start": 2870680,
    "end": 2874610,
    "text": "勾配を使ったアプローチもあるが、遺伝的アルゴリズムには及ばない。"
  },
  {
    "start": 2875610,
    "end": 2893606,
    "text": "生データでニューラルネットワークを訓練し、遺伝的アルゴリズムで生徒の先生のようにやるほうがずっと簡単だ。"
  },
  {
    "start": 2893718,
    "end": 2903066,
    "text": "ニューラルネットワークでこのような多段階の集計をするよりもはるかに効率的だ。"
  },
  {
    "start": 2903098,
    "end": 2904320,
    "text": "それも理由の一つだ。"
  },
  {
    "start": 2904690,
    "end": 2906698,
    "text": "もうひとつの理由は因数分解だ。"
  },
  {
    "start": 2906794,
    "end": 2912910,
    "text": "最初にニューラルネットワークをトレーニングすることで、それを一種の因数分解をして部分式にするようなものだ。"
  },
  {
    "start": 2913270,
    "end": 2921858,
    "text": "その構造がどのように機能すべきかについて、何らかの知識、予備知識が必要だ。"
  },
  {
    "start": 2921944,
    "end": 2925586,
    "text": "つまり、あなたが提案したように、それを学ぶこともできる。"
  },
  {
    "start": 2925688,
    "end": 2936360,
    "text": "この例では、プーリングが合計であることを課しているが、異なるプーリングを用意して、そのうちのひとつを選ぶようにスパース化することもできるだろう。"
  },
  {
    "start": 2937870,
    "end": 2941050,
    "text": "それはある種、あなたが課さなければならないものだ。"
  },
  {
    "start": 2946670,
    "end": 2952050,
    "text": "この象徴的な蒸留を試みてもうまくいかない場合は、間違った誘導バイアスを選んでいることになる。"
  },
  {
    "start": 2952230,
    "end": 2958640,
    "text": "それは、間違った帰納的バイアスを選択したかどうかを見分ける一般的な方法のようなものだと思う。"
  },
  {
    "start": 2959090,
    "end": 2969090,
    "text": "幸いなことに、物理学の多くの問題では、特定の法則を知らなくても、因果関係が破られることはないことが分かっているので、帰納的バイアスをうまく使うことができる。"
  },
  {
    "start": 2969430,
    "end": 2977374,
    "text": "エネルギー保存とか、科学にはいろいろな対称性がある。"
  },
  {
    "start": 2977422,
    "end": 2988194,
    "text": "もう少し哲学的かもしれませんが、あなたは講演の冒頭で、AI科学者を作りたいと言いました。"
  },
  {
    "start": 2988242,
    "end": 2994922,
    "text": "その点では、これは明らかに大きなパズルの1ピースのように思える。"
  },
  {
    "start": 2995056,
    "end": 2997100,
    "text": "ああ、いい質問だね。"
  },
  {
    "start": 2999070,
    "end": 3001442,
    "text": "行動への呼びかけのようなものだ。"
  },
  {
    "start": 3001526,
    "end": 3009150,
    "text": "もっと多くの人が、自然科学のように科学研究を自動化することに取り組んでほしい。"
  },
  {
    "start": 3010610,
    "end": 3014154,
    "text": "これはモデル発掘のようなものだ。"
  },
  {
    "start": 3014282,
    "end": 3016382,
    "text": "理論展開の面ですらない。"
  },
  {
    "start": 3016446,
    "end": 3019582,
    "text": "データから経験式になるようなものだ。"
  },
  {
    "start": 3019726,
    "end": 3025774,
    "text": "経験的な関連性を与える理論を構築する方法が必要なのだ。"
  },
  {
    "start": 3025902,
    "end": 3029826,
    "text": "実験計画法が必要だ。"
  },
  {
    "start": 3030018,
    "end": 3039510,
    "text": "全体像がどのように見えるか、私はまだよく把握していないが、科学を実際に自動化するためには、最終的にこのループ全体が必要だ。"
  },
  {
    "start": 3041370,
    "end": 3042920,
    "text": "やるべきことはたくさんある。"
  },
  {
    "start": 3045690,
    "end": 3050942,
    "text": "さて、そろそろ質問も終わりに近づいているようだ。"
  },
  {
    "start": 3050996,
    "end": 3052362,
    "text": "ありがとう、マイルズ。"
  },
  {
    "start": 3052426,
    "end": 3053040,
    "text": "ありがとう。"
  },
  {
    "start": 3058050,
    "end": 3062814,
    "text": "いや、分かっている。"
  },
  {
    "start": 3062932,
    "end": 3063210,
    "text": "ネットワーク"
  }
]