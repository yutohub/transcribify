[
  {
    "start": 410,
    "end": 2206,
    "text": "やあ、みんな。"
  },
  {
    "start": 2308,
    "end": 5386,
    "text": "今日は、検索拡張世代について話そう。"
  },
  {
    "start": 5498,
    "end": 11774,
    "text": "つまり、ご存知のように、大規模な言語モデルは、学習データに基づいて質問に答えたり、テキストを生成したりすることしかできない。"
  },
  {
    "start": 11892,
    "end": 26726,
    "text": "例えば、2018年に訓練された言語モデルがあるとして、それにCOVIDについて質問すると、言語モデルはおそらくCOVIDについて何も知らないでしょう。言語モデルの知識を増強する方法の1つは、最新のデータでモデルを微調整することです。"
  },
  {
    "start": 26828,
    "end": 33314,
    "text": "検索拡張生成と呼ばれる別のテクニックがあり、これは特に質問応答において非常に有用である。"
  },
  {
    "start": 33442,
    "end": 38978,
    "text": "最近では、TwitterがGrokと呼ばれる新しい言語モデルを作成する際にも使われている。"
  },
  {
    "start": 39074,
    "end": 45574,
    "text": "Grokはツイートからリアルタイムですべてのデータにアクセスし、最新のトレンドに関する質問に答えることができる。"
  },
  {
    "start": 45702,
    "end": 48982,
    "text": "このビデオでは、検索拡張世代とは何か？"
  },
  {
    "start": 49046,
    "end": 55258,
    "text": "すべてのパイプライン、パイプラインのすべての部分、そしてそれぞれの構成要素の背後にあるアーキテクチャ。"
  },
  {
    "start": 55434,
    "end": 58160,
    "text": "では、今日のトピックをおさらいしよう。"
  },
  {
    "start": 58530,
    "end": 61194,
    "text": "言語モデルについて簡単に紹介しよう。"
  },
  {
    "start": 61242,
    "end": 64526,
    "text": "それが何なのか、どのように機能するのか、どのように推論するのか。"
  },
  {
    "start": 64628,
    "end": 73890,
    "text": "次に、検索補強世代を構成するパイプラインについて、特に埋め込みベクトルとその構築方法、その正体について言及する。"
  },
  {
    "start": 73960,
    "end": 81826,
    "text": "また、文の埋め込みを生成する方法の1つである、文誕生の背後にあるアーキテクチャについても探求する。"
  },
  {
    "start": 82018,
    "end": 94730,
    "text": "次に、ベクトル・データベースとは何か、どのように使うのか、クエリに似た特定のベクトルを見つけるためにベクトル・データベースのアルゴリズムがどのように働くのかについて説明する。"
  },
  {
    "start": 95390,
    "end": 101366,
    "text": "このビデオを見る前に、みなさんがトランスフォーマーについて少しは知っていることを期待している。"
  },
  {
    "start": 101398,
    "end": 110874,
    "text": "今回はそれほど詳しく説明するつもりはないが、少なくともトランスの基本的な構成要素や、誕生に関する前回のビデオをご覧になったことについてはご理解いただけたと思う。"
  },
  {
    "start": 110922,
    "end": 117206,
    "text": "もし、これらのトピックに馴染みがなければ、トランスフォーマーに関する前回のビデオとバートに関する前回のビデオをご覧になることをお勧めする。"
  },
  {
    "start": 117258,
    "end": 121634,
    "text": "彼らは、現在の映像を完全に理解するために必要な背景をすべて教えてくれる。"
  },
  {
    "start": 121832,
    "end": 123710,
    "text": "さあ、旅を始めよう。"
  },
  {
    "start": 123870,
    "end": 132582,
    "text": "まずは、私の愛猫オレオの紹介から始めよう。"
  },
  {
    "start": 132636,
    "end": 140940,
    "text": "中国人でなければ、彼の名前の読み方を知らないだろう。オレオはビスケットのオレオの略で、白黒だからだ。"
  },
  {
    "start": 141390,
    "end": 142906,
    "text": "さあ、始めよう。"
  },
  {
    "start": 143088,
    "end": 146166,
    "text": "最初にお話しするのは言語モデルです。"
  },
  {
    "start": 146278,
    "end": 147802,
    "text": "さて、言語モデルとは何か？"
  },
  {
    "start": 147856,
    "end": 150302,
    "text": "言語モデルは確率的なモデルだ。"
  },
  {
    "start": 150356,
    "end": 153710,
    "text": "これは、単語の列に確率を割り当てるものである。"
  },
  {
    "start": 153860,
    "end": 166002,
    "text": "実際、言語モデルを使えば、次のような計算ができる。 上海という文の後ろにchinaという単語が来る確率は？"
  },
  {
    "start": 166136,
    "end": 177802,
    "text": "言語モデルは、プロンプトが与えられたときに次のトークンが出現する確率をモデル化することができる。"
  },
  {
    "start": 177966,
    "end": 184200,
    "text": "非常に大量のテキストで学習されたニューラルネットワークは、大規模言語モデルとして知られている。"
  },
  {
    "start": 184970,
    "end": 193318,
    "text": "大規模な言語モデルを学習する場合、通常は非常に大きなテキスト・コーポレート、つまり文書のコレクションを用意する。"
  },
  {
    "start": 193494,
    "end": 211422,
    "text": "言語モデルには可能な限り多くの知識を獲得させたいので、多くの場合、言語モデルはウィキペディア全体や数百万のウェブページ、あるいは数千の書籍で学習される。"
  },
  {
    "start": 211476,
    "end": 224514,
    "text": "例えば、llamaは通常デコーダのみのネットワークとして知られ、Bertは通常エンコーダのみのネットワークとして知られている。"
  },
  {
    "start": 224552,
    "end": 234486,
    "text": "Llamaは基本的に、トランスフォーマーのデコーダーからクロスアテンションを除いたものにリニアレイヤーとソフトマックスを加えたもので、Bertはエンコーダー側だけを使っている。"
  },
  {
    "start": 234588,
    "end": 240780,
    "text": "そして、使用するタスクによっては、リニアレイヤーとなり得るヘッドもある。"
  },
  {
    "start": 241230,
    "end": 257402,
    "text": "言語モデルを推論するには、通常、プロンプトを作成し、言語モデルが追加するトークンがプロンプトと連続して意味をなすように、反復的にトークンを追加して、このプロンプトを継続するように言語モデルに依頼する。"
  },
  {
    "start": 257546,
    "end": 269202,
    "text": "例えば、私がChat GPTにジョークの続きをお願いしているとき、chatjbdはさらにいくつかのトークンを追加してジョークを続けました。"
  },
  {
    "start": 269256,
    "end": 272130,
    "text": "私が実際に書いたことと首尾一貫している。"
  },
  {
    "start": 273750,
    "end": 275442,
    "text": "あなたは自分が食べたものなのだ。"
  },
  {
    "start": 275576,
    "end": 280738,
    "text": "言語モデルが出力できるのは、学習したテキストと情報のみである。"
  },
  {
    "start": 280834,
    "end": 289058,
    "text": "つまり、英語のコンテンツだけで言語モデルを学習させた場合、おそらく日本語やフランス語を出力することはできないだろう。"
  },
  {
    "start": 289234,
    "end": 295740,
    "text": "新しいコンセプト、新しいコンテンツ、新しい情報を言語モデルに教えるには、モデルを微調整する必要がある。"
  },
  {
    "start": 296110,
    "end": 300442,
    "text": "しかし、微調整には短所もある。"
  },
  {
    "start": 300576,
    "end": 312994,
    "text": "例えば、モデルのパラメータ数を微調整するのに必要な計算能力という点で、高価になる可能性があり、モデル自体に教えたい知識をすべて把握するには不十分かもしれない。"
  },
  {
    "start": 313032,
    "end": 319118,
    "text": "例えば、llamaは70億、130億、700億のパラメーターで導入された。"
  },
  {
    "start": 319214,
    "end": 319858,
    "text": "なぜですか？"
  },
  {
    "start": 320024,
    "end": 329640,
    "text": "なぜなら、70億のパラメーターがあれば、ある程度の知識を把握することはできるが、700億のパラメーターのモデルほどではないからだ。"
  },
  {
    "start": 330250,
    "end": 335750,
    "text": "パラメータの数は、言語モデルが獲得できる知識の量に対する制限となる。"
  },
  {
    "start": 336410,
    "end": 338870,
    "text": "また、微調整は足し算ではない。"
  },
  {
    "start": 339610,
    "end": 347034,
    "text": "例えば、英語のコンテンツで学習させたモデルを、日本語のコンテンツで微調整するとします。"
  },
  {
    "start": 347232,
    "end": 359018,
    "text": "このモデルは、微調整が終わっても、英語にも日本語にも習熟しているわけではない。"
  },
  {
    "start": 359194,
    "end": 364260,
    "text": "つまり、微調整は言語モデルの知識には加算されないということだ。"
  },
  {
    "start": 366150,
    "end": 372110,
    "text": "もちろん、微調整は迅速なエンジニアリングで補うことができる。"
  },
  {
    "start": 372190,
    "end": 381522,
    "text": "例えば、プロンプトを操作することで、言語モデルに特別に訓練されていない新しいタスクを実行させることができる。"
  },
  {
    "start": 381666,
    "end": 386838,
    "text": "例えば、これはいくつかの短いプロンプト・テクニックで、次のような例がある。"
  },
  {
    "start": 386924,
    "end": 391658,
    "text": "まず言語モデルに指示を与える。"
  },
  {
    "start": 391744,
    "end": 396458,
    "text": "ここに、言語モデルが実行するタスクが何であるかの指示がある。"
  },
  {
    "start": 396624,
    "end": 404266,
    "text": "そして、言語モデルにこのタスクを実行する方法をいくつか例示し、言語モデルに実行してもらう。"
  },
  {
    "start": 404448,
    "end": 413194,
    "text": "たとえば、オレオは友人のウマルにいたずらするのが好きな猫で、彼が書くすべての名前をニャーに置き換える。"
  },
  {
    "start": 413322,
    "end": 420210,
    "text": "例えば、ウマルはBob runs a YouTube channelと書き、オレオはMeow runs a YouTube channelと修正する。"
  },
  {
    "start": 420360,
    "end": 424942,
    "text": "ウマルがアリスはボブと遊ぶのが好きだと書いたらどうなる？"
  },
  {
    "start": 425006,
    "end": 426946,
    "text": "オレオはどう修正するんだ？"
  },
  {
    "start": 427048,
    "end": 432338,
    "text": "言語モデルは正しい答えを導き出す。ニャーは友達のニャーと遊ぶのが好きだ。"
  },
  {
    "start": 432434,
    "end": 444170,
    "text": "この場合のジャッジGPTは、このタスクを実行するためのトレーニングを受けていなかったが、プロンプトと我々が提供した例を見ることで、正しい解決策を導き出すことができた。"
  },
  {
    "start": 444590,
    "end": 450390,
    "text": "プロンプト・エンジニアリングも、同じような推論で質問に答えるために使うことができる。"
  },
  {
    "start": 450470,
    "end": 454638,
    "text": "インストラクションパートを含む非常に大きなプロンプトを作る。"
  },
  {
    "start": 454724,
    "end": 465438,
    "text": "あなたは、与えられたコンテキストを使って質問に答えるように訓練されたアシスタントです。"
  },
  {
    "start": 465604,
    "end": 468210,
    "text": "そして、言語モデルに質問する。"
  },
  {
    "start": 468360,
    "end": 471042,
    "text": "Grok zeroにはいくつのパラメータがありますか？"
  },
  {
    "start": 471096,
    "end": 481080,
    "text": "GrokはTwitterが導入した言語モデルで、最新のツイートにもアクセスできる。"
  },
  {
    "start": 481770,
    "end": 484614,
    "text": "要するに、私は言語モデルを尋ねているのだ。"
  },
  {
    "start": 484652,
    "end": 487222,
    "text": "グロック・ゼロについて教えてくれるチャットGPT。"
  },
  {
    "start": 487276,
    "end": 495638,
    "text": "おそらく、チャージプトはこのグロック・ゼロの存在を知らなかったのだろう。"
  },
  {
    "start": 495814,
    "end": 501318,
    "text": "モデル・チャルは文脈を見て答えを導き出す。"
  },
  {
    "start": 501414,
    "end": 504638,
    "text": "この場合、例えば、グロック・ゼロと書かれている。"
  },
  {
    "start": 504724,
    "end": 520514,
    "text": "というのも、Chatbtは、ロック・ゼロにいくつのパラメータがあるのかについて話しているコンテキスト、つまりこの行にアクセスすることができたからだ。"
  },
  {
    "start": 520552,
    "end": 525954,
    "text": "Xaiを発表した後、プロトタイプLLMを330億パラメータで訓練する。"
  },
  {
    "start": 526082,
    "end": 527960,
    "text": "答えは正しい。"
  },
  {
    "start": 528330,
    "end": 536082,
    "text": "このようなプロンプトの扱い方は、実際とてもパワフルだ。"
  },
  {
    "start": 536226,
    "end": 556346,
    "text": "というのも、通常、特定のコンテンツに対してモデルを微調整すると、プロンプト・エンジニアリングに比べ、より質の高い結果が得られるからです。"
  },
  {
    "start": 556538,
    "end": 563962,
    "text": "また、前に見たように、GPTをシャットダウンするために質問をするには、非常に大きなプロンプトを作らなければならなかった。"
  },
  {
    "start": 564106,
    "end": 568980,
    "text": "これは、モデルに与えるトークンの数がかなり大きいことを意味する。"
  },
  {
    "start": 570150,
    "end": 574340,
    "text": "特定の質問に答えるためには、より大きな文脈を提供する必要がある。"
  },
  {
    "start": 574870,
    "end": 581430,
    "text": "私たちは、より多くの文脈を与えれば与えるほど、言語モデルが正しい答えを導き出すための情報が増えることを知っている。"
  },
  {
    "start": 581500,
    "end": 584070,
    "text": "通常はもっと大きな文脈が必要だ。"
  },
  {
    "start": 585130,
    "end": 589458,
    "text": "問題は、コンテクストが大きいと計算量も多くなるということだ。"
  },
  {
    "start": 589554,
    "end": 601114,
    "text": "というのも、質問する特定のデータについて言語モデルを微調整しているからです。"
  },
  {
    "start": 601152,
    "end": 608986,
    "text": "つまり、微調整された言語モデルに対して、すべてのコンテキストを提供することなく、RoCゼロにはいくつのパラメータがあるのか？"
  },
  {
    "start": 609098,
    "end": 613780,
    "text": "言語モデルが正しく微調整されていれば、正しい答えを導き出すことができるだろう。"
  },
  {
    "start": 615350,
    "end": 625730,
    "text": "さて、検索増強生成パイプラインを紹介する必要がある。それが次のステップであり、パイプラインを構成する各構成ブロックを探求することになるからだ。"
  },
  {
    "start": 625810,
    "end": 634390,
    "text": "プロンプト・エンジニアリングで行っていた質問応答を、検索拡張世代で行うことを想像してみてほしい。"
  },
  {
    "start": 635290,
    "end": 639382,
    "text": "グロック・ゼロにはいくつのパラメータがありますか？"
  },
  {
    "start": 639436,
    "end": 640650,
    "text": "これが私たちの問い合わせだ。"
  },
  {
    "start": 640990,
    "end": 645962,
    "text": "この答えを見つけることができる資料もいくつかあると思う。"
  },
  {
    "start": 646096,
    "end": 652300,
    "text": "これらの文書は、PDF文書かもしれないが、この答えを取得できるウェブページかもしれない。"
  },
  {
    "start": 652990,
    "end": 658990,
    "text": "私たちがすることは、これらの文書やテキストの断片をテキストの塊に分割することだ。"
  },
  {
    "start": 659060,
    "end": 667570,
    "text": "例えば、文書は多くのページで構成され、各ページは段落で構成され、各段落は文章で構成される。"
  },
  {
    "start": 668630,
    "end": 677640,
    "text": "通常、これらの文書はそれぞれ小さな文章に分割され、ウェブページも同じように分割される。"
  },
  {
    "start": 678090,
    "end": 689110,
    "text": "各埋め込みは、それぞれの文の意味をとらえる一定の大きさのベクトルとなるように、これらの文の埋め込みを作成する。"
  },
  {
    "start": 690030,
    "end": 698060,
    "text": "そして、これらの埋め込みをすべてベクトル・データベースに格納する。後で、これらの埋め込みとベクトル・データベースがどのように機能するかを見てみよう。"
  },
  {
    "start": 698590,
    "end": 701290,
    "text": "次に、クエリ（文）を取る。"
  },
  {
    "start": 701370,
    "end": 708910,
    "text": "文書を埋め込みに変換したのと同じモデルを使って、埋め込みに変換する。"
  },
  {
    "start": 709570,
    "end": 724814,
    "text": "私たちは、この埋め込みを、すでに多くの埋め込みを持っている私たちのデータベースに検索します。"
  },
  {
    "start": 724942,
    "end": 730998,
    "text": "各埋め込みは、それが由来するテキストの一部とも関連付けられている。"
  },
  {
    "start": 731084,
    "end": 737186,
    "text": "ベクトル・データベースは、その埋め込みが作成された元のテキストを検索することもできる。"
  },
  {
    "start": 737378,
    "end": 747926,
    "text": "例えば、grox zeroにはいくつのパラメータがあるのか、というような場合、ベクトル・データベースはすべての埋め込みを検索し、私たちのクエリに最もマッチする埋め込みを与えてくれる。"
  },
  {
    "start": 748118,
    "end": 754990,
    "text": "おそらく、grok zeroとそれが含むパラメーターについて述べているすべてのテキストを探すだろう。"
  },
  {
    "start": 755650,
    "end": 761482,
    "text": "コンテキストとクエリができたので、プロンプトのテンプレートを作成する。"
  },
  {
    "start": 761546,
    "end": 768686,
    "text": "以前使ったプロンプトと同じように、あなたは与えられた文脈を使って質問に答えるよう訓練されたアシスタントなのだ。"
  },
  {
    "start": 768878,
    "end": 778070,
    "text": "コンテキストとクエリをプロンプトテンプレートの中に貼り付け、先ほどと同じように言語モデルに送り込む。"
  },
  {
    "start": 778220,
    "end": 783986,
    "text": "そうすれば、言語モデルは提供された文脈を使って質問に答えることができる。"
  },
  {
    "start": 784098,
    "end": 790154,
    "text": "検索拡張世代では、質問に答えるためにモデルを微調整しているわけではない。"
  },
  {
    "start": 790272,
    "end": 793894,
    "text": "私たちは実際にプロンプト・エンジニアリングを使っている。"
  },
  {
    "start": 794022,
    "end": 816260,
    "text": "ベクトル・データベースと呼ばれるデータベースを導入することで、クエリで指定されたコンテキストにアクセスすることができ、特定の質問に答えるために必要なコンテキストを検索して言語モデルに送り、そのコンテキストを使って言語モデルが正しい答えを導き出すことができる。"
  },
  {
    "start": 818470,
    "end": 823620,
    "text": "では、埋め込みベクトルとは何か、どのように機能するのかについて説明しよう。"
  },
  {
    "start": 824710,
    "end": 829062,
    "text": "ではまず、なぜ言葉を表現するのにベクトルを使うのでしょうか？"
  },
  {
    "start": 829196,
    "end": 837010,
    "text": "例えば、チェリー・デジタルとインフォメーションという言葉があるとして、埋め込みベクトルを2次元だけで表すとする。"
  },
  {
    "start": 837090,
    "end": 842214,
    "text": "覚えているように、バニラ・トランスフォームでは、各埋め込みベクトルは500から12次元である。"
  },
  {
    "start": 842262,
    "end": 844170,
    "text": "もっと簡単な言葉を想像してほしい。"
  },
  {
    "start": 844240,
    "end": 850470,
    "text": "我々は2次元しか持っていないので、これらの埋め込みベクトルをxy平面上にプロットすることができる。"
  },
  {
    "start": 850630,
    "end": 862078,
    "text": "似たような意味を持つ言葉や、同じ概念を表す言葉が、空間上で同じ方向を向くように。"
  },
  {
    "start": 862164,
    "end": 873570,
    "text": "例えば、digitalという単語とinformationという単語は空間上で同じ方向を向いており、同じような意味を持つ単語同士の角度は小さい。"
  },
  {
    "start": 873640,
    "end": 881842,
    "text": "デジタルと情報の間の角度は小さく、異なる意味を持つ言葉の間の角度は大きい。"
  },
  {
    "start": 881906,
    "end": 890918,
    "text": "例えば、チェリーとデジタルは、デジタルとインフォメーションに比べて角度が大きく、異なる概念を表していることがわかる。"
  },
  {
    "start": 891094,
    "end": 893606,
    "text": "トマトという別の単語があるとしよう。"
  },
  {
    "start": 893638,
    "end": 900140,
    "text": "チェリーとトマトの間の角度が小さくなるように、この垂直方向を向くと予想される。"
  },
  {
    "start": 901250,
    "end": 903034,
    "text": "この角度はどうやって測るのか？"
  },
  {
    "start": 903082,
    "end": 911070,
    "text": "私たちは通常、ベクトル間の角度を測定するために余弦類似度を使用します。"
  },
  {
    "start": 912050,
    "end": 917918,
    "text": "さて、なぜ単語を埋め込みとして表現することを思いついたのだろうか？"
  },
  {
    "start": 918094,
    "end": 926230,
    "text": "第一の考え方は、類義語である単語は同じ文脈で出現する傾向があるため、同じ単語に囲まれるということである。"
  },
  {
    "start": 926380,
    "end": 937590,
    "text": "例えば、teacherやprofessorという単語は、通常、school、university、examination、lecture、courseなどの単語によって使われる。"
  },
  {
    "start": 938190,
    "end": 944166,
    "text": "また、同じ文脈で出てくる単語は似たような意味を持つ傾向があるとも言える。"
  },
  {
    "start": 944278,
    "end": 947530,
    "text": "これは分布仮説として知られている。"
  },
  {
    "start": 948430,
    "end": 957120,
    "text": "つまり、ある単語の意味を理解するためには、その単語の文脈、つまりその単語を取り巻く単語にもアクセスする必要があるのだ。"
  },
  {
    "start": 957650,
    "end": 967486,
    "text": "これは、各トークンの概念情報を捉えるために、トランスフォーマーモデルで自己注意を採用している理由もここにあることを意味する。"
  },
  {
    "start": 967598,
    "end": 971342,
    "text": "トランスフォーマーのモデルを思い出してほしい。"
  },
  {
    "start": 971406,
    "end": 981074,
    "text": "自己注目は、各トークンが文中で占める位置にも基づいて、各トークンを同じ文中の他のすべてのトークンと関連付ける方法である。"
  },
  {
    "start": 981122,
    "end": 988850,
    "text": "私たちは位置エンコーディングの概念を持っているので、自己の注意はその注意のスコアを計算するために2つのものにアクセスする。"
  },
  {
    "start": 989010,
    "end": 993110,
    "text": "1つ目は、言葉の意味を捉える埋め込みである。"
  },
  {
    "start": 993270,
    "end": 1002620,
    "text": "2つ目の情報は、位置エンコーディングにアクセスすることで、互いに近い単語と遠い単語は異なる関係になる。"
  },
  {
    "start": 1002990,
    "end": 1014900,
    "text": "この自己注意メカニズムは、各単語の埋め込みを、その単語とその単語を囲む単語の文脈情報も取り込むように修正する。"
  },
  {
    "start": 1017990,
    "end": 1028638,
    "text": "私たちは、bertの埋め込み情報を作成するための情報を取得するために、マスケット言語モデルタスクと呼ばれる非常に特殊なタスクでbertを訓練した。"
  },
  {
    "start": 1028814,
    "end": 1034390,
    "text": "このマスケット銃の言語モデルのタスクは、洋服のタスクがベースになっている。"
  },
  {
    "start": 1034460,
    "end": 1035800,
    "text": "例を挙げよう。"
  },
  {
    "start": 1036170,
    "end": 1038258,
    "text": "私が次のような文章を書いたとしよう。"
  },
  {
    "start": 1038354,
    "end": 1045366,
    "text": "ローマはイタリアの \"何か \"である。"
  },
  {
    "start": 1045478,
    "end": 1047914,
    "text": "足りない単語は何ですか？"
  },
  {
    "start": 1048112,
    "end": 1056240,
    "text": "もちろん、欠落している単語は大文字である。"
  },
  {
    "start": 1056610,
    "end": 1060430,
    "text": "その足りない単語をどうやってキャピタルという言葉に置き換えたのだろう？"
  },
  {
    "start": 1060500,
    "end": 1073460,
    "text": "つまり、キャピタル・キャピタルという単語は、その単語が現れる文脈、その単語を囲む単語によって決まるということだ。"
  },
  {
    "start": 1073990,
    "end": 1076078,
    "text": "これがバートを鍛える方法だ。"
  },
  {
    "start": 1076174,
    "end": 1088086,
    "text": "私たちは、自己注意メカニズムがすべての入力トークンを互いに関連付け、バートが欠落した単語の文脈に関する十分な情報を持って、その単語を予測できるようにしたいと考えている。"
  },
  {
    "start": 1088268,
    "end": 1096362,
    "text": "例えば、Bertをマスケット銃の言語モデルタスクで学習させたいと考えて、先ほどと同じようにマスケット銃という単語を含む入力を作成したとする。"
  },
  {
    "start": 1096416,
    "end": 1101754,
    "text": "ローマはイタリアの \"何か \"である。"
  },
  {
    "start": 1101802,
    "end": 1105402,
    "text": "空白をマスクと呼ばれる特別なトークンで置き換える。"
  },
  {
    "start": 1105546,
    "end": 1109626,
    "text": "これが14個のトークンからなるbertの入力となる。"
  },
  {
    "start": 1109818,
    "end": 1111770,
    "text": "バートに食べさせるんだ。"
  },
  {
    "start": 1111930,
    "end": 1116482,
    "text": "バートはトランスフォーマーモデルなので、出力されるのはシーケンスからシーケンスへのモデルだ。"
  },
  {
    "start": 1116536,
    "end": 1120910,
    "text": "入力が14トークンなら、出力も14トークンになる。"
  },
  {
    "start": 1121070,
    "end": 1126578,
    "text": "バートに4つ目のトークンを予想してもらう。"
  },
  {
    "start": 1126664,
    "end": 1128262,
    "text": "私たちはその言葉が何なのかを知っている。"
  },
  {
    "start": 1128396,
    "end": 1129794,
    "text": "この言葉は資本である。"
  },
  {
    "start": 1129922,
    "end": 1137202,
    "text": "予測された4番目のトークンと実際の4番目のトークンに基づいて損失を計算しました。"
  },
  {
    "start": 1137266,
    "end": 1142602,
    "text": "その後、モデルのすべての重みを更新するために損失を逆伝播する。"
  },
  {
    "start": 1142736,
    "end": 1150506,
    "text": "バックプロパゲーション・バートを実行すると、モデルはここで入力埋め込みも更新する。"
  },
  {
    "start": 1150608,
    "end": 1166910,
    "text": "バックプロパゲーション機構によって入力された埋め込みは、この単語、つまり大文字の単語、つまり大文字の単語に関連付けられた埋め込みが、その文脈に関するすべての情報を捉えるように修正される。"
  },
  {
    "start": 1167070,
    "end": 1172530,
    "text": "次回は、バートがその文脈から予測するのに苦労することはないだろう。"
  },
  {
    "start": 1172870,
    "end": 1181190,
    "text": "バックプロパゲーションを実行する理由のひとつは、損失を減らすことでモデルをどんどん良くしていきたいからだ。"
  },
  {
    "start": 1182890,
    "end": 1195370,
    "text": "実は、1つのトークンだけでなく、文全体の埋め込みを作成することも可能で、自己注視のメカニズムを使って文全体の意味も捉えることができると言ったらどうだろう。"
  },
  {
    "start": 1195710,
    "end": 1201178,
    "text": "私たちにできることは、事前に訓練されたBertモデルを使って、文全体の埋め込みを生成することだ。"
  },
  {
    "start": 1201274,
    "end": 1202400,
    "text": "どうやるか見てみよう。"
  },
  {
    "start": 1202770,
    "end": 1206170,
    "text": "さて、単純な入力文があるとしよう。"
  },
  {
    "start": 1206250,
    "end": 1213834,
    "text": "例えば、教授と呼ばれる13個のトークンからなるトークンは、いつも週末にたくさんの課題を出してくる。"
  },
  {
    "start": 1213962,
    "end": 1215578,
    "text": "バートに食べさせるんだ。"
  },
  {
    "start": 1215674,
    "end": 1220930,
    "text": "バートからリニアレイヤーを削除したことに注目してほしい。"
  },
  {
    "start": 1221080,
    "end": 1230914,
    "text": "そこでまず、自己注視の入力が13×768yの行列であることに気づく。"
  },
  {
    "start": 1230962,
    "end": 1241210,
    "text": "各トークンは768次元の埋め込みで表現され、これはバート・ベースにおける埋め込みベクトルのサイズである。"
  },
  {
    "start": 1241280,
    "end": 1251354,
    "text": "より小さいバート前訓練モデル（自己注意メカニズム）は、13×768の別の行列を出力する。"
  },
  {
    "start": 1251392,
    "end": 1257066,
    "text": "つまり、13のトークンがあり、それぞれが768次元の埋め込みを持つ。"
  },
  {
    "start": 1257258,
    "end": 1270340,
    "text": "自己注意の出力は、単語の意味や文中での位置だけでなく、文脈情報、他の単語と現在の単語の関係もすべて捉えた埋め込みである。"
  },
  {
    "start": 1271430,
    "end": 1278934,
    "text": "の場合、出力は13個のトークンとなり、各トークンは768サイズの埋め込みで表現される。"
  },
  {
    "start": 1279132,
    "end": 1284806,
    "text": "今やっていることは、それぞれが一つの単語の意味を表しているんだ。"
  },
  {
    "start": 1284908,
    "end": 1289970,
    "text": "そのすべてを平均して、文の埋め込みを作成するのだ。"
  },
  {
    "start": 1290050,
    "end": 1299734,
    "text": "768次元のベクトルをすべて取り、その平均を計算すると、760次元のベクトルになる。"
  },
  {
    "start": 1299782,
    "end": 1306090,
    "text": "この単一のベクトルは、文全体の意味を捉える文埋め込みを表す。"
  },
  {
    "start": 1306250,
    "end": 1309790,
    "text": "このようにして、文の埋め込みを作成する。"
  },
  {
    "start": 1311170,
    "end": 1318702,
    "text": "さて、2つの文が同じような意味を持っているかどうかを確認するために、文の埋め込みを比較するにはどうすればよいでしょうか？"
  },
  {
    "start": 1318766,
    "end": 1323534,
    "text": "例えば、ある文章がクエリについて話しているとしよう。"
  },
  {
    "start": 1323582,
    "end": 1327298,
    "text": "例えば、以前、グロッシュゼロにはいくつのパラメータがあるかという話をした。"
  },
  {
    "start": 1327384,
    "end": 1331574,
    "text": "そして、grox zeroにはいくつのパラメーターがあるのか、という別の文章がある。"
  },
  {
    "start": 1331612,
    "end": 1333730,
    "text": "この2つの文章をどのように関連づけることができるだろうか？"
  },
  {
    "start": 1333890,
    "end": 1336950,
    "text": "類似性関数を見つける必要がある。"
  },
  {
    "start": 1337100,
    "end": 1341910,
    "text": "というのも、どちらもベクトルだからだ。"
  },
  {
    "start": 1341990,
    "end": 1349734,
    "text": "余弦類似度はベクトル間として計算することができ、2つのベクトル間の角度の余弦を測定する。"
  },
  {
    "start": 1349862,
    "end": 1358026,
    "text": "角度が小さいほど余弦類似度スコアは高くなり、角度が大きいほど余弦類似度スコアは小さくなる。"
  },
  {
    "start": 1358058,
    "end": 1360990,
    "text": "これはコサイン類似度スコアの式である。"
  },
  {
    "start": 1362050,
    "end": 1363378,
    "text": "問題がある。"
  },
  {
    "start": 1363544,
    "end": 1370190,
    "text": "誰もバートには、この埋め込みがコサイン類似度に匹敵するはずだとは言わなかった。"
  },
  {
    "start": 1370270,
    "end": 1375682,
    "text": "Bertはいくつかの埋め込みを出力し、その平均を取る。"
  },
  {
    "start": 1375746,
    "end": 1383910,
    "text": "誰もバートには、これらの埋め込みは、似たような2つの文が似たような埋め込みを生成するようにするべきだとは言わなかった。"
  },
  {
    "start": 1385850,
    "end": 1396810,
    "text": "Bertに、我々が選択した類似度関数（エコサイン類似度やユークリッド距離など）と比較できる埋め込みデータを生成するように教えるにはどうすればいいのだろう。"
  },
  {
    "start": 1397550,
    "end": 1399750,
    "text": "さて、バートを紹介しよう。"
  },
  {
    "start": 1399830,
    "end": 1411082,
    "text": "Sentence Bertは、選択した類似性関数を用いて比較可能な、文全体の埋め込みを生成する最も一般的なモデルの1つである。"
  },
  {
    "start": 1411146,
    "end": 1414138,
    "text": "この場合、コサイン類似度スコアである。"
  },
  {
    "start": 1414314,
    "end": 1418046,
    "text": "というわけで、センテンス・バートはセンテンス・バートという論文で紹介された。"
  },
  {
    "start": 1418078,
    "end": 1420974,
    "text": "シャムバートネットワークを用いた文の埋め込み。"
  },
  {
    "start": 1421022,
    "end": 1422482,
    "text": "私たちはこのすべてを目にすることになる。"
  },
  {
    "start": 1422536,
    "end": 1423362,
    "text": "どういう意味ですか？"
  },
  {
    "start": 1423416,
    "end": 1425250,
    "text": "シャム・ネットワークとは？"
  },
  {
    "start": 1425590,
    "end": 1429842,
    "text": "ここで、意味の似た2つの文があるとしよう。"
  },
  {
    "start": 1429906,
    "end": 1433314,
    "text": "例えば、父は公園で私と遊んでくれる。"
  },
  {
    "start": 1433442,
    "end": 1436310,
    "text": "パパと公園で遊ぶんだ。"
  },
  {
    "start": 1438650,
    "end": 1443500,
    "text": "最初のものを文aと呼び、2番目のものを文bと呼ぶことにする。"
  },
  {
    "start": 1444030,
    "end": 1449894,
    "text": "私たちはそれらをバートに与えるので、それぞれがトークンの列になる。"
  },
  {
    "start": 1449942,
    "end": 1453094,
    "text": "例えば、この1は10トークンかもしれないし、この1は8トークンかもしれない。"
  },
  {
    "start": 1453142,
    "end": 1458826,
    "text": "これをバートに送ると、バートは10トークンと8トークンを出力する。"
  },
  {
    "start": 1459018,
    "end": 1462062,
    "text": "それからプーリング、つまり前にやったミニプーリングをするんだ。"
  },
  {
    "start": 1462116,
    "end": 1470114,
    "text": "そこで、これらの出力トークンをすべて取り出し、それらの平均を計算して、768次元の1つのベクトルだけを生成する。"
  },
  {
    "start": 1470152,
    "end": 1474530,
    "text": "バースベースを使う場合、あるいはもっと大きな鳥を使う場合。"
  },
  {
    "start": 1475670,
    "end": 1478454,
    "text": "最初のものを、ここではセンテンス・エンベッディングaと呼ぶことにする。"
  },
  {
    "start": 1478492,
    "end": 1484662,
    "text": "最初のベクトルは文aの埋め込み、2番目のベクトルは文bの埋め込みである。"
  },
  {
    "start": 1484796,
    "end": 1489366,
    "text": "次に、これら2つのベクトル間の余弦類似度を測定する。"
  },
  {
    "start": 1489558,
    "end": 1494922,
    "text": "このモデルをトレーニングするため、目標とする余弦類似度がある。"
  },
  {
    "start": 1495056,
    "end": 1505918,
    "text": "つまり、例えば、意味のよく似た2つの文があったとして、その間の角度が小さくなるため、1つの文に非常に近いターゲットができる可能性がある。"
  },
  {
    "start": 1506084,
    "end": 1508510,
    "text": "両者の角度が小さくなることを願っている。"
  },
  {
    "start": 1508580,
    "end": 1518018,
    "text": "ターゲットとモデルの出力があるので、損失を計算し、逆伝播を実行してこのモデルのすべての重みを更新する。"
  },
  {
    "start": 1518184,
    "end": 1523442,
    "text": "さて、おわかりのように、このモデルは同じ2つのブランチで構成されている。"
  },
  {
    "start": 1523576,
    "end": 1539158,
    "text": "これはシャムネットワークと呼ばれるもので、アーキテクチャーだけでなく、モデルの重みに関しても互いに同じである2つ以上の枝からなるネットワークである。"
  },
  {
    "start": 1539244,
    "end": 1544614,
    "text": "シャムネットワークを表現する場合、2つの枝で表現する。"
  },
  {
    "start": 1544662,
    "end": 1549978,
    "text": "このモデルをコーディングするとき、実際にはモデルは1つしかない。"
  },
  {
    "start": 1550064,
    "end": 1554058,
    "text": "コサインの類似性を生み出す分岐はここにひとつしかない。"
  },
  {
    "start": 1554154,
    "end": 1563470,
    "text": "オペレーティング・レベルでは、まずaの文章をこのモデルに通し、次にbの文章を通す。"
  },
  {
    "start": 1563540,
    "end": 1567934,
    "text": "また、このモデルを通じて、これら2つの出力間の余弦類似度を計算する。"
  },
  {
    "start": 1567982,
    "end": 1575506,
    "text": "その場合、逆伝播はこのモデルのパラメーターだけを変更するように逆伝播を実行する。"
  },
  {
    "start": 1575608,
    "end": 1581682,
    "text": "それを見せるために2本の枝として表現する。"
  },
  {
    "start": 1581746,
    "end": 1584434,
    "text": "実際には2つの枝があるわけではないことを覚えておいてほしい。"
  },
  {
    "start": 1584482,
    "end": 1589590,
    "text": "1つのブランチ、1つのウェイト、1つのアーキテクチャー、同じ数のパラメーターだ。"
  },
  {
    "start": 1590890,
    "end": 1595826,
    "text": "こうすることで、鳥は鳥降下を訓練する。"
  },
  {
    "start": 1595858,
    "end": 1612030,
    "text": "このようにbirdは埋め込みを生成するが、コサイン類似度が似ている文はコサイン類似度が高いので、コサイン類似度を使って比較することができる。"
  },
  {
    "start": 1612770,
    "end": 1620482,
    "text": "また、Bertは、少なくともBertベースは768サイズのエンベッディングを生成する。"
  },
  {
    "start": 1620536,
    "end": 1629202,
    "text": "もし768次元より小さい文の埋め込みを作りたいなら、ここで線形レイヤーを入れて次元を小さくすることができる。"
  },
  {
    "start": 1629346,
    "end": 1636242,
    "text": "例えば、768から512にしたい。"
  },
  {
    "start": 1636306,
    "end": 1649814,
    "text": "実は、BeRtが出力する全トークンの平均を計算し、文全体の意味を表す1つのベクトルを生成するために、私たちが使った平均プーリングだけでなく、最大プーリングも使っている。"
  },
  {
    "start": 1649942,
    "end": 1652854,
    "text": "もう一つのテクニックはCLSトークンだ。"
  },
  {
    "start": 1652902,
    "end": 1660762,
    "text": "思い出してほしいのだが、CLSトークンはBeRtに入力する最初のトークンであり、Bertが出力する最初のトークンでもある。"
  },
  {
    "start": 1660906,
    "end": 1672980,
    "text": "通常、自己注意メカニズムが働くため、このCLSトークンは他のすべてのトークンから情報を取得する。"
  },
  {
    "start": 1674310,
    "end": 1683634,
    "text": "しかし、国勢調査の鳥の論文では、最大プーリングとCLSトークンの両方の方法が最小プーリングよりも優れていないことを示している。"
  },
  {
    "start": 1683682,
    "end": 1689930,
    "text": "彼らはミン・プーリングの使用を推奨している。"
  },
  {
    "start": 1691310,
    "end": 1697334,
    "text": "では、検索オーグメンテッド世代のパイプラインとは何なのか、もう一度おさらいしてみよう。"
  },
  {
    "start": 1697382,
    "end": 1705280,
    "text": "さて、エンベッディングの仕組みがわかったところで、groc zeroにはいくつのパラメータがあるのかというクエリを用意しよう。"
  },
  {
    "start": 1705730,
    "end": 1708878,
    "text": "それなら、この答えを見つけることができる資料がある。"
  },
  {
    "start": 1708964,
    "end": 1712350,
    "text": "ドキュメントはPDFドキュメントでもウェブページでもかまいません。"
  },
  {
    "start": 1712710,
    "end": 1719810,
    "text": "それを1つのセンテンスに分割し、センテンスバートを使ってセンテンスを埋め込む。"
  },
  {
    "start": 1720150,
    "end": 1729938,
    "text": "文Bertは固定サイズ（仮に768）のベクトルを生成し、これらのベクトルをすべてベクトルDbに格納する。"
  },
  {
    "start": 1730034,
    "end": 1731960,
    "text": "どのように機能するかは、後で見てみよう。"
  },
  {
    "start": 1733210,
    "end": 1745366,
    "text": "また、クエリは768次元のベクトルに変換され、このクエリをベクトルDBSで検索する。"
  },
  {
    "start": 1745478,
    "end": 1746810,
    "text": "どうやって探すのか？"
  },
  {
    "start": 1746960,
    "end": 1751750,
    "text": "我々は、我々のクエリに最もマッチするすべての埋め込みを見つけたい。"
  },
  {
    "start": 1751830,
    "end": 1752934,
    "text": "これは何を意味しているのか？"
  },
  {
    "start": 1752992,
    "end": 1755982,
    "text": "我々は、それを持つすべてのエンベッディングを意味している。"
  },
  {
    "start": 1756036,
    "end": 1765662,
    "text": "コサイン類似度スコアをクエリで計算すると、高い値が得られるが、他の類似度スコア、例えばユークリッド距離を使っている場合は、その値が高くなる。"
  },
  {
    "start": 1765806,
    "end": 1772530,
    "text": "この距離は、コサイン類似度とユークリッド距離のどちらを使うかによって小さくなる。"
  },
  {
    "start": 1773110,
    "end": 1781640,
    "text": "これにより、クエリに最もマッチする埋め込みが生成され、それらを元のテキストにマッピングする。"
  },
  {
    "start": 1782410,
    "end": 1786946,
    "text": "これで、プロンプト・テンプレートに入力するコンテキストが生成される。"
  },
  {
    "start": 1787138,
    "end": 1795242,
    "text": "クエリとともに、答えを生成する大規模な言語モデルにクエリを与える。"
  },
  {
    "start": 1795296,
    "end": 1801718,
    "text": "カスタムデータセットでファインチューニングするか、エンベッディングで構成されたベクトルデータベースを使うか、2つの戦略がある。"
  },
  {
    "start": 1801814,
    "end": 1810750,
    "text": "例えば、数回のエポックで微調整を行い、その後ベクトルデータベースを使用してウェブから取得した知識で補完するなど、両方を組み合わせて使用することもできる。"
  },
  {
    "start": 1810900,
    "end": 1819534,
    "text": "どのような戦略で進めるにせよ、検索増強世代パイプラインを構築するためには、信頼性が高く、スケーラブルで、使いやすいサービスが必要だ。"
  },
  {
    "start": 1819662,
    "end": 1822030,
    "text": "だから私はグラデーションを勧める。"
  },
  {
    "start": 1822190,
    "end": 1830050,
    "text": "GradientはスケーラブルなAIクラウドプラットフォームであり、モデルの微調整、埋め込みデータの生成、推論の実行のためのシンプルなAPIを提供する。"
  },
  {
    "start": 1830130,
    "end": 1837398,
    "text": "一般的なライブラリllama indexとの統合により、数行のコードで検索拡張生成パイプラインを構築することができる。"
  },
  {
    "start": 1837484,
    "end": 1840710,
    "text": "例えば、使いたいモデルを選択する。"
  },
  {
    "start": 1840780,
    "end": 1842442,
    "text": "私たちの場合はラマ2だ。"
  },
  {
    "start": 1842496,
    "end": 1847174,
    "text": "モデルが回答を検索するために使用できるカスタム文書のセットを定義します。"
  },
  {
    "start": 1847222,
    "end": 1850710,
    "text": "埋め込みを生成するために使いたいモデルを定義する。"
  },
  {
    "start": 1850870,
    "end": 1854442,
    "text": "例えば、オレオという名前の人を知っていますか？"
  },
  {
    "start": 1854506,
    "end": 1854782,
    "text": "それは？"
  },
  {
    "start": 1854836,
    "end": 1855402,
    "text": "どうだ。"
  },
  {
    "start": 1855466,
    "end": 1862346,
    "text": "検索拡張世代のパワーのおかげで、このモデルはチャンネルのマスコット、オレオについての情報を検索できるようになった。"
  },
  {
    "start": 1862538,
    "end": 1867002,
    "text": "Gradientは、検索増強生成パイプラインのあらゆる面を構築するのに役立つ。"
  },
  {
    "start": 1867066,
    "end": 1872094,
    "text": "例えば、埋め込みデータを生成するだけでなく、カスタムデータでモデルを微調整することもできる。"
  },
  {
    "start": 1872222,
    "end": 1877102,
    "text": "gradientを使えば、微調整されたモデルの重みだけでなく、データの完全なオーナーシップを持つことができる。"
  },
  {
    "start": 1877166,
    "end": 1885842,
    "text": "オープンソースのモデルは、そのアーキテクチャにアクセスでき、膨大な開発者コミュニティのサポートが受けられるため、開発やデバッグの時間を節約できるという点で優れている。"
  },
  {
    "start": 1885906,
    "end": 1890610,
    "text": "Gradientは、人気のあるライブラリLlama indexやLangchainとも統合されている。"
  },
  {
    "start": 1890770,
    "end": 1893734,
    "text": "説明文のリンクをチェックして、5ドルクーポンをご利用ください。"
  },
  {
    "start": 1893782,
    "end": 1904158,
    "text": "今日から勾配を使い始めるために、ベクトル・データベースとは何か、そしてそのマッチング・アルゴリズムがどのように機能するかについて説明しよう。"
  },
  {
    "start": 1904244,
    "end": 1909710,
    "text": "ベクター・データベースは、どのようにして保存されているすべてのベクターからクエリーを検索できるのでしょうか？"
  },
  {
    "start": 1910530,
    "end": 1928898,
    "text": "さて、ベクトルデータベースは、埋め込みと呼ばれる一定の次元のベクトルを保存しており、データベースに問い合わせることで、コサイン類似度やユークリッド距離といった距離メトリックを使って、問い合わせに最も近い、あるいは類似している埋め込みをすべて見つけることができる。"
  },
  {
    "start": 1929074,
    "end": 1938226,
    "text": "ベクトルデータベースは、ニーエストネイバーアルゴリズムの略であるKNNアルゴリズムの変種、または他の類似性検索アルゴリズムを使用する。"
  },
  {
    "start": 1938258,
    "end": 1941622,
    "text": "通常、これはKNNアルゴリズムの一種である。"
  },
  {
    "start": 1941766,
    "end": 1949142,
    "text": "ベクトル・データベースは、検索拡張生成パイプラインで使用されるだけでなく、例えば似たような曲を見つけるためにも使用される。"
  },
  {
    "start": 1949206,
    "end": 1952254,
    "text": "曲があれば、そのエンベッディングを作ることができる。"
  },
  {
    "start": 1952292,
    "end": 1959870,
    "text": "その曲に関するすべての情報をとらえたベクトルを埋め込むことで、似たような曲を見つけることができる。"
  },
  {
    "start": 1960850,
    "end": 1964990,
    "text": "例えば、ある曲に似た曲をすべて探したいユーザーがいるとする。"
  },
  {
    "start": 1965060,
    "end": 1968066,
    "text": "私たちは、その曲と他のすべての曲のエンベッディングを作成します。"
  },
  {
    "start": 1968168,
    "end": 1973602,
    "text": "ある類似性スコア、たとえばコサイン類似性スコアなどを使って比較する。"
  },
  {
    "start": 1973656,
    "end": 1977586,
    "text": "グーグル画像も同様の手法で類似画像を検索している。"
  },
  {
    "start": 1977618,
    "end": 1987494,
    "text": "埋め込み空間を使って、特定の画像と他のすべての画像の埋め込みを生成し、最もよく一致するものをチェックする。"
  },
  {
    "start": 1987692,
    "end": 1990650,
    "text": "製品についても同じことができる。"
  },
  {
    "start": 1991470,
    "end": 2008282,
    "text": "knは、データベースに保存されているすべてのベクトルと特定のクエリーを比較し、距離順または類似度順にソートし、ベストマッチの上位を保持するアルゴリズムである。"
  },
  {
    "start": 2008426,
    "end": 2010522,
    "text": "例えば、あるクエリーがあるとする。"
  },
  {
    "start": 2010586,
    "end": 2013114,
    "text": "Grokにはいくつのパラメータがありますか？"
  },
  {
    "start": 2013162,
    "end": 2021442,
    "text": "100万個の埋め込みからなるデータベース、ベクトル・データベースがあるとしよう。"
  },
  {
    "start": 2021576,
    "end": 2033270,
    "text": "というのも、例えばGrokが毎日リアルタイムでツイートにアクセスしているとすると、何百万とは言わないまでも、何千、何十万というツイートがあると思う。"
  },
  {
    "start": 2033610,
    "end": 2040738,
    "text": "実際、ベクターの数は数十億、いや、数百万にもならないだろう。"
  },
  {
    "start": 2040834,
    "end": 2046202,
    "text": "実際、数百万という数字は大きいように見えるが、テキストデータを扱う場合はそうでもない。"
  },
  {
    "start": 2046256,
    "end": 2050598,
    "text": "また、ウェブからは、何十億というウェブページをインデックス化する必要があるかもしれない。"
  },
  {
    "start": 2050774,
    "end": 2072370,
    "text": "例えば、このknでは、ナイーブアプローチで、クエリを他のすべてのベクトルにマッチさせる最も単純な方法です。"
  },
  {
    "start": 2072870,
    "end": 2079214,
    "text": "そしてコサイン類似度スコアでソートする。"
  },
  {
    "start": 2079262,
    "end": 2086066,
    "text": "だから、例えば、一番高いのが、例えば、これが1番で、次にこれが2番で、エトセトラ、エトセトラ。"
  },
  {
    "start": 2086098,
    "end": 2091180,
    "text": "つまり、kの選び方次第では、上位3人、あるいは上位2人ということになる。"
  },
  {
    "start": 2091710,
    "end": 2101894,
    "text": "というのも、埋め込みベクトルがn個（この場合は100万個）あり、それぞれがd次元だからだ。"
  },
  {
    "start": 2101942,
    "end": 2106734,
    "text": "この場合、例えばバースベースの場合は768だ。"
  },
  {
    "start": 2106932,
    "end": 2113690,
    "text": "計算量はnにdを掛けたオーダーで、非常に遅い。"
  },
  {
    "start": 2113850,
    "end": 2116478,
    "text": "もっといいアプローチがないか考えてみよう。"
  },
  {
    "start": 2116654,
    "end": 2127160,
    "text": "ここでは特に、最もポピュラーなベクトルDBでも使われている、階層的ナビゲーショナル・スモールワードと呼ばれるアルゴリズムについて説明する。"
  },
  {
    "start": 2128650,
    "end": 2133618,
    "text": "今、私たちが考えているのは、正確さとスピードを引き換えにするということだ。"
  },
  {
    "start": 2133714,
    "end": 2147238,
    "text": "前に見たアルゴリズム、つまりナイーブなknnの前に、これは非常にゆっくりと実行されますが、クエリがそれぞれのベクトルと比較されるため、実際には正確です。"
  },
  {
    "start": 2147414,
    "end": 2160238,
    "text": "を使えば、可能な限りの比較を行っているため、常に正確な結果を得ることができる。"
  },
  {
    "start": 2160324,
    "end": 2165374,
    "text": "類似性検索で通常気にする指標は想起である。"
  },
  {
    "start": 2165502,
    "end": 2186086,
    "text": "リコールは基本的に、ベスト・マッチング・ベクトルが例えばこれとこれとこれだと仮定した場合、上位3つのクエリをKNNが検索することを示している。"
  },
  {
    "start": 2186188,
    "end": 2188102,
    "text": "が2つしか返されないことを想像してほしい。"
  },
  {
    "start": 2188156,
    "end": 2194326,
    "text": "この場合、クエリーはベスト・マッチング・ベクトルのうち2つだけを返したことになる。"
  },
  {
    "start": 2194358,
    "end": 2199046,
    "text": "リコール率は66％、つまり3分の2である。"
  },
  {
    "start": 2199238,
    "end": 2208654,
    "text": "基本的に、リコールは、検索から検索されるはずだったすべての関連項目のうち、どれだけの関連項目が検索されたかを測定する。"
  },
  {
    "start": 2208852,
    "end": 2217380,
    "text": "ここでは、特に近似最近傍のアルゴリズムとして、階層的ナビゲート可能な小単語と呼ばれるものを紹介する。"
  },
  {
    "start": 2218070,
    "end": 2224798,
    "text": "現在、ベクトル・データベースでは、階層的にナビゲート可能な小単語が実際に使われており、非常に人気がある。"
  },
  {
    "start": 2224894,
    "end": 2234114,
    "text": "特に、TwitterのGrok LLMで使用されているオープンソースのベクトル・データベースであるquadrantデータベースを動かしているのも同じアルゴリズムだ。"
  },
  {
    "start": 2234162,
    "end": 2252134,
    "text": "例えば、これは先日見たイーロン・マスクとクワドラントのチームとのツイートのやり取りだが、クワドラントは、実はGrokはクワドラントであるベクター・データベースを使ってリアルタイムですべてのツイートにアクセスしていると言っている。"
  },
  {
    "start": 2252262,
    "end": 2262026,
    "text": "このベクター・データベースのドキュメントをチェックすると、現在、象限儀は階層的にナビゲート可能なスモールワードだけをベクターのインデックスとして使っていることがわかる。"
  },
  {
    "start": 2262058,
    "end": 2272514,
    "text": "これは、現在Twitterがその大規模な言語モデルに検索拡張世代を導入するために使用しているデータベースを強化するアルゴリズムである。"
  },
  {
    "start": 2272552,
    "end": 2283042,
    "text": "この階層的にナビゲート可能なスモールワードの背後にある最初のアイデアは、6段階の進化というアイデアである。"
  },
  {
    "start": 2283106,
    "end": 2297542,
    "text": "実は、この階層型ナビゲーシブル・スモールワードは、ナビゲーシブル・スモールワードと呼ばれる以前のアルゴリズムを進化させたもので、後述する近似最近傍のアルゴリズムである。"
  },
  {
    "start": 2297686,
    "end": 2307802,
    "text": "1960年代、アメリカでミルグラム実験と呼ばれる、人々の社会的つながりを検証しようとした実験があった。"
  },
  {
    "start": 2307946,
    "end": 2315490,
    "text": "当初ネブラスカ州とカンザス州にいた参加者は、見知らぬ人に手紙を届けることになった。"
  },
  {
    "start": 2315560,
    "end": 2318942,
    "text": "彼らは知らなかったし、その人物はボストンにいた。"
  },
  {
    "start": 2319086,
    "end": 2323406,
    "text": "しかし、手紙を直接相手に送ることは許されなかった。"
  },
  {
    "start": 2323518,
    "end": 2331000,
    "text": "その代わり、その対象者を最もよく知る人物に送るよう指示された。"
  },
  {
    "start": 2331770,
    "end": 2346294,
    "text": "ミルグラムの言葉の実験の最後に、彼らは手紙が5、6段階で最終的な受取人に届くことを発見し、世界中の人々が6度の隔たりでつながっているという概念を作り出した。"
  },
  {
    "start": 2346422,
    "end": 2359306,
    "text": "実際、2016年にフェイスブックは、15億9000万人のフェイスブックのアクティブユーザーが平均3.5度の隔たりでつながっていると主張するブログ記事を発表した。"
  },
  {
    "start": 2359418,
    "end": 2371650,
    "text": "つまり、私とマーク・ザッカーバーグの間には3.5人のコネクションがあり、平均すると、もちろん、私の友人にマーク・スタッケンバーグを知っている友人がいるということになる。"
  },
  {
    "start": 2371990,
    "end": 2375450,
    "text": "これが分離度の考え方である。"
  },
  {
    "start": 2375630,
    "end": 2377014,
    "text": "さて、これは何だろう？"
  },
  {
    "start": 2377052,
    "end": 2390330,
    "text": "ナビゲーシブル・スモールワード・アルゴリズムは、フェイスブックの友達のように、親しいベクトル同士を結びつけながら、つながりの総数を少なくするグラフを構築する。"
  },
  {
    "start": 2390480,
    "end": 2398022,
    "text": "例えば、すべてのベクトルは、6次の分離度を模倣するように、他の6つのベクトルまで接続することができる。"
  },
  {
    "start": 2398166,
    "end": 2411018,
    "text": "例えば、15個のベクトルしかない非常に小さなデータベースがあり、各ベクトルは知識源（文書やウェブページなど）から取得した特定の文章を表しているとする。"
  },
  {
    "start": 2411194,
    "end": 2420786,
    "text": "例えば、最初のテキストはトランスフォーマーに関するもので、そのテキストはトランスフォーマーのモデルに関する別のテキストにつながっている。"
  },
  {
    "start": 2420888,
    "end": 2427266,
    "text": "そして、この2つのツリーをつなぐもう1つの文章がある。"
  },
  {
    "start": 2427298,
    "end": 2432226,
    "text": "AIによる癌の診断、その他もろもろ。"
  },
  {
    "start": 2432338,
    "end": 2437122,
    "text": "さて、この特定のグラフの中で、与えられたクエリをどのように見つけるのだろうか？"
  },
  {
    "start": 2437186,
    "end": 2443882,
    "text": "トランスフォーマーモデルにエンコーダーレイヤーがいくつあるかというクエリーを想像してほしい。"
  },
  {
    "start": 2444016,
    "end": 2447174,
    "text": "アルゴリズムはどのようにk個の最近傍を見つけるのですか？"
  },
  {
    "start": 2447222,
    "end": 2451050,
    "text": "クエリに最もマッチするk個のベクトル。"
  },
  {
    "start": 2451470,
    "end": 2453614,
    "text": "アルゴリズムは次のように進む。"
  },
  {
    "start": 2453652,
    "end": 2457626,
    "text": "まず、ランダムに選ばれたグラフのエントリーポイントを見つける。"
  },
  {
    "start": 2457658,
    "end": 2475246,
    "text": "これらのベクトルの中からランダムに1つのノードをエントリーポイントとして選び、そのノードを訪問し、このクエリとこのノードの類似度スコアを比較し、このノードの友人とクエリの類似度スコアと比較する。"
  },
  {
    "start": 2475278,
    "end": 2479350,
    "text": "ノード番号7とノード番号2。"
  },
  {
    "start": 2479500,
    "end": 2485462,
    "text": "もしどちらかの友人の方が類似度が高ければ、そちらに移動させる。"
  },
  {
    "start": 2485596,
    "end": 2492346,
    "text": "例えば、2番は5番よりもqとの類似度が高いかもしれない。"
  },
  {
    "start": 2492448,
    "end": 2495706,
    "text": "そして2番へ進み、またこのプロセスを繰り返す。"
  },
  {
    "start": 2495808,
    "end": 2507034,
    "text": "3番目のノードとクエリとの間の余弦類似度スコアをチェックし、2番目のベクトルとクエリとの余弦類似度と比較する。"
  },
  {
    "start": 2507162,
    "end": 2526566,
    "text": "もし3番がより良いコサイン類似度スコアを持っていれば、そこに移動し、このノードの友人、つまり8番と6番が現在のノードと比較して、クエリに関してより良いコサイン類似度スコアを持っていないノードに到達するまで、このようなことを続ける。"
  },
  {
    "start": 2526668,
    "end": 2535094,
    "text": "この4番は、10番、8番、6番のすべてのコネクションの中で最高のコサイン類似度スコアを持っている。"
  },
  {
    "start": 2535212,
    "end": 2539654,
    "text": "多くのノードを訪問した。"
  },
  {
    "start": 2539782,
    "end": 2545820,
    "text": "基本的には、ベストマッチングから最低マッチングまで順番に並べ、トップkをキープする。"
  },
  {
    "start": 2546750,
    "end": 2552494,
    "text": "また、異なるランダムなエントリーポイントを選んで、この探索を何度も繰り返す。"
  },
  {
    "start": 2552612,
    "end": 2557866,
    "text": "毎回、すべての結果を類似スコアでソートする。"
  },
  {
    "start": 2557898,
    "end": 2566210,
    "text": "そして、上位k個の単語を保持し、これがナビゲート可能な小単語アルゴリズムを使って最もよくマッチするニーエストネイバーとなる。"
  },
  {
    "start": 2566630,
    "end": 2571650,
    "text": "このグラフにベクトルを挿入したい場合は、前と同じようにすればいい。"
  },
  {
    "start": 2571720,
    "end": 2579046,
    "text": "例えば、このクエリを挿入したい場合、検索と同じように実行します。"
  },
  {
    "start": 2579148,
    "end": 2587990,
    "text": "そしてトップkを見つけたら、クエリーとトップkを結びつけ、このグラフに新しいアイテムを挿入する。"
  },
  {
    "start": 2589150,
    "end": 2597418,
    "text": "階層的なナビゲーションが可能なスモールワードの2つ目のアイデアは、スキップリストと呼ばれる別のデータ構造に基づいている。"
  },
  {
    "start": 2597584,
    "end": 2606154,
    "text": "ナビゲート可能な小単語から階層的なナビゲート可能な小単語へ移行するには、この新しいデータ構造を導入する必要がある。"
  },
  {
    "start": 2606282,
    "end": 2617570,
    "text": "スキップリストは、ソートされたリストを保持するデータ構造であり、平均対数時間の複雑さで検索と挿入を可能にする。"
  },
  {
    "start": 2618150,
    "end": 2623682,
    "text": "例えば、9という数字を検索したい場合、こうすればいい。"
  },
  {
    "start": 2623736,
    "end": 2627938,
    "text": "まず第一に、おわかりのように、これは1つのリンクリストだけではない。"
  },
  {
    "start": 2628104,
    "end": 2630902,
    "text": "リンクリストには多くのレベルがある。"
  },
  {
    "start": 2630956,
    "end": 2634294,
    "text": "レベルゼロ、レベル1、レベル2、レベル3がある。"
  },
  {
    "start": 2634492,
    "end": 2637922,
    "text": "最下層が最もアイテム数が多い。"
  },
  {
    "start": 2637986,
    "end": 2641294,
    "text": "上に行けば行くほど、品数は少なくなる。"
  },
  {
    "start": 2641442,
    "end": 2648346,
    "text": "このリンクリスト内の数列を検索する場合、このスキップリストでは最上位から始める。"
  },
  {
    "start": 2648448,
    "end": 2650682,
    "text": "まずは3番ヘッドから。"
  },
  {
    "start": 2650816,
    "end": 2653334,
    "text": "次の項目は何かをチェックする。"
  },
  {
    "start": 2653462,
    "end": 2655598,
    "text": "と比較する。"
  },
  {
    "start": 2655764,
    "end": 2657710,
    "text": "最初の項目は5番だ。"
  },
  {
    "start": 2657780,
    "end": 2659834,
    "text": "次の項目と比較する。"
  },
  {
    "start": 2659882,
    "end": 2663950,
    "text": "この場合、9番が倒れなければならない。"
  },
  {
    "start": 2664100,
    "end": 2674530,
    "text": "つまり、このノードの後にはありえないということだ。"
  },
  {
    "start": 2674600,
    "end": 2678726,
    "text": "下に降りて、次のノードと比較する。"
  },
  {
    "start": 2678828,
    "end": 2681462,
    "text": "よし、9番だ。"
  },
  {
    "start": 2681596,
    "end": 2683702,
    "text": "さて、別の数字を求めるとしよう。"
  },
  {
    "start": 2683756,
    "end": 2685320,
    "text": "仮に12としよう。"
  },
  {
    "start": 2685690,
    "end": 2688150,
    "text": "また3番から始める。"
  },
  {
    "start": 2688300,
    "end": 2691238,
    "text": "最初の項目に行き、次の項目と比較する。"
  },
  {
    "start": 2691324,
    "end": 2693322,
    "text": "よし、もう終わりだ。"
  },
  {
    "start": 2693456,
    "end": 2695194,
    "text": "そしてここに到着する。"
  },
  {
    "start": 2695232,
    "end": 2696234,
    "text": "次と比較するんだ。"
  },
  {
    "start": 2696272,
    "end": 2698938,
    "text": "17歳だから、12歳より大きい。"
  },
  {
    "start": 2699024,
    "end": 2700378,
    "text": "我々は沈む。"
  },
  {
    "start": 2700544,
    "end": 2702266,
    "text": "それから9人だとわかった。"
  },
  {
    "start": 2702368,
    "end": 2704862,
    "text": "次の項目は9番だ。"
  },
  {
    "start": 2704916,
    "end": 2706320,
    "text": "私たちは9人を訪ねる。"
  },
  {
    "start": 2707330,
    "end": 2711710,
    "text": "次の項目は17なので、12より大きい。"
  },
  {
    "start": 2711780,
    "end": 2718094,
    "text": "そして次の項目、12番と比較する。"
  },
  {
    "start": 2718132,
    "end": 2719954,
    "text": "これが探している番号だ。"
  },
  {
    "start": 2719992,
    "end": 2720994,
    "text": "私たちはそこで止まる。"
  },
  {
    "start": 2721112,
    "end": 2723298,
    "text": "これがスキップリストの仕組みだ。"
  },
  {
    "start": 2723464,
    "end": 2726790,
    "text": "さて、階層的にナビゲート可能なスモールワードを作成する。"
  },
  {
    "start": 2726860,
    "end": 2737074,
    "text": "ナビゲート可能な小単語の概念とスキップリストの概念を組み合わせて、階層的なナビゲート可能な小単語アルゴリズムを作成する。"
  },
  {
    "start": 2737202,
    "end": 2745862,
    "text": "より多くのノードとより多くのコネクションを持つ下位レベルと、より少ないノードとより少ないコネクションを持つ上位レベルがある。"
  },
  {
    "start": 2745926,
    "end": 2749850,
    "text": "こっちの方が密度が高く、こっちの方が疎だと言う。"
  },
  {
    "start": 2750670,
    "end": 2754190,
    "text": "このグラフでの検索はどうなっているのか？"
  },
  {
    "start": 2754690,
    "end": 2758826,
    "text": "先ほどと同じようなクエリがあって、このグラフの中を検索したいとする。"
  },
  {
    "start": 2758938,
    "end": 2765358,
    "text": "このグラフの最上位にランダムなエントリーポイントを見つけ、そこを訪れる。"
  },
  {
    "start": 2765444,
    "end": 2774098,
    "text": "そして、このノードとクエリとの余弦類似度と、彼の友人全員とクエリとの余弦類似度を比較し、このノードがベストであることがわかる。"
  },
  {
    "start": 2774184,
    "end": 2775330,
    "text": "我々は沈む。"
  },
  {
    "start": 2775480,
    "end": 2780262,
    "text": "降りて、もう一度同じテストをする。"
  },
  {
    "start": 2780316,
    "end": 2786034,
    "text": "このノードとクエリとの余弦類似度、およびクエリとのすべての友人の余弦類似度をチェックする。"
  },
  {
    "start": 2786162,
    "end": 2789586,
    "text": "この友人の方がコサイン類似度のスコアが高いことがわかる。"
  },
  {
    "start": 2789618,
    "end": 2790758,
    "text": "私たちはここに移る。"
  },
  {
    "start": 2790924,
    "end": 2795994,
    "text": "そして、このノードを彼の友人全員とチェックし、このノードがベストであることを確認する。"
  },
  {
    "start": 2796032,
    "end": 2798874,
    "text": "私たちはこの試合でも倒れる。"
  },
  {
    "start": 2798912,
    "end": 2803674,
    "text": "コサイン類似度スコアでは、彼の友人たちの中で最高のものであることがわかる。"
  },
  {
    "start": 2803722,
    "end": 2815354,
    "text": "このノードとクエリ、そしてクエリを持つ彼の友人全員のコサイン類似度スコアはいくらかということになる。"
  },
  {
    "start": 2815402,
    "end": 2820690,
    "text": "このノード、このノード、このノードの中で、最適なものがあればそれに移る。"
  },
  {
    "start": 2820760,
    "end": 2823794,
    "text": "地元のベストを見つけたら、すぐにやめる。"
  },
  {
    "start": 2823912,
    "end": 2828360,
    "text": "彼の友人たちよりも悪くない唯一のノードだ。"
  },
  {
    "start": 2829450,
    "end": 2834918,
    "text": "先ほどと同じように、ランダムに選んだエントリーポイントを使ってこの探索を繰り返す。"
  },
  {
    "start": 2835084,
    "end": 2846220,
    "text": "訪問したすべてのベクトルをソートし、使用している類似度スコアまたは使用している距離関数に基づいて、上位k個のベストマッチングを保持する。"
  },
  {
    "start": 2847870,
    "end": 2859358,
    "text": "さて、ファクター・データベースがどのように機能するのかもわかったところで、これまで見てきたことを総括して、検索増強生成のパイプラインをもう一度おさらいしてみよう。"
  },
  {
    "start": 2859444,
    "end": 2861630,
    "text": "またもやクエリである。"
  },
  {
    "start": 2862050,
    "end": 2864886,
    "text": "我々は知識を検索したい文書を持っている。"
  },
  {
    "start": 2864938,
    "end": 2866898,
    "text": "テキストに分割した。"
  },
  {
    "start": 2866984,
    "end": 2869246,
    "text": "センテンス・バードを用いてエンベッディングを作成する。"
  },
  {
    "start": 2869278,
    "end": 2873182,
    "text": "例えば、これらのベクトルをすべてベクトル・データベースに保存する。"
  },
  {
    "start": 2873326,
    "end": 2879894,
    "text": "クエリを埋め込みに変換し、前に見たアルゴリズムを使ってベクトルデータベース内を検索する。"
  },
  {
    "start": 2879932,
    "end": 2882518,
    "text": "階層的にナビゲート可能なスモールワード。"
  },
  {
    "start": 2882684,
    "end": 2887874,
    "text": "これにより、クエリに最もマッチする上位k個の埋め込みが生成され、そこから関連付けが行われる。"
  },
  {
    "start": 2887922,
    "end": 2890780,
    "text": "原文に戻ろう。"
  },
  {
    "start": 2891950,
    "end": 2895942,
    "text": "クエリと取得したコンテキストをテンプレートにまとめる。"
  },
  {
    "start": 2896006,
    "end": 2901100,
    "text": "それを大規模言語モデルに送り、最終的に大規模言語モデルが答えを出す。"
  },
  {
    "start": 2902670,
    "end": 2904382,
    "text": "僕のビデオを見てくれてありがとう。"
  },
  {
    "start": 2904436,
    "end": 2909550,
    "text": "今日、多くのことを学んでもらえたと思う。実は、このビデオを長い間作りたかったんだ。"
  },
  {
    "start": 2909620,
    "end": 2914734,
    "text": "私はまた、このパイプラインの背後にあるすべてのアルゴリズムを自分自身で理解したかった。"
  },
  {
    "start": 2914862,
    "end": 2918450,
    "text": "皆さんも、これらのコンセプトはすべてご理解いただけたと思う。"
  },
  {
    "start": 2919190,
    "end": 2922578,
    "text": "実際にラグ・パイプラインを導入するのはとても簡単だと知っている。"
  },
  {
    "start": 2922664,
    "end": 2933880,
    "text": "ラマ指数や肺連鎖のような有名なライブラリはたくさんあるが、私は実際にどのように機能するのか、それぞれの構成要素が実際にどのように連動しているのか、深く掘り下げてみたかった。"
  },
  {
    "start": 2934650,
    "end": 2937638,
    "text": "分からないことがあったら教えてください。"
  },
  {
    "start": 2937724,
    "end": 2940962,
    "text": "コメント欄の質問にはすべて答えるつもりだ。"
  },
  {
    "start": 2941106,
    "end": 2947062,
    "text": "また、私の今後のビデオでもっと明確にしてほしいことがあれば教えてください。"
  },
  {
    "start": 2947116,
    "end": 2949302,
    "text": "あるいは、どうすれば今後のビデオを改善できるでしょうか？"
  },
  {
    "start": 2949356,
    "end": 2952382,
    "text": "より分かりやすくするために、私のチャンネルを購読してください。"
  },
  {
    "start": 2952436,
    "end": 2956382,
    "text": "これは、質の高いコンテンツを作り続け、ビデオを気に入ってもらうための最高のモチベーションだ。"
  },
  {
    "start": 2956436,
    "end": 2961550,
    "text": "もし気に入ったら、このビデオを友人や教授、学生などにシェアしてください。"
  },
  {
    "start": 2961890,
    "end": 2963100,
    "text": "良い一日を"
  }
]