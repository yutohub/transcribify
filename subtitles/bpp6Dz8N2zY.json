[
  {
    "start": 1400,
    "end": 1920,
    "text": "皆さん、こんにちは。"
  },
  {
    "start": 2006,
    "end": 8850,
    "text": "言語モデルの物理学パート2.1、小学校の算数と隠れた推論プロセスについてのオンライン・トークへようこそ。"
  },
  {
    "start": 9350,
    "end": 17478,
    "text": "私のSML 2024のチュートリアルからここに来られた方は、その講演では、結果のハイレベルな概要を20分ほど説明しただけです。"
  },
  {
    "start": 17614,
    "end": 20850,
    "text": "今日はその詳細を掘り下げてみようと思う。"
  },
  {
    "start": 21950,
    "end": 26006,
    "text": "というわけで、動機は、言語モデルがどのように推論するのかを研究したいということだ。"
  },
  {
    "start": 26078,
    "end": 30406,
    "text": "少なくとも2つの異なる視点から研究することができる。"
  },
  {
    "start": 30598,
    "end": 32102,
    "text": "ひとつは行動プロセスだ。"
  },
  {
    "start": 32166,
    "end": 33814,
    "text": "もうひとつは精神的なプロセスだ。"
  },
  {
    "start": 33982,
    "end": 42502,
    "text": "つまり、言語モデルや行動プロセスを研究するためには、タスクを設計し、言語モデルが何を言うかを観察することが想像できる。"
  },
  {
    "start": 42566,
    "end": 50730,
    "text": "モデルの正しさ、推定解の長さ、解のタイプなどを見積もることができる。"
  },
  {
    "start": 52270,
    "end": 56646,
    "text": "一方、言語モデルの精神的プロセスを研究することもできる。"
  },
  {
    "start": 56798,
    "end": 66246,
    "text": "つまり、言語モデルの内部状態を見て、彼らの思考がどのようなものなのか、彼らが実際にどのように推論しているのかを発見するのだ。"
  },
  {
    "start": 66398,
    "end": 71334,
    "text": "その頭脳の内部で、彼らがどのようなプランニング戦略を用いていたかを知ることができる。"
  },
  {
    "start": 71462,
    "end": 74450,
    "text": "試行錯誤を繰り返した。"
  },
  {
    "start": 75590,
    "end": 83270,
    "text": "実際、人類はほぼ1世紀にわたって行動と精神的プロセスを研究してきた。"
  },
  {
    "start": 83390,
    "end": 85814,
    "text": "言語モデルのためではない。"
  },
  {
    "start": 85982,
    "end": 91414,
    "text": "動物や人間には、例えば動物行動学や行動心理学がある。"
  },
  {
    "start": 91582,
    "end": 98238,
    "text": "一方、脳や認知科学、認知心理学などもある。"
  },
  {
    "start": 98374,
    "end": 110372,
    "text": "この2つの単語、つまり1つの単語で、つまり言語モデルを研究したい、もう1つの単語は動物に似ている、ということをまとめると、少なくとも3つの理由から、言語モデルを研究する方が実は簡単だと私は主張する。"
  },
  {
    "start": 110556,
    "end": 113548,
    "text": "第一に、データを完全にコントロールできる。"
  },
  {
    "start": 113724,
    "end": 120548,
    "text": "つまり、言語モデルが事前に訓練されたデータセットで何を見たかを正確に制御し、データセットなどを微調整することができる。"
  },
  {
    "start": 120644,
    "end": 126116,
    "text": "そして、例えば、分布の一般化からモデルをテストしたりする。"
  },
  {
    "start": 126268,
    "end": 142106,
    "text": "動物の場合、サルが異なる色に対する分布汎化をテストしたい場合、サルが生まれた最初の日から、赤色を見たことがないように閉じ込めておき、最終的に赤色に対する反応をテストすることを想像してほしい。"
  },
  {
    "start": 142258,
    "end": 146426,
    "text": "それはかなり非人道的な行為だ。"
  },
  {
    "start": 146498,
    "end": 152270,
    "text": "だから、湿度の問題がないから、言語モデルを研究するのは簡単なんだ。"
  },
  {
    "start": 153570,
    "end": 155826,
    "text": "もうひとつは、プロービングができることだ。"
  },
  {
    "start": 155898,
    "end": 165282,
    "text": "私たちは実際に言語モデルの内部を見て、さまざまなレイヤー、さまざまなトークンの位置、そして隠れた状態が実際に何を表しているかを見ることができる。"
  },
  {
    "start": 165466,
    "end": 172546,
    "text": "これに対して、動物にこれをやっても、差し込める電気プローブはせいぜい100種類程度だろう。"
  },
  {
    "start": 172738,
    "end": 177790,
    "text": "また、動物を殺す可能性があるため、あまり頻繁には行いたくない。"
  },
  {
    "start": 178250,
    "end": 186890,
    "text": "3つ目は、言語モデルのトレーニングには費用がかかりますが、1週間もあれば非常に優れた言語モデルをトレーニングできるということです。"
  },
  {
    "start": 186970,
    "end": 196338,
    "text": "例えば、1億個のパラメーターしか持たないモデルを訓練するのであれば、1つのGPUで1日以内に訓練できる。"
  },
  {
    "start": 196474,
    "end": 201178,
    "text": "動物の場合、ウサギのように研究すれば、毎年1世代しか成長できない。"
  },
  {
    "start": 201274,
    "end": 203642,
    "text": "サルを研究するのであれば、何年もかかるだろう。"
  },
  {
    "start": 203666,
    "end": 207626,
    "text": "そのため、言語モデルを研究する方が実は簡単なのだ。"
  },
  {
    "start": 207818,
    "end": 223620,
    "text": "一方、私たちが言語モデルの精神的プロセスをあまり研究しないのは、言語モデルがあまりにも多くの異なるタスクをこなし、非常に複雑な理由を実行できるからである。"
  },
  {
    "start": 223730,
    "end": 239220,
    "text": "例えば、チックタックトークとかね。YouTubeでサルがチックタックトークをしているビデオを見たんだけど、スライドを準備している間にね。"
  },
  {
    "start": 240000,
    "end": 249542,
    "text": "従って、言語モデルは非常にハードな勉強ができ、非常にハードな推論ができると思う。"
  },
  {
    "start": 249656,
    "end": 260434,
    "text": "少なくとも、言語モデルがどのように推論するのかを理解するための第一歩を踏み出すためには、そのような非常に複雑なタスクは使わないようにしよう。"
  },
  {
    "start": 260562,
    "end": 269898,
    "text": "例えば、小学生レベルの推論課題で言語モデルがどのように推論するかを研究することである。"
  },
  {
    "start": 270074,
    "end": 279350,
    "text": "小学生レベルの算数に関する小学生レベルの推論といえば、誰もが最初に思い浮かべるデータセットがGSMAKデータセットだろう。"
  },
  {
    "start": 279470,
    "end": 283286,
    "text": "このデータには数学の問題が8000問しかない。"
  },
  {
    "start": 283398,
    "end": 285006,
    "text": "いいベンチマークだよ。"
  },
  {
    "start": 285038,
    "end": 289526,
    "text": "トレーニングしたモデルに適用できる。"
  },
  {
    "start": 289718,
    "end": 296094,
    "text": "たった8000の問題で、とても小さいから楽しめないし、本当に好きじゃない。"
  },
  {
    "start": 296142,
    "end": 302050,
    "text": "通常、モデルが非常に高いスコアを出したとしても、それはデータの汚染によるものかもしれない。"
  },
  {
    "start": 302650,
    "end": 313562,
    "text": "そのため、例えばDRPT4を使って、ここに10個の問題があると言ってデータを補強し、同じような方法で新しい数学の問題を生成しようとする試みがなされてきた。"
  },
  {
    "start": 313666,
    "end": 320130,
    "text": "このように、3ビット4ビットを使って数学の問題を無限に生成することができる。"
  },
  {
    "start": 320290,
    "end": 332340,
    "text": "それでも、生成された問題は非常に偏っているかもしれないし、テンプレートが少なすぎるかもしれない。"
  },
  {
    "start": 333480,
    "end": 338528,
    "text": "そのため、私たちは自分たちのデータセットを使って推論を研究したいと切に願っている。"
  },
  {
    "start": 338664,
    "end": 342304,
    "text": "データをどのように構築するかについては、これから詳しく説明する。"
  },
  {
    "start": 342352,
    "end": 343840,
    "text": "非常に高いレベルで。"
  },
  {
    "start": 343960,
    "end": 358662,
    "text": "例えば、ろうそくが燃えると、その長さは伸びる代わりに縮む。"
  },
  {
    "start": 358766,
    "end": 365142,
    "text": "あるいは、仕事に行けばお金を稼ぎ、タンクを感じれば水位が上がる。"
  },
  {
    "start": 365286,
    "end": 383902,
    "text": "モデルが実際に数学の問題を正しく解くためには、インターネット上のデータから、例えば一般的な群衆のデータセットから、このような常識的な知識を学ばなければならない。"
  },
  {
    "start": 383966,
    "end": 385022,
    "text": "そうしなければならない。"
  },
  {
    "start": 385126,
    "end": 396022,
    "text": "というのも、インターネットのデータを使ってモデルをトレーニングすると、データが汚染される危険性があるからだ。"
  },
  {
    "start": 396086,
    "end": 409238,
    "text": "また、数学の問題を解くのに役立つ常識をすべて学習するためには、本当に数十億、少なくとも70億規模のモデルを訓練する必要があるのではないでしょうか。"
  },
  {
    "start": 409374,
    "end": 411252,
    "text": "それはあまりいいことじゃない。"
  },
  {
    "start": 411406,
    "end": 425104,
    "text": "そのため、私たちは、常識的なセットを削減したデータセットを作ったら、そのデータセットでモデルを直接事前訓練できるような方法で、常識を本当に削除または削減することを提案する。"
  },
  {
    "start": 425192,
    "end": 434304,
    "text": "そして、モデルが見た訓練済みデータセットをコントロールすることで、モデルがどのように推論するかを研究することがより簡単になる。"
  },
  {
    "start": 434472,
    "end": 435940,
    "text": "それが動機だ。"
  },
  {
    "start": 436360,
    "end": 449076,
    "text": "常識的なことは減らしているが、小学生レベルの算数の問題などでよく見かける、少なくともいくつかの重要な側面は捉えておきたい。"
  },
  {
    "start": 449108,
    "end": 457868,
    "text": "例えば、あるパラメータ、例えばaが、別の2つのパラメータの合計の5倍に依存する場合、それを捕捉する直接的依存関係。"
  },
  {
    "start": 457924,
    "end": 460188,
    "text": "このような依存関係を把握したい。"
  },
  {
    "start": 460324,
    "end": 472940,
    "text": "例えば、x個の教室があり、それぞれにy個のバッグがある場合、合計でx倍のy個のバッグがあることをモデルが理解できるようにしたい。"
  },
  {
    "start": 473280,
    "end": 475304,
    "text": "最後に、暗黙の依存関係。"
  },
  {
    "start": 475392,
    "end": 485500,
    "text": "つまり、ボブはアリスより3倍多くの果物を持っていて、アリスは4つのxを持っている場合、xは果物ではない、などと理解できるモデルにしたい。"
  },
  {
    "start": 486080,
    "end": 494432,
    "text": "詳細は省くが、私たちの数学の問題がどのようなものなのか、その一例を直接お見せしよう。"
  },
  {
    "start": 494496,
    "end": 496610,
    "text": "だから、これはIの例だ。"
  },
  {
    "start": 497390,
    "end": 515510,
    "text": "この例では、問題記述にはいくつもの文があり、それぞれの問題記述には、この問題で取り上げられる可能性のあるインスタンス・パラメータを定義する基礎となる構造グラフが実際にある。"
  },
  {
    "start": 515630,
    "end": 521982,
    "text": "例えば、この端はリバービューの高校にある映画スタジオの数を表している。"
  },
  {
    "start": 522086,
    "end": 526102,
    "text": "これは、何らかの値を割り当てることができるサブパラメーターである。"
  },
  {
    "start": 526206,
    "end": 528166,
    "text": "これがインスタンス・パラメーターと呼ばれるものだ。"
  },
  {
    "start": 528238,
    "end": 533446,
    "text": "構造グラフの各辺はインスタンス・パラメータを表す。"
  },
  {
    "start": 533598,
    "end": 540302,
    "text": "中央高校内のバッグの数といった抽象的なパラメーターもある。"
  },
  {
    "start": 540446,
    "end": 543166,
    "text": "この構造グラフの最初の行は高校である。"
  },
  {
    "start": 543198,
    "end": 545190,
    "text": "列目は教室だ。"
  },
  {
    "start": 545230,
    "end": 548768,
    "text": "3列目はバッグを捉える。"
  },
  {
    "start": 548814,
    "end": 557332,
    "text": "従って、中央高校にバックパックが何個あるかと言われれば、これは質問された数量である。"
  },
  {
    "start": 557476,
    "end": 559892,
    "text": "これが抽象パラメータと呼ばれるものだ。"
  },
  {
    "start": 559996,
    "end": 570040,
    "text": "つまり、抽象的なパラメータは、その値が問題の基礎となる構造グラフによって暗黙のうちに定義されているため、値を割り当てることができない。"
  },
  {
    "start": 570420,
    "end": 580358,
    "text": "今、問題文の各センテンスは、基本的に1つの値を割り当てている。"
  },
  {
    "start": 580414,
    "end": 584590,
    "text": "例えば、リバービューズ高校のフィルムスタジオ。"
  },
  {
    "start": 584670,
    "end": 586598,
    "text": "これはインスタンス・パラメーターである。"
  },
  {
    "start": 586734,
    "end": 592806,
    "text": "最初の文では、このインスタンス・パラメーターはこの2つの合計の5倍である、とだけ書いてある。"
  },
  {
    "start": 592918,
    "end": 602530,
    "text": "従って、依存関係グラフを描けば、それはDAC、つまり有向無サイクルグラフであり、基本的にどのパラメータが、どのパラメータに依存しているかを把握する。"
  },
  {
    "start": 603410,
    "end": 608630,
    "text": "では、実際に解決策の例をお見せしよう。"
  },
  {
    "start": 608970,
    "end": 613802,
    "text": "どのように解決策を構築するかについては、非常に慎重にあなたと一緒に考えている。"
  },
  {
    "start": 613866,
    "end": 621030,
    "text": "その時点で、私たちの問題がどうなっているのか、私たちの問題が何を意味するのか、そういったことが理解できると思う。"
  },
  {
    "start": 622570,
    "end": 630068,
    "text": "そもそもこの問題は、中央高校に何個のバックパックがあるのか、ということですよね？"
  },
  {
    "start": 630154,
    "end": 634896,
    "text": "この抽象的なパラメーターを計算する必要がある。"
  },
  {
    "start": 634968,
    "end": 639776,
    "text": "前のスライドで述べたように、これはこれら2つのパラメーターの合計に等しい。"
  },
  {
    "start": 639848,
    "end": 645420,
    "text": "したがって、質問に答える前に、この2つのパラメータを計算しなければならない。"
  },
  {
    "start": 645800,
    "end": 650568,
    "text": "例えば、このパラメーターの場合は、これとこれの合計に等しい。"
  },
  {
    "start": 650704,
    "end": 660844,
    "text": "つまり、撮影所内にあるバックパックの数は、通学用パックの数とメッセンジャーバッグの数に等しい。"
  },
  {
    "start": 660972,
    "end": 667040,
    "text": "したがって、モデルはまずコンピュータに答える必要がある。"
  },
  {
    "start": 667500,
    "end": 674340,
    "text": "さて、問題文によると、これはこれとこれの和と定義されている。"
  },
  {
    "start": 674380,
    "end": 677740,
    "text": "したがって、まず黄色の2つを計算しなければならない。"
  },
  {
    "start": 677900,
    "end": 685356,
    "text": "例えば、問題文に「この部分のパラメータは、他の2つの事柄に依存している」と書かれていることがある。"
  },
  {
    "start": 685508,
    "end": 689980,
    "text": "を計算する前に、グレーの2つを計算する必要がある。"
  },
  {
    "start": 690100,
    "end": 693236,
    "text": "最後に、グレーの2つにはすでに値が与えられている。"
  },
  {
    "start": 693268,
    "end": 694604,
    "text": "問題提起の中で"
  },
  {
    "start": 694732,
    "end": 697720,
    "text": "一人は17歳、もう一人は13歳だ。"
  },
  {
    "start": 698180,
    "end": 699268,
    "text": "これで終わりだ。"
  },
  {
    "start": 699364,
    "end": 717330,
    "text": "基本的には、正しい順序で計算を積み重ねていけば、最終的に中央高校内のバックパックの数を計算することができ、この質問に答えることができる。"
  },
  {
    "start": 718630,
    "end": 726570,
    "text": "その理由は、推論だけに焦点を当てたいので、算数の難易度を下げたいからである。"
  },
  {
    "start": 726910,
    "end": 729630,
    "text": "この点については、また後ほど。"
  },
  {
    "start": 729750,
    "end": 739570,
    "text": "さて、このようにして、数学的な問題がどのように構成され、その解がどのように見えるかの一例をお見せした。"
  },
  {
    "start": 740310,
    "end": 742934,
    "text": "データセットについて、いくつかの特性を挙げておこう。"
  },
  {
    "start": 743022,
    "end": 758950,
    "text": "その第一は、私たちはこの6つの単語だけを見るのではなく、約1600のパラメータ名を含む独自の語彙を構築し、それを4つの異なるカテゴリーに分類しているということです。"
  },
  {
    "start": 759030,
    "end": 760940,
    "text": "各カテゴリーには4つのレイヤーがある。"
  },
  {
    "start": 761070,
    "end": 764576,
    "text": "例えば、このカテゴリーでは、第1層は高校である。"
  },
  {
    "start": 764608,
    "end": 767256,
    "text": "第2層は教室だ。"
  },
  {
    "start": 767288,
    "end": 769024,
    "text": "第3層はバッグだ。"
  },
  {
    "start": 769072,
    "end": 774232,
    "text": "第4のレイヤーは、実はステーショナリーと呼ばれるものだ。"
  },
  {
    "start": 774296,
    "end": 782624,
    "text": "定規や鉛筆、ホッチキスなど、どのバッグにも入れられるものだ。"
  },
  {
    "start": 782792,
    "end": 795074,
    "text": "そして、各問題について、1600の名前から、多くて4層、多くて16のような名前をランダムに選んで構造グラフを構築した。"
  },
  {
    "start": 795202,
    "end": 802910,
    "text": "この構造化グラフは、基本的に、この数学の問題でどのようなオブジェクトが出現しうるかを示している。"
  },
  {
    "start": 803850,
    "end": 807842,
    "text": "各問題について、ランダムな依存関係グラフも作成した。"
  },
  {
    "start": 807946,
    "end": 812872,
    "text": "これは、基本的にどのパラメータが他のどのパラメータに依存しているかを表すタグである。"
  },
  {
    "start": 812986,
    "end": 820180,
    "text": "サイクルがないことを確認し、各ノードの完全性は最大でも4であることを確認した。"
  },
  {
    "start": 820220,
    "end": 826012,
    "text": "ということは、ジェネレーターの乱数を除けば、最大でも4つの異なるものに依存することができる。"
  },
  {
    "start": 826076,
    "end": 831588,
    "text": "もしそれが乱数発生器に依存するようなものなら、数字がある。"
  },
  {
    "start": 831644,
    "end": 834800,
    "text": "例えば、この5という数字はランダムに生成される。"
  },
  {
    "start": 836060,
    "end": 845836,
    "text": "非常に重要なのは、問題文の説明で、最後の問題文以外はすべてランダムにシャッフルするようにしたことだ。"
  },
  {
    "start": 846028,
    "end": 862340,
    "text": "というのも、解答が常に問題文と同じ順序になることは避けたいからだ。同じ順序であれば、モデルがすべきことは、すべての文章を数学の計算に変換するだけになってしまう。"
  },
  {
    "start": 862380,
    "end": 863516,
    "text": "それは簡単すぎる。"
  },
  {
    "start": 863628,
    "end": 876464,
    "text": "したがって、モデルがこの問題を解くことができるのであれば、ただ指示に従うのではなく、最初に何を計算するかを決める必要があることを確認するために、問題の文章をランダムにシャッフルしたようなものだ。"
  },
  {
    "start": 876632,
    "end": 879928,
    "text": "これは我々のデータの重要な側面だ。"
  },
  {
    "start": 879984,
    "end": 882552,
    "text": "つまり、ここではランダムに文章をシャッフルする。"
  },
  {
    "start": 882696,
    "end": 893336,
    "text": "このデータのもうひとつの重要な点は、この問題を解くために必要な操作の回数をコントロールするopというパラメーターを持っていることだ。"
  },
  {
    "start": 893488,
    "end": 904206,
    "text": "例えば、ここには6つのステップがあり、そのうちの1つには2つの演算が含まれている。"
  },
  {
    "start": 904318,
    "end": 907454,
    "text": "したがって、この問題ではopは7に等しい。"
  },
  {
    "start": 907582,
    "end": 911690,
    "text": "問題の難易度をコントロールするために、このパラメーターがある。"
  },
  {
    "start": 913430,
    "end": 922410,
    "text": "これをオリジナルのGSmAkデータセットと比較すると、一方では推論の難易度を大幅に高めたことがわかる。"
  },
  {
    "start": 922790,
    "end": 933164,
    "text": "例えば、ここではオプデンヌを使った非常に簡単な例しか見ていないが、もしオプが21になることを許せば、少なくとも人間にとっては非常に些細な問題ではなくなる。"
  },
  {
    "start": 933212,
    "end": 938080,
    "text": "この問題を解くには、5分か10分とスクラッチペーパーが必要だろう。"
  },
  {
    "start": 938540,
    "end": 944760,
    "text": "そこで、推理の難易度を大幅に高め、GSmakよりもさらに難しくした。"
  },
  {
    "start": 945060,
    "end": 950084,
    "text": "その一方で、先ほど述べたように、我々はデータから常識を取り除いている。"
  },
  {
    "start": 950132,
    "end": 954960,
    "text": "gsmakを解決するよりも、常識が非常に重要であることは認める。"
  },
  {
    "start": 955080,
    "end": 961960,
    "text": "ろうそくが燃えたらその長さが短くなるとか、誰かのために働いたらお金がもらえるとか、そういうことを知る必要がある。"
  },
  {
    "start": 962120,
    "end": 966720,
    "text": "また、管理された実験をするのはずっと難しくなると思う。"
  },
  {
    "start": 966760,
    "end": 976904,
    "text": "世界中の常識をすべてカバーしようと思ったら、おそらく非常に大きなモデルと、それを実現するためのインターネットデータなどに対する非常に長い学習時間が必要になる。"
  },
  {
    "start": 976992,
    "end": 982620,
    "text": "そのため、データセットを自己完結させるために、意図的に常識を取り除いた。"
  },
  {
    "start": 982740,
    "end": 991160,
    "text": "インターネット上のデータも、インターネット上の知識も使わずに、ランダムな初期化からこのデータで直接訓練することができる。"
  },
  {
    "start": 992260,
    "end": 998660,
    "text": "3つ目の大きな違いは、問題の算数の難易度を下げたことだ。"
  },
  {
    "start": 998820,
    "end": 1004960,
    "text": "例えば、3つの数字を足し合わせる必要があるときは、2進数の演算に分解する。"
  },
  {
    "start": 1005260,
    "end": 1008062,
    "text": "これで私のような計算は簡単になる。"
  },
  {
    "start": 1008246,
    "end": 1012318,
    "text": "また、モジュラー23算法しか使わなかったようなものだ。"
  },
  {
    "start": 1012454,
    "end": 1014702,
    "text": "それには2つの理由がある。"
  },
  {
    "start": 1014766,
    "end": 1024326,
    "text": "もしモデルがこのタスクで失敗した場合、それが演算によるものでないことを確認するためだ。"
  },
  {
    "start": 1024438,
    "end": 1031606,
    "text": "だって結局のところ、23×23の掛け算表を覚えるのだって、とても小さなモデルにとっては些細なことなんだから。"
  },
  {
    "start": 1031758,
    "end": 1036142,
    "text": "理屈と算数を分けて考えたい。"
  },
  {
    "start": 1036326,
    "end": 1046214,
    "text": "今日の話の場合、23を例えば2003と置き換えてもいい。"
  },
  {
    "start": 1046302,
    "end": 1055302,
    "text": "しかし、極端な数字にするために小さい数字を選び、計算が本当に簡単であることを確認する。"
  },
  {
    "start": 1055326,
    "end": 1057330,
    "text": "推理の部分だけが難しい。"
  },
  {
    "start": 1058390,
    "end": 1070006,
    "text": "第二の理由は、結局のところ、私たちは計算機を持っているので、言語モデルに難しい計算をさせたくないからです。"
  },
  {
    "start": 1070118,
    "end": 1083054,
    "text": "例えば、言語モデルのサイズを2倍にすることで、30桁×30桁の掛け算のようなものをすべて解くことができるようになるのだとしたら、そんなことをする価値があるのだろうか？"
  },
  {
    "start": 1083142,
    "end": 1088680,
    "text": "電卓を呼び出すことで、パラメーターの数を半分に減らすことができるとか？"
  },
  {
    "start": 1088790,
    "end": 1090732,
    "text": "その方がいい選択かもしれない。"
  },
  {
    "start": 1090796,
    "end": 1093900,
    "text": "したがって、私たちは算数に焦点を当てたくない。"
  },
  {
    "start": 1094020,
    "end": 1097320,
    "text": "このデータセットでは、推論だけに注目したい。"
  },
  {
    "start": 1098820,
    "end": 1106292,
    "text": "このデータセットを使って最初にやったことは、GPT-2でこのデータセットに言語モデルを描き出すことだ。"
  },
  {
    "start": 1106436,
    "end": 1112452,
    "text": "本講演を通して、GPT-2と言うときはいつでも、回転埋め込みを持つGPT-2を意味する。"
  },
  {
    "start": 1112636,
    "end": 1113244,
    "text": "いいかい？"
  },
  {
    "start": 1113332,
    "end": 1120490,
    "text": "GPT-2スモールと呼ばれるものです。"
  },
  {
    "start": 1120870,
    "end": 1127410,
    "text": "このデータでどのようなパフォーマンスを示したかを観察してみると、すでにかなり驚くべき精度であることがわかる。"
  },
  {
    "start": 1128390,
    "end": 1142776,
    "text": "対照的に、GPT4.0にこのデータを直接適用し、このデータを解決するために数発学習させると、かなり失敗することも、今日の講演の後半でお見せします。"
  },
  {
    "start": 1142808,
    "end": 1144936,
    "text": "opのように、11より大きい。"
  },
  {
    "start": 1145048,
    "end": 1153824,
    "text": "したがって、このデータは些細なものではなく、1億の大きさのパラメータはすでに非常に良い精度で解くことができる。"
  },
  {
    "start": 1153952,
    "end": 1157260,
    "text": "実は、ここでいくつかコメントしたいことがある。"
  },
  {
    "start": 1157600,
    "end": 1162192,
    "text": "その1は、これらの精度は答えの精度ではないということだ。"
  },
  {
    "start": 1162296,
    "end": 1163880,
    "text": "モジュール式ではなく、10対3の精度だ。"
  },
  {
    "start": 1163920,
    "end": 1166336,
    "text": "我々は数字だけを直接比較しているわけではない。"
  },
  {
    "start": 1166368,
    "end": 1173004,
    "text": "私たちは実際に、出力の一文一文を正確に解析するパーサーを書いたんだ。"
  },
  {
    "start": 1173052,
    "end": 1180316,
    "text": "例えば99％とか87％というのは、パーサーの精度のことだ。"
  },
  {
    "start": 1180428,
    "end": 1189320,
    "text": "つまり、最終的な答えも含めて、すべての文章、すべての解答ステップが正しいのだ。"
  },
  {
    "start": 1189820,
    "end": 1196258,
    "text": "つ目のコメントは、トレーニングセットとテストセットが完全に破壊されていることを確認したということです。"
  },
  {
    "start": 1196354,
    "end": 1204410,
    "text": "特に、たとえばテストでは、事前トレーニングのどの問題よりもはるかに難しい問題でモデルをテストしました。"
  },
  {
    "start": 1204450,
    "end": 1209698,
    "text": "事前訓練では、例えば、中程度のデータセットでは、事前訓練データはopが15に等しくなる程度である。"
  },
  {
    "start": 1209834,
    "end": 1214930,
    "text": "テストでは、オペでも23番までの精度を調べます。"
  },
  {
    "start": 1215050,
    "end": 1219282,
    "text": "トレーニング中のハードデータについては、21で制限されている。"
  },
  {
    "start": 1219306,
    "end": 1223170,
    "text": "テストでは、32でもモデルをチェックする。"
  },
  {
    "start": 1223250,
    "end": 1226294,
    "text": "したがって、この推理の長さを一般化と呼ぶ。"
  },
  {
    "start": 1226402,
    "end": 1241350,
    "text": "つまり、GPT-2モデルは、ある難易度のデータで学習し、より難しい推論やより複雑な推論ステップを必要とする問題に汎化する能力を持っていることがよくわかる。"
  },
  {
    "start": 1241470,
    "end": 1248070,
    "text": "これはトークンの長さの一般化とは大きく異なることを指摘しておきたい。"
  },
  {
    "start": 1248230,
    "end": 1264728,
    "text": "最近では、例えばマンバやロープについて、特にこれらのモデルがどれだけ長さを汎化できるか、より長い長さの問題に対してトークンの長さを汎化できるか、といった文脈で語られることが多い。"
  },
  {
    "start": 1264744,
    "end": 1275128,
    "text": "選択的コピーや算数のように、ある長さでモデルを訓練し、より長いコンテクストウィンドウでそれをテストする。"
  },
  {
    "start": 1275224,
    "end": 1278016,
    "text": "実は、ここで話しているのは別のことなんだ。"
  },
  {
    "start": 1278048,
    "end": 1286616,
    "text": "つまり、このテストセットでは、トレーニングデータと比較して、テスト問題がほぼ同じ長さであることまで確認している。"
  },
  {
    "start": 1286728,
    "end": 1292200,
    "text": "例えば、このデータでは、124のコンテキストレンズウィンドウでトレーニングしているようなものだ。"
  },
  {
    "start": 1292240,
    "end": 1299420,
    "text": "そしてテスト中、すべての問題が124トークン以内でも解決できることを確認した。"
  },
  {
    "start": 1300120,
    "end": 1303408,
    "text": "それでも精度が落ちるのは目に見えている。"
  },
  {
    "start": 1303464,
    "end": 1309176,
    "text": "つまり、流通に乗れば精度は99だが、流通から外れれば精度は落ちるという感じだ。"
  },
  {
    "start": 1309328,
    "end": 1313776,
    "text": "この減少は、推論長生成の一般化のためである。"
  },
  {
    "start": 1313848,
    "end": 1316216,
    "text": "トークンの長さの一般化のせいではない。"
  },
  {
    "start": 1316368,
    "end": 1320992,
    "text": "これは、我々の論文などを見ればわかる微妙なところだ。"
  },
  {
    "start": 1321096,
    "end": 1331940,
    "text": "結局のところ、私たちはこのモデルが、より難しい問題を解くために、分布外の一般化が可能であることを見てきた。"
  },
  {
    "start": 1333120,
    "end": 1340436,
    "text": "ここで最後に申し上げたいのは、私たちは実際にデータセットの大きさをチェックし、推定もしているということです。"
  },
  {
    "start": 1340608,
    "end": 1344508,
    "text": "これは解決策の一例のようなものだ。"
  },
  {
    "start": 1344564,
    "end": 1355244,
    "text": "私たちは実際に、それぞれの解答について、英語を削除し、数字を削除し、変数がa、b、cの順になるようにすることで、解答テンプレートに変換します。"
  },
  {
    "start": 1355372,
    "end": 1361596,
    "text": "この移行後、データ上の問題をカウントした。"
  },
  {
    "start": 1361708,
    "end": 1365188,
    "text": "では、彼らはいくつのソリューション・テンプレートを共有しているのだろうか？"
  },
  {
    "start": 1365284,
    "end": 1369498,
    "text": "そのため、例えば、同じソリューション・テンプレートを使用した複数の問題が存在する可能性がある。"
  },
  {
    "start": 1369644,
    "end": 1377206,
    "text": "ということは、このデータセットにはまだ数十億、少なくとも数兆の解のテンプレートがあることになる。"
  },
  {
    "start": 1377318,
    "end": 1381410,
    "text": "データがいかに豊富かを感じていただければ幸いだ。"
  },
  {
    "start": 1381790,
    "end": 1386430,
    "text": "基本的には、グラフの同型性テストなどのようなものだ。"
  },
  {
    "start": 1386470,
    "end": 1389134,
    "text": "私たちは、このデータがかなり巨大なものだと推測している。"
  },
  {
    "start": 1389262,
    "end": 1395654,
    "text": "この程度であれば、私たちはそれが言語になるだけだと主張する。"
  },
  {
    "start": 1395702,
    "end": 1411766,
    "text": "一方では、暗記できない問題が多すぎるし、他方では、このような難易度の長さの問題を見たことがなくても、解くことができるのだから。"
  },
  {
    "start": 1411838,
    "end": 1422558,
    "text": "したがって、この最初の結果を見ただけで、言語モデルが実際に問題を解決するスキルを学習するという驚くべき能力を持つことがわかる。"
  },
  {
    "start": 1422614,
    "end": 1427448,
    "text": "問題を暗記するのではなく、実際にアルゴリズムを学んでいるのだ。"
  },
  {
    "start": 1427584,
    "end": 1436940,
    "text": "さて、残りのトークでは、彼らがどのようなスキルを身につけ、どのようにこの問題を解決したのかを実際にお見せしよう。"
  },
  {
    "start": 1437920,
    "end": 1448280,
    "text": "このデータで学習したGPT-2は、私たちがレベル1と呼ぶ推論能力を獲得している。"
  },
  {
    "start": 1448320,
    "end": 1449592,
    "text": "これを定義してみよう。"
  },
  {
    "start": 1449776,
    "end": 1458936,
    "text": "まず、レベルゼロのリーディング・スキルを想像してみよう。"
  },
  {
    "start": 1458968,
    "end": 1467464,
    "text": "例えば、問題に登場したすべてのパラメーターを、登場した順番に調べていくようなイメージだ。"
  },
  {
    "start": 1467632,
    "end": 1474544,
    "text": "そうすれば、この順番ですべてを調べ、それぞれを最大に計算しようとすることができる。"
  },
  {
    "start": 1474592,
    "end": 1481732,
    "text": "例えば、最初のパラメーターについては、すでに問題を定義していることがわかったかもしれない。"
  },
  {
    "start": 1481856,
    "end": 1482700,
    "text": "これで13人目だ。"
  },
  {
    "start": 1482820,
    "end": 1488900,
    "text": "ここで2番目のパラメーターを見れば、まだ計算の準備ができていないことがわかるだろう。"
  },
  {
    "start": 1488940,
    "end": 1491684,
    "text": "まだ計算されていない何かに依存しているのかもしれない。"
  },
  {
    "start": 1491772,
    "end": 1492772,
    "text": "スキップする"
  },
  {
    "start": 1492876,
    "end": 1496532,
    "text": "第3パラメータを見れば、すでに準備が整っていることがわかる。"
  },
  {
    "start": 1496636,
    "end": 1498772,
    "text": "4番目の1番は、まだ準備ができていないかもしれない。"
  },
  {
    "start": 1498796,
    "end": 1507052,
    "text": "このループを数回繰り返した後、すでに計算可能なパラメータにぶつかるたびに、それを計算する。"
  },
  {
    "start": 1507156,
    "end": 1511490,
    "text": "これはおそらく、あなたが想像できるひとつの推論スケールだ。"
  },
  {
    "start": 1511530,
    "end": 1514530,
    "text": "それがレベルゼロの推論スケールと呼ばれるものだ。"
  },
  {
    "start": 1514690,
    "end": 1517554,
    "text": "レベル1の推理力なら、もっと賢いことをするだろう。"
  },
  {
    "start": 1517602,
    "end": 1530154,
    "text": "つまり、トポロジックな並べ替えを行い、計算の順序や、特に非常に重要なことだが、問題を止めるのに必要のないパラメータを見つけ出すのだ。"
  },
  {
    "start": 1530282,
    "end": 1533710,
    "text": "となると、結局は最短の解しか生まれない。"
  },
  {
    "start": 1534140,
    "end": 1548996,
    "text": "つまり、データ、モデル、トレーニングに問題を与えるとき、解が常に最短になるようにデータを推論した。"
  },
  {
    "start": 1549068,
    "end": 1551908,
    "text": "不必要なパラメータを計算することはない。"
  },
  {
    "start": 1552044,
    "end": 1553548,
    "text": "そして、モデルを事前にトレーニングする。"
  },
  {
    "start": 1553684,
    "end": 1560502,
    "text": "ここで疑問なのだが、非常に良い精度が得られているが、モデルは実際にレベルゼロまたはレベル1の増加スケールを使用することを学習したのだろうか？"
  },
  {
    "start": 1560556,
    "end": 1563610,
    "text": "では、モデルは不足の解決策も出力できるのか？"
  },
  {
    "start": 1563770,
    "end": 1568842,
    "text": "その結果、このモデルは実にうまく、より短い解を生成できることがわかった。"
  },
  {
    "start": 1568986,
    "end": 1570794,
    "text": "例えば、これがその表だ。"
  },
  {
    "start": 1570842,
    "end": 1580386,
    "text": "論文から、我々は、ほぼすべての100の問題でモデルが好きではない、1つ以上のような不要な計算を生成することがわかります。"
  },
  {
    "start": 1580458,
    "end": 1585226,
    "text": "そのため、モデルが不必要なパラメータを計算することはほとんどない。"
  },
  {
    "start": 1585378,
    "end": 1589906,
    "text": "同じ論文で、私たちはreask分布と呼ぶものも紹介している。"
  },
  {
    "start": 1590018,
    "end": 1596350,
    "text": "そこでの目標は、評価データの分布をトレーニングからできるだけ遠ざけることだ。"
  },
  {
    "start": 1596390,
    "end": 1603718,
    "text": "この例では、モデルが不必要なパラメータを計算し始めたことがわかるが、それでもごくまれである。"
  },
  {
    "start": 1603774,
    "end": 1609726,
    "text": "1つの問題につき平均して1回以下の無駄な計算しかしていないことになる。"
  },
  {
    "start": 1609918,
    "end": 1620462,
    "text": "このことを念頭に置くと、このモデルは実際、より短い解を生成できることがわかる。"
  },
  {
    "start": 1620526,
    "end": 1624374,
    "text": "考えてみれば、これは実につまらない技術なのだ。"
  },
  {
    "start": 1624502,
    "end": 1625590,
    "text": "そんなことが可能なのか？"
  },
  {
    "start": 1625670,
    "end": 1639622,
    "text": "それは特に、モデルが最初の文章を言い始める前に、問題を解決するために必要なことが何かをすでに精神的に処理し、把握する必要があることを意味する。"
  },
  {
    "start": 1639766,
    "end": 1647350,
    "text": "たとえば、すでに定義済みの値が与えられているパラメータがある。"
  },
  {
    "start": 1647390,
    "end": 1649510,
    "text": "これがデッキの葉だ。"
  },
  {
    "start": 1649670,
    "end": 1656490,
    "text": "最終的な質問に答えるために、すべての葉が必要であるとは限らない。"
  },
  {
    "start": 1657190,
    "end": 1670046,
    "text": "したがって、モデルが最初の文章を言い始める前に、何が葉なのかを把握するだけでなく、問題解決に向けて必要な葉も把握する必要がある。"
  },
  {
    "start": 1670238,
    "end": 1671382,
    "text": "これは些細なことではない。"
  },
  {
    "start": 1671486,
    "end": 1673498,
    "text": "このモデルはどのようにしてこれを達成できるのだろうか？"
  },
  {
    "start": 1673694,
    "end": 1677390,
    "text": "それに答えるために、私たちは実際にプロービングを行った。"
  },
  {
    "start": 1678370,
    "end": 1682362,
    "text": "少なくとも以下の3種類のプロービングを行った。"
  },
  {
    "start": 1682506,
    "end": 1690914,
    "text": "その1は、モデルが解を生成し始める前、つまり解のトークンよりも前に理解したいということだ。"
  },
  {
    "start": 1691042,
    "end": 1695794,
    "text": "この時点で、私たちは問題の後を理解したい。"
  },
  {
    "start": 1695922,
    "end": 1702240,
    "text": "では、この質問に答えるためにパラメータaが必要かどうか、モデルはすでに知っているのだろうか？"
  },
  {
    "start": 1702540,
    "end": 1705720,
    "text": "すべてのパラメータについて、それはすでに知っているのか？"
  },
  {
    "start": 1706300,
    "end": 1722340,
    "text": "2番目は、解生成プロセスの途中、たとえば文のペアごとに、次の文の中でパラメータaが計算できるかどうか、モデルは知っているのか、それはすでに計算可能なのか、ということです。"
  },
  {
    "start": 1722420,
    "end": 1727040,
    "text": "つまり、それに依存するすべてのパラメータはすでに計算済みということですか？"
  },
  {
    "start": 1727540,
    "end": 1731940,
    "text": "モデルがこのパラメーターを計算する準備ができていることを知っていることは間違いない。"
  },
  {
    "start": 1731980,
    "end": 1738240,
    "text": "モデルは、次に計算できるパラメータの全セットを知っているか？"
  },
  {
    "start": 1739180,
    "end": 1751440,
    "text": "では、私たちが3番目に探ろうとしているのは、質問される前から、このトークンの位置は、どのパラメータaがどのパラメータbに依存しているか、モデルがすでに知っているかということです。"
  },
  {
    "start": 1751900,
    "end": 1761016,
    "text": "私たちは、モデルが内部状態で実際にどのように考えているかを理解するために、このようなプロービングを行おうとした。"
  },
  {
    "start": 1761208,
    "end": 1765180,
    "text": "私たちのテクニックは、Vプロービング・テクニックと呼ばれるものだ。"
  },
  {
    "start": 1765680,
    "end": 1767420,
    "text": "次のスライドでお見せしましょう。"
  },
  {
    "start": 1769040,
    "end": 1785404,
    "text": "このVプロービングでは、例えばGPTが小さすぎる場合、12層のデコーダー層を持つ学習済みの言語モデルを使用する。"
  },
  {
    "start": 1785532,
    "end": 1791320,
    "text": "事前に訓練されたモデルを使い、その重みを凍結させ、そこからプロービングを試みる。"
  },
  {
    "start": 1791860,
    "end": 1802492,
    "text": "まず最初にしたことは、ソリューションを削除し、それを特別な開始トークンと特別な終了トークンを持つ任意のパラメータ名に置き換えることである。"
  },
  {
    "start": 1802596,
    "end": 1808290,
    "text": "この新しい入力をモデルに転送し、伝播させる。"
  },
  {
    "start": 1808330,
    "end": 1813266,
    "text": "最終的には、最後のレイヤーの最後のトークンの位置を読み取ろうとする。"
  },
  {
    "start": 1813378,
    "end": 1826778,
    "text": "これは隠れた状態であり、我々はその上に追加の学習可能な線形層を含め、パラメータaがこの質問に答えるために必要かどうかを予測しようとする。"
  },
  {
    "start": 1826834,
    "end": 1829470,
    "text": "これがバイナリ分類タスクである。"
  },
  {
    "start": 1830250,
    "end": 1836848,
    "text": "これは線形層である。"
  },
  {
    "start": 1836904,
    "end": 1841696,
    "text": "768という数字は、GPT-2のスモールを使っていることに由来する。"
  },
  {
    "start": 1841808,
    "end": 1846000,
    "text": "隠された次元はこの768の次元である。"
  },
  {
    "start": 1846120,
    "end": 1848216,
    "text": "これをリニアプロービングと呼ぶ。"
  },
  {
    "start": 1848248,
    "end": 1853232,
    "text": "つまり、何も変えずに、最後のレイヤーから何かを読み取ろうとする。"
  },
  {
    "start": 1853256,
    "end": 1858832,
    "text": "リニアプロービングは、例えばここでは入力分布を変えるという意味で非常に弱い。"
  },
  {
    "start": 1858896,
    "end": 1862146,
    "text": "例えば、特別な開始と終了のトークン化のようなものを追加している。"
  },
  {
    "start": 1862208,
    "end": 1868130,
    "text": "したがって、リニア・プロービングを直接行ったとしても、このネットワークからはほとんど何もプロービングできない。"
  },
  {
    "start": 1868470,
    "end": 1880966,
    "text": "その代わりに私たちが行ったのは、Vプロービングと呼んでいるもので、ネットワークの本体ではなく、入力層だけに訓練可能な変更を加えるというものだ。"
  },
  {
    "start": 1881118,
    "end": 1888302,
    "text": "つまり、この場合、入力埋め込み層は760分の2000程度しかないことを覚えておいてほしい。"
  },
  {
    "start": 1888406,
    "end": 1898886,
    "text": "というのも、この問題の語彙数は2000しかないからだ。英語のトークンをすべて網羅したわけではないので、2000しかカバーしていない。"
  },
  {
    "start": 1898998,
    "end": 1903238,
    "text": "その上で、小さなランク8のアップデートを行おう。"
  },
  {
    "start": 1903374,
    "end": 1913620,
    "text": "これは非常に小さく、エンベデッドレイヤーのパラメーターの1％程度に過ぎず、全パラメーターの0.02％程度に過ぎない。"
  },
  {
    "start": 1913750,
    "end": 1923608,
    "text": "そして、出力に線形層を追加するだけでなく、入力層にも低ランクの更新を追加して訓練する。"
  },
  {
    "start": 1923744,
    "end": 1926540,
    "text": "そして、このバイナリータスクを実行しようとする。"
  },
  {
    "start": 1927760,
    "end": 1936344,
    "text": "ネットワーク全体、つまりデコーダーの12レイヤーがフリーズしたままであることがわかるだろう。"
  },
  {
    "start": 1936472,
    "end": 1952490,
    "text": "つまり、この新しいプロービング・タスク、例えば、このパラメータaが最終的な問題を解決するために必要かどうかを予測する二値分類タスクのために訓練したいのです。"
  },
  {
    "start": 1952870,
    "end": 1962550,
    "text": "ある操作より小さいか等しい操作の問題でもう一度訓練し、もっと難しい問題で分布から外してテストする、といった具合だ。"
  },
  {
    "start": 1962670,
    "end": 1980370,
    "text": "つまり、aがこのプロービングの位置で精神的に必要かどうかにかかわらず、モデルはすでにこの量を知っていると言うのだ。"
  },
  {
    "start": 1981230,
    "end": 1983046,
    "text": "これが私たちのメインコンセプトだ。"
  },
  {
    "start": 1983078,
    "end": 1991490,
    "text": "つまり、このプロービングが成功するたびに、モデルはこのトークンの位置でこの量をすでに知っていることになる。"
  },
  {
    "start": 1992280,
    "end": 1993140,
    "text": "いいかい？"
  },
  {
    "start": 1993960,
    "end": 2006528,
    "text": "なぜこのようなことを言いたいかというと、私たちはネットワーク全体をほぼ凍結し、その上にごく少数の訓練可能なパラメーターを追加しただけだと考えているからだ。"
  },
  {
    "start": 2006624,
    "end": 2013976,
    "text": "したがって、このテスト精度が高いのであれば、パラメーターを追加でトレーニングしていることに由来するはずがない。"
  },
  {
    "start": 2014088,
    "end": 2020330,
    "text": "たしかにパラメーターを追加でトレーニングしているが、それはごくわずかで、入力層と出力層だけだ。"
  },
  {
    "start": 2020450,
    "end": 2024114,
    "text": "ほぼリニアなプロービングという感じですね？"
  },
  {
    "start": 2024202,
    "end": 2027778,
    "text": "さらに、トレーニングセットとタスクセットを分離した。"
  },
  {
    "start": 2027914,
    "end": 2047302,
    "text": "このことは、もしモデルがこのプロービングタスクのより難しい問題に対して本当に汎化することができ、非常に高い精度で予測することができるのであれば、この知識は事前に訓練された重みから得たものであって、新たに訓練された重みから得たものではないことを意味する。"
  },
  {
    "start": 2047406,
    "end": 2049130,
    "text": "それが私たちのメインコンセプトだ。"
  },
  {
    "start": 2050870,
    "end": 2054286,
    "text": "それは、ここにある必要な仕事のためだった。"
  },
  {
    "start": 2054318,
    "end": 2082681,
    "text": "例えば、依存性タスクを調査したい場合、つまり、パラメータaがbに依存するかどうかを、この位置のモデルがすでに知っているかどうかを判断したい場合、何をするかというと、おそらく2つのパラメータを導入し、最後のトークンの位置、最後のレイヤーで、別のバイナリ分類タスク、つまり、aがbに依存するかどうかを再度調査し、モデルが良い精度を出せるかどうかなどをチェックするのです。"
  },
  {
    "start": 2082705,
    "end": 2086229,
    "text": "これが、この依存タスクを探る方法だ。"
  },
  {
    "start": 2087209,
    "end": 2099996,
    "text": "最終的に私たちが発見したのは、3つのプロービング・タスクのすべてにおいて、私たちがテストした最も難しい問題でさえも、99％という非常に高い精度が得られたということです。"
  },
  {
    "start": 2100108,
    "end": 2114196,
    "text": "例えば、コネクストのプロービング・タスクの場合、ミディアムとハードのデータセットでテストしてみると、最も難しい問題でも約99％の精度があることがわかります。"
  },
  {
    "start": 2114348,
    "end": 2126214,
    "text": "もしこれを多数決に当てはめ、ベースラインを推測し、つまり、常に推測し、パラメータが計算の準備ができていないとノーと言えば、精度は50％程度にしかならない。"
  },
  {
    "start": 2126382,
    "end": 2134134,
    "text": "我々はまた、非常に重要な比較作業を行った。それは、ランダムなトランス・モデルでリニア・プロービングを行うことである。"
  },
  {
    "start": 2134262,
    "end": 2137718,
    "text": "精度はベースラインより高いが、わずか60％であることがわかる。"
  },
  {
    "start": 2137774,
    "end": 2138846,
    "text": "これはとても重要なことだ。"
  },
  {
    "start": 2138918,
    "end": 2148590,
    "text": "つまり、プロービングの際に、学習可能なパラメータが追加されるのだが、プロービングの精度は、それらの追加パラメータからは生まれないという点を伝えたい。"
  },
  {
    "start": 2148710,
    "end": 2150370,
    "text": "この点をどう伝えるか。"
  },
  {
    "start": 2151000,
    "end": 2157616,
    "text": "その一つの方法は、ランダムに初期化されたモデルから直接プロービングを行うことだと考えている。"
  },
  {
    "start": 2157808,
    "end": 2160352,
    "text": "こうすることで、ある程度の精度を保つことができる。"
  },
  {
    "start": 2160416,
    "end": 2163820,
    "text": "これは基本的にニューラル・タンジェント・カーネルである。"
  },
  {
    "start": 2164120,
    "end": 2168296,
    "text": "このランダムモデルがまったく役に立たないわけではないのには理由がある。"
  },
  {
    "start": 2168448,
    "end": 2177532,
    "text": "それでもランダムな接線カーネルとして機能するため、そのランダムなカーネルを使ってプロービングを行なえば、些細な精度ではない精度を得ることができる。"
  },
  {
    "start": 2177636,
    "end": 2182740,
    "text": "これは、本当に事前に訓練されたモデルから得られる精度よりもずっとずっと低い。"
  },
  {
    "start": 2182780,
    "end": 2190692,
    "text": "こうすることで、このプロービング精度は、実際にはほとんどが事前に訓練されたモデルの重みに由来しているという点を指摘することができる。"
  },
  {
    "start": 2190836,
    "end": 2194240,
    "text": "これはコネクストのタスクに似ている。"
  },
  {
    "start": 2195140,
    "end": 2203584,
    "text": "論文では、3つのプロービング・タスクだけでなく、さらに3つのプロービング・タスクも行ったが、いずれも高い精度を示した。"
  },
  {
    "start": 2203772,
    "end": 2219312,
    "text": "ひとつ指摘しておかなければならないのは、いくつかのプロービング・タスク、特に依存関係のプロービング・タスクでは、これは実際には非常に偏りのないタスクだということだ。"
  },
  {
    "start": 2219456,
    "end": 2229250,
    "text": "というのも、ランダムなグラフの2つの頂点、たとえばaとBをランダムに選ぶと、ほとんどの場合、aはBjDevに依存しないからだ。"
  },
  {
    "start": 2229400,
    "end": 2232966,
    "text": "したがって、この結果をできるだけ科学的なものにしたい。"
  },
  {
    "start": 2233118,
    "end": 2235566,
    "text": "精度と再現率も調べた。"
  },
  {
    "start": 2235638,
    "end": 2245490,
    "text": "例えば、依存性テストでは、ポジティブ・ラベルとネガティブ・ラベルの間でどの程度の精度があるのかもテストしたが、精度は非常に高いままであることがわかった。"
  },
  {
    "start": 2246390,
    "end": 2262668,
    "text": "そこで私たちは、プロービングを可能な限り科学的なものにしようと努め、最終的に、たとえばこれら3つのタスクのモデルは、ほぼ99％のプロービング精度を達成したという結論を導き出した。"
  },
  {
    "start": 2262724,
    "end": 2268524,
    "text": "GPT-2がどのようにしてレベル1の推論を実現しているのか、それを実際に説明するために使う。"
  },
  {
    "start": 2268652,
    "end": 2271172,
    "text": "そうやって不足の解決策を生み出すのだ。"
  },
  {
    "start": 2271316,
    "end": 2291434,
    "text": "例えば、モデルは必要なパラメータが何であるかをすでに知っており、解生成の各ステップにおけるパラメータは、計算可能なパラメータが何であるかを知っているので、モデルはその2つの間のロジックnを取るだけでよく、このようにして常に最短解を生成することになる。"
  },
  {
    "start": 2291602,
    "end": 2298550,
    "text": "これが、GPT-2がレベル1の推論を達成する方法を説明するためのプロービングの使い方だ。"
  },
  {
    "start": 2299530,
    "end": 2301578,
    "text": "ひとこと"
  },
  {
    "start": 2301634,
    "end": 2304538,
    "text": "つまり、このV字プロービングのテクニックを紹介したわけだ。"
  },
  {
    "start": 2304634,
    "end": 2310848,
    "text": "私は実際に10分、実際には多くの労力を費やして、プロービングが意味のあるものであることを確認するために論文を書いた。"
  },
  {
    "start": 2310944,
    "end": 2312260,
    "text": "私たちはいろいろなことをした。"
  },
  {
    "start": 2312880,
    "end": 2321192,
    "text": "その1は、プロービング・タスクのためのプロービング・トレーニング、トレーニング・データセットとタスク・データセットがまだ分離していないことを確認したことだ。"
  },
  {
    "start": 2321376,
    "end": 2330064,
    "text": "例えば、プロービングトレーニングはオペアンプの数が少ない問題に対して行ったが、テストではオペアンプの数が多い問題に対してテストを行った、といった具合だ。"
  },
  {
    "start": 2330152,
    "end": 2338750,
    "text": "そしてもうひとつは、学習可能なパラメーターが非常に少なく、しかもほぼ線形になるようにしたことだ。"
  },
  {
    "start": 2338790,
    "end": 2350050,
    "text": "出力層と入力層にのみ変更を加え、モデルの重みの大部分、特にトランスフォーマーの12層は変更しなかった。"
  },
  {
    "start": 2350430,
    "end": 2356094,
    "text": "その3は、ランダムに初期化されたモデルではプロービングが機能しないことを確認したことだ。"
  },
  {
    "start": 2356142,
    "end": 2365336,
    "text": "なぜなら、プロービング中に学習可能なパラメーターが追加され、ランダムに初期化されたモデルを使っても、実際に何かを学習することができるからだ。"
  },
  {
    "start": 2365368,
    "end": 2371296,
    "text": "いわゆるニューラル・タンジェント・カーネルと呼ばれるものだ。"
  },
  {
    "start": 2371408,
    "end": 2375920,
    "text": "この方法では、プロービングの精度が非常に低いことを確認したい。"
  },
  {
    "start": 2376040,
    "end": 2387460,
    "text": "これは、プロービングの精度が、プロービング・パラメーターではなく、事前に訓練された重みに由来するものであることを確認するための、非常に良い比較タスクです。"
  },
  {
    "start": 2388200,
    "end": 2393012,
    "text": "最後に、プロービングタスクのいくつかは、依存関係タスクのように偏っている。"
  },
  {
    "start": 2393116,
    "end": 2398204,
    "text": "私たちは、精度と想起の両側面もチェックするようにした。"
  },
  {
    "start": 2398292,
    "end": 2399500,
    "text": "どちらも高い。"
  },
  {
    "start": 2399580,
    "end": 2403868,
    "text": "その場合のみ、このプロービングは非常に成功したと言える。"
  },
  {
    "start": 2403964,
    "end": 2408028,
    "text": "このトークンの位置で、モデルはすでにすべてを知っている。"
  },
  {
    "start": 2408124,
    "end": 2412920,
    "text": "これが、我々のV字プロービング・テクニックの簡単な要約である。"
  },
  {
    "start": 2414340,
    "end": 2425590,
    "text": "プロービングから読み取れる次の結果は、実はこの99％という高い精度が、不要なパラメーターでも維持されているということだ。"
  },
  {
    "start": 2426850,
    "end": 2438230,
    "text": "例えば、コネクストの場合、最終的な質問に答えるために不要なパラメータaを条件とする。"
  },
  {
    "start": 2438610,
    "end": 2448320,
    "text": "ネガティブ・ラベルとポジティブ・ラベルの両方をチェックすると、プロービングの精度が非常に高いことがわかる。"
  },
  {
    "start": 2448900,
    "end": 2457988,
    "text": "それだけでなく、ディペンデント・タスクでも、不要なパラメーターの中ではプロービング精度が非常に高い。"
  },
  {
    "start": 2458164,
    "end": 2471928,
    "text": "これは非常に重要な特性である。なぜなら、たとえばこの位置では、質問がなされる前であっても、すべてのパラメータのペアの間に、すべてのペアの依存関係グラフが存在することを意味するからである。"
  },
  {
    "start": 2472084,
    "end": 2481656,
    "text": "このトークンの位置では、モデルは問題が終わったことを知らないからだ。"
  },
  {
    "start": 2481768,
    "end": 2484872,
    "text": "従って、いくつかの未来文のようなものがあるかもしれない。"
  },
  {
    "start": 2484936,
    "end": 2491240,
    "text": "モデルはすでに何を知っているのか、次の文がいつ、何を受信するかにかかっている。"
  },
  {
    "start": 2491360,
    "end": 2497874,
    "text": "例えば、グラフのある連結成分と別の連結成分を結ぶ場合。"
  },
  {
    "start": 2497962,
    "end": 2508522,
    "text": "モデルは、その1文の中で突然、このインクリメンタルな文の意味を理解し、全パワーの依存関係グラフを完全に更新しなければならない。"
  },
  {
    "start": 2508666,
    "end": 2513842,
    "text": "これは数学の問題を解くのに必要なスキルではない。"
  },
  {
    "start": 2513986,
    "end": 2517642,
    "text": "それはまた、人間の推論方法とも大きく異なる。"
  },
  {
    "start": 2517666,
    "end": 2528836,
    "text": "人間の場合、疑問が提起されるまで待ち、疑問から出発して、この問題を解決するために必要なパラメータを逆算して特定するのが一般的だ。"
  },
  {
    "start": 2528988,
    "end": 2532964,
    "text": "私たちは通常、そのようなパラメーターを見過ごすことさえある。"
  },
  {
    "start": 2533052,
    "end": 2538452,
    "text": "この数学の問題を出せば、おそらく不必要なパラメータは何なのかさえ見落とすだろう。"
  },
  {
    "start": 2538476,
    "end": 2542560,
    "text": "おそらく、必要ないから読むこともないだろう。"
  },
  {
    "start": 2543140,
    "end": 2547604,
    "text": "私たちはこれをレベル2のリーディングスキルと呼んでいる。"
  },
  {
    "start": 2547692,
    "end": 2554522,
    "text": "つまり、私たちの疑問が提起される前に、モデルは実際にこのような全パワー依存グラフを作成しているのだ。"
  },
  {
    "start": 2554666,
    "end": 2575522,
    "text": "というのも、私たちはこのスキルのラベルをモデルに提供したわけではなく、モデルが独自にスキルを開発したのです。"
  },
  {
    "start": 2575706,
    "end": 2591822,
    "text": "一方、このスキルは非常に重要である。というのも、モデルはオブジェクト間の因果関係の束を見ただけで、実際にはそれらのオブジェクト間の関係グラフ全体を整理する能力を持っているからである。"
  },
  {
    "start": 2591926,
    "end": 2598286,
    "text": "これは間違いなく、後に他の仕事などで微調整できるスキルだ。"
  },
  {
    "start": 2598398,
    "end": 2605326,
    "text": "モデルは、私たちが設計した数学のデータセットから、このスキルを学ぶことができる。"
  },
  {
    "start": 2605438,
    "end": 2609510,
    "text": "これはすでにかなり驚くべきことで、私たちはこれをレベル2の読解力と呼んでいる。"
  },
  {
    "start": 2609550,
    "end": 2625350,
    "text": "つまり、人間よりも進化しているという意味ではなく、人間とはまったく異なる方法で物事を学習し、前処理してきたということだ。"
  },
  {
    "start": 2626570,
    "end": 2629130,
    "text": "ここまでの話を要約しよう。"
  },
  {
    "start": 2629250,
    "end": 2650310,
    "text": "基本的に、私たちは新しいデータセットを構築し、ある意味では無限のデータセットを構築した。"
  },
  {
    "start": 2650850,
    "end": 2659082,
    "text": "2つ目の結果では、言語モデルはレベル1の推論能力を示し、レベル2の推論能力も密かに学んでいる。"
  },
  {
    "start": 2659186,
    "end": 2664074,
    "text": "トルクの実験と論文を読めば、それがGBDのためだけのものであることがわかるだろう。"
  },
  {
    "start": 2664122,
    "end": 2672572,
    "text": "実際、Lamaなどの他のアーキテクチャと同様に簡単にテストしてみたが、結果に違いは見られなかった。"
  },
  {
    "start": 2672636,
    "end": 2674508,
    "text": "私たちはそれを提示しなかった。"
  },
  {
    "start": 2674684,
    "end": 2683200,
    "text": "次にお見せする結果は、言語モデルが実際にどのような推論ミスを犯すかに関するものです。"
  },
  {
    "start": 2684620,
    "end": 2689220,
    "text": "私たちが構築したこのデータセットで、彼らはどのように推論ミスを犯すのだろうか？"
  },
  {
    "start": 2689260,
    "end": 2698946,
    "text": "つまり、私たちが描いたGPT-2を見ると、99％以上の確率で、ミスを犯すとすれば次の2つのタイプのどちらかでなければならないと主張している。"
  },
  {
    "start": 2699098,
    "end": 2704026,
    "text": "その1は、不必要なパラメータを計算することがあることだ。"
  },
  {
    "start": 2704178,
    "end": 2714826,
    "text": "モデルが不必要な計算をすることはほとんどないと言ったが、テストは常に可能な限り難しく、トレーニングとは可能な限り異なる問題にすることができる。"
  },
  {
    "start": 2714938,
    "end": 2717490,
    "text": "最終的にはこの動作を表示させることができる。"
  },
  {
    "start": 2717610,
    "end": 2725156,
    "text": "たとえモデルが不必要な計算をしたとしても、それが不正確だということにはならないからだ。"
  },
  {
    "start": 2725268,
    "end": 2740156,
    "text": "つまり、モデルがパラメータを生成して定義しようとするが、実際にはそのパラメータが計算できる状態になっていないことがあるのだ。"
  },
  {
    "start": 2740268,
    "end": 2746668,
    "text": "というのも、まだ計算不可能なものに依存しているからだ。"
  },
  {
    "start": 2746764,
    "end": 2752710,
    "text": "そのため、正しい解答を出すことはできない。"
  },
  {
    "start": 2753050,
    "end": 2761346,
    "text": "実際、事前に訓練したGPT-2をチェックしたところ、この2種類のミスが見られた。"
  },
  {
    "start": 2761378,
    "end": 2773274,
    "text": "つまり、私たちのデータセットを使って、GPT 4と4.0に適用して、そのパフォーマンスをチェックしたのです。"
  },
  {
    "start": 2773442,
    "end": 2778732,
    "text": "GPT 4.0では、このデータセットで非常に悪い結果が出ました。"
  },
  {
    "start": 2778836,
    "end": 2784876,
    "text": "特に、GPTの4人もこの2種類のミスを犯していることがわかった。"
  },
  {
    "start": 2785068,
    "end": 2787400,
    "text": "ひとつ例を挙げよう。"
  },
  {
    "start": 2787980,
    "end": 2801116,
    "text": "つまり、例えばopが10に等しいとすると、問題を適用する前に、GPT4に問題を与える前に、実は最初に非常に詳細なシステム・メッセージを提供している。"
  },
  {
    "start": 2801148,
    "end": 2808196,
    "text": "例えば、私たちはGPTの4番、つまりモジュールの23番ではなく、モジュールの5番しか考えていない。"
  },
  {
    "start": 2808268,
    "end": 2810268,
    "text": "モジュール5のように、さらにシンプルにした。"
  },
  {
    "start": 2810364,
    "end": 2816316,
    "text": "また、モジュラーの五則計算をしていることをモデルが理解できるように、いくつかの例を示す。"
  },
  {
    "start": 2816428,
    "end": 2825560,
    "text": "私たちはまた、英語を特定のカテゴリー、つまり学校の教室やバックパック、文房具だけに限定した。"
  },
  {
    "start": 2825900,
    "end": 2827476,
    "text": "可能な限りシンプルにした。"
  },
  {
    "start": 2827548,
    "end": 2834160,
    "text": "最大で16の英語パラメータしか見ていないほどだ。"
  },
  {
    "start": 2835140,
    "end": 2843956,
    "text": "覚えているだろうか、私たちは1600人ほどいたが、GPT4に問題を渡す前に、実際にはもっともっと語彙を減らしたんだ。"
  },
  {
    "start": 2844108,
    "end": 2859708,
    "text": "また、具体的な問題をGPT4に渡す前に、この具体的な問題について、検討している高校はどこか、検討している教室はどこか、などをGPT4に正確に伝えました。"
  },
  {
    "start": 2859724,
    "end": 2873452,
    "text": "基本的には、私たちの背景をモデルに伝え、この問題を解決するために必要な基本的概念が何であるかを理解してもらえるよう、できる限りモデルを手助けする。"
  },
  {
    "start": 2873596,
    "end": 2879200,
    "text": "その上で、モデルに5つの短い例を与えて、モデルに生成させる。"
  },
  {
    "start": 2879540,
    "end": 2884388,
    "text": "GPT4もこの2種類のミスを犯すことがわかった。"
  },
  {
    "start": 2884444,
    "end": 2893318,
    "text": "つまり、必要のない計算をすることもあれば、まだ計算の準備ができていないインスタンスの計算を始めることもある。"
  },
  {
    "start": 2893414,
    "end": 2897690,
    "text": "したがって、GPTの4人もこの種のミスを犯していることがわかった。"
  },
  {
    "start": 2898230,
    "end": 2909410,
    "text": "この点については、GPT 4040が9または11より大きいopで失敗すると主張する理由を説明することができます。"
  },
  {
    "start": 2909710,
    "end": 2918680,
    "text": "というのも、私たちは実際に、このような5回の短期学習でGPT4に直接データセットを適用したからだ。"
  },
  {
    "start": 2918840,
    "end": 2921856,
    "text": "ここでは答えを確認するだけである。"
  },
  {
    "start": 2921888,
    "end": 2924528,
    "text": "部分的な正確さではなく、答えの正確さだ。"
  },
  {
    "start": 2924584,
    "end": 2927528,
    "text": "もしゼロを推測するなら、それは5のモデルだと覚えておいてほしい。"
  },
  {
    "start": 2927624,
    "end": 2932032,
    "text": "いつもゼロなら、すでに30数パーセントの精度が出せる。"
  },
  {
    "start": 2932136,
    "end": 2935208,
    "text": "GPTの4番はランダムな推測に近いことがわかる。"
  },
  {
    "start": 2935344,
    "end": 2938936,
    "text": "ここまでのopはそれぞれ9か11に相当する。"
  },
  {
    "start": 2939008,
    "end": 2940080,
    "text": "ランダムガスと同じだ。"
  },
  {
    "start": 2940120,
    "end": 2947550,
    "text": "従って、GPT 4はこの閾値以上のopで失敗すると主張したわけだ。"
  },
  {
    "start": 2947670,
    "end": 2951590,
    "text": "いずれにせよ、オプトがどれくらいの大きさで解けるかなどは重要ではない。"
  },
  {
    "start": 2951630,
    "end": 2957090,
    "text": "GPT4もまた、この2つの間違いのために失敗していることを見ることは重要だ。"
  },
  {
    "start": 2957630,
    "end": 2963878,
    "text": "そのため、モデルがなぜこの2種類のミスを犯すのかを研究することが非常に重要になる。"
  },
  {
    "start": 2964054,
    "end": 2970376,
    "text": "最初のタイプの間違いについて見てみよう。モデルが解を生成する前に、モデルが実際に解を生成することを思い出してほしい。"
  },
  {
    "start": 2970568,
    "end": 2980140,
    "text": "私たちのプロービング技術は、モデルがすでに、質問に答えるために必要なパラメータを精神的に計算していることを示している。"
  },
  {
    "start": 2980680,
    "end": 2988768,
    "text": "したがって、モデルが精神的に考えていることと、実際にモデルが言うことの相関関係をチェックすることができる。"
  },
  {
    "start": 2988944,
    "end": 2992318,
    "text": "実際、この2つには非常に大きな相関関係が見られた。"
  },
  {
    "start": 2992464,
    "end": 3014410,
    "text": "つまり、プランニングの段階、つまりモデルが口を開く前に、あるパラメータが真であるために必要な計算をモデルが間違っていた場合、モデルがすでにこの間違いを犯していれば、最終的に生成される解の中でモデルがそう言う可能性が非常に高く、その相関は約80％であることがわかった。"
  },
  {
    "start": 3014570,
    "end": 3021054,
    "text": "つまり、モデルが必要のない解のステップを生成するような場合である。"
  },
  {
    "start": 3021102,
    "end": 3028398,
    "text": "プランニングの段階では、プロービングの精度は20％程度しかなかった。"
  },
  {
    "start": 3028494,
    "end": 3032390,
    "text": "したがって、この種のミスは組織的なものであることがわかる。"
  },
  {
    "start": 3032510,
    "end": 3038118,
    "text": "つまり、プロービングを使えば、モデルが口を開く前にそのようなミスを発見できるのだ。"
  },
  {
    "start": 3038254,
    "end": 3047762,
    "text": "したがって、このようなミスはシステマティックなものであり、生成過程によるランダムなものではないのだから、ミスが起こるずっと前にすでに発見できる。"
  },
  {
    "start": 3047906,
    "end": 3063910,
    "text": "この種のミスは、ステップ2だけでなく、時にはステップ15でも起こりうるが、そのずっと前、モデルが最初のトークンを生成し始める前でも、すでにこのミスが起こる可能性が非常に高いことを検知できる。"
  },
  {
    "start": 3065290,
    "end": 3068322,
    "text": "これは、最初のタイプのミスがどのようになされるかを説明している。"
  },
  {
    "start": 3068386,
    "end": 3070974,
    "text": "では、2つ目のタイプのミスに移ろう。"
  },
  {
    "start": 3071042,
    "end": 3078166,
    "text": "つまり、モデルが、まだ計算の準備ができていない量の計算を開始するときである。"
  },
  {
    "start": 3078318,
    "end": 3093262,
    "text": "実際に同じようなことをやってみたところ、もしモデルがパラメーターのひとつを誤って計算し、計算可能な状態になっていたとしても、実際にはそうではなかった場合、この場合、モデルは次の文章でそれを言う可能性が高いことがわかった。"
  },
  {
    "start": 3093366,
    "end": 3102314,
    "text": "モデルがミスをするときとしないときの相関関係を再度チェックし、プロービング精度の間にギャップがあることを確認する。"
  },
  {
    "start": 3102442,
    "end": 3109962,
    "text": "つまり、モデルの推論精度を向上させるためには、接続精度を向上させることが非常に重要なのだ。"
  },
  {
    "start": 3110066,
    "end": 3116842,
    "text": "つまり、どのようなパラメータが必要なのか、申し訳ないが、計算の準備ができているのか、などをよりよく知る必要がある。"
  },
  {
    "start": 3116866,
    "end": 3132430,
    "text": "実際、我々はタオの2.2部のような論文で、この精度を向上させる方法を研究し、モデルの推論精度を向上させることに費やしている。"
  },
  {
    "start": 3134290,
    "end": 3138458,
    "text": "次にお見せしたいのは、スケーリング法則に関する結果である。"
  },
  {
    "start": 3138514,
    "end": 3146370,
    "text": "つまり、ネットワークの幅や深さは本当に重要ではないというOpenAIのオリジナルのスケーリングの言い伝えをショートさせるのだ。"
  },
  {
    "start": 3146490,
    "end": 3155446,
    "text": "3.3では、知識スキルのみ、モデルのサイズが重要であることを発見したと説明した。"
  },
  {
    "start": 3155518,
    "end": 3163730,
    "text": "層構造の変圧器さえあれば、事実上の知識を記憶するには十分なのだ。"
  },
  {
    "start": 3164190,
    "end": 3168410,
    "text": "今日のトークでは、深さも重要だということをお話しします。"
  },
  {
    "start": 3168710,
    "end": 3171598,
    "text": "推理には深さが重要なのだ。"
  },
  {
    "start": 3171774,
    "end": 3173830,
    "text": "これが私たちが行った実験である。"
  },
  {
    "start": 3173870,
    "end": 3180788,
    "text": "我々は5つの異なるモデルの深さと、2つの異なるモデルのサイズをまとめた。"
  },
  {
    "start": 3180924,
    "end": 3189172,
    "text": "例えば、12レイヤーと12ヘッドで、各ヘッドが64次元のようなものだとしたら、それはGPDが小さすぎる。"
  },
  {
    "start": 3189316,
    "end": 3199620,
    "text": "ここでは縦方向と同じように、縦方向に4つ、5つのケースでモデルがほぼ同じサイズになるように、実際にヘッドの数を調整した。"
  },
  {
    "start": 3199780,
    "end": 3206356,
    "text": "これらのケースのモデルもほぼ同じ大きさで、サイズ1の2倍の大きさである。"
  },
  {
    "start": 3206548,
    "end": 3210116,
    "text": "各モデル10種類をまとめた。"
  },
  {
    "start": 3210148,
    "end": 3220460,
    "text": "事前訓練を2回行い、精度の最大値を取り、作戦規模や作戦回数などをすべて変えて評価した。"
  },
  {
    "start": 3220580,
    "end": 3222012,
    "text": "これがその結果だ。"
  },
  {
    "start": 3222116,
    "end": 3230882,
    "text": "レイヤーの数が増えると、テストの精度が非常に明確なパターンになることがわかった。"
  },
  {
    "start": 3230976,
    "end": 3239170,
    "text": "このパラメーターの範囲では、モデルの幅にはあまり依存しないようだ。"
  },
  {
    "start": 3239470,
    "end": 3246606,
    "text": "例えば、この2つのポジションを比較した場合、最初のケースではモデルが2倍大きく、奥行きが小さい。"
  },
  {
    "start": 3246678,
    "end": 3252350,
    "text": "2つ目のケースでは、モデルは2倍小さくなり、深さも大きくなる。"
  },
  {
    "start": 3252390,
    "end": 3259336,
    "text": "この場合、94％の精度が得られるが、この場合は60数％の精度しか得られない。"
  },
  {
    "start": 3259528,
    "end": 3262432,
    "text": "これは実際には、非常に予備的な証拠である。"
  },
  {
    "start": 3262496,
    "end": 3267832,
    "text": "つまり、推論にはモデルの深さが非常に重要なのだ。"
  },
  {
    "start": 3268016,
    "end": 3272112,
    "text": "だからもちろん、10人くらいのモデルしかまとめてないよね？"
  },
  {
    "start": 3272176,
    "end": 3279232,
    "text": "より包括的な実験をしたいのであれば、より大きなサイズのモデルなどをもっと増やす必要がある。"
  },
  {
    "start": 3279336,
    "end": 3281336,
    "text": "将来、そうするかもしれない。"
  },
  {
    "start": 3281368,
    "end": 3293590,
    "text": "今日は、なぜこのような現象が起こるのかをさらに調査するために、GPUのリソースを実際に使っていることをお話しします。"
  },
  {
    "start": 3295770,
    "end": 3302470,
    "text": "ネットワークの異なる層で、パラメータtに対するプロービング精度をチェックした。"
  },
  {
    "start": 3302930,
    "end": 3307698,
    "text": "ここでパラメータtは、質問に対するパラメータの距離を表す。"
  },
  {
    "start": 3307754,
    "end": 3317688,
    "text": "例えば、質問があるパラメータaに直接依存する場合、質問に対するaの距離は1であると言う。"
  },
  {
    "start": 3317784,
    "end": 3324360,
    "text": "質問がaに依存し、aがbに依存する場合、質問に対するbの距離は2であると言う。"
  },
  {
    "start": 3324480,
    "end": 3338160,
    "text": "そこで、この必要なプロービングの精度を、質問に対するパラメータの距離の違いによって実際にプロットしてみた。"
  },
  {
    "start": 3338900,
    "end": 3340308,
    "text": "私たちは確認した。"
  },
  {
    "start": 3340364,
    "end": 3347964,
    "text": "これは、20層の変圧器に対する20層のニューラルネットワークの実験であり、各層ごとにチェックした。"
  },
  {
    "start": 3348052,
    "end": 3355332,
    "text": "では、質問に対する異なる距離のパラメータに対するプロービング精度はどの程度なのだろうか？"
  },
  {
    "start": 3355476,
    "end": 3356684,
    "text": "私たちは2つのことを発見した。"
  },
  {
    "start": 3356732,
    "end": 3362812,
    "text": "その1は、一般的に質問から遠いパラメータほど精度が落ちるということだ。"
  },
  {
    "start": 3362876,
    "end": 3381040,
    "text": "というのも、tが8であれば、その質問がこのパラメータに依存していると結論づけるためには、少なくとも8つの異なるメンタルステップを踏む必要があるからだ。"
  },
  {
    "start": 3381540,
    "end": 3392160,
    "text": "というのも、それらのパラメータが質問に答えるために必要であると判断するのがずっとずっと難しいからだ。"
  },
  {
    "start": 3392860,
    "end": 3398380,
    "text": "同時に、このモデルがこのようなレイヤーごとの推論を採用していることもわかった。"
  },
  {
    "start": 3398420,
    "end": 3402440,
    "text": "つまり、一般的に言えば、レイヤーは多ければ多いほどいい。"
  },
  {
    "start": 3402860,
    "end": 3416400,
    "text": "例えば、20のレイヤーがある場合、モデルは必要なパラメーターを計算する難しさを各レイヤーにほぼ平均的に分散させることができる。"
  },
  {
    "start": 3416480,
    "end": 3424688,
    "text": "例えば、浅いレイヤーはより簡単なパラメーターに取り組み、深いレイヤーはより難しいパラメーターなどに取り組むようにする。"
  },
  {
    "start": 3424784,
    "end": 3426360,
    "text": "それが私たちが発見したことだ。"
  },
  {
    "start": 3426400,
    "end": 3431416,
    "text": "なぜレイヤーが多ければ多いほどいいのか、その理由を説明するために使っている。"
  },
  {
    "start": 3431528,
    "end": 3440090,
    "text": "というのも、モデルは非常に複雑な操作を精神的に行う必要があり、そのためには多くのレイヤーが必要になるからだ。"
  },
  {
    "start": 3440870,
    "end": 3442582,
    "text": "という免責事項がある。"
  },
  {
    "start": 3442606,
    "end": 3446198,
    "text": "それぞれの精神的なステップに1つのレイヤーが必要だと言っているのではない。"
  },
  {
    "start": 3446254,
    "end": 3449862,
    "text": "私は、1層のトランスフォーマーが完全にチューニングされているから言っているのではない。"
  },
  {
    "start": 3449886,
    "end": 3452494,
    "text": "何でも計算できるんだろ？"
  },
  {
    "start": 3452582,
    "end": 3458918,
    "text": "例えば、1つのトークンの位置で、アテンション・メカニズムを使い、現在位置までのすべてを回収することができる。"
  },
  {
    "start": 3459054,
    "end": 3463450,
    "text": "MLPは2層ネットワークだから、計算ができる。"
  },
  {
    "start": 3463830,
    "end": 3464822,
    "text": "チューリングの完成だ。"
  },
  {
    "start": 3464846,
    "end": 3466184,
    "text": "どんな関数でも計算できる。"
  },
  {
    "start": 3466302,
    "end": 3467520,
    "text": "そんなことは言っていない。"
  },
  {
    "start": 3468220,
    "end": 3479492,
    "text": "私が言いたいのは、1つのレイヤーしかない場合、計算可能ではあるが、隠れ次元が非常に高くなり、トレーニングが非常に低くなるということだ。"
  },
  {
    "start": 3479636,
    "end": 3484540,
    "text": "私が言いたいのは、その代わり、それは合理的な範囲内だということだ。"
  },
  {
    "start": 3484580,
    "end": 3499530,
    "text": "例えば、この実験では、トランスフォーマーのGPT-2の総サイズを制限している場合、また、同じ時間トレーニングされたモデル同士を比較した場合、このようなパターンが見られます。"
  },
  {
    "start": 3500310,
    "end": 3503910,
    "text": "なるほど、こういう結論になるのか。"
  },
  {
    "start": 3503950,
    "end": 3507350,
    "text": "つまり、推論には深さが必要だと考えている。"
  },
  {
    "start": 3507390,
    "end": 3511214,
    "text": "その理由は、精神的な計算ステップがあるからだ。"
  },
  {
    "start": 3511382,
    "end": 3519702,
    "text": "つまり、例えば知識で例えるなら、事実の知識を暗記する場合、多くの精神的な計算ステップは必要ない。"
  },
  {
    "start": 3519766,
    "end": 3521558,
    "text": "ひとつだけ記録しておく必要がある。"
  },
  {
    "start": 3521694,
    "end": 3530366,
    "text": "というのも、このような推論作業をする場合、質問に答えるために必要なパラメータを事前に計算しなければならないからだ。"
  },
  {
    "start": 3530478,
    "end": 3533342,
    "text": "これには、たぶん、いくつもの精神的なステップが必要だ。"
  },
  {
    "start": 3533446,
    "end": 3535730,
    "text": "そのためには深さが必要だ。"
  },
  {
    "start": 3536470,
    "end": 3538166,
    "text": "それが私たちのやり方だ。"
  },
  {
    "start": 3538318,
    "end": 3539610,
    "text": "何を、どうやって。"
  },
  {
    "start": 3540430,
    "end": 3544814,
    "text": "なぜ深さが推論に必要なのか？"
  },
  {
    "start": 3544902,
    "end": 3551614,
    "text": "最後に言っておきたいのは、この現象はコットでは軽減できないということだ。"
  },
  {
    "start": 3551662,
    "end": 3553462,
    "text": "ベビーベッドはすでに使っています。"
  },
  {
    "start": 3553566,
    "end": 3557214,
    "text": "私たちは、数学の問題に直接答えることをモデルに求めているのではない。"
  },
  {
    "start": 3557262,
    "end": 3560430,
    "text": "一歩一歩、問題を解決していくようにお願いしています。"
  },
  {
    "start": 3560550,
    "end": 3566090,
    "text": "私は、最初のステップが何であるかを決定するモデルでさえ、そう主張する。"
  },
  {
    "start": 3567110,
    "end": 3573656,
    "text": "これにはすでに、1ステップではなく、複数のステップを必要とする精神的な計算が必要だ。"
  },
  {
    "start": 3573758,
    "end": 3582988,
    "text": "そのため、このような精神的なプロセスは、深さを要求するCO2の最初のステップを決める計画段階でもある。"
  },
  {
    "start": 3583124,
    "end": 3590548,
    "text": "そのため、より難しい仕事をこなすためには、最終的にはディープ・トランスフォーマーが必要になる。"
  },
  {
    "start": 3590724,
    "end": 3594640,
    "text": "これでパート8は終了だ。"
  },
  {
    "start": 3595500,
    "end": 3610108,
    "text": "つまり、この3つの結果をまとめると、言語モデルがどのように妥当な間違いを犯すか、そしてそれはプロービングやモデルが精神的にどのように物事を考えるかに強く相関している、ということになります。"
  },
  {
    "start": 3610244,
    "end": 3619900,
    "text": "それはプロービングが間違っていたからであり、プロービングによって言語モデルが何か間違ったことを考えていたことが明らかになり、それが推論ミスを犯す理由なのだ。"
  },
  {
    "start": 3620060,
    "end": 3624716,
    "text": "最後の結果では、長い推論タスクには深さが重要であることを示した。"
  },
  {
    "start": 3624828,
    "end": 3631690,
    "text": "それは精神的な事前計算が必要だからだ。"
  },
  {
    "start": 3631730,
    "end": 3636190,
    "text": "何段階もの精神的な計算が必要で、非常にコストがかかる。"
  },
  {
    "start": 3636570,
    "end": 3639466,
    "text": "最後に、素晴らしい共著者たちに感謝の意を表したい。"
  },
  {
    "start": 3639538,
    "end": 3645330,
    "text": "Tian YeはCMUの大学院生で私のインターン、ZhichengxueはMedaの同僚だ。"
  },
  {
    "start": 3645410,
    "end": 3649490,
    "text": "ヤン・ジーリーはMBZウアイの教授で、それが私だ。"
  },
  {
    "start": 3649610,
    "end": 3651070,
    "text": "ご清聴ありがとうございました。"
  },
  {
    "start": 3651490,
    "end": 3652210,
    "text": "それではまた次回。"
  }
]