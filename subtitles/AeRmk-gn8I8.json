[
  {
    "start": 41810,
    "end": 44710,
    "text": "さて、そろそろ時間だ。"
  },
  {
    "start": 44780,
    "end": 69860,
    "text": "コリン・ラフェルは、実際に大規模な言語モデルを構築し、それがどのように機能するかを知り尽くしている人物の一人です。"
  },
  {
    "start": 70870,
    "end": 71620,
    "text": "ありがとう。"
  },
  {
    "start": 71990,
    "end": 74322,
    "text": "ああ、ここにいられて本当に嬉しいよ。"
  },
  {
    "start": 74456,
    "end": 87346,
    "text": "実は今、引っ越しの真っ最中なんだ。でも、この講演に招待されたとき、すべてを一時中断して来てみようと決めたんだ。本当にエキサイティングなグループだし、エキサイティングなトピックだと思うからね。"
  },
  {
    "start": 87538,
    "end": 91898,
    "text": "そういえば、今日はセッションが終わったらすぐに帰らないといけないんだ。"
  },
  {
    "start": 91984,
    "end": 97050,
    "text": "私の荷物の引き渡しをフォローしたい人がいれば。"
  },
  {
    "start": 98110,
    "end": 104830,
    "text": "もしこの件についてフォローアップしたい人がいれば、私は近くにいないのでチャットすることはできないが、オフラインでこのようなことについてチャットすることはもちろん歓迎だ。"
  },
  {
    "start": 105250,
    "end": 115050,
    "text": "というわけで、ともかく今日私がやろうとすることは、汎用AIシステムの構築方法が見当違いであることを納得してもらうことだ。"
  },
  {
    "start": 115130,
    "end": 122130,
    "text": "私は、このようなシステムを構築する別の方法を提案しようと思う。それも見当違いかもしれないが、少なくともそれは違う。"
  },
  {
    "start": 122280,
    "end": 131702,
    "text": "見当違いだと思うなら、そう言ってくれれば、それについて話し合うことができるかもしれない。"
  },
  {
    "start": 131756,
    "end": 133206,
    "text": "さっそく始めよう。"
  },
  {
    "start": 133308,
    "end": 144506,
    "text": "4年ほど前までは、これがNLPやその他多くの機械学習の応用分野で問題に取り組む主な方法だった。"
  },
  {
    "start": 144608,
    "end": 158954,
    "text": "真ん中の小さな虹色のニューラルネットワークは、一般的なタスク（言語モデリングとか、たくさんのクラスが存在する画像分類タスクとか）で事前に訓練されたモデルだ。"
  },
  {
    "start": 159082,
    "end": 166458,
    "text": "そして、モデルに特定のタスクを実行させたい場合は、そのタスクを表すデータセットを取り出し、それを微調整する。"
  },
  {
    "start": 166484,
    "end": 173554,
    "text": "そのタスクに特化したモデルを作るために、そのタスクで訓練する。"
  },
  {
    "start": 173592,
    "end": 176254,
    "text": "これは信じられないほど効果的なパラダイムだ。"
  },
  {
    "start": 176302,
    "end": 182758,
    "text": "事前学習を使用することで、モデルのマージが促進され、より良い解に素早く収束する。"
  },
  {
    "start": 182844,
    "end": 185142,
    "text": "これは誰もがやったことだ。"
  },
  {
    "start": 185196,
    "end": 190410,
    "text": "申し上げたように、非常に効果的で、非常に有用で、非常に価値のあるものだった。"
  },
  {
    "start": 191790,
    "end": 207082,
    "text": "最近では、大規模な言語モデルがあり、大規模な言語モデルは、多くのタスクをこなすことができる汎用システムとして扱えることが多いからだ。"
  },
  {
    "start": 207146,
    "end": 212746,
    "text": "うまくいけば、非常に多くのタスクをこなすことができる。"
  },
  {
    "start": 212778,
    "end": 216270,
    "text": "訓練を受けていない多くのタスクに汎化する。"
  },
  {
    "start": 216340,
    "end": 223234,
    "text": "追加トレーニングをする代わりに、単一のモデルを固定し、モデルに私たちが望むことをさせる。"
  },
  {
    "start": 223352,
    "end": 225614,
    "text": "言語モデルの場合、自然言語で。"
  },
  {
    "start": 225662,
    "end": 227922,
    "text": "次の記事を要約してください。"
  },
  {
    "start": 228066,
    "end": 230280,
    "text": "これらの質問は同じことを尋ねているのだろうか？"
  },
  {
    "start": 231290,
    "end": 233586,
    "text": "基本的には質問したり、リクエストしたりする。"
  },
  {
    "start": 233618,
    "end": 236742,
    "text": "そうすれば、私たちが求めたことを言語モデルに実行させることができる。"
  },
  {
    "start": 236876,
    "end": 239734,
    "text": "これは非常に効果的だろう？"
  },
  {
    "start": 239772,
    "end": 241574,
    "text": "私たちには巨大なモデルがある。"
  },
  {
    "start": 241702,
    "end": 245990,
    "text": "私たちは彼らに、自然言語で考えられるあらゆるタスクを実行するよう求める。"
  },
  {
    "start": 246070,
    "end": 249594,
    "text": "合理的な範囲では、実際に機能している。"
  },
  {
    "start": 249712,
    "end": 252974,
    "text": "これらのモデルを大きくすればするほど、よりうまく機能するようになる。"
  },
  {
    "start": 253092,
    "end": 254842,
    "text": "極めて予測しやすい。"
  },
  {
    "start": 254986,
    "end": 264050,
    "text": "そのため、汎用AIシステムを構築するための、この分野の主流となるパラダイムとなった。"
  },
  {
    "start": 264870,
    "end": 267860,
    "text": "についてお話しします。"
  },
  {
    "start": 271270,
    "end": 272334,
    "text": "いや、これはおかしい。"
  },
  {
    "start": 272382,
    "end": 274062,
    "text": "実はスライドが欠けているんだ。"
  },
  {
    "start": 274206,
    "end": 277590,
    "text": "スライドがなくなった理由を考えてみよう。"
  },
  {
    "start": 279050,
    "end": 285826,
    "text": "さて、言っておくが、この講演をするのは今回が初めてで、スライドが1枚足りない。"
  },
  {
    "start": 285938,
    "end": 287670,
    "text": "口頭で説明するだけだ。"
  },
  {
    "start": 289630,
    "end": 311360,
    "text": "システムを構築する際に期待することのひとつは、それなりに有能なシステムを手に入れ、システムをより良くしたいのであれば、それを捨ててゼロから新しいシステムを構築することだ。"
  },
  {
    "start": 312450,
    "end": 322754,
    "text": "このアプローチは、昨日のインタットのコメントに代表されるもので、あなたをいじめるようで申し訳ないのですが、そのコメントは、まあ、GPT5を待てばいいんでしょ？"
  },
  {
    "start": 322792,
    "end": 324354,
    "text": "まさにその通りだ。"
  },
  {
    "start": 324392,
    "end": 324594,
    "text": "そうだね。"
  },
  {
    "start": 324632,
    "end": 335126,
    "text": "より良いモデルが欲しければ、古いモデルを捨てて、より大きな新しいモデルを作る。"
  },
  {
    "start": 335308,
    "end": 337554,
    "text": "またまた、これは超効果的だった。"
  },
  {
    "start": 337682,
    "end": 340454,
    "text": "別の方法について話そう。"
  },
  {
    "start": 340492,
    "end": 340994,
    "text": "いいかい？"
  },
  {
    "start": 341132,
    "end": 348730,
    "text": "このパラダイムはまだ存在しないが、うまくいく可能性はあると思う。"
  },
  {
    "start": 348800,
    "end": 351466,
    "text": "それがうまくいくかもしれないという証拠をお見せしようと思う。"
  },
  {
    "start": 351488,
    "end": 354640,
    "text": "まず、それを簡単に説明しよう。"
  },
  {
    "start": 355170,
    "end": 363870,
    "text": "私が作ったこの図、この漫画でまず気づくことは、真ん中の箱以外はすべてまったく同じに見えるということだ。"
  },
  {
    "start": 364020,
    "end": 370434,
    "text": "私たちは、どんなタスクでもこなせる汎用システムを構築しようとしている。"
  },
  {
    "start": 370552,
    "end": 381538,
    "text": "違いは、すべてを実行することを目的とした単一のモノリシックなモデルを真ん中に置くのではなく、エコシステム、つまり1トンもの特化したモデルの集合体を持つということだ。"
  },
  {
    "start": 381714,
    "end": 393980,
    "text": "このエコシステムの中で、モデルをさまざまな方法で組み合わせることで、エコシステム全体として、基本的にどんなタスクでも実行できることを期待している。"
  },
  {
    "start": 394350,
    "end": 400886,
    "text": "このエコシステムの中には、何千ものモデル、何万ものモデル、何十万もの特殊なモデルがあるかもしれない。"
  },
  {
    "start": 400918,
    "end": 405850,
    "text": "それぞれのモデルは、ある特定の特定の仕事をこなす能力がある。"
  },
  {
    "start": 405930,
    "end": 407470,
    "text": "それが成分1だ。"
  },
  {
    "start": 407620,
    "end": 419486,
    "text": "要素2は、大規模な言語モデルを使うときのように、エコシステムを一種のブラックボックスとして扱うためには、特定のタスクにどの特化したモデルを使うかを自動的に選択する方法が必要だということだ。"
  },
  {
    "start": 419598,
    "end": 421998,
    "text": "それは、この小さな線が表しているようなものだ。"
  },
  {
    "start": 422094,
    "end": 424340,
    "text": "質問があったので、本当にすぐにやめます。"
  },
  {
    "start": 427030,
    "end": 431586,
    "text": "異なるモデルを異なる場所、異なるロケーションで開催することも可能ですか？"
  },
  {
    "start": 431698,
    "end": 432406,
    "text": "ああ、もちろんだ。"
  },
  {
    "start": 432508,
    "end": 439402,
    "text": "実は一番最後に、このようなシステムで推論を実行する方法について、システムの側面から少しお話します。"
  },
  {
    "start": 439536,
    "end": 442074,
    "text": "ほとんど、その話をするつもりはない。"
  },
  {
    "start": 442272,
    "end": 449626,
    "text": "分散推論などには、こうしたことをいくつか単純化するような興味深い考察が可能だと思う。"
  },
  {
    "start": 449728,
    "end": 455310,
    "text": "効率性についてはもう少し詳しく話すつもりだが、そうだね。"
  },
  {
    "start": 456530,
    "end": 456798,
    "text": "そうだね。"
  },
  {
    "start": 456804,
    "end": 463046,
    "text": "何十万、何百万、何千という専門的なモデルがある。"
  },
  {
    "start": 463178,
    "end": 466050,
    "text": "私たちはそれらのモデルの中で自動ルーティングを行っている。"
  },
  {
    "start": 466390,
    "end": 479570,
    "text": "また、モデルを組み合わせたり、能力を組み合わせたりする方法も必要だと思う。今日、千二が指摘したように、ある特定のタスクを遂行するためには、スキルを組み合わせる必要があることが多い。"
  },
  {
    "start": 479650,
    "end": 488646,
    "text": "宇宙物理学についてトゥパック風の詩を書けるような、そんなモデルが一人できる可能性は低いかもしれない。"
  },
  {
    "start": 488748,
    "end": 494794,
    "text": "詩作モデルとトゥパック世代モデルと天体物理学知識モデルを組み合わせる必要があるかもしれない。"
  },
  {
    "start": 494832,
    "end": 496394,
    "text": "それについては後述する。"
  },
  {
    "start": 496592,
    "end": 525506,
    "text": "もしこのようなシステムを構築できたとしたら、重要なのは、このような特殊化されたモデルは、大規模な汎用モノリス・モデルよりもはるかに安価で、時には高性能であることが多いため、このシステム全体は、計算コストが劇的に低くなり、劇的に向上する可能性があるということだ。"
  },
  {
    "start": 525608,
    "end": 526662,
    "text": "何か質問は？"
  },
  {
    "start": 526716,
    "end": 547018,
    "text": "エコシステムのコンセプトはとても気に入っているのですが、実装の可能性という点では、GPT4やフランなど、別々のモデルになるのでしょうか？"
  },
  {
    "start": 547104,
    "end": 553710,
    "text": "また、人選の面でも、ある程度は専門家の混合に格下げされるのではないか？"
  },
  {
    "start": 554290,
    "end": 560702,
    "text": "この2つについては、後で触れることにしよう。"
  },
  {
    "start": 560756,
    "end": 568034,
    "text": "最初の質問については、これはどちらでもいいと思うが、ほとんどの選手が同じベースモデルを使えば、少し楽になるものもある。"
  },
  {
    "start": 568072,
    "end": 569090,
    "text": "それについては後述する。"
  },
  {
    "start": 569160,
    "end": 572002,
    "text": "また、専門家の混合とのつながりについても後で少し触れよう。"
  },
  {
    "start": 572056,
    "end": 573250,
    "text": "ああ、質問があるんだ。"
  },
  {
    "start": 573320,
    "end": 578626,
    "text": "これはある種の生物学にインスパイアされた建築のようだ。"
  },
  {
    "start": 578738,
    "end": 584150,
    "text": "ニューロンのいくつかは、異なるサブネットワーク間で共有されているようなものだ。"
  },
  {
    "start": 584650,
    "end": 587206,
    "text": "そういう方向ですか？"
  },
  {
    "start": 587308,
    "end": 598674,
    "text": "エコシステムなど生物学的な用語を使っていることは承知しているが、生物学的な概念との関連は偶発的なものだ。"
  },
  {
    "start": 598802,
    "end": 599410,
    "text": "そうでなければ"
  },
  {
    "start": 599490,
    "end": 601550,
    "text": "ええ、私は生物学者ではありません。"
  },
  {
    "start": 604610,
    "end": 607200,
    "text": "ええ、それもすぐにやります。"
  },
  {
    "start": 608290,
    "end": 618242,
    "text": "オレンジ、失礼、ブルーの塊が、あるシステムから次のシステムへ行くにつれて大きくなっていくのを除けば、欠落したスライドはこんな感じだった。"
  },
  {
    "start": 618296,
    "end": 619998,
    "text": "このスライドは過去の講演でも使ったことがある。"
  },
  {
    "start": 620014,
    "end": 622514,
    "text": "私の過去の講演をご覧になった方なら、もしかしたらご存じかもしれない。"
  },
  {
    "start": 622632,
    "end": 641080,
    "text": "エコシステムが成長し、より多くのモデルが導入され、組み替えられ、追加されたり削除されたりするにつれて、時間経過とともにシステムができること、つまり能力の抽象的な概念はこのようになると思う。"
  },
  {
    "start": 641530,
    "end": 662842,
    "text": "私がここで言いたいのは、共同開発を可能にするエコシステムがあれば、多くの人々が協力してエコシステムに追加したり、何かを組み合わせたりすることで、突然の投資増加による大きなステップの改善ではなく、継続的な改善が見られるということだ。"
  },
  {
    "start": 662986,
    "end": 671922,
    "text": "さて、では基本的に残りのトークはすべて、どのように、そしてなぜ建設すべきなのかをスケッチして話すつもりだ。"
  },
  {
    "start": 671976,
    "end": 675790,
    "text": "一枚岩のモデルではなく、専門モデルの生態系を構築することは可能だろうか？"
  },
  {
    "start": 675870,
    "end": 680530,
    "text": "繰り返すが、この話はほとんど推測にすぎない。"
  },
  {
    "start": 680690,
    "end": 683366,
    "text": "これがうまくいくかもしれないという証拠を提示しようと思う。"
  },
  {
    "start": 683468,
    "end": 686102,
    "text": "この証拠の多くは、他の人たちの仕事から来ている。"
  },
  {
    "start": 686156,
    "end": 696010,
    "text": "私の研究室の仕事についても少し話そうと思う。多くのことが関連しているからだ。うまくいけば、この話が終わる頃には、これがまったく馬鹿げた話ではないということを、少なくともほんの少しは納得してもらえるだろう。"
  },
  {
    "start": 697310,
    "end": 700314,
    "text": "それで、途中で2、3点指摘したいことがある。"
  },
  {
    "start": 700352,
    "end": 706606,
    "text": "第一のポイントは、このワークショップですでによく説明されていると思うが、スペシャリストのモデルの方が安く、時には優れていることが多いということだ。"
  },
  {
    "start": 706708,
    "end": 710846,
    "text": "昨日のインテの話とイェジンの話で、その良い例が2つあった。"
  },
  {
    "start": 711028,
    "end": 715262,
    "text": "教科書データで学習したコードモデルと言い換えモデル。"
  },
  {
    "start": 715316,
    "end": 720818,
    "text": "そのどちらも、一枚岩のジェネラリスト・モデルよりもはるかに小さなモデルで、もしかしたらうまく機能していたかもしれない。"
  },
  {
    "start": 720984,
    "end": 724574,
    "text": "まだ納得されていない方のために、もう少し例を挙げよう。"
  },
  {
    "start": 724702,
    "end": 734520,
    "text": "というのも、比較対象としているモノリシック・モデルはGPT-3であり、これは現在では恐ろしく時代遅れだからだ。"
  },
  {
    "start": 735450,
    "end": 740262,
    "text": "これは私たちが昨年ニューロプスに発表した論文からの引用である。"
  },
  {
    "start": 740406,
    "end": 776274,
    "text": "このプロットが提供する非常にハイレベルな要点は、合理的な事前学習済みモデルを使用し、非常にシンプルで標準化されたレシピでそれを微調整する場合です。"
  },
  {
    "start": 776472,
    "end": 783986,
    "text": "その結果、微調整されたモデルがよりうまく機能するようになり、精度が上がり、価格も劇的に安くなる。"
  },
  {
    "start": 784098,
    "end": 786786,
    "text": "劇的に安いのは、かなり小さいからだ。"
  },
  {
    "start": 786898,
    "end": 789474,
    "text": "このベースモデルには110億ものパラメータがある。"
  },
  {
    "start": 789522,
    "end": 793346,
    "text": "GBT3は最大1億7500万個のパラメータを持つが、フーゾットもある。"
  },
  {
    "start": 793378,
    "end": 798838,
    "text": "文脈学習は、予測を行う前に文脈内の例を処理しなければならないため、とんでもなくコストがかかる。"
  },
  {
    "start": 798934,
    "end": 803690,
    "text": "一方、この場合は微調整をした後、ゼロショットの予測をしているようなものだと考えればいい。"
  },
  {
    "start": 804270,
    "end": 809470,
    "text": "そう言っても何の意味もないが、要は、クエリーの例を入力するだけで、すぐに予測が得られるということだ。"
  },
  {
    "start": 809540,
    "end": 810160,
    "text": "そうだね。"
  },
  {
    "start": 813090,
    "end": 816580,
    "text": "この例では微調整のコストを考慮する必要があるのでは？"
  },
  {
    "start": 819670,
    "end": 820946,
    "text": "イエスでもありノーでもある。"
  },
  {
    "start": 821128,
    "end": 824194,
    "text": "ファインチューニングをすれば、コストは償却されるからだ。"
  },
  {
    "start": 824232,
    "end": 825122,
    "text": "もう二度とやる必要はない。"
  },
  {
    "start": 825176,
    "end": 826450,
    "text": "あなたには専門的なモデルがある。"
  },
  {
    "start": 826520,
    "end": 828386,
    "text": "微調整はある意味一度だけでいい。"
  },
  {
    "start": 828408,
    "end": 831526,
    "text": "フュージョンやコンテキスト学習を行う場合、データセットを毎回処理することになる。"
  },
  {
    "start": 831628,
    "end": 844694,
    "text": "微調整のコストを考えても、このモデルの微調整は、fuchsiaとGPT-3によるコンテキスト学習で1つの例を処理するよりも約16倍コストがかかる。"
  },
  {
    "start": 844742,
    "end": 846794,
    "text": "GPT-3を使えば、モデルを微調整できたはずだ。"
  },
  {
    "start": 846832,
    "end": 847706,
    "text": "もう二度とする必要はない。"
  },
  {
    "start": 847728,
    "end": 850454,
    "text": "すると突然、あなたのモデルは1000倍速くなる。"
  },
  {
    "start": 850502,
    "end": 854362,
    "text": "なるほど、これは1000倍の効率改善だ。"
  },
  {
    "start": 854426,
    "end": 857882,
    "text": "NLPでは、このX軸を描くことはもうないんだ。"
  },
  {
    "start": 857946,
    "end": 859146,
    "text": "このY軸だけを描く。"
  },
  {
    "start": 859178,
    "end": 860846,
    "text": "このX軸は非常に重要だ。"
  },
  {
    "start": 860948,
    "end": 863774,
    "text": "ああ、愚問だね。"
  },
  {
    "start": 863812,
    "end": 865870,
    "text": "微調整しているベースモデルは？"
  },
  {
    "start": 866210,
    "end": 871298,
    "text": "そう、ここではハイレベルな指摘をしているので詳しくは書かないが、このベースモデルはtゼロと呼ばれている。"
  },
  {
    "start": 871384,
    "end": 875220,
    "text": "Tファイブのマルチタスク版といったところか。"
  },
  {
    "start": 875590,
    "end": 879490,
    "text": "それは、それなりに有能な、事前に訓練された言語モデルのようなものだと考えればいい。"
  },
  {
    "start": 879650,
    "end": 889122,
    "text": "この場合、tゼロのトレーニングミックスから意図的に除外された、さまざまなタスクをカバーするタスクのサブセットである。"
  },
  {
    "start": 889186,
    "end": 890578,
    "text": "そんなことは覚えていないよ。"
  },
  {
    "start": 890604,
    "end": 896678,
    "text": "サーシャはタスクのリストを実際に覚えているのかもしれないが、NLPはNLPのデータセットだ。"
  },
  {
    "start": 896694,
    "end": 901914,
    "text": "繰り返しになるが、これはチャットGPT以前の作品であるため、時代遅れの作品であることを指摘しておきたい。"
  },
  {
    "start": 902032,
    "end": 905946,
    "text": "ポエトリー・ジェネレーションのような、オープンエンドな対話のことではない。"
  },
  {
    "start": 905978,
    "end": 908990,
    "text": "NLIみたいなもので、本当に退屈な学問的なものなんだ。"
  },
  {
    "start": 909060,
    "end": 914542,
    "text": "では、データをひとつ。"
  },
  {
    "start": 914596,
    "end": 917186,
    "text": "では、もう少し最新のものにしてみよう。"
  },
  {
    "start": 917288,
    "end": 922754,
    "text": "GPT-3について話す代わりに、パーム2の論文からいくつかの結果を引用する。"
  },
  {
    "start": 922792,
    "end": 927714,
    "text": "この論文は、特殊なモデルがうまく機能することを納得させようともしていない。"
  },
  {
    "start": 927832,
    "end": 931206,
    "text": "私はただ、彼らがこの表を新聞に掲載したことを指摘したいだけだ。"
  },
  {
    "start": 931388,
    "end": 936130,
    "text": "これは、彼らの常識的な理性的ベンチマーク表のようなものだ。"
  },
  {
    "start": 936210,
    "end": 938070,
    "text": "これが彼らの翻訳表だ。"
  },
  {
    "start": 938430,
    "end": 945194,
    "text": "というのも、彼らは常に最先端技術を引用しているからだ。"
  },
  {
    "start": 945312,
    "end": 947018,
    "text": "ちなみにAはGPT4。"
  },
  {
    "start": 947104,
    "end": 960142,
    "text": "ジェネラリスト的なモノリスモデルが勝つこともあるが、特にこの2つのデータセットでは、その差は些細なものではなく、スペシャリスト的なモデルが優位に立っている。"
  },
  {
    "start": 960196,
    "end": 962542,
    "text": "これは微調整されたバートモデルのようなものなんだ。"
  },
  {
    "start": 962596,
    "end": 966730,
    "text": "これらは非常に安価で、効果的な翻訳モデルである。"
  },
  {
    "start": 966810,
    "end": 973806,
    "text": "実際、彼らはいつもグーグル翻訳に勝っているが、間違いなく、つまり、グーグル翻訳をスペシャリストモデルと呼ぶのはおかしいが、翻訳しかできない。"
  },
  {
    "start": 973998,
    "end": 975282,
    "text": "スペシャリスト・モデルである。"
  },
  {
    "start": 975336,
    "end": 983282,
    "text": "グーグル翻訳やパームツーのシステムの詳細が分からないので、実際のところは分からないが、グーグル翻訳を使った推論の方がずっと安上がりなのではないだろうか。"
  },
  {
    "start": 983346,
    "end": 983814,
    "text": "オーケー。"
  },
  {
    "start": 983932,
    "end": 991802,
    "text": "パーム2は常にグーグル翻訳に勝っている。"
  },
  {
    "start": 991856,
    "end": 995466,
    "text": "スペシャリストモデルではないが、その差は非常に小さい。"
  },
  {
    "start": 995568,
    "end": 995930,
    "text": "そうだね。"
  },
  {
    "start": 996000,
    "end": 999626,
    "text": "より効率的で、うまく機能する専門的なモデルがある。"
  },
  {
    "start": 999728,
    "end": 1003550,
    "text": "繰り返すが、これはこの点を指摘しようともしていない論文からの抜粋である。"
  },
  {
    "start": 1003620,
    "end": 1014270,
    "text": "繰り返しになるが、このことをあまり深く考えたくはない。この時点で少なくとも、スペシャリストのモデルはより安く、より効果的なものになり得るということを、みなさんが思い描くことができればいいと思うからだ。"
  },
  {
    "start": 1014610,
    "end": 1015706,
    "text": "ドロップはバートだと思う。"
  },
  {
    "start": 1015738,
    "end": 1017266,
    "text": "電卓でしょ？"
  },
  {
    "start": 1017368,
    "end": 1017730,
    "text": "もちろんだ。"
  },
  {
    "start": 1017800,
    "end": 1018082,
    "text": "そうだね。"
  },
  {
    "start": 1018136,
    "end": 1020498,
    "text": "専門家モデルは電卓でも何でも使える。"
  },
  {
    "start": 1020584,
    "end": 1021860,
    "text": "ああ、もちろんだ。"
  },
  {
    "start": 1025830,
    "end": 1028450,
    "text": "特殊なモデルには他にも欠点がある。"
  },
  {
    "start": 1029510,
    "end": 1034962,
    "text": "サイズもいいし、性能もいい。"
  },
  {
    "start": 1035106,
    "end": 1038034,
    "text": "攻撃したければ、もっと簡単だ。"
  },
  {
    "start": 1038162,
    "end": 1041130,
    "text": "そうだね、それを特徴づけるのは難しいと思う。"
  },
  {
    "start": 1041550,
    "end": 1042362,
    "text": "それはいい指摘だ。"
  },
  {
    "start": 1042416,
    "end": 1043526,
    "text": "それを特徴づけるのは難しいと思う。"
  },
  {
    "start": 1043558,
    "end": 1046070,
    "text": "これらのモデルはすべて攻撃可能だと思う。"
  },
  {
    "start": 1046150,
    "end": 1046442,
    "text": "そうだね。"
  },
  {
    "start": 1046496,
    "end": 1064020,
    "text": "特に最近は、大規模な言語モデルに対する敵対的な例と呼ばれるようなデモがあり、脱獄のようなことだけでなく、仮定を別の方法で記述することによって、モデルに別の予測を出力させるようなこともできる。"
  },
  {
    "start": 1066310,
    "end": 1078466,
    "text": "しかし、それは良い点だと思うし、全体として考えるのは興味深いことだと思う。"
  },
  {
    "start": 1078648,
    "end": 1085138,
    "text": "また、モデルの能力を組み合わせることで、ロバスト性が向上することが示されている。"
  },
  {
    "start": 1085234,
    "end": 1087910,
    "text": "そのことについては全く話しませんが、喜んでヒントを提供します。"
  },
  {
    "start": 1089470,
    "end": 1089930,
    "text": "クールだ。"
  },
  {
    "start": 1090000,
    "end": 1094698,
    "text": "ベースライン、スペシャリスト・モデル、合理的なスタート地点だ。"
  },
  {
    "start": 1094784,
    "end": 1099414,
    "text": "さて、これはディギーの言う通り、必要ないことだ。"
  },
  {
    "start": 1099542,
    "end": 1109406,
    "text": "スペシャリスト・モデルというのは、必ずしもそういう意味ではないということだ。"
  },
  {
    "start": 1109428,
    "end": 1116578,
    "text": "何十万ものまったく異なるモデル・アーキテクチャがあり、共通点は何もない。"
  },
  {
    "start": 1116744,
    "end": 1125442,
    "text": "また、共通のベースモデルに対して安価で伝達可能なアップデートを行うことで、専門的なモデルを構築することも可能である。"
  },
  {
    "start": 1125496,
    "end": 1126786,
    "text": "だからといって、このやり方でやらなければいけないというわけではない。"
  },
  {
    "start": 1126808,
    "end": 1129480,
    "text": "この方法を選択すれば、これは嬉しいボーナスに過ぎない。"
  },
  {
    "start": 1130010,
    "end": 1144074,
    "text": "つまり、先ほど私が自慢したTfueの論文に戻ると、これらの結果は、フルモデル・ファインチューニングではなく、むしろ皆さんの多くがよくご存知の、周辺効率ファインチューニングと呼ばれるものから得られたものなのです。"
  },
  {
    "start": 1144112,
    "end": 1147510,
    "text": "例えばロラは、パラメータを効率的に微調整する方法、つまりプロンプトチューニングだ。"
  },
  {
    "start": 1147590,
    "end": 1151786,
    "text": "これは、我々がIa 3と呼んでいる、もうひとつのパラメータ効率的な微調整方法だった。"
  },
  {
    "start": 1151968,
    "end": 1159562,
    "text": "Ia threeの考え方は、トランスフォーマーの中間的なアクティベーションを取り出し、要素ごとに学習されたベクトルを掛けるというものだ。"
  },
  {
    "start": 1159706,
    "end": 1169694,
    "text": "重要なのは、これらのベクトルをいくつか導入するだけで、モデルのすべてのパラメーターを更新するわけではないので、実際にはパラメーターの数のごく一部を更新するだけだということだ。"
  },
  {
    "start": 1169742,
    "end": 1174398,
    "text": "フルモデル・ファインチューニングとして、特に数ショットのセッティングではかなりうまくいっている。"
  },
  {
    "start": 1174414,
    "end": 1186246,
    "text": "実際、モデルの全パラメータを更新するよりも優れているので、このケースではモデルを微調整するためのより良い方法であり、他のパラメータを効率的に微調整する方法よりも優れている。"
  },
  {
    "start": 1186428,
    "end": 1196454,
    "text": "要点は、ここでのベースモデルはTゼロで、ディスクに保存するには、保存方法にもよるが、数十ギガバイトかかるということだ。"
  },
  {
    "start": 1196582,
    "end": 1199078,
    "text": "IAの3つのベクトルを保存するのに数メガバイトかかる。"
  },
  {
    "start": 1199094,
    "end": 1202174,
    "text": "フロッピーディスクに収まるほど小さくはないが、そう遠くはない。"
  },
  {
    "start": 1202212,
    "end": 1219922,
    "text": "先ほどストレージの話が出ましたが、1万個の専門家モデルを保存する場合、IA3ベクトルをトレーニングして専門家モデルを作れば、ストレージスペースはそれほどかかりません。"
  },
  {
    "start": 1220056,
    "end": 1225770,
    "text": "実際、後で簡単に説明するが、分散推論という点では、よりシンプルになる。"
  },
  {
    "start": 1225950,
    "end": 1241546,
    "text": "また、このようなことをする必要はありません。もしあなたがよく知らないのであれば、私が説明しているようなエコシステムではなく、モデルの自動的な組み換えやルーティングなどによる巨大なエコシステムをすでに構築している人たちがいます。"
  },
  {
    "start": 1241568,
    "end": 1249654,
    "text": "少なくとも、パラメーターの効率的な微調整によって微調整されたモデルの大きなコレクションは、人々がよくアダプターのように呼ぶものだ。"
  },
  {
    "start": 1249782,
    "end": 1261630,
    "text": "ハグする顔のモデルハブで、パラメータ効率微調整ライブラリであるpeftを使用してアップロードされたモデルを探すと、4500のモデルがあることがわかる。"
  },
  {
    "start": 1261700,
    "end": 1264754,
    "text": "アダプターハブ（別のレポジトリ）には500ほどある。"
  },
  {
    "start": 1264872,
    "end": 1277298,
    "text": "今回は言語モデルのワークショップなので、このような例は挙げなかったが、例えば安定した普及のためのアダプターのリポジトリには、毎日何千ものアップロードがある。"
  },
  {
    "start": 1277464,
    "end": 1295034,
    "text": "これらのアダプターは、多くの場合Loraをベースにしていて、これもまたパラメータを効率的に微調整する方法なのですが、安定した拡散を利用して、常にアニメのようなものを出力するようにしたり、特定の種類の背景を出力するのが得意になるようにしたり、そのようなことをするのです。"
  },
  {
    "start": 1295072,
    "end": 1296634,
    "text": "何をしているのか？"
  },
  {
    "start": 1296752,
    "end": 1302394,
    "text": "安定した拡散のような汎用のモノリシックなモデルを、スペシャリストのモデルに変えているのだ。"
  },
  {
    "start": 1302442,
    "end": 1303038,
    "text": "いいかい？"
  },
  {
    "start": 1303204,
    "end": 1313790,
    "text": "ある意味、このようなモデル・ハブに頼れば、エコシステムを構築するために使用する専門家モデルのコレクションをすでに多く持っていることになる。"
  },
  {
    "start": 1313870,
    "end": 1321186,
    "text": "さて、これが2つ目のポイント、3つ目のポイントだ。"
  },
  {
    "start": 1321368,
    "end": 1337378,
    "text": "先に述べた重要な要素とは、エコシステムをジェネラリスト・モデルの代わりとして扱いたいのであれば、与えられたクエリを実行するためにどのスペシャリスト・モデルを使うかを選択する方法が必要だということだ。"
  },
  {
    "start": 1337474,
    "end": 1338214,
    "text": "いいかい？"
  },
  {
    "start": 1338412,
    "end": 1341814,
    "text": "これは、能力を組み合わせるためにモデルを組み合わせることにつながる。"
  },
  {
    "start": 1341862,
    "end": 1344326,
    "text": "少なくとも、このルーティングを自動的に行う方法が必要だ。"
  },
  {
    "start": 1344358,
    "end": 1352650,
    "text": "つまり、適切なモデルを選択することをユーザーに依存することも考えられるが、それは負担が大きいかもしれない。"
  },
  {
    "start": 1354350,
    "end": 1360046,
    "text": "GPT fourを使うと、可能性のあるリクエストのリストをスクロールして選ぶ必要がない。"
  },
  {
    "start": 1360068,
    "end": 1362126,
    "text": "どんなリクエストにも応えてくれる。"
  },
  {
    "start": 1362148,
    "end": 1369410,
    "text": "もし本当にジェネラリストモデルを置き換えたいのであれば、スペシャリストモデルを自動的に選択する方法が必要だ。"
  },
  {
    "start": 1370390,
    "end": 1374434,
    "text": "では、それができるかもしれないという証拠をお見せしましょう。"
  },
  {
    "start": 1374632,
    "end": 1384530,
    "text": "最初の証拠は、tasktevecに掲載された6、7年前の古い論文である。"
  },
  {
    "start": 1384690,
    "end": 1415434,
    "text": "このベクトルは、フィッシャー情報行列の対角の要約のようなものとして計算されます。混乱しているように聞こえますが、実際にやっていることは、データセットに対する期待値で計算されたパラメータに対するモデルの出力の勾配を測定し、その量を要約して、それをタスクの表現として使っているのです。"
  },
  {
    "start": 1415562,
    "end": 1418894,
    "text": "なぜこれがタスクを表すのに便利なのか？"
  },
  {
    "start": 1419022,
    "end": 1429478,
    "text": "というのも、モデルを同じように変化させる2つのデータセットがあり、勾配を計算するときにパラメータを同じように更新させるのであれば、それらのデータセットは似ているかもしれない。"
  },
  {
    "start": 1429564,
    "end": 1431720,
    "text": "彼らはモデルに似たようなことをさせている。"
  },
  {
    "start": 1432410,
    "end": 1435186,
    "text": "本稿では、主に画像分類に焦点を当てている。"
  },
  {
    "start": 1435298,
    "end": 1452394,
    "text": "これらのタスクをVecベクトルに使用すると、衣服に関連する画像分類タスクのように、ある領域にクラスター化され、色によって別の領域にクラスター化され、異なる種は別の領域に入ることがわかった。"
  },
  {
    "start": 1452432,
    "end": 1455450,
    "text": "これは一種の衣類の仕事であり、それらは種の仕事だ。"
  },
  {
    "start": 1455530,
    "end": 1467458,
    "text": "詳細はさほど重要ではないが、与えられたデータやデータの塊に対して、どのモデルが適切であるかという概念を関連付ける方法があるということを除いては。"
  },
  {
    "start": 1467544,
    "end": 1472690,
    "text": "オーケー、詳しくはその論文を参照してもらえれば嬉しい。"
  },
  {
    "start": 1473270,
    "end": 1492374,
    "text": "面白いのは、パラメータ効率微調整のおまけとして、最近の論文によると、パラメータ効率微調整から吐き出される値を使えば、どの方法を使うかにもよるが、タスクのいい表現にもなるということだ。"
  },
  {
    "start": 1492492,
    "end": 1499098,
    "text": "ベクターを積み上げれば、それほど長いベクターにはならない。"
  },
  {
    "start": 1499264,
    "end": 1508778,
    "text": "あるタスクのIA3ベクトルと別のタスクのIA3ベクトルを比較すると、その類似性は実はタスクの類似性の妥当な概念を表している。"
  },
  {
    "start": 1508874,
    "end": 1519300,
    "text": "よし、これでまた、データを特定の適切なモデルに関連付ける方法を無料で手に入れたようなものだ。"
  },
  {
    "start": 1520550,
    "end": 1534514,
    "text": "さて、特化されたモデル間のルーティングについて最後に申し上げるのは、まさに先ほどディーが申し上げた点で、特化されたネットワーク間の自動ルーティングを行うモデル・アーキテクチャの例がすでにあるということです。"
  },
  {
    "start": 1534642,
    "end": 1540482,
    "text": "この場合、私たちは彼らをエキスパートと呼んでいるが、この場合、彼らは実際には大きなモデルのサブレイヤーのようなものだ。"
  },
  {
    "start": 1540636,
    "end": 1554878,
    "text": "エキスパートスタイルの混合モデルは、モデル内のいくつかの中間アクティブを入力とし、そのアクティブをどのサブネットワークに送るかを適応的に選択するルーターを持つモデルと考えることができる。"
  },
  {
    "start": 1555044,
    "end": 1563726,
    "text": "これはモデルのエコシステムというより、レイヤーのようなサブモデルのエコシステムかもしれないが、これらはかなり効果的だ。"
  },
  {
    "start": 1563758,
    "end": 1575810,
    "text": "このようなエコシステムにとって望ましいと私が考える挙動を示す、エキスパート・スタイルの混合アーキテクチャについて、いくつかのスライドでお話ししよう。"
  },
  {
    "start": 1577050,
    "end": 1582840,
    "text": "特化したネットワークの専門家を選ぶことが有効であることを示す証拠があればいいのだが......。"
  },
  {
    "start": 1585290,
    "end": 1598390,
    "text": "さて、あるクエリに対してどのモデルを使うべきか選択する方法を教えてほしい。"
  },
  {
    "start": 1598550,
    "end": 1607070,
    "text": "また、いろいろなスキルを必要とする仕事があるとしたら？"
  },
  {
    "start": 1607220,
    "end": 1608186,
    "text": "だから本当に嬉しい。"
  },
  {
    "start": 1608218,
    "end": 1617906,
    "text": "私たちはすでに、タスクがスキルの構成要素としてどのように考えられるかについて議論した。"
  },
  {
    "start": 1617928,
    "end": 1620210,
    "text": "そして、それらはまさに千のスキルなのだ。"
  },
  {
    "start": 1621430,
    "end": 1631666,
    "text": "つまり、少なくとも概念的には、私たちが実行したいと思うようなタスクは、スキルの組み合わせとして考えることができるということだ。"
  },
  {
    "start": 1631698,
    "end": 1654730,
    "text": "例えば、エンパイアステートビルのてっぺんから1円玉が地面に落ちるのに何分かかるか、というクエリでは、物理的な推論、世界知識、算数が必要になるかもしれません。例えば、世界知識のようなこれらのスキルの多くは、それが実際のスキルであると考えるなら、その特定のスキルに焦点を当てたタスクにカプセル化することもできます。"
  },
  {
    "start": 1654890,
    "end": 1665646,
    "text": "例えば、何千という専門モデルの中で、ある特定の技術を実行できる専門モデルを持っていることがよくあるとしよう。"
  },
  {
    "start": 1665758,
    "end": 1671870,
    "text": "ここで問題になるのは、各モデルの能力を組み合わせる方法はあるのかということだ。"
  },
  {
    "start": 1671950,
    "end": 1672194,
    "text": "そうだろう？"
  },
  {
    "start": 1672232,
    "end": 1685138,
    "text": "というのも、先ほどサンジーブが素晴らしい議論をしていたが、スケールのメリットのひとつは、より多くのスキルを組み合わせることで、ある種の構成的一般化を行う能力を高めることだ。"
  },
  {
    "start": 1685234,
    "end": 1690860,
    "text": "3つのスキルを組み合わせることから、規模を拡大するにつれて4つのスキルを組み合わせるようになる。"
  },
  {
    "start": 1691470,
    "end": 1695846,
    "text": "これはモノリシック・モデルの信じられないほど便利な機能だ。"
  },
  {
    "start": 1695878,
    "end": 1703370,
    "text": "エコシステムに同じ能力を持たせたいのであれば、専門家モデルを組み合わせてスキルを構成的に組み合わせる方法が必要だ。"
  },
  {
    "start": 1703450,
    "end": 1707934,
    "text": "さて、それはどんなものだろう？"
  },
  {
    "start": 1708052,
    "end": 1720046,
    "text": "まあ、専門化されたモデルを組み合わせて、専門化されたモデルの個々のタスクを実行できる複合モデルを作ることを、私はこのアクションと呼ぶことにしよう。"
  },
  {
    "start": 1720078,
    "end": 1722654,
    "text": "私はそれをモデルの融合と呼ぶつもりだ。"
  },
  {
    "start": 1722782,
    "end": 1728022,
    "text": "私がこの用語を選んだ理由は後で明らかになるかもしれないが、この分野では一般的な用語でもある。"
  },
  {
    "start": 1728156,
    "end": 1736434,
    "text": "合併について重要なのは、もしこれができれば、モデル間で能力を移転するための新たな道が開けるということだ。"
  },
  {
    "start": 1736482,
    "end": 1743210,
    "text": "というのも、通常、モデルに特定の能力を持たせたい場合、トレーニングを行いますよね？"
  },
  {
    "start": 1743360,
    "end": 1749530,
    "text": "昔の転移学習では、特定のタスクを表すデータセットに対して微調整を行っていた。"
  },
  {
    "start": 1749870,
    "end": 1758026,
    "text": "現在のところ、私たちはただ規模を拡大するだけで、モデルがデータのどこかでそのタスクを学習し、他のタスクと合成できることを期待している。"
  },
  {
    "start": 1758218,
    "end": 1774734,
    "text": "例えば、2つの特化されたモデルを組み合わせて複合モデルを作ることができる。"
  },
  {
    "start": 1774862,
    "end": 1778866,
    "text": "また、より変わったモデルの組み合わせ方も可能になる。"
  },
  {
    "start": 1778898,
    "end": 1778998,
    "text": "そうだね。"
  },
  {
    "start": 1779004,
    "end": 1792770,
    "text": "私たちのエコシステムで想像してほしいのは、逐次勾配ベースのトレーニングによって得られたモデルがあり、その元のモデルを別のデータセットでトレーニングすると、これらのモデルをマージして新しい複合モデルを得ることができるということだ。"
  },
  {
    "start": 1792860,
    "end": 1795546,
    "text": "なるほど、合併することでそれが可能になるかもしれない。"
  },
  {
    "start": 1795568,
    "end": 1800886,
    "text": "これから述べるいくつかの異なる方法を説明するために、一般的な用語としてマージという言葉を使うつもりだ。"
  },
  {
    "start": 1800928,
    "end": 1807770,
    "text": "そう、それぞれの円は、アダプター・モデルのひとつであり、特殊なモデル・パラメーターだと考えるべきだ。"
  },
  {
    "start": 1807850,
    "end": 1808382,
    "text": "その通りだ。"
  },
  {
    "start": 1808516,
    "end": 1816074,
    "text": "繰り返しになるが、これから説明する方法のほとんどは、マージされるモデル間のモデル・アーキテクチャが一様であることを前提としている。"
  },
  {
    "start": 1816122,
    "end": 1816766,
    "text": "一貫している。"
  },
  {
    "start": 1816798,
    "end": 1820050,
    "text": "これらはパラメータ間の1対1の対応と同じアーキテクチャを持っている。"
  },
  {
    "start": 1820870,
    "end": 1826414,
    "text": "この仮定をしない合併方法も考えられるが、すべての合併方法がそうであり、パラメーターを効率的に使う必要はない。"
  },
  {
    "start": 1826462,
    "end": 1828242,
    "text": "微調整されたモデルである可能性もある。"
  },
  {
    "start": 1828376,
    "end": 1832070,
    "text": "そのための最も妥当な道筋を考えるためだ。"
  },
  {
    "start": 1832140,
    "end": 1834920,
    "text": "確かに、私たちはそれらをパラメータ効率の良い方法と考えることもできる。"
  },
  {
    "start": 1835450,
    "end": 1844550,
    "text": "この能力を示す、問題なく機能することが示されているマージに見えるものの例を挙げよう。"
  },
  {
    "start": 1844630,
    "end": 1876066,
    "text": "この構成的汎化能力は、破滅的忘却とゼロショット・クロス・シングル・ジェネレーションの克服という素晴らしい論文に由来している。そこでは、プロンプト・チューニングと呼ばれるパラメーターの効率的な微調整の1つの特殊な形式を使って訓練した場合、パラメーターの効率的なアダプター、つまり言語とタスクの学習済みプロンプトを訓練した場合、それらのプロンプト・ベクトルを構成し、新しい言語タスクの組み合わせに汎化できることを示している。"
  },
  {
    "start": 1876178,
    "end": 1876838,
    "text": "そうだろう？"
  },
  {
    "start": 1877004,
    "end": 1888706,
    "text": "大量のベトナム語データで言語モデルをトレーニングし、このアダプター、このプロンプト、この学習済みプロンプトのパラメーターだけを、このパラメーターの効率的な微調整方法で更新する。"
  },
  {
    "start": 1888818,
    "end": 1893194,
    "text": "同じ言語モデルを使い、同じようにさまざまなタスクで訓練する。"
  },
  {
    "start": 1893312,
    "end": 1904686,
    "text": "私のモデルにベトナム語でタスク2を実行させたい場合、これら2つのプロンプトベクトルを連結するだけで、ベトナム語でタスクを合理的に実行するモデルを得ることができる。"
  },
  {
    "start": 1904788,
    "end": 1906334,
    "text": "よし、これでうまくいく。"
  },
  {
    "start": 1906372,
    "end": 1908480,
    "text": "なるほど、興味深い論文だ。"
  },
  {
    "start": 1910210,
    "end": 1912202,
    "text": "もう一つの例、タスク・ベクター。"
  },
  {
    "start": 1912266,
    "end": 1919860,
    "text": "これはとてもシンプルで、驚くほどうまくいくので、とても楽しいアイデアだ。"
  },
  {
    "start": 1920710,
    "end": 1929970,
    "text": "事前学習したモデルを微調整して、微調整したパラメータ値を取って、事前学習した値を引く。"
  },
  {
    "start": 1930050,
    "end": 1936774,
    "text": "モデルを微調整したときのパラメータ空間での動きを表すベクトルを取得しているんだ。"
  },
  {
    "start": 1936892,
    "end": 1939106,
    "text": "これを私のタスク・ベクターと呼ぶことにする。"
  },
  {
    "start": 1939298,
    "end": 1942934,
    "text": "今、私はこのタスクベクトルを使って、あらゆる奇妙なことができる。"
  },
  {
    "start": 1943062,
    "end": 1958826,
    "text": "例えば、言語モデルを有毒なテキストの束で訓練して、ここからここまで行ってタスクベクトルを計算し、それを否定して訓練前のモデルから差し引くと、言語モデルは有毒なタスクを生成しなくなる。"
  },
  {
    "start": 1959018,
    "end": 1960080,
    "text": "とても興味深い。"
  },
  {
    "start": 1961570,
    "end": 1968354,
    "text": "つまり、これはとてもクールなんだけど、今日の話とはちょっと違うんだ。"
  },
  {
    "start": 1968472,
    "end": 1979366,
    "text": "モデルにタスクを合成させたい場合は、2つのタスクのタスク・ベクタを取り、それらを足し合わせれば、マルチタスク・モデルができあがる。"
  },
  {
    "start": 1979468,
    "end": 1981830,
    "text": "では、また別の例を。"
  },
  {
    "start": 1981900,
    "end": 1993894,
    "text": "この地点とこの地点に対応する特化したモデルがあり、それぞれのモデルが単独で実行できるタスクを実行できるマルチタスク・モデルを得たい場合、それぞれのタスク・ベクトルを足し合わせるだけでいい。"
  },
  {
    "start": 1993942,
    "end": 1994346,
    "text": "そうだね。"
  },
  {
    "start": 1994448,
    "end": 1996698,
    "text": "これには追加のトレーニングは必要ない。"
  },
  {
    "start": 1996864,
    "end": 1997530,
    "text": "オーケー。"
  },
  {
    "start": 1997680,
    "end": 2001230,
    "text": "どのような入力クエリに対しても適応的に行うことができる。"
  },
  {
    "start": 2003410,
    "end": 2004782,
    "text": "ちょっと行ってみよう。"
  },
  {
    "start": 2004916,
    "end": 2007054,
    "text": "もしかしたら、もう少し驚かせることができるかもしれない。"
  },
  {
    "start": 2007092,
    "end": 2011070,
    "text": "まだ驚いていないなら、もしかしたらまったく驚かないかもしれない。"
  },
  {
    "start": 2011140,
    "end": 2015998,
    "text": "変化は小さいから、必ずしも小さいとは限らない。"
  },
  {
    "start": 2016084,
    "end": 2018306,
    "text": "この仕事については触れなかった。"
  },
  {
    "start": 2018408,
    "end": 2023278,
    "text": "このトークでは、低ランクであったり、固有次元が低かったりすることが判明した。"
  },
  {
    "start": 2023374,
    "end": 2024020,
    "text": "そうだろう？"
  },
  {
    "start": 2025430,
    "end": 2040710,
    "text": "ニューラルネットワークと勾配ベースのトレーニングについて奇妙なことのひとつは、ニューラルネットワークを長く長くトレーニングしていくと、損失が最小値にプラトーになっても、パラメーターの値はしばしば大きくなったり小さくなったり変化し続けるということだ。"
  },
  {
    "start": 2040780,
    "end": 2045978,
    "text": "モデルはパラメータ空間の中で、損失が少ない領域を周回しているようなものだ。"
  },
  {
    "start": 2046144,
    "end": 2049706,
    "text": "また、ニューラルネットワークにはおかしな対称性がある。"
  },
  {
    "start": 2049728,
    "end": 2055582,
    "text": "例えば、あるレイヤーのウェイトを上げ、別のレイヤーのウェイトを下げても、ネットワークは同じ関数を計算する。"
  },
  {
    "start": 2055716,
    "end": 2058842,
    "text": "私は彼らに巨大な価値を持たせることができる。"
  },
  {
    "start": 2058906,
    "end": 2059134,
    "text": "そうだろう？"
  },
  {
    "start": 2059172,
    "end": 2067890,
    "text": "タスクベクトルの実際の大きさは非常に大きくなる可能性があるが、実際には、例えば、固有次元は低い。"
  },
  {
    "start": 2072870,
    "end": 2087510,
    "text": "NLPタスクの演算は楽しいものだが、非常に複雑な組み合わせや合成を記述できる言語ができた今、それはワードからベックの時代よりもずっと豊かな表層のように感じられる。"
  },
  {
    "start": 2087660,
    "end": 2091560,
    "text": "なぜ、今ある言葉を使わないのか？"
  },
  {
    "start": 2093210,
    "end": 2108282,
    "text": "でも、タスク命令、データセット、モデルの共同埋め込み空間を構築するのは、とてもクールだと思います。"
  },
  {
    "start": 2108426,
    "end": 2109022,
    "text": "そうだろう？"
  },
  {
    "start": 2109156,
    "end": 2111338,
    "text": "特殊なモデルをたくさん持っているとしよう。"
  },
  {
    "start": 2111434,
    "end": 2119220,
    "text": "モデルが実行するタスクの記述から、そのモデルのベクトルに近いベクトルまで持っていく方法が欲しいんだよね。"
  },
  {
    "start": 2120070,
    "end": 2134390,
    "text": "あるいは、タスクの例をいくつか持っているのですが、たぶん私は違う質問をしているのだと思います。このようなことはすべて、私たちが仮定する必要のない、また人々がタスクを説明するときに自然に仮定するような、ある種の直線性を仮定しているのです。"
  },
  {
    "start": 2134970,
    "end": 2143818,
    "text": "リニアリティという点では、あなたはタスクのメトリック空間を持っている。"
  },
  {
    "start": 2143904,
    "end": 2145434,
    "text": "何か見落としているのかもしれない。"
  },
  {
    "start": 2145552,
    "end": 2152574,
    "text": "まあ、タスクの組み合わせが直線的であることは、これが機能するための必要条件とは言えないだろう。"
  },
  {
    "start": 2152612,
    "end": 2164610,
    "text": "私が言いたいのは、興味深いことに、モデルをトレーニングすると、実際にそういうことが起こるということだ。"
  },
  {
    "start": 2165030,
    "end": 2165986,
    "text": "オーケー、フェアだ。"
  },
  {
    "start": 2166088,
    "end": 2166740,
    "text": "そうだね。"
  },
  {
    "start": 2172230,
    "end": 2179906,
    "text": "メトリザビリティに関する質問と、メトリクスのない、割り当てられたものを一般化するのかどうかについてのフォローアップをしたい。"
  },
  {
    "start": 2179938,
    "end": 2185640,
    "text": "ベクトルを扱うように、何かを扱っている例はありますか？"
  },
  {
    "start": 2187210,
    "end": 2200662,
    "text": "私の知る限り、タスクベクターの論文は比較的最近のもので、これは信じられないほどシンプルで、驚くほどうまく機能しているが、おそらく十分ではない例だと思う。"
  },
  {
    "start": 2200726,
    "end": 2203020,
    "text": "たぶん、それがあなたの質問の答えにもなるよね？"
  },
  {
    "start": 2203710,
    "end": 2216020,
    "text": "実際には、すでに人々がモデルをマージしており、それなりにうまく機能していることをお見せした後で、私のグループで、単にタスクベクトルを足し合わせるだけではない、より洗練されたマージの方法について説明します。"
  },
  {
    "start": 2216710,
    "end": 2219860,
    "text": "という疑問の答えになるかもしれない。"
  },
  {
    "start": 2220230,
    "end": 2223970,
    "text": "繰り返しになるが、もしかしたら驚くかもしれないし、驚かないかもしれない。"
  },
  {
    "start": 2224310,
    "end": 2245030,
    "text": "この最近の論文では、共有トランスフォーマー、バックボーン・トレイン、テキスト・トランスフォーマー、ビジョン・トランスフォーマーを使って、それらを統合することが可能であることを示した。マルチモーダルモデルとは、テキストと画像のための共有表現空間を持つモデルのことで、画像と言語のタスクのような標準的なタスクを実行することができる。"
  },
  {
    "start": 2245190,
    "end": 2257566,
    "text": "モデルのパラメーターを平均化するだけでも、タスクベクトル演算でも、モダリティ演算と呼ばれるものでも、もっと洗練されたものでも、うまくいくことがわかった。"
  },
  {
    "start": 2257748,
    "end": 2258910,
    "text": "これは大丈夫だ。"
  },
  {
    "start": 2258980,
    "end": 2261950,
    "text": "完璧ではないが、コンセプトの証明としては存在する。"
  },
  {
    "start": 2262630,
    "end": 2268222,
    "text": "画像の処理とテキストの処理では、異なるスキルが必要だと考える人もいるだろう。"
  },
  {
    "start": 2268286,
    "end": 2273890,
    "text": "ここでは、組み合わせて安くコンポジット・モデルを手に入れることができるスキルをいくつか紹介しよう。"
  },
  {
    "start": 2274970,
    "end": 2283186,
    "text": "実際、私はこのような専門モデルのコミュニティ・コレクションについて述べたことがある。"
  },
  {
    "start": 2283378,
    "end": 2288706,
    "text": "人々はすでにこれらのモデルを統合し、個々のモデルの能力を内在させた複合モデルを作っている。"
  },
  {
    "start": 2288738,
    "end": 2294380,
    "text": "このように構成されていると説明されている、抱き合う顔に関する言語モデルの例である。"
  },
  {
    "start": 2294910,
    "end": 2297590,
    "text": "私はこの構文を理解していない。"
  },
  {
    "start": 2297670,
    "end": 2317070,
    "text": "彼らはこの構文をまったく定義していないが、この構文は、個々のモデル、この場合はローラの、パラメーターを効率的に更新するモデルを、さまざまな方法で足し合わせることを含む。"
  },
  {
    "start": 2317150,
    "end": 2323122,
    "text": "繰り返しになるが、ここでは安定した拡散の例を挙げていないが、安定した拡散のコミュニティでは、これはさらに一般的なことである。"
  },
  {
    "start": 2323256,
    "end": 2339990,
    "text": "例えば、先ほど私は、漫画のような画像を作るロラ、シンセパンクの背景を作るもの、本当にきれいな髭を生やした人物を作るものなど、パラメーターを効率的に微調整するアップデートと安定した拡散のためのアダプターを持っているかもしれないと言いました。"
  },
  {
    "start": 2340150,
    "end": 2349750,
    "text": "私はこれらのアダプターを組み合わせ、安定した拡散バリアントというモデルを作ることができる。"
  },
  {
    "start": 2349910,
    "end": 2351870,
    "text": "これはもう超普通のことだ。"
  },
  {
    "start": 2351940,
    "end": 2353840,
    "text": "つまり、コンセプトの証明だ。"
  },
  {
    "start": 2354850,
    "end": 2370434,
    "text": "でも、ただ足し算をしたり、平均化したりするだけでは、洗練されたことをやろうとするときには不十分なのかもしれない。"
  },
  {
    "start": 2370472,
    "end": 2379590,
    "text": "私の研究室と共同研究者たちによる、より洗練されたモデルの統合方法に関する2つの研究について話そう。"
  },
  {
    "start": 2380010,
    "end": 2385318,
    "text": "この最初の研究は、モデル・マージを最適化問題として形式化したものである。"
  },
  {
    "start": 2385484,
    "end": 2393862,
    "text": "このように定式化すると、この目的を最大化するパラメータ値を見つけようとする、ということになる。"
  },
  {
    "start": 2394006,
    "end": 2401830,
    "text": "この目的は、統合される個々のモデルの事後評価の合計である。"
  },
  {
    "start": 2401990,
    "end": 2413130,
    "text": "このハイパーパラメーター、ラムダ・サブIは、モデル固有のハイパーパラメーターであり、チューニングすることができる。"
  },
  {
    "start": 2413290,
    "end": 2422686,
    "text": "このように定式化するのが難しいのは、最尤法でニューラルネットワークを訓練するとき、実際にはニューラルネットワークの事後分布を知ることができないということだ。"
  },
  {
    "start": 2422718,
    "end": 2424558,
    "text": "私たちが持っているのは推定値だ。"
  },
  {
    "start": 2424734,
    "end": 2429990,
    "text": "だから、私が説明している仕事では、ラプラス近似を使っている。"
  },
  {
    "start": 2430730,
    "end": 2440134,
    "text": "馴染みのない方のために説明しておくと、我々は基本的に最尤訓練で求めたパラメータ値を中心に、モード周りの事後分布を2次のテイラー展開をしている。"
  },
  {
    "start": 2440252,
    "end": 2455210,
    "text": "これは、パラメータ値が、最尤学習で求めたパラメータ値を平均とするガウス分布であり、精度行列がヘシアンであると仮定することに対応する。"
  },
  {
    "start": 2455290,
    "end": 2463994,
    "text": "このような大規模なニューラルネットワークではヘシアンを計算できないので、ヘシアンに非常に近い亀裂情報を使う。"
  },
  {
    "start": 2464042,
    "end": 2468158,
    "text": "事後モードでは、亀裂も計算できないので、対角線の亀裂を使う。"
  },
  {
    "start": 2468254,
    "end": 2469886,
    "text": "これらはどうでもいいことだ。"
  },
  {
    "start": 2469918,
    "end": 2489542,
    "text": "重要なのは、モデルの事後分布に対してこの近似を行えば、この最大化問題を閉形式で解くことができ、モデルパラメータに対する閉形式の解が得られるということです。"
  },
  {
    "start": 2489596,
    "end": 2495290,
    "text": "基本的には、あるモデルにとって重要なパラメータをアップウェイトし、そうでないパラメータをダウンウェイトしている。"
  },
  {
    "start": 2496190,
    "end": 2499418,
    "text": "もしそうするなら、1つだけ例を挙げよう。"
  },
  {
    "start": 2499504,
    "end": 2506218,
    "text": "論文には他にもたくさんの実験が載っているが、これをやれば、先ほど説明したような難解な融合パターンが可能になる。"
  },
  {
    "start": 2506314,
    "end": 2519170,
    "text": "例えば、MNLIで中間タスクトレーニングを受けたRTEモデルを、ドナータスクとマージすることで、元のモデルよりもパフォーマンスを向上させることができる。"
  },
  {
    "start": 2519320,
    "end": 2521102,
    "text": "それは能力を組み合わせるためだけではない。"
  },
  {
    "start": 2521166,
    "end": 2526950,
    "text": "また、マージを行うことで、特定のタスクに対するモデルのパフォーマンスを向上させることもできる。"
  },
  {
    "start": 2528010,
    "end": 2534758,
    "text": "さて、もうひとつ、もう少し洗練された方法でマージを実行する作業について説明しよう。"
  },
  {
    "start": 2534844,
    "end": 2540098,
    "text": "今回の件には、そのような論証された原則的な裏付けはない。"
  },
  {
    "start": 2540114,
    "end": 2541770,
    "text": "ただ、ヒューリスティックに見つけただけなんだ。"
  },
  {
    "start": 2542190,
    "end": 2550374,
    "text": "私たちがここでやっているのは、あるモデルについて、特に重要なパラメーターがあるかもしれないし、そうでないものもあるかもしれない、ということだ。"
  },
  {
    "start": 2550422,
    "end": 2557946,
    "text": "あるモデルにとって重要でないものと、別のモデルにとって重要なものを平均化すれば、もしかしたら、どちらかのモデルへの変更を帳消しにできるかもしれない。"
  },
  {
    "start": 2558058,
    "end": 2568338,
    "text": "タスクベクターがあると仮定して、タスクベクター内の値が小さければすべてゼロにする。"
  },
  {
    "start": 2568424,
    "end": 2582262,
    "text": "パラメータがそれほど変化していない場合は、最終的な符号をどうするかについてモデルに投票させることで、パラメータ値がどちらに変化したかという観点から符号の違いを解決することになる。"
  },
  {
    "start": 2582316,
    "end": 2590402,
    "text": "であれば、符号がこの集約された符号と一致するパラメータ値だけを平均化することになる。"
  },
  {
    "start": 2590556,
    "end": 2596234,
    "text": "繰り返すが、これはヒューリスティックに発見されたもので、なぜそれが有効なのか正当な根拠はない。"
  },
  {
    "start": 2596272,
    "end": 2612346,
    "text": "私が今話しているような、基本的に個々のタスクモデルをマージしてマルチタスクモデルを得るという用途に焦点を当てれば、多くの人が使っている単純な平均化よりも、タスクベクトル演算よりもはるかにうまくいく。"
  },
  {
    "start": 2612378,
    "end": 2617854,
    "text": "これは個々のタスクモデルに関して正規化されたパフォーマンスである。"
  },
  {
    "start": 2617902,
    "end": 2623982,
    "text": "2つ、あるいは3つのモデルを統合しても、基本的にパフォーマンスを犠牲にすることはないことがわかるだろう。"
  },
  {
    "start": 2624126,
    "end": 2627590,
    "text": "我々は本当に個々の能力を保持している。"
  },
  {
    "start": 2629210,
    "end": 2638978,
    "text": "最後に紹介する、異なるサブ・モデルの能力を組み合わせる作業では、エキスパート・スタイルのフレーミングがより多く使われている。"
  },
  {
    "start": 2639074,
    "end": 2646422,
    "text": "この場合、実際には、専門化したモデル全体を組み合わせるのではなく、専門化したエキスパートを組み合わせることを想定している。"
  },
  {
    "start": 2646566,
    "end": 2656954,
    "text": "特定の専門家の間でばらばらにルーティングするのではなく、専門家全体の分布を把握し、それらの専門家の加重平均を計算するのだ。"
  },
  {
    "start": 2657082,
    "end": 2658782,
    "text": "我々は基本的にそれらを統合するつもりだ。"
  },
  {
    "start": 2658916,
    "end": 2675598,
    "text": "そうして、マルチタスクを混合したデータセット（この場合は接着剤のデータセット）で訓練すると、少なくともある程度直感的なパターンが浮かび上がってくる。"
  },
  {
    "start": 2675694,
    "end": 2686440,
    "text": "独自のエキスパートを得られないタスクや、多くの異なる能力を必要とするタスクは、MNLIのように多くのエキスパートを使うことが多い。"
  },
  {
    "start": 2687390,
    "end": 2689434,
    "text": "これで納得してくれればいいのだが。"
  },
  {
    "start": 2689472,
    "end": 2690460,
    "text": "はい、どうぞ。"
  },
  {
    "start": 2695230,
    "end": 2697482,
    "text": "簡単なことだが、私はそれを見落としていた。"
  },
  {
    "start": 2697536,
    "end": 2708206,
    "text": "プロンプトや例ごとにマージするのか、それとも組み合わせたい4つのタスクが与えられて、それらを組み合わせたモデルを作るのか。"
  },
  {
    "start": 2708308,
    "end": 2708622,
    "text": "そうだね。"
  },
  {
    "start": 2708676,
    "end": 2723058,
    "text": "というのも、どのモデルを使うかについて話していた部分と、能力の組み合わせに相当するこの部分の橋渡しがまだできていないからだ。"
  },
  {
    "start": 2723144,
    "end": 2729094,
    "text": "仕事を組み合わせることで、どのモデルを仕事に使うかを選択できるようになることを期待している。"
  },
  {
    "start": 2729212,
    "end": 2739370,
    "text": "例えば、タスク・ベクトルか何かに基づいたインテリジェント・ルーティングがあるとしよう。"
  },
  {
    "start": 2739790,
    "end": 2750538,
    "text": "マージは、これらのモデルを即座に安価に結合し、個々のモデルの能力を継承した複合モデルを得る方法を提供する。"
  },
  {
    "start": 2750634,
    "end": 2755520,
    "text": "実際には誰もやっていないが、この過去の仕事を考えると、実際にうまくいくかもしれない。"
  },
  {
    "start": 2757810,
    "end": 2758560,
    "text": "そうだね。"
  },
  {
    "start": 2760850,
    "end": 2771780,
    "text": "あなたの方法、エコシステムの方法、そして指導的学習を使ってすべてのデータをまとめる方法を実験的に比較しましたか？"
  },
  {
    "start": 2772230,
    "end": 2772980,
    "text": "そうだね。"
  },
  {
    "start": 2774470,
    "end": 2778946,
    "text": "いや、部分的には、私が説明しているようなエコシステムが存在しないからだ。"
  },
  {
    "start": 2778978,
    "end": 2780520,
    "text": "まだ建設されていない。"
  },
  {
    "start": 2781690,
    "end": 2800380,
    "text": "ご質問の答えに一番近いのは、k個のデータセットを使って、k個の特化したモデルを訓練し、それらを統合して、そのk個のデータセットでマルチタスク訓練と同じくらいうまく機能するモデルを得ることができるか、ということだと思います。"
  },
  {
    "start": 2802510,
    "end": 2807418,
    "text": "一緒に言葉を交わすことは、傷の治療にとても役立つんだ。"
  },
  {
    "start": 2807514,
    "end": 2808110,
    "text": "ああ、間違いない。"
  },
  {
    "start": 2808180,
    "end": 2808554,
    "text": "素晴らしい。"
  },
  {
    "start": 2808602,
    "end": 2820942,
    "text": "これらのタスクとこのタスクは、おそらく異なるタスク間で比較される必要がある。"
  },
  {
    "start": 2821006,
    "end": 2821620,
    "text": "もちろんだ。"
  },
  {
    "start": 2822150,
    "end": 2825378,
    "text": "私が目指していた方向性を完成させるためにね。"
  },
  {
    "start": 2825544,
    "end": 2832546,
    "text": "合併することで、あなたが言っているようなポジティブな干渉のような恩恵が得られることは、人々が証明している。"
  },
  {
    "start": 2832578,
    "end": 2832774,
    "text": "そうだね。"
  },
  {
    "start": 2832812,
    "end": 2838962,
    "text": "2つのデータセットや2つのタスクに共通点がある場合、それらのモデルをマージすることで、片方のタスクのパフォーマンスを向上させることができる。"
  },
  {
    "start": 2839026,
    "end": 2845062,
    "text": "共通点のある言語でマルチタスク・トレーニングを行うことで、リソースの少ない言語のパフォーマンスが向上するのと同じだ。"
  },
  {
    "start": 2845126,
    "end": 2849100,
    "text": "しかし、一般的にはマルチタスク学習ほどうまくはいかない。"
  },
  {
    "start": 2850270,
    "end": 2851760,
    "text": "質問の答えになっているか？"
  },
  {
    "start": 2853490,
    "end": 2856622,
    "text": "言語のケースでの仕事は知らないが、マルチタスクのケースでは間違いない。"
  },
  {
    "start": 2856676,
    "end": 2862278,
    "text": "ああ、これはよくある実験結果だ。"
  },
  {
    "start": 2862474,
    "end": 2863122,
    "text": "そうだね。"
  },
  {
    "start": 2863256,
    "end": 2870834,
    "text": "私が紹介した論文のほとんどは、一度にk個のデータセットでトレーニングを比較するという、まさにこの設定を含んでいる。"
  },
  {
    "start": 2870872,
    "end": 2876402,
    "text": "マルチタスク学習と同じように、多言語学習も、それぞれを別々に訓練してから統合する。"
  },
  {
    "start": 2876456,
    "end": 2879960,
    "text": "さっきも言ったように、一般的にはそれほどうまくはいかないが、かなり近い。"
  },
  {
    "start": 2881690,
    "end": 2882440,
    "text": "素晴らしい。"
  },
  {
    "start": 2883290,
    "end": 2894890,
    "text": "そうそう、この際だから言っておくが、マージングもまた、ロバスト性と領域外汎化を向上させる方法として示されている。"
  },
  {
    "start": 2896990,
    "end": 2915090,
    "text": "このようなエコシステムを構築するためには、このようなコラボレーションを実現し、推論を実行し、これらの操作をすべて簡単に行えるようなシステムを構築することが有効でしょう。"
  },
  {
    "start": 2915590,
    "end": 2919346,
    "text": "そのうちの2つについて説明しよう。"
  },
  {
    "start": 2919368,
    "end": 2922478,
    "text": "さて、最初のものはgit thetaと呼ばれている。"
  },
  {
    "start": 2922654,
    "end": 2935522,
    "text": "git thetaのアイデアは、モデル・チェックポイントを使ってソースコードをバージョン管理するために、人々がすでに使っているワークフローをそのまま再現しようというものだ。"
  },
  {
    "start": 2935586,
    "end": 2938594,
    "text": "では、そのワークフローを再現するとはどういうことなのか？"
  },
  {
    "start": 2938642,
    "end": 2949238,
    "text": "つまり、モデルのチェックポイントへの変更を追跡したり、モデルのチェックポイントをマージしたりするために、まったく同じgitコマンドを実行できるようになるということだ。"
  },
  {
    "start": 2949254,
    "end": 2951600,
    "text": "これがマージの語源である。"
  },
  {
    "start": 2952370,
    "end": 2959866,
    "text": "これは、モデル開発のためのパイプラインの擬似的な例である。"
  },
  {
    "start": 2960058,
    "end": 2963754,
    "text": "まずgitzataに、元のチェックポイントを追跡したいと伝える。"
  },
  {
    "start": 2963882,
    "end": 2965902,
    "text": "いつものようにモデルをコミットするだけだ。"
  },
  {
    "start": 2966046,
    "end": 2971538,
    "text": "微調整のステップを行ってから、新しいモデルをコミットする。"
  },
  {
    "start": 2971704,
    "end": 2974542,
    "text": "では、RTEという新しいブランチをチェックしてみよう。"
  },
  {
    "start": 2974686,
    "end": 2987794,
    "text": "ここで微調整を行い、メインブランチをチェックアウトして微調整を行い、git merge RTEを実行してgit thetaにRTEブランチをメインブランチにマージしたいことを伝え、マージを実行します。"
  },
  {
    "start": 2987842,
    "end": 2996780,
    "text": "先ほど説明したような方法をいくつか使ってマージを行い、それをコミットして、さらに他の操作を行う。"
  },
  {
    "start": 2998110,
    "end": 3000234,
    "text": "git thetaはそれなりにうまくいく。"
  },
  {
    "start": 3000272,
    "end": 3001478,
    "text": "GitHubにある。"
  },
  {
    "start": 3001654,
    "end": 3008158,
    "text": "実際、あなたのgitリモートがgit lfsをサポートしている限り、GitHubは抱きつき顔モデルのhubをサポートしている。"
  },
  {
    "start": 3008324,
    "end": 3011230,
    "text": "そのリモートでgit thetaをネイティブに使うことができる。"
  },
  {
    "start": 3012450,
    "end": 3016606,
    "text": "では、git thetaを使うことの利点をどう考えればいいのだろうか？"
  },
  {
    "start": 3016638,
    "end": 3025502,
    "text": "git thetaを使うのではなく、バージョン管理パイプラインの各ポイントでチェックポイントを追跡するためにgit lfsを使うだけだ。"
  },
  {
    "start": 3025566,
    "end": 3027578,
    "text": "その場合、マージはできない。"
  },
  {
    "start": 3027694,
    "end": 3036722,
    "text": "例えば、モデルのチェックポイントのコピーを123456部保存するのと比較して、どれだけの容量を節約できるかを問うことができる。"
  },
  {
    "start": 3036786,
    "end": 3038950,
    "text": "このグラフはそれを示している。"
  },
  {
    "start": 3039020,
    "end": 3045274,
    "text": "この例は、パラメーターを効率的に微調整すれば、ディスク容量を大幅に節約できることを示している。"
  },
  {
    "start": 3045312,
    "end": 3051514,
    "text": "また、今回の操作のように、モデルのパラメーターの一部を削除するようなことをすれば、スペースの節約にもなる。"
  },
  {
    "start": 3051552,
    "end": 3059950,
    "text": "実際には、git thetaがチェックポイントを圧縮する方法のおかげで、モデルのパラメータを更新すれば、高密度の更新を行ったとしても少し容量を節約できます。"
  },
  {
    "start": 3060930,
    "end": 3069646,
    "text": "この例では、実際にこのパイプラインを実行すると、モデルのパフォーマンスが、今議論しているタスクで時間の経過とともに向上していくのがわかる。"
  },
  {
    "start": 3069758,
    "end": 3072900,
    "text": "テータが機能するのは、やや現実的な例である。"
  },
  {
    "start": 3074630,
    "end": 3077918,
    "text": "なるほど、モデルのエコシステムの出所を追跡できるわけだ。"
  },
  {
    "start": 3078014,
    "end": 3089830,
    "text": "誰かがモデル・エコシステムに新しいモデルを提供し、私たちがそのモデルを何らかの方法で改良したり、別のモデルとマージしたりする場合、私たちはgit thetaを使って、ただモデルの大混乱を抱えるのではなく、原則的な方法でそれを追跡することができる。"
  },
  {
    "start": 3090650,
    "end": 3097402,
    "text": "では最後に、先ほども出てきたと思うが、基本的にこの場合、分散推論はどのように行うのかという質問に入ろう。"
  },
  {
    "start": 3097456,
    "end": 3102118,
    "text": "一般的には、分散推論がよりシンプルになると思う。"
  },
  {
    "start": 3102294,
    "end": 3109098,
    "text": "私たちは、多くの同業者が特化したモデルを提供し、適切なルーティングを行うというシステムを構築していない。"
  },
  {
    "start": 3109194,
    "end": 3114500,
    "text": "我々は、単一の大規模なモデルの分散推論を簡単に行えるシステムを構築した。"
  },
  {
    "start": 3115110,
    "end": 3126958,
    "text": "このシステムでできることは、モデルのレイヤーをグループごとに切り分け、参加者がボランティアで、このレイヤーで推論を実行します、と言えるようにすることだ。"
  },
  {
    "start": 3127134,
    "end": 3143260,
    "text": "そしてレイヤーを圧縮して推論を高速化し、起動を圧縮して、インターネット経由で次のピアに送信する。"
  },
  {
    "start": 3144910,
    "end": 3152582,
    "text": "また、インテリジェント・ルーティングを行い、特定のリクエストにどのピアを使うべきかを決定する。"
  },
  {
    "start": 3152726,
    "end": 3154494,
    "text": "それなりにうまくいく。"
  },
  {
    "start": 3154612,
    "end": 3157546,
    "text": "ボランティアの推論を実行するまっとうな方法だ。"
  },
  {
    "start": 3157578,
    "end": 3163658,
    "text": "実際、今システムが呼んでいるペタルを使って、さまざまなモデルの推論を行うことができる。"
  },
  {
    "start": 3163674,
    "end": 3168782,
    "text": "これはヘルスダッシュボードで、基本的にあるチェックポイントを表示している。"
  },
  {
    "start": 3168846,
    "end": 3172610,
    "text": "モデルのレイヤーのブロックをサーブしてくれるピアはいるか？"
  },
  {
    "start": 3173350,
    "end": 3182834,
    "text": "ひとつ重要なことは、いいスライドや図がないのだが、ペタルがネイティブでホットスワップ・アダプター、パラメータ分裂の微調整ベクトルをサポートしていることだ。"
  },
  {
    "start": 3182882,
    "end": 3194870,
    "text": "安定したベルーガのベースモデルを持っていて、特化したモデルを作り、その特化したモデルを使って推論を実行したい場合、ペタルにアダプターを渡せば、そのアダプターを使って推論を実行してくれる。"
  },
  {
    "start": 3194950,
    "end": 3212430,
    "text": "また、これらのモノリスモデルやジェネラリストモデルが、高価なスーパーコンピューターにアクセスできる企業によって提供されることで、実にうまく機能するのと同じように、アダプターの微調整も行う。"
  },
  {
    "start": 3213030,
    "end": 3222900,
    "text": "花びらのようなシステムを使えば、中央集権的なスーパーコンピューターや企業に頼ることなく、ボランティア・コンピューティングで生態系の推論が可能になる。"
  },
  {
    "start": 3224230,
    "end": 3233170,
    "text": "今日、私が講演をするときはいつも、最後のスライドにこのURLを貼っている。"
  },
  {
    "start": 3233250,
    "end": 3238278,
    "text": "もし、私の話が早すぎるとか、遅すぎるとか、カバーする範囲が広すぎるとか、狭すぎるとか、何でも結構です。"
  },
  {
    "start": 3238364,
    "end": 3246140,
    "text": "質問が終わった後、フォローアップを希望される方は、私のメールにご連絡ください。"
  },
  {
    "start": 3251630,
    "end": 3253950,
    "text": "こんにちは、素晴らしいお話をありがとうございました。"
  },
  {
    "start": 3254100,
    "end": 3268798,
    "text": "エコシステムには2種類あって、モデルのエコシステムがある場合と、さまざまな人々のエコシステムがあり、所有権が分散されている場合です。"
  },
  {
    "start": 3268894,
    "end": 3281974,
    "text": "あなたは、より経済的に効率的なさまざまなモデルのエコシステムを持つことができると主張したと思いますが、一企業がモデルのエコシステムを持つという特定のアーキテクチャを選択することができると想像できます。"
  },
  {
    "start": 3282092,
    "end": 3285202,
    "text": "その意味では、所有権はまだ一枚岩だ。"
  },
  {
    "start": 3285346,
    "end": 3296394,
    "text": "一企業がこのようなエコシステムを構築するよりも、コミュニティがこのようなエコシステムを構築する方が比較優位にあると思いますが？"
  },
  {
    "start": 3296512,
    "end": 3297340,
    "text": "ああ、もちろんだ。"
  },
  {
    "start": 3297950,
    "end": 3302730,
    "text": "このエコシステムが単一の事業体によって構築されたと想像できるのは、まったくその通りだ。"
  },
  {
    "start": 3303150,
    "end": 3308240,
    "text": "ちなみに、私はブランディングが得意ではないので、エコシステムより良い言葉があれば教えてほしい。"
  },
  {
    "start": 3309570,
    "end": 3334950,
    "text": "そうですね、ご指摘の点については、私は、つまり、これは動機として言及しませんでしたが、私は、この非常に強力で、非常に有用な技術、つまり、このような汎用のAIシステムのようなものが、非常に資源が豊富で、利潤を追求する比較的少数の機関や団体によってコントロールされる未来について、非常に心配しています。"
  },
  {
    "start": 3336490,
    "end": 3339466,
    "text": "どこまで詳しく説明する必要があるのかわからない。"
  },
  {
    "start": 3339648,
    "end": 3342780,
    "text": "直感だけど、そう感じるんだ。"
  },
  {
    "start": 3344270,
    "end": 3352782,
    "text": "一般的に、権力を分散させ、責任を分散させれば、物事はたいていうまくいくと思う。"
  },
  {
    "start": 3352836,
    "end": 3359310,
    "text": "私は、このシステム、あるいはこのアプローチは、権力を分散させる方法を提供すると思う。"
  },
  {
    "start": 3359380,
    "end": 3359614,
    "text": "クールだ。"
  },
  {
    "start": 3359652,
    "end": 3360480,
    "text": "ああ、ありがとう。"
  },
  {
    "start": 3361650,
    "end": 3362830,
    "text": "素晴らしい話をありがとう。"
  },
  {
    "start": 3362900,
    "end": 3366898,
    "text": "では、将来これらのエコシステムで想定しているモデルについて質問してください。"
  },
  {
    "start": 3366984,
    "end": 3381746,
    "text": "多くのモデルからなる生態系に傾倒しているのでしょうか？中には非常に冗長なモデルもあるのでしょうか？それとも、非常に多様でほとんど直交するモデルの本当に小さな集合を目指し、より効率的にしようとしているのでしょうか？"
  },
  {
    "start": 3381858,
    "end": 3389814,
    "text": "ええ、それは興味深い指摘だと思いますし、私が言い出したことではありませんが、エコシステムを剪定するようなことは絶対に想像できますよね？"
  },
  {
    "start": 3389852,
    "end": 3401670,
    "text": "例えば、冗長なモデルがたくさんある場合、それらのモデルを1つの集合的なモデルにまとめ、個々のモデルの能力とパフォーマンスを維持し、エコシステムを小さくすることができるかもしれない。"
  },
  {
    "start": 3401830,
    "end": 3405630,
    "text": "今日説明したことの多くは、かなりスケーラブルだと思うんだけどね？"
  },
  {
    "start": 3405780,
    "end": 3412366,
    "text": "繰り返しになるが、パラメーターの効率的な微調整を使えば、ストレージ要件は大きな問題ではない。"
  },
  {
    "start": 3412548,
    "end": 3422494,
    "text": "どの特化したモデルを使うかを選択するために使える手法のほとんどは、モデルベクトルのドット積を計算するだけで、かなり効率的にできると想像できる。"
  },
  {
    "start": 3422622,
    "end": 3423886,
    "text": "かなり拡張性があると思う。"
  },
  {
    "start": 3423918,
    "end": 3429462,
    "text": "モデルの刈り込み方などを考えるのは、まったく意味のあることだと私も思う。"
  },
  {
    "start": 3429516,
    "end": 3432200,
    "text": "ああ、素晴らしい話だ。"
  },
  {
    "start": 3432890,
    "end": 3456830,
    "text": "この世界におけるビットの腐敗の対極にあるものについて考えているのですが、具体的に言うと、gpt-2を中心にこのようなエコシステムが構築され、何千ものものが出現し、人々がそれらを組み合わせ、多くのものが機能するようになり、そして誰かが投資してGPT-3を育成したとしますよね？"
  },
  {
    "start": 3456980,
    "end": 3458526,
    "text": "さて、どうする？"
  },
  {
    "start": 3458708,
    "end": 3469122,
    "text": "エコシステム内の個々のモデルに共通点がなく、ベースとなるモデルも必要ないという一般的なケースを考えてみると、確かにそうだと思う。"
  },
  {
    "start": 3469176,
    "end": 3473502,
    "text": "gpt-3ベースのモデルとgpt-2ベースのモデルを持つエコシステムがあってもいい。"
  },
  {
    "start": 3473566,
    "end": 3476958,
    "text": "私が説明したほとんどの合併方法は、そのような環境では実際に機能しないだろう。"
  },
  {
    "start": 3477054,
    "end": 3484200,
    "text": "互換性のあるモデルに対してのみこれらのマージメソッドを使用するといった、インテリジェントなことをすることも想像できるだろう。"
  },
  {
    "start": 3484970,
    "end": 3492522,
    "text": "今後の課題としては話しませんでしたが、共通のアーキテクチャを持たないモデルをどのように融合させるかについて考え始めることは価値があると思います。"
  },
  {
    "start": 3492576,
    "end": 3503626,
    "text": "例えば、これはデーブがすぐに投げかけた質問に戻るんだけど、基本的にモデルが同じでないときにこれは機能するのか？"
  },
  {
    "start": 3503648,
    "end": 3506560,
    "text": "ただ、そうなったときのメリットはあると思う。"
  },
  {
    "start": 3507490,
    "end": 3514980,
    "text": "また、うまく機能していないモデルをどのように刈り込んでいるのか、あるいはプレドが最適でないとか、そういう問題にも関連してくる。"
  },
  {
    "start": 3516630,
    "end": 3529082,
    "text": "そうですね。有償のシステムのようなもので、人々は払い戻しを受けたりすることができるとおっしゃいましたが、個々のほくろの利益をどのように帰属させるのかは明確ではありません。"
  },
  {
    "start": 3529166,
    "end": 3532278,
    "text": "そう言ったかどうかはわからない。"
  },
  {
    "start": 3532364,
    "end": 3542550,
    "text": "私は自分の記憶よりもあなたのことを信じています。ペダルで具体的な例を挙げれば、私たちは仲間にインセンティブを与える方法を考えました。"
  },
  {
    "start": 3542630,
    "end": 3547660,
    "text": "例えば、これらの仲間はボランティアでコンピューティングを行っているため、基本的に自分のGpusを無料で提供している。"
  },
  {
    "start": 3549390,
    "end": 3557754,
    "text": "最も単純なのは、分散型クラウドがある場合、コンピートをボランティアで提供することで、優先的にアクセスできるようにすることだ。"
  },
  {
    "start": 3557882,
    "end": 3565506,
    "text": "似たようなインセンティブとして、より頻繁に使用されるモデルには優先的に推論を行うとか、そういうことも考えられるだろう。"
  },
  {
    "start": 3565688,
    "end": 3572980,
    "text": "何がうまくいくのか、具体的な例はまだないんだ。"
  },
  {
    "start": 3574310,
    "end": 3575058,
    "text": "質問だ。"
  },
  {
    "start": 3575224,
    "end": 3581126,
    "text": "マージに関するほとんどの作業では、ユーザーが何をどのようにマージするかを指定する必要があると思う。"
  },
  {
    "start": 3581148,
    "end": 3582214,
    "text": "そうなのか？"
  },
  {
    "start": 3582412,
    "end": 3589490,
    "text": "マージする特殊なモデルの空間における検索プロセスのように、これを自動化する研究はあるのだろうか？"
  },
  {
    "start": 3589570,
    "end": 3596490,
    "text": "ああ、コンセプチュアルな飛躍があったんだけど、それはまだ作品が存在しないから明示しなかったんだ。"
  },
  {
    "start": 3596560,
    "end": 3605358,
    "text": "私は、適切なモデル選択のようなモデルルーティングと、モデルの統合という興味深い仕事が交差していると思う。"
  },
  {
    "start": 3605524,
    "end": 3618362,
    "text": "確かに、自分のモデルが最終的に実行させたいタスクに類似したタスクで訓練されたモデルがある場合、それらのモデルを統合すると、ターゲットタスクでのパフォーマンスが向上することを示した研究もある。"
  },
  {
    "start": 3618506,
    "end": 3619662,
    "text": "それはうまくいくものだ。"
  },
  {
    "start": 3619716,
    "end": 3628066,
    "text": "どのモデルが似たようなタスクで訓練されたかを識別する洗練された方法があれば、あなたが言っているようなメリットが得られると思います。"
  },
  {
    "start": 3628178,
    "end": 3632054,
    "text": "さて、今日はパネルやレセプションではなく、別の話をしよう。"
  },
  {
    "start": 3632172,
    "end": 3635140,
    "text": "コリンに感謝して、次に進むべきだと思う。"
  }
]