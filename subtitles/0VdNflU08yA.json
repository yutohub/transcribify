[
  {
    "start": 570,
    "end": 2286,
    "text": "やあ、みんな。"
  },
  {
    "start": 2388,
    "end": 4570,
    "text": "今日は量子化について話そう。"
  },
  {
    "start": 4730,
    "end": 6558,
    "text": "今日のトピックをおさらいしよう。"
  },
  {
    "start": 6724,
    "end": 10458,
    "text": "量子化とは何か、なぜ量子化が必要なのかを示すことから始めよう。"
  },
  {
    "start": 10554,
    "end": 17242,
    "text": "後ほど、我々のハードウェアにおける整数と浮動小数点数の数値表現について簡単に紹介する。"
  },
  {
    "start": 17306,
    "end": 25650,
    "text": "CPUとGPUの量子化については、後でいくつかのサンプルをお見せすることで、ニューラルネットワーク・レベルでの量子化について説明しよう。"
  },
  {
    "start": 25730,
    "end": 33570,
    "text": "量子化の種類、つまり非対称量子化と対称量子化、範囲と粒度の意味については後で詳しく説明する。"
  },
  {
    "start": 33650,
    "end": 37766,
    "text": "後ほど、トレーニング後の量子化と量子化を考慮したトレーニングについても説明する。"
  },
  {
    "start": 37948,
    "end": 39282,
    "text": "これらすべてのトピックについて。"
  },
  {
    "start": 39346,
    "end": 44678,
    "text": "pytorchとpythonのコードもお見せします。"
  },
  {
    "start": 44774,
    "end": 50118,
    "text": "実際には、Pytorchを使って非対称量子化と量子化、対称量子化をゼロから構築する。"
  },
  {
    "start": 50134,
    "end": 56320,
    "text": "その後、ポストトレーニング量子化と量子化を意識したトレーニングを使って、サンプルのニューラルネットワークに適用する。"
  },
  {
    "start": 56850,
    "end": 67438,
    "text": "このビデオを見る前に知っておいてほしいことは、基本的にニューラルネットワークの基本的な理解があり、数学の素養があるということだ。"
  },
  {
    "start": 67534,
    "end": 69700,
    "text": "高校数学だけで十分だ。"
  },
  {
    "start": 70470,
    "end": 72206,
    "text": "さあ、旅を始めよう。"
  },
  {
    "start": 72318,
    "end": 74706,
    "text": "まず量子化とは何かを見てみよう。"
  },
  {
    "start": 74808,
    "end": 77038,
    "text": "量子化は、ある問題を解決することを目的としている。"
  },
  {
    "start": 77144,
    "end": 83266,
    "text": "問題は、最新のディープ・ニューラル・ネットワークのほとんどが、数十億とは言わないまでも、数百万ものパラメーターで構成されていることだ。"
  },
  {
    "start": 83378,
    "end": 87074,
    "text": "例えば、最小のラマ2匹には70億のパラメータがある。"
  },
  {
    "start": 87202,
    "end": 95110,
    "text": "さて、すべてのパラメーターが32ビットだとすると、パラメーターをディスクに保存するだけで28GBが必要になる。"
  },
  {
    "start": 95270,
    "end": 99782,
    "text": "また、モデルを推論する際には、モデルのすべてのパラメータをメモリにロードする必要がある。"
  },
  {
    "start": 99846,
    "end": 103706,
    "text": "推論などでCPUを使う場合は、ラムにロードする必要がある。"
  },
  {
    "start": 103738,
    "end": 107466,
    "text": "GPUを使用する場合は、GPUのメモリにロードする必要があります。"
  },
  {
    "start": 107658,
    "end": 118050,
    "text": "もちろん、標準的なパソコンやスマートフォンのような小型デバイスを使用している場合、大きなモデルはRAMやGPUに簡単にロードすることはできない。"
  },
  {
    "start": 118470,
    "end": 126670,
    "text": "また、人間と同じように、コンピュータは整数演算に比べて浮動小数点演算が遅い。"
  },
  {
    "start": 126750,
    "end": 134806,
    "text": "例えば、3×6を精神的にやってみたり、1.21×2.897を精神的にやってみたりする。"
  },
  {
    "start": 134908,
    "end": 141350,
    "text": "もちろん、3×6の掛け算はもっと速くできる。"
  },
  {
    "start": 141510,
    "end": 143702,
    "text": "解決策は量子化である。"
  },
  {
    "start": 143846,
    "end": 155850,
    "text": "量子化は基本的に、浮動小数点数を整数に変換することで、通常各パラメータを表現するのに必要なビット数を減らすことを目的としている。"
  },
  {
    "start": 156010,
    "end": 163806,
    "text": "こうすることで、例えば、通常は何ギガバイトもあるモデルを、もっと小さいサイズに圧縮することができる。"
  },
  {
    "start": 163988,
    "end": 170862,
    "text": "また、量子化というのは、浮動小数点数のすべてを最も近い整数に切り上げたり切り捨てたりすることではないことに注意してほしい。"
  },
  {
    "start": 170926,
    "end": 172770,
    "text": "これは量子化とは違う。"
  },
  {
    "start": 172840,
    "end": 176526,
    "text": "どう機能するかは後でわかるので、混乱しないでほしい。"
  },
  {
    "start": 176718,
    "end": 182946,
    "text": "量子化によって、より小さなデータ型を扱うことが速くなるため、計算も速くなる。"
  },
  {
    "start": 182978,
    "end": 191020,
    "text": "例えば、整数からなる行列の掛け算は、浮動小数点数からなる2つの行列の掛け算よりもはるかに高速である。"
  },
  {
    "start": 191470,
    "end": 196540,
    "text": "後で、この行列の乗算がGPUレベルでもどのように機能するかを実際に見てみよう。"
  },
  {
    "start": 197070,
    "end": 199750,
    "text": "量子化の利点は何ですか？"
  },
  {
    "start": 199830,
    "end": 211466,
    "text": "まず、モデルをロードする際のメモリ消費量が少ないため、モデルをより小さなサイズに圧縮することができ、より単純なデータ型を使用するため推論時間が短縮される。"
  },
  {
    "start": 211498,
    "end": 214322,
    "text": "例えば、浮動小数点数の代わりに整数を使う。"
  },
  {
    "start": 214456,
    "end": 221250,
    "text": "この2つの組み合わせは、例えばスマートフォンにとって非常に重要なエネルギー消費の削減につながる。"
  },
  {
    "start": 222230,
    "end": 226494,
    "text": "それでは、ハードウェアの中で数字がどのように表現されるかをおさらいしてみよう。"
  },
  {
    "start": 226542,
    "end": 229206,
    "text": "CPUレベルでもGPUレベルでも。"
  },
  {
    "start": 229388,
    "end": 235494,
    "text": "そのため、コンピュータはどのようなデータも決まった数のビットで表現する。"
  },
  {
    "start": 235612,
    "end": 240870,
    "text": "例えば、数字や文字、ピクセルの色を表現するには、常に固定ビット数を使用する。"
  },
  {
    "start": 241020,
    "end": 246810,
    "text": "nビットで構成されるビット列は、最大でnの2乗の異なる数を表すことができる。"
  },
  {
    "start": 246960,
    "end": 252334,
    "text": "例えば、3ビットで、0から7までの8つの数字を表すことができる。"
  },
  {
    "start": 252452,
    "end": 255978,
    "text": "各数値が2進数表現であることがわかる。"
  },
  {
    "start": 256154,
    "end": 270850,
    "text": "各桁に、ビット列内の桁の位置の2のべき乗を掛けることで、常に2進数表現を10進数表現に変換することができる。"
  },
  {
    "start": 271190,
    "end": 282262,
    "text": "ほとんどのCPUでは、整数は2の補数を使って表現される。"
  },
  {
    "start": 282396,
    "end": 293366,
    "text": "ゼロは正、1は負を意味し、残りのビットは、正の場合はその絶対値を、負の場合はその補数を示す。"
  },
  {
    "start": 293558,
    "end": 298602,
    "text": "2の補数を使うのは、ゼロを一意に表現したいからである。"
  },
  {
    "start": 298656,
    "end": 302586,
    "text": "プラスゼロとマイナスゼロは同じ2進数表現を持つ。"
  },
  {
    "start": 302778,
    "end": 308814,
    "text": "もちろん、コンピューターは決まったビット数で数字を表現している、と反論されるかもしれない。"
  },
  {
    "start": 308932,
    "end": 313118,
    "text": "Pythonはどうしてこのような大きな数字を問題なく扱えるのですか？"
  },
  {
    "start": 313204,
    "end": 326114,
    "text": "例えば、パイソンで2の9乗を実行すると、64ビットの数値よりもはるかに大きな結果が得られますが、パイソンはこのような巨大な数値を問題なく処理できるのでしょうか？"
  },
  {
    "start": 326232,
    "end": 330114,
    "text": "さて、Pythonはいわゆるビッグ・ナンバー算法を使っている。"
  },
  {
    "start": 330242,
    "end": 342246,
    "text": "この表で前に見たように、6という数字を10の底で表すと1桁で済むが、2の底で表すと3桁になる。"
  },
  {
    "start": 342358,
    "end": 343606,
    "text": "これは実はルールなのだ。"
  },
  {
    "start": 343638,
    "end": 350486,
    "text": "Pythonはその逆を行う。"
  },
  {
    "start": 350518,
    "end": 358910,
    "text": "この配列は、各桁が底2の30の累乗の数字となる。"
  },
  {
    "start": 358980,
    "end": 362558,
    "text": "全体として、非常に大きな数字を保存するために必要な桁数は少なくなる。"
  },
  {
    "start": 362724,
    "end": 373774,
    "text": "例えば、2の9乗である9を10進数で表した場合、これをメモリに格納するには3000桁の配列が必要になる。"
  },
  {
    "start": 373902,
    "end": 380262,
    "text": "一方、Pythonはこの数値を2の底の30乗の数字の配列として格納する。"
  },
  {
    "start": 380316,
    "end": 390326,
    "text": "の場合、必要なのは334個の要素だけで、そのうちの最上位1個以外はすべて0であり、512に等しい。"
  },
  {
    "start": 390348,
    "end": 397418,
    "text": "実のところ、512をベースに掛けることで自分で確認することができる。"
  },
  {
    "start": 397504,
    "end": 399290,
    "text": "の30乗である。"
  },
  {
    "start": 399360,
    "end": 407840,
    "text": "そして、この桁の位置の2乗が、9の9乗となる。"
  },
  {
    "start": 408530,
    "end": 415786,
    "text": "また、これはCPUではなく、PythonのインタープリターであるcPythonによって実装されるものであることにも気づいてほしい。"
  },
  {
    "start": 415818,
    "end": 420782,
    "text": "この大きな数字の計算をしているのはCPUではなく、Pythonのインタープリターなのだ。"
  },
  {
    "start": 420926,
    "end": 433654,
    "text": "例えば、c plus plusのコードをコンパイルすると、そのコードはCPU上のハードウェア上で直接実行される。"
  },
  {
    "start": 433692,
    "end": 441018,
    "text": "Pythonコードをコンパイルすることはないが、cpythonがPython命令をマシン・コードに変換してくれるからだ。"
  },
  {
    "start": 441104,
    "end": 444570,
    "text": "ジャスト・イン・タイム・コンパイルと呼ばれるプロセスである。"
  },
  {
    "start": 446110,
    "end": 449770,
    "text": "さて、浮動小数点数がどのように表現されるかをおさらいしよう。"
  },
  {
    "start": 450190,
    "end": 455358,
    "text": "さて、10進数とは、単に底の負のべき乗も含む数である。"
  },
  {
    "start": 455444,
    "end": 465310,
    "text": "例えば、85.612という数字は、各桁の数字に底の10の累乗を掛けたものと書くことができる。"
  },
  {
    "start": 465460,
    "end": 469074,
    "text": "の場合、小数部は10の負の累乗を持つ。"
  },
  {
    "start": 469112,
    "end": 472226,
    "text": "ご覧の通り、10のマイナス1乗、マイナス2乗、マイナス3乗である。"
  },
  {
    "start": 472328,
    "end": 482280,
    "text": "これと同じ理屈が、32ビットの浮動小数点数の表現を定義した標準IEEE seven five fourでも使われている。"
  },
  {
    "start": 482890,
    "end": 486162,
    "text": "基本的には、32ビットの文字列を3つの部分に分割する。"
  },
  {
    "start": 486226,
    "end": 490038,
    "text": "最初のビットは符号を示すので、ゼロは正を意味する。"
  },
  {
    "start": 490134,
    "end": 494874,
    "text": "次の8ビットは指数を示し、これは数値の大きさも示す。"
  },
  {
    "start": 494912,
    "end": 496506,
    "text": "どのくらいの数字ですか？"
  },
  {
    "start": 496688,
    "end": 501162,
    "text": "最後の23ビットは数値の小数部を示す。"
  },
  {
    "start": 501216,
    "end": 506046,
    "text": "の負のべき乗に対応するすべての桁。"
  },
  {
    "start": 506228,
    "end": 510782,
    "text": "このビット列を10進数の値に変換するには、次のようにすればよい。"
  },
  {
    "start": 510836,
    "end": 525460,
    "text": "指数-127の2乗に分数1を掛けたものに2の負のべき乗をすべて掛けた正弦は、数字のゼロ点15に対応するはずである。"
  },
  {
    "start": 528410,
    "end": 554000,
    "text": "ほとんどの最新GPUは16ビットの浮動小数点数もサポートしているが、端数部分専用のビットが少なく、指数専用のビットが少ないので、もちろん精度は低くなる。"
  },
  {
    "start": 555410,
    "end": 558458,
    "text": "では、量子化の詳細を説明しよう。"
  },
  {
    "start": 558554,
    "end": 562062,
    "text": "さて、まずはニューラルネットワークがどのように機能するかをおさらいする。"
  },
  {
    "start": 562116,
    "end": 571054,
    "text": "テンソルである可能性のある入力から始めて、それを例えば線形層である可能性のある層に与え、さらに別の線形層にマッピングする。"
  },
  {
    "start": 571102,
    "end": 573134,
    "text": "最後に出力がある。"
  },
  {
    "start": 573262,
    "end": 574910,
    "text": "私たちは通常、目標を定めている。"
  },
  {
    "start": 574990,
    "end": 586470,
    "text": "出力と目標を損失関数で比較し、各パラメータに対する損失関数の勾配を計算し、逆伝播を実行してこれらのパラメータを更新する。"
  },
  {
    "start": 587850,
    "end": 591542,
    "text": "ニューラルネットワークは多くの異なる層で構成することができる。"
  },
  {
    "start": 591606,
    "end": 594822,
    "text": "例えば、リニアレイヤーは2つのマトリックスで構成されている。"
  },
  {
    "start": 594886,
    "end": 601040,
    "text": "ひとつはウェイト、もうひとつはバイアスと呼ばれ、一般的には浮動小数点数で表される。"
  },
  {
    "start": 601410,
    "end": 608318,
    "text": "量子化は、モデルの精度を維持しながら、これら2つの行列を整数で表現することを目的としている。"
  },
  {
    "start": 608404,
    "end": 611386,
    "text": "このリニアレイヤーがどのようなものか見てみよう。"
  },
  {
    "start": 611418,
    "end": 627458,
    "text": "例えば、このニューラルネットワークの最初の線形層は、入力に、この線形層のパラメータである重み行列と、同じくこの線形層のパラメータであるバイアスを掛け合わせた演算を表す。"
  },
  {
    "start": 627634,
    "end": 645126,
    "text": "量子化の目的は、入力、重み行列、バイアスメトリクスを整数に量子化することであり、浮動小数点演算に比べてはるかに高速であるため、ここではこれらの演算をすべて整数演算として実行する。"
  },
  {
    "start": 645238,
    "end": 657786,
    "text": "そして、その出力をデカンテーションして次の層に送り、次の層が前の層で量子化が行われたことに気づかないようにデカンテーションする。"
  },
  {
    "start": 657818,
    "end": 664538,
    "text": "量子化によってモデルの出力が変化しないように量子化を行いたい。"
  },
  {
    "start": 664634,
    "end": 672958,
    "text": "モデルの性能、モデルの精度は維持したいが、これらの演算はすべて整数を用いて実行したい。"
  },
  {
    "start": 673054,
    "end": 688360,
    "text": "浮動小数点数から整数へ、整数から浮動小数点数へと、モデルの精度を失わないように、可逆的なマッピングを見つける必要がある。"
  },
  {
    "start": 688810,
    "end": 695686,
    "text": "同時に、ラム内とディスク上のモデルの占有スペースを最適化したい。"
  },
  {
    "start": 695798,
    "end": 705870,
    "text": "前に見たように、整数演算は浮動小数点演算よりもずっと速いからだ。"
  },
  {
    "start": 706930,
    "end": 712390,
    "text": "主な利点は、ほとんどのハードウェアにおいて、整数演算が浮動小数点演算よりもはるかに高速であることだ。"
  },
  {
    "start": 712490,
    "end": 724850,
    "text": "さらに、ほとんどの組み込みハードウェア、特に非常に小さな組み込みデバイスでは、浮動小数点数さえ持っていないので、それらのデバイスでは整数演算を使わざるを得ない。"
  },
  {
    "start": 725750,
    "end": 727202,
    "text": "よし、どう動くか見てみよう。"
  },
  {
    "start": 727256,
    "end": 733846,
    "text": "例えば、このヒディアン・レイヤーにはウェイト・マトリクスがあり、ここに見えるように5×5のマトリクスになっている。"
  },
  {
    "start": 733948,
    "end": 745494,
    "text": "量子化の目的は、より少ないビット数の範囲にマッピングすることで、この行列に表示される各数値の精度を下げることである。"
  },
  {
    "start": 745542,
    "end": 751798,
    "text": "これは浮動小数点数で、4バイト、つまり32ビットを占める。"
  },
  {
    "start": 751974,
    "end": 754830,
    "text": "8ビットだけで量子化したい。"
  },
  {
    "start": 754900,
    "end": 758350,
    "text": "各数値は8ビットのみで表現されるべきである。"
  },
  {
    "start": 758500,
    "end": 769838,
    "text": "さて、8ビットを使えば、-128からプラス127までの範囲を表現できるが、通常は対称的な範囲を得るために-128を犠牲にする。"
  },
  {
    "start": 770014,
    "end": 782550,
    "text": "各数値を8ビット表現にマッピングし、元の配列に戻す操作を量子化と呼び、次にデカンタイズと呼ぶ。"
  },
  {
    "start": 782890,
    "end": 792102,
    "text": "さて、量子化中に元の配列、元のテンソルや行列が得られるはずだが、通常は精度が落ちる。"
  },
  {
    "start": 792166,
    "end": 801454,
    "text": "例えば、最初の値を見ると、元の行列とまったく同じだが、ここにある2番目の値は似ているが、まったく同じではない。"
  },
  {
    "start": 801572,
    "end": 818290,
    "text": "つまり、量子化によって多少の誤差が生じるため、量子化されていないモデルほど正確なモデルにはならないが、可能な限り精度を落とさないように量子化処理を行いたい。"
  },
  {
    "start": 818360,
    "end": 823090,
    "text": "精度を失いたくないので、誤差を最小限に抑えたい。"
  },
  {
    "start": 824150,
    "end": 827406,
    "text": "では、量子化の詳細を説明しよう。"
  },
  {
    "start": 827448,
    "end": 835426,
    "text": "まず最初に、非対称量子化と対称量子化の違いを説明します。"
  },
  {
    "start": 835538,
    "end": 857258,
    "text": "非対称量子化の目的は、元のテンソルをこの範囲にマッピングすることである。このテンソルは、-44.93というテンソルの中で最小の数値と、43.31というテンソルの中で最大の数値の間に分布している。"
  },
  {
    "start": 857354,
    "end": 868500,
    "text": "それを、たとえば8ビットで表現できるゼロから255までの整数で構成される別の範囲にマッピングしたい。"
  },
  {
    "start": 869110,
    "end": 880870,
    "text": "この操作を行えば、例えば、この最初の数字を255に、この数字を0に、この数字を130に、といったように対応付ける新しいテンソルが得られる。"
  },
  {
    "start": 881690,
    "end": 890102,
    "text": "量子化のもう一つのタイプは対称的な量子化で、対称的な範囲をマッピングすることを目的としている。"
  },
  {
    "start": 890166,
    "end": 907518,
    "text": "このテンソルを使って、対称でなくても対称な範囲として扱います。見ての通り、ここで最大の値は43.31、最小の値は-44.93なので、ゼロに関して対称ではありません。"
  },
  {
    "start": 907684,
    "end": 925380,
    "text": "もしそうなら、元の対称範囲を別の対称範囲に基本的にマッピングすることを目的とした対称範囲を使用することができる。"
  },
  {
    "start": 926330,
    "end": 929794,
    "text": "この計算を実際にどのように行うかは、後でお見せしよう。"
  },
  {
    "start": 929842,
    "end": 937346,
    "text": "元のテンソルを使って量子化されたバージョンをどのように計算するか、またどのように逆量子化するか。"
  },
  {
    "start": 937468,
    "end": 938620,
    "text": "行こう"
  },
  {
    "start": 939230,
    "end": 945770,
    "text": "非対称量子化の場合、次のような元のテンソルがあるとする。"
  },
  {
    "start": 945840,
    "end": 951622,
    "text": "ここで見ることができる10個の項目は、以下の式を使って量子化する。"
  },
  {
    "start": 951686,
    "end": 956494,
    "text": "これらの数値の量子化バージョンは、それぞれ浮動小数点数と等しい。"
  },
  {
    "start": 956532,
    "end": 969860,
    "text": "元の浮動小数点数をsと呼ばれるパラメータ（scaleを意味する）で割ると、最も近い整数にzを足した値に切り捨てまたは切り上げられる。"
  },
  {
    "start": 970470,
    "end": 983046,
    "text": "この演算の結果が0より小さければ0にクランプし、nから1を引いた2のべき乗より大きければnから1を引いた2のべき乗にクランプする。"
  },
  {
    "start": 983148,
    "end": 984038,
    "text": "nとは？"
  },
  {
    "start": 984124,
    "end": 987138,
    "text": "Nは量子化に使いたいビット数。"
  },
  {
    "start": 987234,
    "end": 991814,
    "text": "例えば、これらの浮動小数点数をすべて8ビットに量子化したい。"
  },
  {
    "start": 991862,
    "end": 994140,
    "text": "ここではnを8とする。"
  },
  {
    "start": 994670,
    "end": 996710,
    "text": "このsパラメータはどのように計算するのか？"
  },
  {
    "start": 996790,
    "end": 1003550,
    "text": "sパラメータは、基本的にアルファからベータを引いたものを出力レンジで割ったもので与えられる。"
  },
  {
    "start": 1003620,
    "end": 1006830,
    "text": "出力範囲はいくつの数値を表すことができるか？"
  },
  {
    "start": 1007730,
    "end": 1009066,
    "text": "ベータとアルファとは何か？"
  },
  {
    "start": 1009098,
    "end": 1014198,
    "text": "これらは、元のテンソルの中で最大の数と最小の数である。"
  },
  {
    "start": 1014234,
    "end": 1026082,
    "text": "は、基本的に元のテンソルの範囲を取り、このスケールパラメータによって出力範囲に絞り込み、zパラメータを使ってセンタリングする。"
  },
  {
    "start": 1026226,
    "end": 1035442,
    "text": "このzパラメータは、マイナス1にベータを掛け、sで割って、最も近い整数に丸めたものとして計算される。"
  },
  {
    "start": 1035506,
    "end": 1042220,
    "text": "zパラメータは整数、scaleパラメータは整数ではなく浮動小数点数。"
  },
  {
    "start": 1042990,
    "end": 1051290,
    "text": "東の浮動小数点をこの式に当てはめると、量子化されたベクトルが得られる。"
  },
  {
    "start": 1052270,
    "end": 1062922,
    "text": "まずわかることは、非対称量子化を使った最大の数値は常に出力範囲の最大の数値にマッピングされ、最小の数値は常に出力範囲のゼロにマッピングされるということだ。"
  },
  {
    "start": 1063066,
    "end": 1067454,
    "text": "元のベクトルのゼロがzパラメータにマッピングされる。"
  },
  {
    "start": 1067502,
    "end": 1077734,
    "text": "この130を計算すると、実際にはzパラメータとなり、他の数値はすべてゼロから255の間にマッピングされる。"
  },
  {
    "start": 1077932,
    "end": 1080978,
    "text": "そして、以下の式を使ってデカンティファイすることができる。"
  },
  {
    "start": 1080994,
    "end": 1093910,
    "text": "をデカント化し、浮動小数点数を元に戻すには、スケールに量子化された数からzを引いた値を掛けるだけで、元のテンソルが得られるはずだ。"
  },
  {
    "start": 1093990,
    "end": 1111546,
    "text": "というのも、量子化によって誤差が生じるからだ。32ビットを使えば、非常に大きな範囲を8ビットでより小さな範囲に表現できる。"
  },
  {
    "start": 1111578,
    "end": 1113970,
    "text": "もちろん、多少の誤差は生じる。"
  },
  {
    "start": 1115270,
    "end": 1117310,
    "text": "対称量子化を見てみよう。"
  },
  {
    "start": 1117470,
    "end": 1124446,
    "text": "対称量子化 前に見たように、対称な入力範囲を対称な出力範囲に変換することを目指す。"
  },
  {
    "start": 1124558,
    "end": 1126802,
    "text": "まだこのテンソルがあることを想像してほしい。"
  },
  {
    "start": 1126866,
    "end": 1130722,
    "text": "量子化された値を次のように計算する。"
  },
  {
    "start": 1130786,
    "end": 1135318,
    "text": "各数値は、浮動小数点数をパラメータsで割ったものである。"
  },
  {
    "start": 1135404,
    "end": 1143846,
    "text": "nは量子化に使用するビット数である。"
  },
  {
    "start": 1144038,
    "end": 1152240,
    "text": "sパラメータはalphaの絶対値として計算され、ここでalphaは絶対値で最大の数値となる。"
  },
  {
    "start": 1152610,
    "end": 1165250,
    "text": "この場合、-44.93という数字が絶対値として最大の値なので、このテンソルを量子化すると次のようになる。"
  },
  {
    "start": 1165400,
    "end": 1171410,
    "text": "この場合、ゼロがゼロにマッピングされることに注目すべきである。"
  },
  {
    "start": 1171830,
    "end": 1175906,
    "text": "そして、ここにある式を使ってデカンティファイすることができる。"
  },
  {
    "start": 1175928,
    "end": 1184470,
    "text": "浮動小数点数を得るには、量子化された数値をscaleパラメータとsパラメータで掛け合わせ、元のベクトルを得ればよい。"
  },
  {
    "start": 1184810,
    "end": 1192186,
    "text": "もちろん、多少の精度は落ちるので、元の数字が43.31であったことがわかるように、多少の精度は落ちる。"
  },
  {
    "start": 1192208,
    "end": 1195622,
    "text": "量子化された数字は43.16。"
  },
  {
    "start": 1195686,
    "end": 1197510,
    "text": "私たちは精度を失った。"
  },
  {
    "start": 1197590,
    "end": 1203386,
    "text": "私たちの目標はもちろん、オリジナルの配列にできるだけ近づけることだ。"
  },
  {
    "start": 1203578,
    "end": 1209630,
    "text": "もちろん、量子化のビット数を増やす最善の方法もある。"
  },
  {
    "start": 1210210,
    "end": 1221150,
    "text": "というのも、前に見たように、行列の乗算を線形レイヤーで実行し、CPUで高速化したいからだ。"
  },
  {
    "start": 1221230,
    "end": 1224254,
    "text": "CPUは常に固定ビット数で動作する。"
  },
  {
    "start": 1224302,
    "end": 1228378,
    "text": "CPU側の演算は、固定ビット数に最適化されている。"
  },
  {
    "start": 1228414,
    "end": 1233462,
    "text": "例えば、8ビット、16ビット、32ビット、64ビットに対する最適化がある。"
  },
  {
    "start": 1233516,
    "end": 1242614,
    "text": "もちろん、量子化のために11ビットを選択した場合、CPUは11ビットを使用した演算の高速化をサポートしない可能性がある。"
  },
  {
    "start": 1242662,
    "end": 1250418,
    "text": "ビット数とハードウェアの可用性との間の良い妥協点を選ぶように注意しなければならない。"
  },
  {
    "start": 1250614,
    "end": 1256880,
    "text": "後で、GPUがどのように行列の乗算を加速形式で計算するかも見てみましょう。"
  },
  {
    "start": 1258210,
    "end": 1261898,
    "text": "さて、対称と非対称の量子化をお見せしました。"
  },
  {
    "start": 1261994,
    "end": 1266510,
    "text": "では、実際にどのように実装されているのか、実際にコードを見てみよう。"
  },
  {
    "start": 1266590,
    "end": 1267730,
    "text": "見てみよう。"
  },
  {
    "start": 1267880,
    "end": 1277906,
    "text": "さて、私は非常に単純なノートを作った。基本的には、-50から150の間で20個の乱数を生成した。"
  },
  {
    "start": 1278088,
    "end": 1285942,
    "text": "最初の数字が一番大きく、2番目の数字が一番小さく、そして3番目の数字がゼロになるように、これらの数字を修正した。"
  },
  {
    "start": 1285996,
    "end": 1290700,
    "text": "最大数、最小数、そしてゼロに対する量子化の効果をチェックすることができる。"
  },
  {
    "start": 1291470,
    "end": 1293434,
    "text": "これが元の数字だとする。"
  },
  {
    "start": 1293552,
    "end": 1300262,
    "text": "この20個の数値の配列で、このベクトルを量子化する関数を定義する。"
  },
  {
    "start": 1300406,
    "end": 1306942,
    "text": "つまり、非対称量子化は、基本的にアルファを最大値、ベータを最小値として計算する。"
  },
  {
    "start": 1306996,
    "end": 1315058,
    "text": "前のスライドで見た計算式を使ってスケールとゼロを計算し、前に見たのと同じ計算式を使ってクオンタイズする。"
  },
  {
    "start": 1315144,
    "end": 1317422,
    "text": "対称量子化も同様である。"
  },
  {
    "start": 1317486,
    "end": 1322930,
    "text": "アルファ値、スケールパラメータ、クランピングの上限値、下限値を計算する。"
  },
  {
    "start": 1323350,
    "end": 1327294,
    "text": "また、スライドで見たのと同じ式を使ってデカント化することもできる。"
  },
  {
    "start": 1327342,
    "end": 1334520,
    "text": "非対称の場合、ゼロがあるのはこの方で、対称の場合、ゼロは常にゼロにマッピングされるため、ゼロはない。"
  },
  {
    "start": 1335450,
    "end": 1343414,
    "text": "また、平均二乗誤差を用いて元の値とデカント化された値を比較することで、量子化誤差を計算することもできる。"
  },
  {
    "start": 1343542,
    "end": 1346230,
    "text": "量子化にどのような影響があるか見てみよう。"
  },
  {
    "start": 1346310,
    "end": 1349530,
    "text": "これが浮動小数点数の配列である。"
  },
  {
    "start": 1349600,
    "end": 1362122,
    "text": "これを非対称量子化を使って量子化すると、このような配列になり、最大の値が255にマッピングされていることがわかる。"
  },
  {
    "start": 1362186,
    "end": 1368594,
    "text": "最小値はゼロにマップされ、ゼロは61であるzパラメータにマップされる。"
  },
  {
    "start": 1368632,
    "end": 1377734,
    "text": "ご覧のように、ゼロは61にマッピングされるが、対称量子化ではゼロはゼロにマッピングされる。"
  },
  {
    "start": 1377772,
    "end": 1383640,
    "text": "の場合、元のベクトルの3番目の要素は対称範囲の3番目の要素にマッピングされ、それがゼロとなる。"
  },
  {
    "start": 1384170,
    "end": 1394010,
    "text": "量子化されたパラメーターをデキャンタライズし直すと、元のベクトルと似ているが、まったく同じではないことがわかる。"
  },
  {
    "start": 1394080,
    "end": 1400380,
    "text": "おわかりのように、私たちは精度を少し失っており、例えば平均二乗誤差を使ってこの精度を測定することができる。"
  },
  {
    "start": 1400690,
    "end": 1405482,
    "text": "対称量子化の方が誤差が大きいことがわかる。"
  },
  {
    "start": 1405626,
    "end": 1406270,
    "text": "なぜですか？"
  },
  {
    "start": 1406420,
    "end": 1409530,
    "text": "なぜなら、元のベクトルは対称的ではないからだ。"
  },
  {
    "start": 1409610,
    "end": 1413378,
    "text": "元のベクトルは-50から150の間である。"
  },
  {
    "start": 1413464,
    "end": 1418978,
    "text": "対称量子化でやっていることは、ここで見てみよう。"
  },
  {
    "start": 1419144,
    "end": 1425538,
    "text": "対称量子化では、基本的に絶対値で最大の値をチェックする。"
  },
  {
    "start": 1425624,
    "end": 1436406,
    "text": "つまり、対称量子化は-127からプラス127の範囲をマッピングすることになる。"
  },
  {
    "start": 1436508,
    "end": 1443318,
    "text": "この配列には-127から-40までの数字はすべて含まれていない。"
  },
  {
    "start": 1443414,
    "end": 1444614,
    "text": "私たちは犠牲になっている。"
  },
  {
    "start": 1444662,
    "end": 1453134,
    "text": "そのため、他のすべての数字がこの悪い分布に苦しむことになる。"
  },
  {
    "start": 1453332,
    "end": 1456666,
    "text": "そのため、対称量子化では誤差が大きくなる。"
  },
  {
    "start": 1456778,
    "end": 1462778,
    "text": "では、リニア・レイヤーの場合、量子化がどのように機能するか、もう一度おさらいしてみよう。"
  },
  {
    "start": 1462874,
    "end": 1468446,
    "text": "このネットワークを量子化しなければ、重み行列とバイアス行列ができる。"
  },
  {
    "start": 1468558,
    "end": 1477042,
    "text": "このレイヤーの出力は、このレイヤーの入力にバイアスをかけた重みになり、出力は別の浮動小数点数になる。"
  },
  {
    "start": 1477096,
    "end": 1479846,
    "text": "これらの行列はすべて浮動小数点数である。"
  },
  {
    "start": 1480028,
    "end": 1488162,
    "text": "量子化すると、重み行列を量子化することになるが、この行列は固定行列である。"
  },
  {
    "start": 1488306,
    "end": 1497670,
    "text": "重み行列は固定であり、対称量子化または非対称量子化を使用して、前に見たアルファとベータを計算することによって量子化することができます。"
  },
  {
    "start": 1497830,
    "end": 1508590,
    "text": "ベータ・パラメーターも量子化できる。固定ベクトルなので、このベクトルのアルファとベータを計算し、8ビットで量子化できるからだ。"
  },
  {
    "start": 1510370,
    "end": 1514170,
    "text": "我々の目標は、整数を用いてこれらの演算をすべて実行することである。"
  },
  {
    "start": 1514250,
    "end": 1516222,
    "text": "x行列を量子化するには？"
  },
  {
    "start": 1516286,
    "end": 1521890,
    "text": "なぜなら、x行列はネットワークが受け取る入力に依存する入力だからである。"
  },
  {
    "start": 1522230,
    "end": 1525038,
    "text": "ひとつの方法は動的量子化と呼ばれるものだ。"
  },
  {
    "start": 1525134,
    "end": 1532834,
    "text": "動的量子化とは、その場で受け取る入力ごとにアルファとベータを計算することである。"
  },
  {
    "start": 1532882,
    "end": 1537278,
    "text": "アルファとベータを計算し、それをその場で量子化することができる。"
  },
  {
    "start": 1537474,
    "end": 1542662,
    "text": "さて、これで入力マトリックスも、例えば動的量子化を使って量子化したことになる。"
  },
  {
    "start": 1542806,
    "end": 1549334,
    "text": "この行列の乗算を実行すれば、整数行列の乗算になる。"
  },
  {
    "start": 1549462,
    "end": 1553870,
    "text": "出力は整数行列yとなる。"
  },
  {
    "start": 1554450,
    "end": 1562826,
    "text": "この行列は、量子化されていないネットワークの元の浮動小数点数ではなく、量子化された値である。"
  },
  {
    "start": 1562948,
    "end": 1567060,
    "text": "元の浮動小数点数に戻すにはどうすればいいのか？"
  },
  {
    "start": 1567430,
    "end": 1571134,
    "text": "キャリブレーションと呼ばれる作業が必要だ。"
  },
  {
    "start": 1571262,
    "end": 1581350,
    "text": "キャリブレーションとは、ネットワークを使っていくつかの入力を実行し、yの典型的な値をチェックすることである。"
  },
  {
    "start": 1581500,
    "end": 1591034,
    "text": "これらの典型的なyの値を使うことで、私たちが観測したyの値に対して、妥当なアルファと妥当なベータをチェックすることができる。"
  },
  {
    "start": 1591152,
    "end": 1605694,
    "text": "であれば、この整数行列の乗算の出力を使用し、このyに関する統計情報を収集することによって計算したスケールとゼロパラメータを使用することができる。"
  },
  {
    "start": 1605812,
    "end": 1619854,
    "text": "この出力行列をデカンテーションして浮動小数点数に戻し、ネットワークの出力が量子化されていないネットワークとあまり変わらないようにする。"
  },
  {
    "start": 1619902,
    "end": 1628222,
    "text": "量子化の目的は、各パラメーターを表現するのに必要なビット数を減らし、計算を高速化することである。"
  },
  {
    "start": 1628286,
    "end": 1636178,
    "text": "私たちの目標は、同じ入力に対して同じ出力を得ること、あるいは少なくとも、同じ入力に対してよく似た出力を得ることである。"
  },
  {
    "start": 1636274,
    "end": 1638210,
    "text": "正確さを失いたくない。"
  },
  {
    "start": 1638290,
    "end": 1646070,
    "text": "もちろん、各リニア・レイヤーの各出力を浮動小数点数にマップバックする方法を見つける必要がある。"
  },
  {
    "start": 1646150,
    "end": 1647482,
    "text": "これが我々のやり方だ。"
  },
  {
    "start": 1647536,
    "end": 1651990,
    "text": "入力行列は、動的量子化によって毎回観測することができる。"
  },
  {
    "start": 1652070,
    "end": 1654238,
    "text": "その場でクオンタイズできる。"
  },
  {
    "start": 1654324,
    "end": 1664666,
    "text": "出力は、いくつかのサンプルについて観察することができるので、典型的な最大値と最小値を知ることができ、それをアルファ値とベータ値として使うことができる。"
  },
  {
    "start": 1664698,
    "end": 1670318,
    "text": "そして、観測されたこれらの値を使って、出力yをデキャンタ化することができる。"
  },
  {
    "start": 1670414,
    "end": 1676760,
    "text": "この後、トレーニング後の量子化について、実際にどのように機能するのか、コードを見ながら見ていくことにしよう。"
  },
  {
    "start": 1677770,
    "end": 1683298,
    "text": "また、GPUがどのように行列の掛け算を行うかについても、ちらっとお見せしたい。"
  },
  {
    "start": 1683474,
    "end": 1704494,
    "text": "行列の掛け算の後に行列の足し算を行う、xにwとbを掛けた積を計算した場合、結果はx行列の各行とy行列の各列の間のドット積のリストになり、バイアスベクトルbの対応する要素を合計したものになる。"
  },
  {
    "start": 1704692,
    "end": 1718670,
    "text": "例えば、各行列が4要素のベクトルで構成されているとします。"
  },
  {
    "start": 1718830,
    "end": 1730422,
    "text": "vex行列の最初の行とw行列の最初の列のベクトルをロードし、対応する積を計算する。"
  },
  {
    "start": 1730476,
    "end": 1737442,
    "text": "x11をw11で、x12をw12で、x13をw3oneで、などなど。"
  },
  {
    "start": 1737506,
    "end": 1741702,
    "text": "次に、この値をすべてアキュムレータと呼ばれるレジスタに合計する。"
  },
  {
    "start": 1741846,
    "end": 1745686,
    "text": "さて、これは8ビットの整数である。"
  },
  {
    "start": 1745798,
    "end": 1749434,
    "text": "量子化するため、これは8ビットの整数となる。"
  },
  {
    "start": 1749632,
    "end": 1755722,
    "text": "2つの8ビット整数の乗算結果は、8ビット整数ではないかもしれない。"
  },
  {
    "start": 1755786,
    "end": 1758606,
    "text": "もちろん、これは16ビット以上でも構わない。"
  },
  {
    "start": 1758788,
    "end": 1766546,
    "text": "このため、ここでは通常32ビットで使用されるアキュムレータを使用する。"
  },
  {
    "start": 1766648,
    "end": 1776550,
    "text": "このベクトルを32ビットとして量子化するのは、このアキュムレータがすでにバイアス要素で初期化されているからである。"
  },
  {
    "start": 1776890,
    "end": 1786230,
    "text": "GPUは、このように多数のブロックを使って、初期行列のすべての行と列に対してこの処理を並列に実行します。"
  },
  {
    "start": 1786300,
    "end": 1789782,
    "text": "これがGPUアクセラレーションによる行列乗算の仕組みだ。"
  },
  {
    "start": 1789926,
    "end": 1804190,
    "text": "アルゴリズムレベルでどのようにこれが起こるかに興味があるなら、グーグルの一般的な行列乗算ライブラリ（低精度行列乗算ライブラリ）の記事を見ることをお勧めする。"
  },
  {
    "start": 1805810,
    "end": 1817666,
    "text": "さて、対称量子化と非対称量子化の違いを理解したところで、先ほど見たベータとアルファのパラメータをどのように選択するかについても理解しておきたい。"
  },
  {
    "start": 1817768,
    "end": 1830374,
    "text": "もちろん、非対称量子化の場合はベータとアルファを最小値と最大値に選び、対称量子化の場合はアルファを絶対値で最大値に選ぶという方法もある。"
  },
  {
    "start": 1830492,
    "end": 1833430,
    "text": "これが唯一の戦略ではないし、長所も短所もある。"
  },
  {
    "start": 1833500,
    "end": 1835720,
    "text": "私たちが持っているすべての戦略を見直そう。"
  },
  {
    "start": 1836250,
    "end": 1846214,
    "text": "つまり、αを元のテンソルの中で最大の値として選び、βを元のテンソルの中で最小の値として選ぶのである。"
  },
  {
    "start": 1846342,
    "end": 1858942,
    "text": "というのも、-50からプラス50のあたりに分布しているベクトルがあるとする。"
  },
  {
    "start": 1859076,
    "end": 1865374,
    "text": "この戦略の問題点は、外れ値がデキャンタ化を行うことである。"
  },
  {
    "start": 1865502,
    "end": 1869682,
    "text": "すべての数値の量子化誤差は非常に大きい。"
  },
  {
    "start": 1869736,
    "end": 1881026,
    "text": "すべての数字を量子化した後、最小・最大戦略の非対称量子化を使ってデキャンタイズすると、ご覧のように、すべての数字がオリジナルとあまり似ていないことがわかる。"
  },
  {
    "start": 1881058,
    "end": 1882262,
    "text": "実際にはまったく異なるものだ。"
  },
  {
    "start": 1882316,
    "end": 1885814,
    "text": "これが43.31、これが45.8だ。"
  },
  {
    "start": 1885852,
    "end": 1888918,
    "text": "実際、量子化にはかなり大きな誤差がある。"
  },
  {
    "start": 1889094,
    "end": 1897014,
    "text": "外れ値が入力範囲を台無しにするのを避けるより良い方法は、パーセンタイル戦略を使うことである。"
  },
  {
    "start": 1897062,
    "end": 1902714,
    "text": "アルファとベータは基本的に元の分布のパーセンタイルになるように設定する。"
  },
  {
    "start": 1902762,
    "end": 1908510,
    "text": "最大値や最小値ではなく、パーセンタイル、たとえば99％パーセンタイルを使う。"
  },
  {
    "start": 1909410,
    "end": 1915358,
    "text": "パーセンタイルを用いれば、すべての項で量子化誤差が減少していることがわかる。"
  },
  {
    "start": 1915444,
    "end": 1920340,
    "text": "量子化誤差の影響を大きく受ける唯一の項は、外れ値そのものである。"
  },
  {
    "start": 1921270,
    "end": 1927480,
    "text": "では、この最小最大戦略とパーセンタイル戦略がどう違うのか、コードを見てみよう。"
  },
  {
    "start": 1927930,
    "end": 1934710,
    "text": "今回もまた、たくさんの数字が並ぶ。"
  },
  {
    "start": 1934780,
    "end": 1938166,
    "text": "10,000の数字が-50から150の間で分布している。"
  },
  {
    "start": 1938188,
    "end": 1940126,
    "text": "その場合、外れ値を導入することになる。"
  },
  {
    "start": 1940178,
    "end": 1943660,
    "text": "最後の数字は外れ値なので、1000に等しいとしよう。"
  },
  {
    "start": 1944030,
    "end": 1947740,
    "text": "他の数字はすべて-50から150の間に分布している。"
  },
  {
    "start": 1948990,
    "end": 1957194,
    "text": "2つの戦略を比較する。すなわち、最小最大戦略を用いた非対称量子化と、パーセンテージ戦略を用いた非対称量子化である。"
  },
  {
    "start": 1957242,
    "end": 1962042,
    "text": "おわかりのように、この2つの方法の唯一の違いは、アルファとベータを計算する方法です。"
  },
  {
    "start": 1962106,
    "end": 1964162,
    "text": "ここで、アルファは最大値として計算される。"
  },
  {
    "start": 1964216,
    "end": 1969540,
    "text": "ここでアルファはパーセンタイルとして計算され、パーセンタイルは99.99である。"
  },
  {
    "start": 1970070,
    "end": 1978898,
    "text": "量子化された値を比較することができる。"
  },
  {
    "start": 1979074,
    "end": 1987122,
    "text": "デカンタイズしてみると、ミニマックス戦略を使ったすべての値が大きな量子化誤差に苦しんでいることがわかる。"
  },
  {
    "start": 1987186,
    "end": 1993980,
    "text": "一方、パーセンタイルを使用した場合、大きな量子化誤差に悩まされるのは異常値だけであることがわかるだろう。"
  },
  {
    "start": 1994510,
    "end": 2010190,
    "text": "見てわかるように、外れ値を除外して他の項の量子化誤差を計算すると、パーセンタイルでは誤差がずっと小さいのに対し、ビンマックス戦略では外れ値を除くすべての数値で非常に大きな誤差があることがわかる。"
  },
  {
    "start": 2010770,
    "end": 2017070,
    "text": "アルファとベータを選択するために一般的に使用される他の2つの戦略は、平均二乗誤差とクロスエントロピーです。"
  },
  {
    "start": 2017150,
    "end": 2025406,
    "text": "平均二乗誤差とは、元の値と量子化された値の間の平均二乗誤差が最小になるように、アルファとベータを選ぶことを意味する。"
  },
  {
    "start": 2025598,
    "end": 2035062,
    "text": "私たちは通常、このためにグリッドサーチを使用し、例えば言語モデルを扱うときには、クロスエントロピーを戦略として使用する。"
  },
  {
    "start": 2035116,
    "end": 2043862,
    "text": "ご存知のように、言語モデルの最後のレイヤーは、線形レイヤーにソフトマックスを加えたもので、語彙からトークンを選ぶことができる。"
  },
  {
    "start": 2044006,
    "end": 2052234,
    "text": "このソフトマックス層の目標は、通常グリッド戦略かトップP戦略を使う分布確率分布を作成することである。"
  },
  {
    "start": 2052362,
    "end": 2058910,
    "text": "私たちが関心を持っているのは、この分布の内側にある値ではなく、実は分布そのものなのだ。"
  },
  {
    "start": 2058980,
    "end": 2067182,
    "text": "最大の数値は量子化された値においても最大の数値のままであるべきであり、中間の数値は相対的な分布を変えてはならない。"
  },
  {
    "start": 2067326,
    "end": 2078102,
    "text": "つまり、量子化された値と量子化されていない値の間のクロスエントロピーが大きくなるように、アルファとベータを選ぶのである。"
  },
  {
    "start": 2078156,
    "end": 2082310,
    "text": "を最小化する。"
  },
  {
    "start": 2083770,
    "end": 2091174,
    "text": "もうひとつのトピックは、量子化をするときに、畳み込みレイヤーを持つたびに登場する、粒度だ。"
  },
  {
    "start": 2091302,
    "end": 2101370,
    "text": "ご存知のように、畳み込み層は多くのフィルターやカーネルで構成され、それぞれのカーネルが例えば画像を通して特定の特徴を計算する。"
  },
  {
    "start": 2101790,
    "end": 2107706,
    "text": "さて、例えば、これらのカーネルは異なる分布を持つパラメータで構成されている。"
  },
  {
    "start": 2107738,
    "end": 2121634,
    "text": "例えば、マイナス5とプラス5の間に分布するカーネル、マイナス10とプラス10の間に分布するカーネル、マイナス6とプラス6の間に分布するカーネルがある。"
  },
  {
    "start": 2121832,
    "end": 2126214,
    "text": "すべての選手に同じアルファとベータを使えば、そうなる。"
  },
  {
    "start": 2126252,
    "end": 2132646,
    "text": "例えば、いくつかのカーネルは量子化範囲をこことここで無駄にしている。"
  },
  {
    "start": 2132828,
    "end": 2146570,
    "text": "つまり、各カーネルごとにアルファとベータを計算し、基本的にカーネルごとに異なる値にする。"
  },
  {
    "start": 2147070,
    "end": 2149740,
    "text": "この方が精度を落とさずに済む。"
  },
  {
    "start": 2150110,
    "end": 2153770,
    "text": "ここで、トレーニング後の量子化について見てみよう。"
  },
  {
    "start": 2153850,
    "end": 2158990,
    "text": "つまり、トレーニング後の量子化とは、事前にトレーニングしたモデルを量子化することを意味する。"
  },
  {
    "start": 2159410,
    "end": 2160382,
    "text": "どうすればいいんだ？"
  },
  {
    "start": 2160436,
    "end": 2165182,
    "text": "さて、事前学習されたモデルが必要で、ラベル付けされていないデータも必要だ。"
  },
  {
    "start": 2165236,
    "end": 2167842,
    "text": "オリジナルのトレーニングデータは必要ない。"
  },
  {
    "start": 2167896,
    "end": 2170834,
    "text": "推論を実行できるデータが必要なだけだ。"
  },
  {
    "start": 2170952,
    "end": 2175918,
    "text": "例えば、事前に学習されたモデルが犬と猫を分類できるモデルだとする。"
  },
  {
    "start": 2176094,
    "end": 2182120,
    "text": "データとして必要なのは、犬と猫の写真だけだ。"
  },
  {
    "start": 2182890,
    "end": 2194890,
    "text": "基本的には、事前学習されたモデルを使い、モデルに対して推論を実行している間に統計情報を収集するオブザーバーを追加します。"
  },
  {
    "start": 2194960,
    "end": 2203326,
    "text": "これらの統計量は、モデルの各層の z と s パラメータの計算に使用される。"
  },
  {
    "start": 2203428,
    "end": 2206062,
    "text": "それを使ってモデルを量子化することができる。"
  },
  {
    "start": 2206196,
    "end": 2209120,
    "text": "これがどのように機能するか、コードで見てみよう。"
  },
  {
    "start": 2209650,
    "end": 2213246,
    "text": "今回は非常にシンプルなモデルを作る。"
  },
  {
    "start": 2213348,
    "end": 2216798,
    "text": "まず、いくつかのライブラリをインポートするが、基本的にはトーチだけだ。"
  },
  {
    "start": 2216894,
    "end": 2220210,
    "text": "次にデータセットをインポートする。"
  },
  {
    "start": 2220280,
    "end": 2221662,
    "text": "アムニストを使う予定だ。"
  },
  {
    "start": 2221726,
    "end": 2230310,
    "text": "私たちの場合、mNISTの数字を分類するための非常に単純なモデルを定義する。"
  },
  {
    "start": 2231690,
    "end": 2233394,
    "text": "私はこのネットワークを作っている。"
  },
  {
    "start": 2233522,
    "end": 2235522,
    "text": "私はこのネットワークでトレーニングを行っている。"
  },
  {
    "start": 2235586,
    "end": 2240360,
    "text": "これは基本的なトレーニングのループだ。"
  },
  {
    "start": 2240730,
    "end": 2245734,
    "text": "このネットワークをこのファイルに保存する。"
  },
  {
    "start": 2245782,
    "end": 2249318,
    "text": "何エポックだったか忘れたが、5エポック分トレーニングした。"
  },
  {
    "start": 2249334,
    "end": 2251306,
    "text": "それをファイルに保存する。"
  },
  {
    "start": 2251498,
    "end": 2256138,
    "text": "検証のためのテスト・ループを定義する。"
  },
  {
    "start": 2256314,
    "end": 2258430,
    "text": "このモデルの精度は？"
  },
  {
    "start": 2258580,
    "end": 2262542,
    "text": "ではまず、量子化されていないモデルを見てみよう。"
  },
  {
    "start": 2262596,
    "end": 2268046,
    "text": "例えば、この場合、最初の線形層の重みを見てみよう。"
  },
  {
    "start": 2268078,
    "end": 2276738,
    "text": "この場合、線形レイヤーは32ビットの浮動小数点である多数の数値からなる重み行列で構成されていることがわかる。"
  },
  {
    "start": 2276914,
    "end": 2278946,
    "text": "32ビットの浮動小数点数。"
  },
  {
    "start": 2279058,
    "end": 2283750,
    "text": "量子化前のモデルのサイズは360キロバイト。"
  },
  {
    "start": 2284170,
    "end": 2293100,
    "text": "このモデルでテストループを実行すると、精度が96％であることがわかる。"
  },
  {
    "start": 2293470,
    "end": 2297526,
    "text": "もちろん、私たちの目的は量子化であり、つまり計算をスピードアップすることだ。"
  },
  {
    "start": 2297558,
    "end": 2302814,
    "text": "モデルのサイズは小さくしたいが、精度は維持したい。"
  },
  {
    "start": 2303012,
    "end": 2309258,
    "text": "最初にすることは、オブザーバーを導入してモデルのコピーを作ることだ。"
  },
  {
    "start": 2309354,
    "end": 2312394,
    "text": "見ての通り、これは量子化スタブだ。"
  },
  {
    "start": 2312442,
    "end": 2318930,
    "text": "これは、Pytorchがオンザフライで量子化を行うために使用するデカンテーション・スタブである。"
  },
  {
    "start": 2319430,
    "end": 2324190,
    "text": "そして、すべての中間層にオブザーバーを導入する。"
  },
  {
    "start": 2324270,
    "end": 2328254,
    "text": "つまり、オブザーバーがいる。"
  },
  {
    "start": 2328382,
    "end": 2335106,
    "text": "基本的には、事前に訓練したモデルから重みを取り出し、作成した新しいモデルにコピーする。"
  },
  {
    "start": 2335138,
    "end": 2336582,
    "text": "我々は新しいモデルを育成しているわけではない。"
  },
  {
    "start": 2336636,
    "end": 2346250,
    "text": "私たちは、事前に訓練したモデルの重みを、オブザーバーが何人かいるだけで、元のモデルとまったく同じ、定義した新しいモデルにコピーしているだけなのだ。"
  },
  {
    "start": 2347230,
    "end": 2351546,
    "text": "また、すべての中間層にオブザーバーを挿入する。"
  },
  {
    "start": 2351658,
    "end": 2363210,
    "text": "このオブザーバーは、基本的にはPytorchが提供する特別な節オブジェクトで、各リニアレイヤーについて統計情報を観測する。"
  },
  {
    "start": 2363290,
    "end": 2366178,
    "text": "このモデルで推論を行う。"
  },
  {
    "start": 2366344,
    "end": 2375582,
    "text": "ご覧のように、彼らが収集した統計は、各レイヤーで見た最小値と最大値だけである。"
  },
  {
    "start": 2375646,
    "end": 2378230,
    "text": "そのため、このクオンツのスタブをインプットとして持っている。"
  },
  {
    "start": 2378970,
    "end": 2382022,
    "text": "テストを使ってモデルをキャリブレーションする。"
  },
  {
    "start": 2382076,
    "end": 2394330,
    "text": "例えば、テストセットを使ってモデルに対する推論を実行する場合、つまり、モデルに対する推論を実行するためには、オブザーバーが統計情報を収集するためのデータが必要なのだ。"
  },
  {
    "start": 2394400,
    "end": 2400202,
    "text": "これはモデルに対してすべてのテストセットの推論を実行する。"
  },
  {
    "start": 2400256,
    "end": 2402990,
    "text": "我々は何もトレーニングしていない。"
  },
  {
    "start": 2403650,
    "end": 2408522,
    "text": "オブザーバーは推論を実行した後、いくつかの統計データを収集する。"
  },
  {
    "start": 2408586,
    "end": 2412670,
    "text": "例えば、ここで入力オブザーバーがいくつかの統計情報を収集したとする。"
  },
  {
    "start": 2413010,
    "end": 2419662,
    "text": "リニア第1レイヤーのオブザーバーは、第2、第3レイヤーの統計も取っている。"
  },
  {
    "start": 2419806,
    "end": 2424690,
    "text": "収集した統計を使って、量子化モデルを作ることができる。"
  },
  {
    "start": 2424760,
    "end": 2429874,
    "text": "実際の量子化は、これらの統計情報を収集した後に行われる。"
  },
  {
    "start": 2430002,
    "end": 2435320,
    "text": "そして、量子化変換というメソッドを実行し、量子化モデルを作成する。"
  },
  {
    "start": 2435690,
    "end": 2442774,
    "text": "量子化した後、各レイヤーが量子化されたレイヤーになることがわかる。"
  },
  {
    "start": 2442822,
    "end": 2445286,
    "text": "量子化前はただのリニアレイヤーだ。"
  },
  {
    "start": 2445318,
    "end": 2454282,
    "text": "量子化されたリニアレイヤーになった後、それぞれのリニアレイヤーは特別なパラメータを持つ。"
  },
  {
    "start": 2454346,
    "end": 2456400,
    "text": "目盛りとゼロ点。"
  },
  {
    "start": 2457170,
    "end": 2460730,
    "text": "量子化後のウェイト行列を表示することもできる。"
  },
  {
    "start": 2460810,
    "end": 2464670,
    "text": "ウェイト行列が8ビットの整数になっていることがわかる。"
  },
  {
    "start": 2464750,
    "end": 2470174,
    "text": "このように、デカント化された重みと元の重みを比較することができる。"
  },
  {
    "start": 2470222,
    "end": 2477646,
    "text": "元の重みは32ビットの浮動小数点数であるのに対し、デカント化された重みは32ビットの浮動小数点数である。"
  },
  {
    "start": 2477838,
    "end": 2481974,
    "text": "もちろん、デキャンタすれば、浮動小数点数は戻ってくる。"
  },
  {
    "start": 2482092,
    "end": 2484514,
    "text": "これらはディスクにどのように保存されるかである。"
  },
  {
    "start": 2484562,
    "end": 2496250,
    "text": "もちろん、デカント化する場合、元のウェイト行列に非常に近いものが得られるが、量子化によって多少の誤差が生じるため、完全に同じというわけではない。"
  },
  {
    "start": 2497310,
    "end": 2502650,
    "text": "量子化された重みは元の数値に非常に似ているが、まったく同じではない。"
  },
  {
    "start": 2502800,
    "end": 2505438,
    "text": "例えば、最初の数字はかなり違う。"
  },
  {
    "start": 2505524,
    "end": 2507294,
    "text": "2つ目もよく似ている。"
  },
  {
    "start": 2507412,
    "end": 2508990,
    "text": "3つ目もよく似ている。"
  },
  {
    "start": 2509060,
    "end": 2510270,
    "text": "などなど。"
  },
  {
    "start": 2510610,
    "end": 2517214,
    "text": "量子化後のモデルのサイズを確認すると、新しいモデルのサイズは94kbであることがわかる。"
  },
  {
    "start": 2517262,
    "end": 2521246,
    "text": "私の記憶が正しければ、もともとは360だった。"
  },
  {
    "start": 2521358,
    "end": 2523986,
    "text": "4倍に減少した。"
  },
  {
    "start": 2524088,
    "end": 2524450,
    "text": "なぜですか？"
  },
  {
    "start": 2524520,
    "end": 2533640,
    "text": "なぜなら、各数値は32ビットの浮動小数点数ではなく、整数となり、さらに他のデータを保存する必要があるため、若干のオーバーヘッドが発生するからだ。"
  },
  {
    "start": 2534250,
    "end": 2542598,
    "text": "例えば、スケール、スケール値、ゼロ点値、そしてpytorchが保存しているその他の値をすべて保存する必要があるからだ。"
  },
  {
    "start": 2542774,
    "end": 2552054,
    "text": "量子化されたモデルの精度もチェックすることができ、量子化の影響をあまり受けていないことがわかる。"
  },
  {
    "start": 2552182,
    "end": 2555242,
    "text": "しかし、実際には精度はほとんど変わらなかった。"
  },
  {
    "start": 2555306,
    "end": 2557774,
    "text": "さて、これは非常に単純な例で、モデルはかなり大きい。"
  },
  {
    "start": 2557812,
    "end": 2565074,
    "text": "私は、このモデルには十分な予測パラメータがあると思う。"
  },
  {
    "start": 2565272,
    "end": 2578920,
    "text": "実際には、モデルを量子化すると精度が落ちるので、量子化に強いモデルを作るための学習アプローチを後で紹介する。"
  },
  {
    "start": 2579770,
    "end": 2582930,
    "text": "これはトレーニング後の量子化である。"
  },
  {
    "start": 2583010,
    "end": 2585030,
    "text": "今回は以上だ。"
  },
  {
    "start": 2585180,
    "end": 2589660,
    "text": "次の量子化戦略を見てみよう。量子化を意識したトレーニングである。"
  },
  {
    "start": 2590430,
    "end": 2599498,
    "text": "私たちが基本的に行っているのは、モデルの計算グラフに偽のモデルをいくつか挿入して、トレーニング中の量子化の影響をシミュレートすることです。"
  },
  {
    "start": 2599584,
    "end": 2605486,
    "text": "以前は、モデルをトレーニングした後に量子化する方法について話していた。"
  },
  {
    "start": 2605668,
    "end": 2612286,
    "text": "この場合、量子化効果に対してよりロバストなモデルをトレーニングしたい。"
  },
  {
    "start": 2612468,
    "end": 2617300,
    "text": "これはトレーニングの後ではなく、トレーニング中に行われる。"
  },
  {
    "start": 2617990,
    "end": 2626790,
    "text": "基本的には、入力があり、線形層があり、出力があり、目標があり、損失を計算する。"
  },
  {
    "start": 2627210,
    "end": 2637858,
    "text": "私たちが基本的に行っているのは、各レイヤーの間に量子化演算やデキャンタイズ演算といった特殊な演算を挿入することだ。"
  },
  {
    "start": 2637954,
    "end": 2643078,
    "text": "実際には、モデルがトレーニングされているので、モデルや重みを量子化しているわけではない。"
  },
  {
    "start": 2643174,
    "end": 2646694,
    "text": "その場で量子化を行う。"
  },
  {
    "start": 2646742,
    "end": 2653130,
    "text": "ここで入力を見るたびに、それを量子化し、即座にデキャンタイズして次のレイヤーに走らせる。"
  },
  {
    "start": 2653210,
    "end": 2662026,
    "text": "量子化誤差が生じるので、すぐに量子化してデキャンタし、次の出力に回す。"
  },
  {
    "start": 2662138,
    "end": 2673422,
    "text": "この損失関数が、我々が導入している偽の量子化によってもたらされる量子化誤差を処理するために、よりロバストになることを期待している。"
  },
  {
    "start": 2673566,
    "end": 2685030,
    "text": "これらの演算を導入する目的は、損失関数が量子化の影響に対抗できるように、量子化誤差を導入することにある。"
  },
  {
    "start": 2685930,
    "end": 2688540,
    "text": "そのコードを見てみよう。"
  },
  {
    "start": 2690750,
    "end": 2693578,
    "text": "量子化を意識したトレーニングに移行する。"
  },
  {
    "start": 2693744,
    "end": 2699002,
    "text": "さて、データセットをインポートする前と同じように、必要なライブラリをインポートしよう。"
  },
  {
    "start": 2699056,
    "end": 2700982,
    "text": "我々の場合はMNIstだ。"
  },
  {
    "start": 2701126,
    "end": 2704078,
    "text": "以前とまったく同じモデルを定義する。"
  },
  {
    "start": 2704164,
    "end": 2715230,
    "text": "というのも、ここでは量子化を意識してモデルをトレーニングしたいからだ。"
  },
  {
    "start": 2715310,
    "end": 2717620,
    "text": "だから量子化を意識したトレーニングというわけだ。"
  },
  {
    "start": 2718310,
    "end": 2721954,
    "text": "それ以外のモデルの構造は以前と同じである。"
  },
  {
    "start": 2722152,
    "end": 2726014,
    "text": "各レイヤーのモデルにミニマックス・オブザーバーを挿入する。"
  },
  {
    "start": 2726062,
    "end": 2732626,
    "text": "ご覧のように、このモデルは訓練されておらず、すでに何人かのオブザーバーを挿入している。"
  },
  {
    "start": 2732738,
    "end": 2738090,
    "text": "これらのオブザーバーは、推論を実行することも、このモデルに対してトレーニングを実行することもないため、キャリブレーションされていない。"
  },
  {
    "start": 2738160,
    "end": 2742170,
    "text": "これらの値はすべてプラスマイナス無限大である。"
  },
  {
    "start": 2742910,
    "end": 2748598,
    "text": "次に、amnestを使ってモデルを訓練し、1エポック訓練する。"
  },
  {
    "start": 2748774,
    "end": 2754640,
    "text": "これらのオブザーバーがトレーニング中に収集した統計情報をチェックする。"
  },
  {
    "start": 2755170,
    "end": 2761326,
    "text": "トレーニング中に、最小値と最大値の統計が収集されていることがわかる。"
  },
  {
    "start": 2761508,
    "end": 2767230,
    "text": "量子化を意識したトレーニングでは、このようなウェイトの偽量子を持っていることがわかるだろう。"
  },
  {
    "start": 2767310,
    "end": 2778390,
    "text": "これは実際には、トレーニング中に導入した偽の量子化オブザーバーであり、いくつかの値や統計を収集している。"
  },
  {
    "start": 2778730,
    "end": 2785238,
    "text": "そして、トレーニング中に収集された統計量を使用することで、モデルを量子化することができる。"
  },
  {
    "start": 2785404,
    "end": 2791030,
    "text": "量子化されたモデルの値、スケール、ゼロ点を表示することができる。"
  },
  {
    "start": 2791100,
    "end": 2792840,
    "text": "ここで見ることができる。"
  },
  {
    "start": 2793290,
    "end": 2796090,
    "text": "量子化されたモデルの重みを表示することもできる。"
  },
  {
    "start": 2796160,
    "end": 2800918,
    "text": "を見ると、最初の線形レイヤーの重み行列が実際には整数行列であることがわかる。"
  },
  {
    "start": 2801094,
    "end": 2803174,
    "text": "精度を上げることもできる。"
  },
  {
    "start": 2803222,
    "end": 2806880,
    "text": "このモデルの精度はゼロ点92であることがわかる。"
  },
  {
    "start": 2807810,
    "end": 2812646,
    "text": "まあ、この場合は他のケースより少し悪いけど、これはルールではない。"
  },
  {
    "start": 2812698,
    "end": 2817758,
    "text": "通常、量子化を意識したトレーニングは、モデルを量子化の影響に対してよりロバストにする。"
  },
  {
    "start": 2817854,
    "end": 2824734,
    "text": "通常、トレーニング後に量子化を行うと、量子化よりもモデルの精度が落ちる。"
  },
  {
    "start": 2824862,
    "end": 2827560,
    "text": "量子化を意識したトレーニングでモデルを訓練する場合。"
  },
  {
    "start": 2829050,
    "end": 2830710,
    "text": "スライドに戻ろう。"
  },
  {
    "start": 2831370,
    "end": 2844806,
    "text": "量子化を意識したトレーニングでは、各レイヤー間にいくつかのオブザーバーを導入し、各レイヤー間で特別な量子化とデキャンタ処理を行うことになる。"
  },
  {
    "start": 2844918,
    "end": 2847082,
    "text": "それなら、トレーニング中にやるんだ。"
  },
  {
    "start": 2847216,
    "end": 2858222,
    "text": "つまり、逆伝播アルゴリズムは、我々が行っているこの操作に関して、損失関数の勾配を計算することもできるはずだ。"
  },
  {
    "start": 2858276,
    "end": 2862406,
    "text": "量子化の操作は微分できない。"
  },
  {
    "start": 2862458,
    "end": 2872270,
    "text": "逆伝播アルゴリズムは、順方向ループで行っている量子化演算の勾配をどのように計算するのですか？"
  },
  {
    "start": 2872430,
    "end": 2890390,
    "text": "つまり、ベータパラメータとアルファパラメータの中間に位置するすべての値について、勾配を1とし、この範囲外の値をすべて折りたたむのです。"
  },
  {
    "start": 2890550,
    "end": 2893420,
    "text": "勾配をゼロで近似する。"
  },
  {
    "start": 2893790,
    "end": 2897654,
    "text": "これは、量子化演算が微分可能ではないからだ。"
  },
  {
    "start": 2897702,
    "end": 2901950,
    "text": "このため、この近似を使って勾配を近似する必要がある。"
  },
  {
    "start": 2903330,
    "end": 2908782,
    "text": "次に注目すべきは、なぜ量子化を意識したトレーニングが有効なのかということだ。"
  },
  {
    "start": 2908836,
    "end": 2913086,
    "text": "つまり、量子化を意識したトレーニングが損失関数に与える影響とは？"
  },
  {
    "start": 2913188,
    "end": 2921202,
    "text": "というのも、前にもお話ししたように、私たちの目標は学習中に量子化誤差を導入し、損失関数がそれに反応できるようにすることだからだ。"
  },
  {
    "start": 2921256,
    "end": 2922130,
    "text": "どうやって？"
  },
  {
    "start": 2922280,
    "end": 2929990,
    "text": "ここで、量子化の概念を持たないモデルを訓練するときに、訓練後の量子化を行うとしよう。"
  },
  {
    "start": 2930490,
    "end": 2936550,
    "text": "重みが1つしかなく、この特定の重みに対して損失関数が計算されるとする。"
  },
  {
    "start": 2937370,
    "end": 2950434,
    "text": "逆伝播アルゴリズム、あるいは勾配降下アルゴリズムの目的は、実際には、損失を最小化するようにモデルの重みを計算することである。"
  },
  {
    "start": 2950582,
    "end": 2958000,
    "text": "通常、これが損失関数で、このローカル・ミニマムに行き着くと仮定する。"
  },
  {
    "start": 2958530,
    "end": 2966670,
    "text": "量子化を意識したトレーニングの目的は、モデルをより広い範囲のローカルミニマムに到達させることである。"
  },
  {
    "start": 2966830,
    "end": 2967394,
    "text": "なぜですか？"
  },
  {
    "start": 2967512,
    "end": 2973666,
    "text": "なぜなら、ここでのウェイト値は、量子化した後に変化するからだ。"
  },
  {
    "start": 2973768,
    "end": 2984550,
    "text": "例えば、量子化せずにトレーニングした場合、量子化後に損失がここにあり、重み値がここにあったとすると、この重み値は当然変更される。"
  },
  {
    "start": 2984620,
    "end": 3001662,
    "text": "しかし、量子化を意識したトレーニングでは、ローカル・ミニマムやより幅の広いミニマムを選ぶので、量子化後のウェイトが多少動いても、損失はそれほど大きくなりません。"
  },
  {
    "start": 3001796,
    "end": 3004960,
    "text": "これが、量子化を意識したトレーニングが機能する理由だ。"
  },
  {
    "start": 3005810,
    "end": 3007790,
    "text": "僕のビデオを見てくれてありがとう。"
  },
  {
    "start": 3007940,
    "end": 3010378,
    "text": "量子化について楽しく学んでいただけただろうか。"
  },
  {
    "start": 3010474,
    "end": 3017614,
    "text": "GPTQやAWQのような高度なトピックについては話していない。"
  },
  {
    "start": 3017732,
    "end": 3023760,
    "text": "このビデオを気に入っていただけましたら、ぜひご購読いただき、「いいね！」を押して、ご友人や同僚、生徒の皆さんとシェアしてください。"
  },
  {
    "start": 3024970,
    "end": 3035300,
    "text": "ディープラーニングや機械学習に関するビデオは他にもありますので、分からないことがあれば教えてください。また、LinkedInやソーシャルメディアで気軽に私とつながってください。"
  }
]