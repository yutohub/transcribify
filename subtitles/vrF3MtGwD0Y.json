[
  {
    "start": 1290,
    "end": 3482,
    "text": "こんにちは、マンバについて話しましょう。"
  },
  {
    "start": 3546,
    "end": 8906,
    "text": "マンバについて話し、状態空間モデルについて説明するのはもう時効だ。"
  },
  {
    "start": 9018,
    "end": 26150,
    "text": "マンバは、その登場直後からユビキタス・トランスフォーマーに取って代わる可能性があると言われ話題を呼んだが、iclearとこのAI coffee breakでマンバの論文がボツになったことは誰もが驚いた。"
  },
  {
    "start": 26220,
    "end": 40038,
    "text": "私たちは、状態空間モデルや短いssmsについて理解する必要があるすべての重要なことを説明し、マンバが選択的状態空間モデルでssmsをさらに良くする方法を説明する。"
  },
  {
    "start": 40214,
    "end": 57086,
    "text": "トランスフォーマーは、テキスト、イメージ、オーディオ、ビデオ、ゲノミクスなど、あらゆるデータタイプ、あらゆるモダリティにおいて、長年にわたって最も人気のあるアーキテクチャであった。"
  },
  {
    "start": 57188,
    "end": 63838,
    "text": "ビデオの中のチャプターを使って、ssmsとMambaの興味のある部分に正確にナビゲートしてください。"
  },
  {
    "start": 63924,
    "end": 70562,
    "text": "マンバオはアルバート・グーとスリー・ダオによって紹介される。"
  },
  {
    "start": 70696,
    "end": 82230,
    "text": "マンバの主な貢献は、ssmsを改善することである。ssmsはすでにトランスフォーマーよりも高速で、長いシーケンスをモデリングするときに最も重要な、トランスフォーマーよりも少ないメモリしか使わなかった。"
  },
  {
    "start": 82570,
    "end": 95030,
    "text": "ssmsはまだトランスフォーマーほど高性能ではなかったので、ssmsで節約できる高速化のボーナスやメモリは、精度を犠牲にすることになった。"
  },
  {
    "start": 95190,
    "end": 107274,
    "text": "mambaはsmsを取り込み、選択的smsにすることでそれらを改善し、予測の質から判断して、トランスフォーマーより優れているか匹敵する。"
  },
  {
    "start": 107402,
    "end": 112014,
    "text": "smsは、長いシーケンスであっても処理が速い。"
  },
  {
    "start": 112132,
    "end": 119678,
    "text": "選択的状態空間モデルの改善を理解する前に、まず状態空間モデルを理解する必要がある。"
  },
  {
    "start": 119774,
    "end": 122590,
    "text": "ストラップを締めて、この旅に備えよう。"
  },
  {
    "start": 122750,
    "end": 129830,
    "text": "smsは通常、より大きなニューラルネットワーク・アーキテクチャの一部である。"
  },
  {
    "start": 129980,
    "end": 143270,
    "text": "高レベルの観点からは、これらは線形RNのように動作し、前のトークンの出力表現と現在の入力トークンの埋め込みが変換され、その後結合される。"
  },
  {
    "start": 143430,
    "end": 149290,
    "text": "はい、rnnsと同じように、smsは入力トークンを次々に処理します。"
  },
  {
    "start": 149440,
    "end": 161758,
    "text": "ssmsは4つのモデルとも呼ばれ、入力を処理するための4組の行列とパラメータ、すなわちデルタ、a、b、cを持つ。"
  },
  {
    "start": 161924,
    "end": 164638,
    "text": "これらのマトリックスには、それぞれ異なる仕事がある。"
  },
  {
    "start": 164734,
    "end": 169250,
    "text": "デルタはa行列とB行列の重みを変更する。"
  },
  {
    "start": 169590,
    "end": 178390,
    "text": "そして、修正されたaは、トークンからトークンへ、隠された状態をどれだけ伝播させるかを決定する。"
  },
  {
    "start": 178730,
    "end": 188774,
    "text": "修正されたbは、入力がどれだけ隠れた状態に入るかを決定し、cは隠れた状態がどのように出力に変換されるかを決定する。"
  },
  {
    "start": 188902,
    "end": 192726,
    "text": "ここでさらに詳しく説明すると、smsは2つのステップを踏む。"
  },
  {
    "start": 192838,
    "end": 204426,
    "text": "まず、いわゆる離散化ステップの間に、デルタは行列aとbに何かをする。"
  },
  {
    "start": 204538,
    "end": 207086,
    "text": "の代わりにバーが表示される。"
  },
  {
    "start": 207188,
    "end": 216290,
    "text": "さらに、この別の式に従ってbを修正し、b barを得る。"
  },
  {
    "start": 216630,
    "end": 219890,
    "text": "なぜ離散化が必要なのかは、後ほど説明する。"
  },
  {
    "start": 219960,
    "end": 226702,
    "text": "まずはssmsの全体像を把握し、それからこの離散化のステップを理解しよう。"
  },
  {
    "start": 226856,
    "end": 234470,
    "text": "今のところ、デルタが行列aとbのエントリを修正する離散化ステップが必要だと仮定するだけでよい。"
  },
  {
    "start": 234620,
    "end": 239526,
    "text": "デルタ自体は、SSMを訓練する際にデータから学習できるパラメータだ。"
  },
  {
    "start": 239638,
    "end": 245130,
    "text": "次に、a bar行列とb bar行列を使って、線形Rnnステップに進む。"
  },
  {
    "start": 245280,
    "end": 252474,
    "text": "RNNのように、SSMはトークンごとに働き、前のトークンに基づいて新しい隠れ表現を処理する。"
  },
  {
    "start": 252522,
    "end": 274750,
    "text": "つまり、トークンtの隠された状態を得るには、前のトークンの隠された状態からtを引いたものにバーを掛ける。"
  },
  {
    "start": 274910,
    "end": 287586,
    "text": "そして、現在のトークンの入力エンベッディングにbバーを掛け合わせ、この2つを足す。"
  },
  {
    "start": 287778,
    "end": 293194,
    "text": "ほとんどの場合、私たちはその時間ステップで隠された表現を使って何かをしたい。"
  },
  {
    "start": 293312,
    "end": 301450,
    "text": "例えば、次のトークンを予測したり、配列全体を分類したりする。"
  },
  {
    "start": 301530,
    "end": 310990,
    "text": "例えば、現在のトークンの最終的な表現を得るために、SSMはcとそのトークンの隠された表現を掛け合わせる。"
  },
  {
    "start": 311330,
    "end": 325534,
    "text": "トランスフォーマーと同じように、この表現を使ってソフトマックスを行い、例えば語彙の中の5万語のうち、次に出てきそうな単語を分類することができる。"
  },
  {
    "start": 325662,
    "end": 333922,
    "text": "では、なぜsmsが速くて素晴らしく、トランスフォーマーに匹敵するのか、すぐに説明しよう。"
  },
  {
    "start": 333986,
    "end": 348534,
    "text": "なぜ離散化ステップが必要なのか、なぜaやbを直接学ぶのではなく、まず奇妙な公式によってaやbを修正する必要があるのか。"
  },
  {
    "start": 348662,
    "end": 360490,
    "text": "専門的になりすぎたら、遠慮なく次の章に飛んでほしい。"
  },
  {
    "start": 360650,
    "end": 368820,
    "text": "smsは連続微分方程式を行列方程式に変換したものである。"
  },
  {
    "start": 369190,
    "end": 377414,
    "text": "つまり、連続smsは、変数hが時間とともにどのように変化するかを示す微分方程式である。"
  },
  {
    "start": 377532,
    "end": 387382,
    "text": "例えば、ここでhは正弦波のように変化し、ドットはhの時間微分を表し、hが時間とともにどのように変化するかを表す。"
  },
  {
    "start": 387516,
    "end": 396886,
    "text": "この方程式は、hが将来どのように変化するかは、hの現在の状態と外的要因に依存するということ以外、何も語っていない。"
  },
  {
    "start": 397078,
    "end": 400666,
    "text": "これはリニアRNNを思い出させないだろうか？"
  },
  {
    "start": 400778,
    "end": 412266,
    "text": "そこでは基本的に、トークンの将来の隠された状態は、変更された現在の状態に、たまたま現在の入力トークンである追加要素を加えたものである。"
  },
  {
    "start": 412458,
    "end": 418830,
    "text": "我々のsmsは、この連続微分方程式の離散化バージョンに過ぎない。"
  },
  {
    "start": 418990,
    "end": 431650,
    "text": "この理論によれば、連続的なケースから行列を使った離散的な定式化に変換するためには、どの程度詳細に離散化するかを示すステップ・サイズ・デルタを選択する必要がある。"
  },
  {
    "start": 431810,
    "end": 436306,
    "text": "連続的な場合、曲線に沿って無限小のステップで移動することになる。"
  },
  {
    "start": 436418,
    "end": 441306,
    "text": "離散的なケースでは、状態から状態へと離散的なステップを踏む。"
  },
  {
    "start": 441488,
    "end": 449046,
    "text": "ステップを大きくすればhのジャンプを大きくできるが、大きくしすぎるとカーブから外れてエラーを起こす可能性がある。"
  },
  {
    "start": 449078,
    "end": 462462,
    "text": "このような粗い近似では、デルタは、例えば、シーケンス長で割ったスカラーに設定できるパラメータである。シーケンス長によって、必要なトランジションとステップの数が決まるからだ。"
  },
  {
    "start": 462596,
    "end": 468222,
    "text": "入力埋め込みを高次元のベクトルで初期化するため、hは多次元になる。"
  },
  {
    "start": 468286,
    "end": 476526,
    "text": "異なる次元に進むために異なるステップサイズを選択したい場合、デルタはそのエントリーを変化させることができる行列でもある。"
  },
  {
    "start": 476718,
    "end": 489302,
    "text": "ここでは説明しないが、離散化のステップを正しく行うには、連続設定のaとbを離散設定のバーとバーに変換しなければならない。"
  },
  {
    "start": 489446,
    "end": 491082,
    "text": "よし、これで終わりだ。"
  },
  {
    "start": 491136,
    "end": 495530,
    "text": "では、なぜsmsがこれほど素晴らしいのかを説明しよう。"
  },
  {
    "start": 495680,
    "end": 508970,
    "text": "トランスフォーマーが優れていることは誰もが知っているが、それにもかかわらず、2次関数的にスケールする厄介な部分がある。つまり、セルフ・アテンション・サブレイヤーである。"
  },
  {
    "start": 509050,
    "end": 516798,
    "text": "配列の長さが2倍になると、自己注意の計算に必要なメモリと時間は4倍になる。"
  },
  {
    "start": 516974,
    "end": 519870,
    "text": "一方、smsはリニアにスケールする。"
  },
  {
    "start": 519950,
    "end": 526690,
    "text": "シーケンスにトークンを1つ追加するということは、SSMの計算をもう1回行う必要があるということだ。"
  },
  {
    "start": 526850,
    "end": 531394,
    "text": "シーケンスの長さが2倍になると、計算時間とメモリもちょうど2倍になる。"
  },
  {
    "start": 531442,
    "end": 534290,
    "text": "トランスフォーマーは4倍になった。"
  },
  {
    "start": 534450,
    "end": 541706,
    "text": "トランスフォーマーは偉大な言語モデルとなり、歴史的にもトランスフォーマーは超高速であるため、すぐにrnnに取って代わられた。"
  },
  {
    "start": 541808,
    "end": 548742,
    "text": "他のトークンからの結果を待つことなく、各トークンを並行して処理することができる。"
  },
  {
    "start": 548886,
    "end": 557822,
    "text": "rnnが遅いのは、次のトークンで計算を続けるために、前のトークンの結果を待つ必要があるからである。"
  },
  {
    "start": 557956,
    "end": 563410,
    "text": "今のsmsはrnnのようなもので、トークンを次々に処理する。"
  },
  {
    "start": 563560,
    "end": 565746,
    "text": "そうすると、スピードも遅くなるのか？"
  },
  {
    "start": 565928,
    "end": 594346,
    "text": "なぜなら、ssmsは線形rnnのようなもので、訓練時には高速で並列処理が可能だからです。訓練時には、入力シーケンスのすべてが訓練データに存在するので、ssmsはすべてのトークンに対して並列に線形変換を組み合わせ、事前計算し、実行することができますが、推論時には、入力が1つずつ生成されるので、トークンを次々に処理する必要があります。"
  },
  {
    "start": 594528,
    "end": 598138,
    "text": "でも、トレーニング中の並列化はどうなっているんだ？"
  },
  {
    "start": 598224,
    "end": 603374,
    "text": "先ほど、次のトークンを計算するには最初のトークンの出力が必要だと言った。"
  },
  {
    "start": 603572,
    "end": 609326,
    "text": "まあ、結局のところ、SSMには線形計算があるということだ。"
  },
  {
    "start": 609428,
    "end": 615374,
    "text": "入力トークンに乗算するのは、常に同じ行列a barとb barとcである。"
  },
  {
    "start": 615502,
    "end": 629558,
    "text": "では、ssmsが何をするのか、最初のトークンに対してどのような表現がなされるのか見てみよう。"
  },
  {
    "start": 629724,
    "end": 632982,
    "text": "そして2つ目のトークンはこうなる。"
  },
  {
    "start": 633116,
    "end": 640330,
    "text": "パターンを見るために3つ目のトークンについても書いてみよう。"
  },
  {
    "start": 640480,
    "end": 643926,
    "text": "このパターンで、とても規則的なんだ。"
  },
  {
    "start": 644038,
    "end": 657374,
    "text": "ここでは、入力が何であるかに関係なく、c×t×a棒×b棒のように、たくさんの行列の乗算が次々に行われる。"
  },
  {
    "start": 657572,
    "end": 668946,
    "text": "この行列乗算のスタックの後でのみ、入力と乗算しなければならないので、このような行列乗算やこのような行列乗算をすべて事前に計算することができる。"
  },
  {
    "start": 669128,
    "end": 682018,
    "text": "これらの事前計算された行列の乗算をすべて1つの巨大な行列（ここではkと呼ぶ）にまとめることができるし、入力ベクトルを別の行列に書き込むこともできる。"
  },
  {
    "start": 682194,
    "end": 690022,
    "text": "kとxを互いに畳み込むと、すべてのトークンの出力が一挙に得られる。"
  },
  {
    "start": 690086,
    "end": 694202,
    "text": "コンボリューションはGpusでも速い。"
  },
  {
    "start": 694336,
    "end": 703018,
    "text": "さて、SSMの説明を最後にすると、SSMはトランスフォーマーよりも高速で、メモリ消費量もそれほど多くない。"
  },
  {
    "start": 703194,
    "end": 716606,
    "text": "Ssmsは、トランスフォーマーが終了するのに時間がかかりすぎたり、メモリーエラーを投げたりするような長いシーケンスでも処理を続けることができる。"
  },
  {
    "start": 716798,
    "end": 721794,
    "text": "ssmsは、トランスフォーマーのような甘い高タスク精度をもたらさない。"
  },
  {
    "start": 721832,
    "end": 725698,
    "text": "なぜなら、smsは入力を処理する方法に柔軟性がないからだ。"
  },
  {
    "start": 725794,
    "end": 745430,
    "text": "入力トークンを区別することなく、すべての入力に対して同じ行列デルタ、a、b、cを適用しているが、smsが入力トークンを異なる方法で処理し、あるものを他のものよりも高く評価することができれば、特定の入力を記憶するか無視するかを選択できるようになる。"
  },
  {
    "start": 745510,
    "end": 755866,
    "text": "したがって、δ、b、cは、選択的smsの特徴である個々のトークンの埋め込みに依存するはずである。"
  },
  {
    "start": 756058,
    "end": 764430,
    "text": "選択的ssmsは、各入力トークンに対して異なるデルタ、b、cを計算するために線形レイヤーを使用する。"
  },
  {
    "start": 764510,
    "end": 777222,
    "text": "例えばデルタの場合、デルタに特化した線形層は現在の入力エンベッディングを取り込み、そこからデルタを計算する。"
  },
  {
    "start": 777356,
    "end": 783474,
    "text": "これで、各入力とSSMで異なるデルタ、b、cマトリックスが得られる。"
  },
  {
    "start": 783602,
    "end": 793142,
    "text": "選択的SSMは、あるトークンに他のトークンよりも集中することを学ぶことができる。"
  },
  {
    "start": 793286,
    "end": 800310,
    "text": "これらの入力に依存するデルタ、b、cのパラメーターで唯一の問題は、先ほどのコンボリューションのトリックが使えないことだ。"
  },
  {
    "start": 800470,
    "end": 806462,
    "text": "以前は、どのような入力が表示されるかを知る必要なく、すべての行列乗算を事前に計算できた。"
  },
  {
    "start": 806516,
    "end": 812750,
    "text": "さて、行列とその乗算を計算できるようにするためには、すでに入力を知っている必要がある。"
  },
  {
    "start": 813110,
    "end": 815710,
    "text": "したがってコンボリューションは不可能になる。"
  },
  {
    "start": 815870,
    "end": 822526,
    "text": "mambaの作者たちは、並列連想スキャンと呼ばれる、これらを高速に計算する別のトリックを紹介している。"
  },
  {
    "start": 822638,
    "end": 831702,
    "text": "それは、たとえ何かが本質的に連続的であると感じても、何かを素早く行うための中間ステップを保存できるというアルゴリズム的な考え方に依存している。"
  },
  {
    "start": 831836,
    "end": 832834,
    "text": "合計を考えてみよう。"
  },
  {
    "start": 832882,
    "end": 847894,
    "text": "例えば、31704などの配列があり、その要素の和を計算したい場合、まずすべての前置和を計算するのがよい。"
  },
  {
    "start": 847942,
    "end": 848778,
    "text": "ゼロだ。"
  },
  {
    "start": 848944,
    "end": 856046,
    "text": "そして1に3を足して4、4に7を足して11......。"
  },
  {
    "start": 856148,
    "end": 864674,
    "text": "最後の要素だけの和を計算したい場合は、22から3を引くだけでいいからだ。"
  },
  {
    "start": 864872,
    "end": 876162,
    "text": "物事を効率的に計算するこれらのスキャン・アルゴリズム以上に、物事を計算し、適切なタイプのGPUメモリに格納するハードウェア固有の実装が必要だ。"
  },
  {
    "start": 876226,
    "end": 881010,
    "text": "なぜなら、純粋なPytorchではこのスキャンの方が遅いからだ。"
  },
  {
    "start": 881170,
    "end": 889382,
    "text": "を高速化するために、著者らはデルタa、b、cを低速HPM GPU RAMから高速GPU SRAMに読み込んだ。"
  },
  {
    "start": 889526,
    "end": 893670,
    "text": "SRAMでaとbを離散化するのだ。"
  },
  {
    "start": 893830,
    "end": 902262,
    "text": "その後、SRAMで並列連想走査を行い、Cとの乗算を行い、結果をHPMに書き戻す。"
  },
  {
    "start": 902406,
    "end": 908346,
    "text": "ここで詳しく説明するほどの知識は持ち合わせていないが、2つの素晴らしい情報源を紹介しよう。"
  },
  {
    "start": 908458,
    "end": 914750,
    "text": "純粋なPytorchの実装が必要な場合は、Rudy payによるこの素晴らしいレポをチェックしてください。"
  },
  {
    "start": 914900,
    "end": 920590,
    "text": "連想スキャンに関する完全なCUDAチュートリアルが必要なら、Nvidiaのこのチュートリアルをチェックしてほしい。"
  },
  {
    "start": 920750,
    "end": 936546,
    "text": "mambaについて知っておく必要があるのは、原理的には、選択的なSSmsは畳み込みができないのでssmsより遅くなるが、作者は高速かつ並列に計算する別の方法の高速GPU実装を見つけることができたということだ。"
  },
  {
    "start": 936658,
    "end": 939930,
    "text": "mambaは線形時間アーキテクチャを維持する。"
  },
  {
    "start": 940350,
    "end": 941290,
    "text": "オーケー、素晴らしい。"
  },
  {
    "start": 941360,
    "end": 947830,
    "text": "ssmsと選択的ssmsについて十分に学び、ようやくマンバのレイヤーを理解することができた。"
  },
  {
    "start": 947990,
    "end": 954762,
    "text": "つのマンバ・レイヤーは、選択的状態空間モジュールと、その他以下のようなもので構成されている。"
  },
  {
    "start": 954906,
    "end": 959706,
    "text": "まず、線形層は入力トークン埋込みの次元を2倍にする。"
  },
  {
    "start": 959818,
    "end": 969230,
    "text": "次元が高いほど、ネットワークが情報を押し流すスペースが広くなり、低次元では分離不可能だったクラスが、高次元では分離可能になるかもしれない。"
  },
  {
    "start": 969390,
    "end": 972494,
    "text": "著者らは64次元の入力埋め込みを使用している。"
  },
  {
    "start": 972542,
    "end": 978886,
    "text": "このレイヤーは次元を64から128に増やし、次にカノニカルなものにする。"
  },
  {
    "start": 978908,
    "end": 982162,
    "text": "畳み込み層は前の層の出力を取り込む。"
  },
  {
    "start": 982226,
    "end": 989590,
    "text": "その役割は、リニアにアップスケールされた128次元ベクトルの次元間の情報をプッシュしまくることである。"
  },
  {
    "start": 989930,
    "end": 993994,
    "text": "cルー活性化関数とも呼ばれるスウィッシュを使用する。"
  },
  {
    "start": 994192,
    "end": 1002880,
    "text": "次に、先ほど説明したように、線形RNNのように畳み込みの出力を処理する選択的状態空間モジュールが登場する。"
  },
  {
    "start": 1003250,
    "end": 1006602,
    "text": "そしてマンバはゲート掛け算をする。"
  },
  {
    "start": 1006746,
    "end": 1021410,
    "text": "つまり、入力を別の線形層に通し、それをスウィッシュまたはシルエ活性化関数に投入し、その結果を選択的SSMの出力に掛け合わせる。"
  },
  {
    "start": 1021830,
    "end": 1035250,
    "text": "この操作の背後にある著者の直感は、乗算が、前のトークンからの情報を含むSSMの出力と、現在のトークンの埋め込みとの間の類似性の尺度であるということである。"
  },
  {
    "start": 1035410,
    "end": 1043242,
    "text": "その後、線形レイヤーで次元を128から64に戻す。"
  },
  {
    "start": 1043376,
    "end": 1048922,
    "text": "マンバを手に入れるには、複数のマンバ・レイヤーを数回重ねるだけでいい。"
  },
  {
    "start": 1049056,
    "end": 1061806,
    "text": "他のSSMアーキテクチャーとは異なり、トランスフォーマーが互いにトランスフォーマー層だけを重ねて構成されているのと同じように、同じレイヤーを常に使用することができるからだ。"
  },
  {
    "start": 1061988,
    "end": 1075954,
    "text": "このアーキテクチャーが複雑だと思うのなら、トランスフォーマー・レイヤーにも多くのコンポーネントがあり、自己注意、ネットワーク正規化のためのフィード、残差レイヤーなどがあることを考えてほしい。"
  },
  {
    "start": 1076072,
    "end": 1078598,
    "text": "マンバの技術革新が実を結んだ。"
  },
  {
    "start": 1078684,
    "end": 1081490,
    "text": "マンバはトランスフォーマーに匹敵するパフォーマンスを持っている。"
  },
  {
    "start": 1081570,
    "end": 1083538,
    "text": "このスケーリングの法則を見てほしい。"
  },
  {
    "start": 1083634,
    "end": 1097402,
    "text": "パイルデータセットで言語モデリングを行った場合、マンバの当惑度とトランスフォーマーの当惑度は同じように減少する。"
  },
  {
    "start": 1097536,
    "end": 1105514,
    "text": "マンバはまた、超高速であることによって、他のすべてのsmsまたは注目無料モデル、そしてこのすべてを上回る。"
  },
  {
    "start": 1105712,
    "end": 1123054,
    "text": "128のバッチサイズでは、14億のパラメーターを持つmambaは1秒間に1814のトークンを処理できる。"
  },
  {
    "start": 1123182,
    "end": 1130086,
    "text": "バッチサイズが8個の場合、マンバは1秒間に744個のトークンを処理できるが、トランスフォーマーは265個しか処理できない。"
  },
  {
    "start": 1130108,
    "end": 1148934,
    "text": "というのも、トランスフォーマーのフラッシュ・アテンション2は、信じられないほど最適化されたマンバ・スキャンよりも、シーケンスの長さが長くなるにつれて、より多くの時間を必要とするからだ。"
  },
  {
    "start": 1149062,
    "end": 1155562,
    "text": "マンバ・スキャンには10ミリ秒、フラッシュ・アテンション2には約1000ミリ秒が必要だ。"
  },
  {
    "start": 1155706,
    "end": 1158650,
    "text": "はい、Y軸は対数目盛りを使っています。"
  },
  {
    "start": 1158810,
    "end": 1164330,
    "text": "著者らはマンバを下流のタスクでもテストしており、全体的に素晴らしい精度を示している。"
  },
  {
    "start": 1164500,
    "end": 1170142,
    "text": "大胆な数字であればあるほどいいし、変圧器やアテンションフリーの方法よりもいい。"
  },
  {
    "start": 1170286,
    "end": 1175702,
    "text": "私たちが示したかったのは、マンバがカバーできるデータ型の多様性です。"
  },
  {
    "start": 1175836,
    "end": 1183570,
    "text": "Mambaは、モデルがDNA配列から種を分類するDNA配列分類タスクに最適である。"
  },
  {
    "start": 1183730,
    "end": 1193654,
    "text": "ここで、シーケンスの長さは100万トークンまで大幅に増加し、マンバは自己回帰オーディオモデリングで競合よりも優れたパフォーマンスを発揮する。"
  },
  {
    "start": 1193702,
    "end": 1197734,
    "text": "マンバは、SSMであった以前の最先端技術を凌駕している。"
  },
  {
    "start": 1197862,
    "end": 1208250,
    "text": "まとめると、マンバは多くのデータ型に対応し、トランスフォーマーに挑戦している。"
  },
  {
    "start": 1208410,
    "end": 1220386,
    "text": "もちろん、ssmsがトランスフォーマーに取って代わるかどうかは時間が解決してくれるだろう。"
  },
  {
    "start": 1220568,
    "end": 1225374,
    "text": "面白いことに、イクリアの査読ではマンバが最初に却下された。"
  },
  {
    "start": 1225422,
    "end": 1232034,
    "text": "機械学習に携わる誰もが、マンバの上に構築するために査読の承認を待っている暇があるとは思えない。"
  },
  {
    "start": 1232162,
    "end": 1238002,
    "text": "私たちはすでに、画像を処理するエキスパート・マンバとビジョン・マンバの混合物を持っている。"
  },
  {
    "start": 1238146,
    "end": 1246310,
    "text": "また、面倒なトークナイザーに頼らず、生のバイトから直接学習するためにmambaを使うmamba byteもある。"
  },
  {
    "start": 1246470,
    "end": 1258986,
    "text": "もちろん、バイトを入力とすることで、シーケンスが非常に長くなり、シーケンスの長さに対して2次関数的にスケーリングするトランスフォーマーでは問題となったが、リニアにスケーリングするマンバでは問題ない。"
  },
  {
    "start": 1259178,
    "end": 1263222,
    "text": "これはssmsという形でのRNのカムバックなのだろうか？"
  },
  {
    "start": 1263386,
    "end": 1268562,
    "text": "私の主観では、RNはパワフルだが、ずっと遅かった。"
  },
  {
    "start": 1268696,
    "end": 1278520,
    "text": "足りなかったのは、彼らも速くするための十分な研究上の注意力だけだった。しかし、誰もが代わりに線形注意力と閃光注意力の研究に忙殺されていた。"
  },
  {
    "start": 1279050,
    "end": 1289666,
    "text": "RWKVが発表したトランスフォーマー時代のRNNの再発明は、誰もが改良に取り組んでいたトランスフォーマーにRNNを追いつかせようとした最初の試みだった。"
  },
  {
    "start": 1289778,
    "end": 1292490,
    "text": "今、マンバがついに釘を刺した。"
  },
  {
    "start": 1292560,
    "end": 1292938,
    "text": "すごいね。"
  },
  {
    "start": 1293024,
    "end": 1298442,
    "text": "このビデオの最後までお付き合いいただきありがとうございました。"
  },
  {
    "start": 1298576,
    "end": 1301274,
    "text": "このビデオが気に入ったら、「いいね！」と「購読」をお忘れなく。"
  },
  {
    "start": 1301322,
    "end": 1303438,
    "text": "こちらもご覧ください。"
  },
  {
    "start": 1303524,
    "end": 1306554,
    "text": "AIコーヒーブレイクのグッズをリリースした。"
  },
  {
    "start": 1306682,
    "end": 1312474,
    "text": "単なるチャンネルロゴを少し超えた、素敵なものを作りたかった。"
  },
  {
    "start": 1312602,
    "end": 1314714,
    "text": "私たちは皆さんに着ていただけるクールなデザインを求めていた。"
  },
  {
    "start": 1314762,
    "end": 1318574,
    "text": "当店をチェックして、気に入ったものがあるかどうか確かめてください。"
  },
  {
    "start": 1318692,
    "end": 1328470,
    "text": "何かを購入することで、クールな商品を手に入れることができ、同時にコーヒー豆のカフェイン補給のために数ドルを支援することができる。"
  },
  {
    "start": 1328890,
    "end": 1335640,
    "text": "お気に召すものが見つかれば幸いですが、何よりも、次回のビデオでお会いできることを楽しみにしています。"
  },
  {
    "start": 1336570,
    "end": 1337330,
    "text": "じゃあ、さようなら。"
  }
]