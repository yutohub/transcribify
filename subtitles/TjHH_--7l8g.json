[
  {
    "start": 4840,
    "end": 5232,
    "text": "こんにちは。"
  },
  {
    "start": 5270,
    "end": 8254,
    "text": "私の名前はルイス・セラーノ、ここはセラーノ・アカデミーだ。"
  },
  {
    "start": 8334,
    "end": 12758,
    "text": "このビデオはプロキシマル・ポリシー最適化（PPO）についてです。"
  },
  {
    "start": 12846,
    "end": 26118,
    "text": "近接政策最適化（PPO）は、強化学習や一般的な機械学習において非常に重要である。"
  },
  {
    "start": 26286,
    "end": 33686,
    "text": "大規模な言語モデルは、人間のフィードバックによる強化学習（RLHF）と呼ばれるものを使って学習される。"
  },
  {
    "start": 33750,
    "end": 40438,
    "text": "RLHFは次のビデオのトピックで、ゲームをさせることによって大規模な言語モデルを学習させるというものだ。"
  },
  {
    "start": 40486,
    "end": 45430,
    "text": "このゲームでは、人間の評価が高ければ、モデルは高得点を獲得する。"
  },
  {
    "start": 45502,
    "end": 50190,
    "text": "RLHFで使用されている技術は、近接政策最適化である。"
  },
  {
    "start": 50302,
    "end": 54290,
    "text": "このビデオでは、プロキシマル・ポリシー最適化についてのすべてをお話しします。"
  },
  {
    "start": 54402,
    "end": 62842,
    "text": "要するに、PPOは、前のビデオで見たような、2つのニューラルネットワークを持つ他の強化学習手法と非常によく似ている。"
  },
  {
    "start": 62898,
    "end": 65858,
    "text": "価値観のネットワークとポリシーのネットワークを訓練するのだ。"
  },
  {
    "start": 65946,
    "end": 72482,
    "text": "さて、PPOが特別なのは、バリュー・ネットワークとポリシー・ネットワークが同時に訓練されることだ。"
  },
  {
    "start": 72658,
    "end": 73538,
    "text": "その方法をお見せしよう。"
  },
  {
    "start": 73586,
    "end": 75866,
    "text": "このビデオでは3つのことを学ぶ。"
  },
  {
    "start": 76010,
    "end": 81764,
    "text": "まずバリュー・ニューラル・ネットワーク、次にポリシー・ニューラル・ネットワーク、そしてそれらのトレーニング方法だ。"
  },
  {
    "start": 81844,
    "end": 85224,
    "text": "ここでのコツは、同時にトレーニングするということだ。"
  },
  {
    "start": 89804,
    "end": 94028,
    "text": "前のビデオとよく似たグリッドワールドを紹介しよう。"
  },
  {
    "start": 94116,
    "end": 99684,
    "text": "ここにグリッドがあり、エージェントがこのグリッドの周りを歩く。"
  },
  {
    "start": 99844,
    "end": 102676,
    "text": "さて、このグリッドには特別なセルがある。"
  },
  {
    "start": 102740,
    "end": 104944,
    "text": "例えば、ここにはたくさんのお金がある。"
  },
  {
    "start": 105124,
    "end": 106824,
    "text": "ここには大金もある。"
  },
  {
    "start": 106944,
    "end": 109284,
    "text": "ここにドラゴンがいる。"
  },
  {
    "start": 109664,
    "end": 112088,
    "text": "お金になればそれでいい。"
  },
  {
    "start": 112136,
    "end": 115384,
    "text": "もしドラゴンの中に入ってしまったら、ドラゴンに食べられてしまうからだ。"
  },
  {
    "start": 115464,
    "end": 116920,
    "text": "私たちは数字を出すことができる。"
  },
  {
    "start": 116992,
    "end": 119944,
    "text": "こちらで賞金圏内に入ればプラス5点。"
  },
  {
    "start": 120064,
    "end": 121552,
    "text": "こちらではプラス4点だ。"
  },
  {
    "start": 121648,
    "end": 124568,
    "text": "ドラゴンに食べられたらマイナス1点。"
  },
  {
    "start": 124656,
    "end": 129208,
    "text": "こことここのように、越えてはいけない壁がある。"
  },
  {
    "start": 129336,
    "end": 136968,
    "text": "このゲームは、エージェントがグリッドの中を上下左右に移動しながら歩くというものだ。"
  },
  {
    "start": 137096,
    "end": 140568,
    "text": "特別な箱に当たれば、その数だけ得点が入る。"
  },
  {
    "start": 140616,
    "end": 143136,
    "text": "この試合では4点を獲得した。"
  },
  {
    "start": 143240,
    "end": 146728,
    "text": "またプレーするようなら、何が起こるか見てみよう。"
  },
  {
    "start": 146776,
    "end": 147032,
    "text": "今すぐだ。"
  },
  {
    "start": 147088,
    "end": 150184,
    "text": "ドラゴンに食べられてマイナス1点。"
  },
  {
    "start": 150304,
    "end": 152584,
    "text": "左下から始まる必要はない。"
  },
  {
    "start": 152624,
    "end": 153840,
    "text": "どこからでも始められる。"
  },
  {
    "start": 153912,
    "end": 157384,
    "text": "例えば、この試合で5点を獲得したとしよう。"
  },
  {
    "start": 157544,
    "end": 166384,
    "text": "つまり、オレンジ色のボールであるエージェントに、最適なプレーを学んでもらうということだ。"
  },
  {
    "start": 166464,
    "end": 170824,
    "text": "そのグリッドルールは、強化学習の典型的な例だ。"
  },
  {
    "start": 170864,
    "end": 180096,
    "text": "チェスや囲碁、アタリや任天堂のゲームなど、強化学習を使って解決される問題はたくさんある。"
  },
  {
    "start": 180200,
    "end": 183544,
    "text": "また、ロボットは強化学習を使って動きを学習する。"
  },
  {
    "start": 183704,
    "end": 190884,
    "text": "最も重要なことは、私たちがいつも使っているような大規模な言語モデルは、強化学習を使って学習されるということだ。"
  },
  {
    "start": 195384,
    "end": 197328,
    "text": "州について話そう。"
  },
  {
    "start": 197456,
    "end": 198768,
    "text": "国家はとてもシンプルだ。"
  },
  {
    "start": 198856,
    "end": 201688,
    "text": "エージェントのいる場所ならどこでもいい。"
  },
  {
    "start": 201736,
    "end": 206976,
    "text": "バリアには置けないが、グリッド上のどのセルにも置くことができる。"
  },
  {
    "start": 207040,
    "end": 209846,
    "text": "言い換えれば、状態とは、ゲームが可能なあらゆる位置のことである。"
  },
  {
    "start": 209920,
    "end": 211466,
    "text": "我々には行動もある。"
  },
  {
    "start": 211530,
    "end": 214642,
    "text": "右のような正方形を見てみよう。"
  },
  {
    "start": 214778,
    "end": 217170,
    "text": "隣人が4人いるとしよう。"
  },
  {
    "start": 217282,
    "end": 220626,
    "text": "上下左右に動くことができる。"
  },
  {
    "start": 220730,
    "end": 222082,
    "text": "これが4つのアクションだ。"
  },
  {
    "start": 222138,
    "end": 226722,
    "text": "アクションは基本的に、エージェントが動く可能性のある矢印である。"
  },
  {
    "start": 226778,
    "end": 232786,
    "text": "さて、これを単純化して、エージェントに壁の中を歩かないように指示できるとしよう。"
  },
  {
    "start": 232850,
    "end": 241144,
    "text": "これらの矢印のいくつかを取り除けば、それぞれの状態に対して可能なアクションができる。"
  },
  {
    "start": 245924,
    "end": 247852,
    "text": "さて、価値観について話そう。"
  },
  {
    "start": 247908,
    "end": 250588,
    "text": "例えば、真ん中のセルを見てみよう。"
  },
  {
    "start": 250636,
    "end": 255716,
    "text": "諜報員がここにいるのなら、私は試合について何を知っているというのだ？"
  },
  {
    "start": 255900,
    "end": 262788,
    "text": "まあ、いいプレーができれば、ここから5点に到達できるとしよう。"
  },
  {
    "start": 262836,
    "end": 271284,
    "text": "ということは、私がこの状態にいる場合、5という値を割り当てることができるということだ。"
  },
  {
    "start": 271364,
    "end": 277572,
    "text": "これで、4とマイナス1以外のほとんどのマスが5の値を持つことが明らかになった。"
  },
  {
    "start": 277708,
    "end": 283372,
    "text": "なぜなら、このグリッドのどの場所からでも、一番上の5点に到達できるからだ。"
  },
  {
    "start": 283468,
    "end": 283796,
    "text": "そうだね。"
  },
  {
    "start": 283860,
    "end": 287620,
    "text": "さて、このゲームはあまり面白くないので、もう少し面白くしよう。"
  },
  {
    "start": 287732,
    "end": 294104,
    "text": "このグリッドの1カ所を移動するのに1ポイントかかるとしよう。"
  },
  {
    "start": 294224,
    "end": 296808,
    "text": "ここにいたらどうなる？"
  },
  {
    "start": 296856,
    "end": 303432,
    "text": "まあ、5点に移動して5点を稼ぐことはできるが、そこに移動したことで1点を失った。"
  },
  {
    "start": 303488,
    "end": 305160,
    "text": "この値は4である。"
  },
  {
    "start": 305232,
    "end": 311496,
    "text": "つまり、そのマスに立っていれば、4点がベストスコアということになる。"
  },
  {
    "start": 311560,
    "end": 313504,
    "text": "私がここに立っていたら？"
  },
  {
    "start": 313664,
    "end": 320964,
    "text": "まあ、この方向に最適な方法で歩き回って、私がここにいれば価値があることに気づくことができる。"
  },
  {
    "start": 321454,
    "end": 326950,
    "text": "というのも、何をするにしても、5を得るためには2つのステップを踏む必要があるからだ。"
  },
  {
    "start": 327102,
    "end": 328966,
    "text": "私がここにいれば、値は2だ。"
  },
  {
    "start": 329110,
    "end": 336486,
    "text": "つまり、あのグリッドにいたら、プラス5点まで全力で歩いても2点が精一杯ということだ。"
  },
  {
    "start": 336550,
    "end": 338054,
    "text": "さて、私がここにいたらどうなる？"
  },
  {
    "start": 338174,
    "end": 341134,
    "text": "まあ、プラス5まで歩いてみるか。"
  },
  {
    "start": 341294,
    "end": 350136,
    "text": "このゲームを続けるなら、歩いたことで多くのポイントを失い、マイナス4が精一杯だ。"
  },
  {
    "start": 350200,
    "end": 353032,
    "text": "でも、ただ歩いて行って4番を取ることもできる。"
  },
  {
    "start": 353168,
    "end": 357528,
    "text": "というのも、私は今、2つの点を指摘しているからだ。"
  },
  {
    "start": 357696,
    "end": 361044,
    "text": "ここでの値はマイナス4ではなく、2である。"
  },
  {
    "start": 361344,
    "end": 368712,
    "text": "言い換えれば、価値とは、私ができるだけ賢くプレーした場合に達成できる最大得点のことである。"
  },
  {
    "start": 368808,
    "end": 370920,
    "text": "さて、ここでルールが起きているのがわかるだろう？"
  },
  {
    "start": 370992,
    "end": 375240,
    "text": "箱があって、4つの隣があって、4つのアクションができるとしよう。"
  },
  {
    "start": 375272,
    "end": 380886,
    "text": "4つのアクションを起こすことができ、すべての隣人の値を計算したと仮定しよう。"
  },
  {
    "start": 380910,
    "end": 386774,
    "text": "隣は4、1マイナス3、2である。"
  },
  {
    "start": 386894,
    "end": 393794,
    "text": "そして当然、一番高い4に向かって歩き、そこに行くのに1かかるから1を引く。"
  },
  {
    "start": 394134,
    "end": 402786,
    "text": "つまり、あるセルの値は、隣のセルの値の最大値から1を引いた値となる。"
  },
  {
    "start": 402870,
    "end": 408098,
    "text": "それを使って、実際にセルを伝搬し、すべての値を計算し始めることができる。"
  },
  {
    "start": 408146,
    "end": 410866,
    "text": "の隣はすべて4である。"
  },
  {
    "start": 411010,
    "end": 415330,
    "text": "今、まだ埋まっていないものの中で、4の隣には3がある。"
  },
  {
    "start": 415402,
    "end": 420242,
    "text": "今、私がまだ得点していない3の隣は2の値を持っている。"
  },
  {
    "start": 420378,
    "end": 424018,
    "text": "まだ採点していない「2」の隣は「1」となる。"
  },
  {
    "start": 424146,
    "end": 427698,
    "text": "ゼロとマイナス1でも同じことだ。"
  },
  {
    "start": 427826,
    "end": 434016,
    "text": "ここで、まだラベルを貼っていないマイナス1の隣は、マイナス2の値を持っていることに注目してほしい。"
  },
  {
    "start": 434160,
    "end": 435496,
    "text": "奇妙なことに気づいてほしい。"
  },
  {
    "start": 435600,
    "end": 439976,
    "text": "もし私がここにいるなら、私にできることはマイナス1まで歩いていくことだ。"
  },
  {
    "start": 440040,
    "end": 441648,
    "text": "私はマイナス2の値を持っている。"
  },
  {
    "start": 441776,
    "end": 444608,
    "text": "もし私がここにいるなら、マイナス1まで歩くべきじゃない。"
  },
  {
    "start": 444656,
    "end": 448160,
    "text": "こっちの4番まで歩いていって、マイナス2の値を出さないといけない。"
  },
  {
    "start": 448232,
    "end": 450324,
    "text": "それが価値観だ。"
  },
  {
    "start": 454984,
    "end": 458444,
    "text": "価値観とは何かを学んだところで、ポリシーとは何かを説明しよう。"
  },
  {
    "start": 458544,
    "end": 463264,
    "text": "自由に動き回れた最初の試合に戻ろう。"
  },
  {
    "start": 463684,
    "end": 465076,
    "text": "さて、もし自由に動き回れるなら。"
  },
  {
    "start": 465140,
    "end": 471028,
    "text": "それから、私はいつもこの5番を狙いたいんだ。"
  },
  {
    "start": 471156,
    "end": 477548,
    "text": "この4番を打ちたくないのなら、4番につながるこの2本の矢を禁じるべきだということだ。"
  },
  {
    "start": 477716,
    "end": 482708,
    "text": "もし私がマイナス1を打ちたくないのであれば、この2本の矢を禁じなければならない。"
  },
  {
    "start": 482796,
    "end": 499036,
    "text": "さて、エージェントが何をしようとも、矢印に従っている限り、4やマイナス1に当たることはなく、確率1で必ず最終的に5に到達することに注目しよう。"
  },
  {
    "start": 499180,
    "end": 503292,
    "text": "これらの矢印は、いわゆるポリシーを形成する。"
  },
  {
    "start": 503428,
    "end": 505064,
    "text": "では、政策を見てみよう。"
  },
  {
    "start": 505484,
    "end": 513008,
    "text": "実際に1点をチャージして別のセルに移動するときは、その値が何だったか覚えておくこと。"
  },
  {
    "start": 513136,
    "end": 515104,
    "text": "これが各セルの値である。"
  },
  {
    "start": 515224,
    "end": 517924,
    "text": "良い政策とは何だと思いますか？"
  },
  {
    "start": 518344,
    "end": 526496,
    "text": "まあ、基本的には、隣の選手の中で一番高い値を指す矢印を付ければ、それは良い方針だ。"
  },
  {
    "start": 526640,
    "end": 531944,
    "text": "もし私がここにいるのなら、例えば、この2つに移動するのが良い方針だろう。"
  },
  {
    "start": 531984,
    "end": 535040,
    "text": "2階にも移れるけど、絶対に左には行かない。"
  },
  {
    "start": 535112,
    "end": 540928,
    "text": "もし私がここにいるなら、良い方針は、上に移動して、たぶん右に移動して、それから上に戻ることだ。"
  },
  {
    "start": 540976,
    "end": 544096,
    "text": "ポリシーが何であるかは理解できたと思う。"
  },
  {
    "start": 544160,
    "end": 554960,
    "text": "例えば、4人の隣人がいて、4つの可能な方向があるとする。"
  },
  {
    "start": 555072,
    "end": 562432,
    "text": "その値は3つだが、ポリシーは最高値を指し示す矢印でもある。"
  },
  {
    "start": 562568,
    "end": 566498,
    "text": "その最高値が繰り返されれば、1時間以上出てくるかもしれない。"
  },
  {
    "start": 566576,
    "end": 569374,
    "text": "この試合に対する良い方針はこれだろう。"
  },
  {
    "start": 569454,
    "end": 577806,
    "text": "その矢印に従う限り、私は常にそのセルで達成できる最大の得点数を得ることになる。"
  },
  {
    "start": 577870,
    "end": 579434,
    "text": "それがポリシーだ。"
  },
  {
    "start": 584254,
    "end": 590342,
    "text": "まあ、問題は解決したように見えるよね。価値を計算し、方針を計算したんだから。"
  },
  {
    "start": 590478,
    "end": 596788,
    "text": "価値観と政策、そのどちらか一方があれば、このゲームに勝つことができる。"
  },
  {
    "start": 596876,
    "end": 598196,
    "text": "キャッチは？"
  },
  {
    "start": 598340,
    "end": 605220,
    "text": "さて、お気づきかもしれないが、価値と方針を計算するためには、グリッド上のすべてのマスを訪問しなければならなかった。"
  },
  {
    "start": 605332,
    "end": 614596,
    "text": "それだけでなく、完璧な値と方針を得るまで、この値を何度も繰り返し、実際に再計算する必要があったため、何度も足を運ばなければならなかった。"
  },
  {
    "start": 614700,
    "end": 618332,
    "text": "さて、このグリッド上のすべてのマスを訪問することは、グリッドが小さければ問題ない。"
  },
  {
    "start": 618388,
    "end": 621396,
    "text": "本当に、本当に大きなグリッドを想像してみてほしい。"
  },
  {
    "start": 621460,
    "end": 624852,
    "text": "何千もの列と何千もの行があるとしよう。"
  },
  {
    "start": 624908,
    "end": 630724,
    "text": "このようなグリッドでは、すべてのマスを訪れるのは難しいし、何度も訪れるのはさらに難しい。"
  },
  {
    "start": 630884,
    "end": 632944,
    "text": "何ができるだろうか？"
  },
  {
    "start": 633324,
    "end": 638932,
    "text": "実のところ、この問題はさらに難しくなる可能性がある。"
  },
  {
    "start": 639108,
    "end": 649408,
    "text": "例えば、工場でロボットアームをトレーニングする場合、アームのすべての位置が1つの状態であり、その数は無限にある。"
  },
  {
    "start": 649456,
    "end": 655616,
    "text": "そのためにニューラルネットワークを使う。"
  },
  {
    "start": 655720,
    "end": 657328,
    "text": "どのように機能するのか？"
  },
  {
    "start": 657416,
    "end": 663256,
    "text": "ニューラルネットワークの考え方は、価値ネットワークに連続関数を与えるというものだ。"
  },
  {
    "start": 663360,
    "end": 664440,
    "text": "どういう意味かって？"
  },
  {
    "start": 664512,
    "end": 669448,
    "text": "この正方形の価値がかなり高いことがわかったとしよう。"
  },
  {
    "start": 669576,
    "end": 675064,
    "text": "ということは、周りのマスの値も高いということだろう。"
  },
  {
    "start": 675144,
    "end": 682310,
    "text": "同様に、ある値が本当に低ければ、その周りの正方形や州の値もかなり低い。"
  },
  {
    "start": 682382,
    "end": 684590,
    "text": "すべてのマスを回る必要はない。"
  },
  {
    "start": 684662,
    "end": 690806,
    "text": "そのうちのいくつかを訪れることができれば、残りの部分の関数を計算することができる。"
  },
  {
    "start": 690830,
    "end": 693174,
    "text": "そのためにニューラルネットワークを使用する。"
  },
  {
    "start": 693254,
    "end": 701510,
    "text": "ニューラルネットワークが何なのか知らない人は、私のチャンネルにあるこのビデオでニューラルネットワークの詳細とトレーニング方法について説明しているので、そちらをお勧めする。"
  },
  {
    "start": 701582,
    "end": 713596,
    "text": "知っておくべきことは、このニューラルネットワークでは、状態またはボックスの座標、つまりxとy、水平と垂直の座標を入力とする、ということだ。"
  },
  {
    "start": 713780,
    "end": 717612,
    "text": "出力はニューラルネットワークが考える値である。"
  },
  {
    "start": 717628,
    "end": 719460,
    "text": "この場合、例えば3人。"
  },
  {
    "start": 719532,
    "end": 724276,
    "text": "もちろん、別の問題ではニューラルネットワークの入力を2より大きくすることもできる。"
  },
  {
    "start": 724300,
    "end": 729932,
    "text": "基本的には、状態を記述する変数の束であり、出力は値である。"
  },
  {
    "start": 730028,
    "end": 732846,
    "text": "このニューラルネットワークをこのように描く。"
  },
  {
    "start": 732870,
    "end": 735318,
    "text": "層しかないが、何層にも重ねることができる。"
  },
  {
    "start": 735366,
    "end": 740870,
    "text": "入力は状態であり、出力は値である。"
  },
  {
    "start": 740982,
    "end": 746774,
    "text": "さて、バリュー・ニューラル・ネットワークがあったように、ポリシー・ニューラル・ネットワークもある。"
  },
  {
    "start": 746854,
    "end": 751790,
    "text": "例えば、この州にとって良い政策が計算できたとしよう。"
  },
  {
    "start": 751862,
    "end": 755086,
    "text": "このポリシーには、大きさの異なる4つの矢印があることに注目してほしい。"
  },
  {
    "start": 755150,
    "end": 760160,
    "text": "つまり、政策が推奨しているのは、下がることよりも上がること、そしてその確率が高いということだ。"
  },
  {
    "start": 760272,
    "end": 762472,
    "text": "この方針については、もう少し詳しく説明しよう。"
  },
  {
    "start": 762528,
    "end": 774048,
    "text": "言い換えれば、もし州の政策がこちらの政策であるならば、近隣の州の政策も同様であると考えるのが筋だろう？"
  },
  {
    "start": 774216,
    "end": 781884,
    "text": "そこで、価値でやったのと同じように、ポリシー・ニューラル・ネットワークと呼ばれるニューラルネットワークでポリシーを近似することができる。"
  },
  {
    "start": 782344,
    "end": 785548,
    "text": "を入力する。"
  },
  {
    "start": 785636,
    "end": 791444,
    "text": "この場合、2つの座標が出力され、その状態で進むポリシーが出力される。"
  },
  {
    "start": 791524,
    "end": 794384,
    "text": "さて、政策ニューラルネットワークをこのように描いてみる。"
  },
  {
    "start": 794964,
    "end": 798452,
    "text": "何層にも重ねることもできるが、ここではひとつだけ描くことにする。"
  },
  {
    "start": 798548,
    "end": 802116,
    "text": "入力は状態であり、出力は4つである。"
  },
  {
    "start": 802260,
    "end": 808236,
    "text": "上に行く確率、右に行く確率、下に行く確率、左に行く確率。"
  },
  {
    "start": 808380,
    "end": 818478,
    "text": "この場合、上に行く確率を0.6、右に行く確率を0.25、下に行く確率を0.05、左に行く確率を0.1と出力するとする。"
  },
  {
    "start": 818566,
    "end": 822518,
    "text": "これはストキャスティック・ポリシーと呼ばれるもので、もう少し詳しく説明しよう。"
  },
  {
    "start": 822606,
    "end": 830150,
    "text": "つまり、どの状態でも、4つの方向のいずれかに進む確率があるということだ。"
  },
  {
    "start": 830302,
    "end": 838624,
    "text": "この方針は、今日の状態が上位に行くほど価値が高くなることを示唆している。"
  },
  {
    "start": 838704,
    "end": 842632,
    "text": "つまり、要約すると、私たちはこちらの価値観を持っているということだ。"
  },
  {
    "start": 842728,
    "end": 850464,
    "text": "すべての値を計算するのは面倒なので、値のニューラルネットワークを使ってかなり近似的に計算する。"
  },
  {
    "start": 850544,
    "end": 855064,
    "text": "ということは、かなり優れたニューラルネットワークを訓練するためには、もっと少ない反復回数で済むということだ。"
  },
  {
    "start": 855184,
    "end": 862378,
    "text": "私たちが望んでいる値とは違うかもしれないが、この試合でうまくやるにはどうすればいいか、かなりいいアイデアを与えてくれるに違いない。"
  },
  {
    "start": 862456,
    "end": 865942,
    "text": "それなら、こっちの方針がある。"
  },
  {
    "start": 866078,
    "end": 876326,
    "text": "また、このすべてのポリシーを計算する帯域幅もないため、ポリシー・ニューラル・ネットワークがポリシーを実にうまく近似してくれる。"
  },
  {
    "start": 876390,
    "end": 883702,
    "text": "さて、この右の政策は確率論的なものであるため、これまでとは異なると前述したが、実はこれは良いことなのだ。"
  },
  {
    "start": 883838,
    "end": 889662,
    "text": "左の方針は完全に決定論的で、何をすべきかを正確に指示している。"
  },
  {
    "start": 889758,
    "end": 897254,
    "text": "時には選択肢を与えてくれることもあるが、完璧な場所にたどり着くためにはどのような道を辿ればいいかを教えてくれることがほとんどだ。"
  },
  {
    "start": 897334,
    "end": 906118,
    "text": "しかし、このゲームをする以上、決定論的な政策は望まない。決定論的な政策では、空間をあまり探索できないからだ。"
  },
  {
    "start": 906286,
    "end": 916406,
    "text": "一方、右のような確率的な方針は、確率を与えてくれるため、空間をうまく探索することができる。"
  },
  {
    "start": 916590,
    "end": 928742,
    "text": "毎回ベストスコアが出るとは限らないが、グリッドを歩き回ることで、自分たちの価値観や方針を改善することができる。"
  },
  {
    "start": 928918,
    "end": 933086,
    "text": "確率的政策は決定論的政策より常に優れている。"
  },
  {
    "start": 933150,
    "end": 938198,
    "text": "モデルをトレーニングするとき、機械学習ではexplore（探索）、exploit（利用）という言葉を聞いたことがあるかもしれない。"
  },
  {
    "start": 938326,
    "end": 947866,
    "text": "左のポリシーは、一度良いパスを見つけると、そのパスを取得し続け、可能な限り最高のスコアを得るため、悪用する。"
  },
  {
    "start": 947930,
    "end": 948946,
    "text": "それは知っている。"
  },
  {
    "start": 949010,
    "end": 958530,
    "text": "しかし、右の確率的な方針は、高得点を得ようと懸命になるため、実際にはエクスプロイトとエクスプロイトのバランスが取れている。"
  },
  {
    "start": 958642,
    "end": 968880,
    "text": "また、わずかな確率で空間全体を探索し、価値と方針を改善することができる。"
  },
  {
    "start": 968992,
    "end": 974604,
    "text": "さて、価値とポリシーのニューラルネットワークを手に入れたところで、それらをどのように訓練するかをお見せしよう。"
  },
  {
    "start": 979104,
    "end": 991088,
    "text": "つまり、値を近似するニューラルネットワークと、方針を近似するポリシー・ニューラル・ネットワークだ。"
  },
  {
    "start": 991216,
    "end": 997148,
    "text": "この2つのトレーニング方法を説明するが、まずはバリュー・ニューラル・ネットワークのトレーニング方法を説明しよう。"
  },
  {
    "start": 997236,
    "end": 999756,
    "text": "これが価値観を鍛えるための主な考え方だ。"
  },
  {
    "start": 999780,
    "end": 1004396,
    "text": "少し漠然としているが、心配しないで、ゆっくりと詳細を説明するつもりだ。"
  },
  {
    "start": 1004460,
    "end": 1008052,
    "text": "例えば、値が2の状態があるとしよう。"
  },
  {
    "start": 1008108,
    "end": 1009884,
    "text": "2つの価値観とはどういう意味だろう？"
  },
  {
    "start": 1010004,
    "end": 1014860,
    "text": "つまり、ニューラルネットワークはこの値を2だと考えているのだ。"
  },
  {
    "start": 1014932,
    "end": 1018264,
    "text": "しかし、実際の値が5だとしよう。"
  },
  {
    "start": 1018724,
    "end": 1023140,
    "text": "このニューラルネットワークは値を過小評価した。"
  },
  {
    "start": 1023332,
    "end": 1027556,
    "text": "そのため、ニューラルネットワークにこの値を2増やすように指示する必要がある。"
  },
  {
    "start": 1027620,
    "end": 1029644,
    "text": "ゲインというものを導入する。"
  },
  {
    "start": 1029764,
    "end": 1035780,
    "text": "ここでのゲインは3であり、これは実際の値からニューラルネットワークが考える値を引いたものである。"
  },
  {
    "start": 1035852,
    "end": 1045500,
    "text": "この2つを増やすようにニューラルネットワークに指示するために、そのゲインはニューラルネットワークに入力される損失関数になる。"
  },
  {
    "start": 1045572,
    "end": 1049088,
    "text": "この損失関数については、もう少し具体的に説明しよう。"
  },
  {
    "start": 1049156,
    "end": 1059232,
    "text": "とりあえず、ゲインを使ってニューラルネットワークに2という値を大きくするように指示し、2.1というようにもっと大きくすると考える。"
  },
  {
    "start": 1059288,
    "end": 1065336,
    "text": "逆のシナリオでは、ニューラルネットワークはこの値を5と考え、実際の値は3だとする。"
  },
  {
    "start": 1065400,
    "end": 1072216,
    "text": "これでニューラルネットワークは過大評価し、ゲインは3から5を引いたマイナス2となった。"
  },
  {
    "start": 1072320,
    "end": 1074392,
    "text": "というのは、実際にはマイナスの利益だ。"
  },
  {
    "start": 1074568,
    "end": 1085854,
    "text": "では、このマイナスのゲインを、トレーニングでニューラルネットワークにこの5を減らすように指示する。"
  },
  {
    "start": 1085894,
    "end": 1087590,
    "text": "例えば4.9点とかね。"
  },
  {
    "start": 1087662,
    "end": 1094366,
    "text": "つまり、ニューラルネットワークを訓練して、どんどん実際の値に近づけているのだ。"
  },
  {
    "start": 1094430,
    "end": 1098422,
    "text": "さて、もっと具体的に説明すると約束したので、もっと具体的に説明しよう。"
  },
  {
    "start": 1098518,
    "end": 1102076,
    "text": "左側は、ニューラルネットワークが出した値である。"
  },
  {
    "start": 1102150,
    "end": 1104472,
    "text": "さて、このニューラルネットワークはあまりよく訓練されていない。"
  },
  {
    "start": 1104528,
    "end": 1105736,
    "text": "まだ初期段階だ。"
  },
  {
    "start": 1105800,
    "end": 1113560,
    "text": "ご覧のように、値は前に計算した実際の値とはあまり似ていませんが、より良いものにするために1回反復します。"
  },
  {
    "start": 1113632,
    "end": 1123204,
    "text": "だから、真ん中のこの場所に立っているとしよう。これからすることは、終着駅に着くまで歩き回ることだ。"
  },
  {
    "start": 1123504,
    "end": 1125560,
    "text": "どうやって歩けばいいんだろう？"
  },
  {
    "start": 1125752,
    "end": 1129304,
    "text": "まあ、今までのポリシー・ニューラル・ネットワークを使えばいい。"
  },
  {
    "start": 1129424,
    "end": 1132176,
    "text": "素晴らしいとは言えないかもしれないが、ある程度の道筋はつけられるだろう。"
  },
  {
    "start": 1132240,
    "end": 1135568,
    "text": "さて、ここで私は幸運にも、5人という末期の状態に到達した。"
  },
  {
    "start": 1135616,
    "end": 1138776,
    "text": "私たちは、マイナス1かプラス4という終末状態に到達していたかもしれない。"
  },
  {
    "start": 1138840,
    "end": 1140016,
    "text": "そんなことはどうでもいい。"
  },
  {
    "start": 1140200,
    "end": 1147384,
    "text": "実際、このパスからいくつかの値を計算することができる。"
  },
  {
    "start": 1147424,
    "end": 1149800,
    "text": "こっちの道を真似しよう。"
  },
  {
    "start": 1149952,
    "end": 1161506,
    "text": "さて、このパスの値は、5から1マス離れているので4、次に3、2、10、そしてマイナス1である。"
  },
  {
    "start": 1161610,
    "end": 1164554,
    "text": "これが実際の値である。"
  },
  {
    "start": 1164634,
    "end": 1166994,
    "text": "必ずしも実際の値ではないことに注意してほしい。"
  },
  {
    "start": 1167034,
    "end": 1167186,
    "text": "ここだよ。"
  },
  {
    "start": 1167210,
    "end": 1175130,
    "text": "私は幸運にも、かなり良い道を歩むことになったが、その方針は私を別のところに連れて行くこともできたし、正確ではない別の値を計算することもできた。"
  },
  {
    "start": 1175282,
    "end": 1176330,
    "text": "そんなことはどうでもいい。"
  },
  {
    "start": 1176402,
    "end": 1181130,
    "text": "これを何度も繰り返せば、実際の値に近くなると信じているつもりだ。"
  },
  {
    "start": 1181282,
    "end": 1190568,
    "text": "さて、ニューラルネットワークに、左側のグレーのパスで有効な値が、右側の色のついたパスで有効な値よりどの程度離れているかをどのように伝えればいいのだろう。"
  },
  {
    "start": 1190736,
    "end": 1195328,
    "text": "その差額を二乗するんだ。"
  },
  {
    "start": 1195376,
    "end": 1200104,
    "text": "機械学習の損失関数を見たことがあるなら、これは教科書的な損失関数だ。"
  },
  {
    "start": 1200184,
    "end": 1208880,
    "text": "二乗差であり、二乗差が小さければ小さいほど、ニューラルネットワークが実際の値を計算するのに近いことを意味するからだ。"
  },
  {
    "start": 1208952,
    "end": 1214630,
    "text": "これらの数値の差を二乗して、この値を得よう。"
  },
  {
    "start": 1214782,
    "end": 1220486,
    "text": "さて、損失関数では、合計の代わりに平均を取るのが通例だ。"
  },
  {
    "start": 1220630,
    "end": 1222486,
    "text": "平均を取る。"
  },
  {
    "start": 1222550,
    "end": 1229910,
    "text": "つまり、数字が6つあるので、6で割ることになり、この和を6で割ると3.36となる。"
  },
  {
    "start": 1229982,
    "end": 1243080,
    "text": "3.36の損失は、このニューラルネットワークがどの程度悪いかを数値化したもので、損失が大きければ大きいほど、ニューラルネットワークが値の近似から遠ざかっていることを意味するからだ。"
  },
  {
    "start": 1243152,
    "end": 1245656,
    "text": "損失が少ないほど良い。"
  },
  {
    "start": 1245720,
    "end": 1253480,
    "text": "損失がゼロという理想的な状況では、ニューラルネットワークが実際の値を完璧に近似できたことを意味する。"
  },
  {
    "start": 1253552,
    "end": 1260368,
    "text": "ニューラルネットワークでやるのと同じように、この損失を利用してバリュー・ニューラル・ネットワークをトレーニングする。"
  },
  {
    "start": 1260456,
    "end": 1272210,
    "text": "そのため、このトレーニングを1回繰り返せば、ニューラルネットワークは、それらのセルとその周辺のすべてのセルについて、わずかに良い値を出力できるようになる。"
  },
  {
    "start": 1272322,
    "end": 1284330,
    "text": "ご想像のとおり、さまざまな経路をとりながら、常にニューラルネットワークを訓練し続ければ、ある時点でかなりいいものができるだろう。"
  },
  {
    "start": 1284482,
    "end": 1291108,
    "text": "つまり、値をかなりよく近似するニューラルネットワーク、それがバリュー・ニューラル・ネットワークだ。"
  },
  {
    "start": 1291226,
    "end": 1297424,
    "text": "さて、数式がお好きな方は、バリュー・ニューラル・ネットワークの損失関数の公式をご覧いただきたい。"
  },
  {
    "start": 1297584,
    "end": 1300432,
    "text": "複雑そうに見えるが、まさにその通りだ。"
  },
  {
    "start": 1300488,
    "end": 1300848,
    "text": "ここだよ。"
  },
  {
    "start": 1300936,
    "end": 1301584,
    "text": "お見せしましょう。"
  },
  {
    "start": 1301624,
    "end": 1307424,
    "text": "つまり、このeは期待値だが、期待値は平均値に過ぎない。"
  },
  {
    "start": 1307504,
    "end": 1311680,
    "text": "覚えておいてほしいのは、この6番以上は平均6番以上の時だったということだ。"
  },
  {
    "start": 1311752,
    "end": 1312800,
    "text": "さて、これは何だろう？"
  },
  {
    "start": 1312912,
    "end": 1318562,
    "text": "さて、vは値ニューラルネットワーク、stは状態である。"
  },
  {
    "start": 1318688,
    "end": 1326182,
    "text": "これらの値は、ニューラルネットワークが考える、そのパスにおける特定の状態である。"
  },
  {
    "start": 1326238,
    "end": 1327430,
    "text": "RTとは何か？"
  },
  {
    "start": 1327542,
    "end": 1332350,
    "text": "さて、これらはニューラルネットワークが到達したい実際の値である。"
  },
  {
    "start": 1332502,
    "end": 1335030,
    "text": "今、私が言ったように、それらは実際の値ではないかもしれない。"
  },
  {
    "start": 1335062,
    "end": 1341734,
    "text": "それらは経路に依存するが、これがニューラルネットワークのゴールであるとちょっと考えてみよう。"
  },
  {
    "start": 1341814,
    "end": 1343990,
    "text": "その価値観に到達したいのだ。"
  },
  {
    "start": 1344062,
    "end": 1347920,
    "text": "最後に、この2乗はこっちの2乗だ。"
  },
  {
    "start": 1347992,
    "end": 1351360,
    "text": "これは前述したように、二乗損失関数である。"
  },
  {
    "start": 1351432,
    "end": 1356504,
    "text": "この損失関数を用いてニューラルネットワークを訓練する。"
  },
  {
    "start": 1356584,
    "end": 1365640,
    "text": "損失関数を可能な限り小さくすることができれば、ニューラルネットワークが出力する値は実際の値に限りなく近いということになる。"
  },
  {
    "start": 1365712,
    "end": 1368160,
    "text": "これがバリュー・ニューラル・ネットワークをトレーニングする方法だ。"
  },
  {
    "start": 1368272,
    "end": 1370924,
    "text": "では、ポリシー・ニューラル・ネットワークの訓練方法をお見せしよう。"
  },
  {
    "start": 1375524,
    "end": 1380044,
    "text": "つまり、価値ニューラルネットワークと政策ニューラルネットワークの2つのニューラルネットワークがあることを思い出してほしい。"
  },
  {
    "start": 1380124,
    "end": 1382476,
    "text": "バリュー・ニューラル・ネットワークのトレーニング方法はすでに説明した。"
  },
  {
    "start": 1382540,
    "end": 1385724,
    "text": "では、どのようにポリシー・ニューラル・ネットワークを訓練するかを説明しよう。"
  },
  {
    "start": 1385804,
    "end": 1389460,
    "text": "似たようなアイデアだが、もう少し複雑だ。"
  },
  {
    "start": 1389612,
    "end": 1390428,
    "text": "それを乗り越えよう。"
  },
  {
    "start": 1390476,
    "end": 1400696,
    "text": "つまり、2つの状態があって、左の状態から右の状態になる確率が0.2だとする。"
  },
  {
    "start": 1400760,
    "end": 1403600,
    "text": "この確率は政策ニューラルネットワークによって与えられる。"
  },
  {
    "start": 1403672,
    "end": 1410712,
    "text": "私たちの目標は、この確率をよりよく推定するために、このポリシー・ニューラル・ネットワークを微調整し、訓練することです。"
  },
  {
    "start": 1410808,
    "end": 1412416,
    "text": "同時にトレーニングしているからね。"
  },
  {
    "start": 1412480,
    "end": 1421224,
    "text": "値ニューラル・ネットワークを見てみよう。値ニューラル・ネットワークは右の状態の値を2だと考えているとしよう。"
  },
  {
    "start": 1421304,
    "end": 1423816,
    "text": "では、実際の値が5だとしよう。"
  },
  {
    "start": 1424000,
    "end": 1426112,
    "text": "実際の価値というのは、以前と同じという意味だ。"
  },
  {
    "start": 1426208,
    "end": 1431440,
    "text": "ある経路をとり、その経路を終点までたどり、そこから値を計算する。"
  },
  {
    "start": 1431512,
    "end": 1435752,
    "text": "というポリシー・ニューラル・ネットワークを使ってパスを与えた。"
  },
  {
    "start": 1435808,
    "end": 1441592,
    "text": "また、それは最高の価値を与えてくれる道かもしれないし、他の価値を与えてくれる道かもしれない。"
  },
  {
    "start": 1441648,
    "end": 1443192,
    "text": "今は関係ない。"
  },
  {
    "start": 1443328,
    "end": 1447320,
    "text": "ニューラルネットワークが2だと思った値が、実際には5だったからだ。"
  },
  {
    "start": 1447472,
    "end": 1451608,
    "text": "つまり、3点のゲインがあったことになる。"
  },
  {
    "start": 1451776,
    "end": 1459038,
    "text": "つまり、バリュー・ニューラル・ネットワークは3つ過小評価していたのだ。"
  },
  {
    "start": 1459206,
    "end": 1469750,
    "text": "ひとつは、ニューラルネットワークがステートの値を低く見積もりすぎたので、その値を上げるように指示すること。"
  },
  {
    "start": 1469822,
    "end": 1478930,
    "text": "なぜなら、両方のニューラルネットワークが同時にトレーニングされるからだ。"
  },
  {
    "start": 1479062,
    "end": 1490274,
    "text": "二人ともこの箱を過小評価しているので、箱の価値と箱に辿り着く確率を上げるよう、二人に伝える必要がある。"
  },
  {
    "start": 1490434,
    "end": 1495690,
    "text": "を増やすように指示され、仮に0.3に増やすとする。"
  },
  {
    "start": 1495762,
    "end": 1496954,
    "text": "そういうことだ。"
  },
  {
    "start": 1497074,
    "end": 1505170,
    "text": "例えば、バリュー・ニューラル・ネットワークが6点という高い値を示しているのに、実際の値はそれほど高くないという逆のシナリオがあるとしよう。"
  },
  {
    "start": 1505202,
    "end": 1506400,
    "text": "それは一つだ。"
  },
  {
    "start": 1506592,
    "end": 1510720,
    "text": "となると、利益はマイナス5、つまり5損ということになる。"
  },
  {
    "start": 1510832,
    "end": 1513032,
    "text": "私たちは2つのことを伝える必要がある。"
  },
  {
    "start": 1513128,
    "end": 1521200,
    "text": "バリュー・ニューラル・ネットワークに、その状態の値を減らして1に近づけるように指示する必要がある。"
  },
  {
    "start": 1521392,
    "end": 1535690,
    "text": "どちらのニューラルネットワークもこの状態を過大評価したため、ポリシー・ニューラル・ネットワークはこの確率を例えば0.1に下げることになる。"
  },
  {
    "start": 1535762,
    "end": 1538762,
    "text": "これが、ポリシー・ニューラル・ネットワークをトレーニングする方法だ。"
  },
  {
    "start": 1538858,
    "end": 1541454,
    "text": "2人とも同時にトレーニングを受けていることに注目してほしい。"
  },
  {
    "start": 1541874,
    "end": 1549354,
    "text": "しかし、ポリシー・ニューラル・ネットワークのトレーニングには特別な要素がある。"
  },
  {
    "start": 1549434,
    "end": 1553322,
    "text": "簡単な例で、勢いがどのように働くかをお見せしよう。"
  },
  {
    "start": 1553458,
    "end": 1564566,
    "text": "例えば、走っている子供がいるとしよう。これがシナリオ1で、その子供は速度10で右に向かって走っている。"
  },
  {
    "start": 1564750,
    "end": 1569014,
    "text": "つまり、子供が走っている方向と同じ方向に力を加えるのだ。"
  },
  {
    "start": 1569054,
    "end": 1572694,
    "text": "例えば、スピードを10倍から20倍にする。"
  },
  {
    "start": 1572774,
    "end": 1574302,
    "text": "では、別のシナリオを見てみよう。"
  },
  {
    "start": 1574398,
    "end": 1581134,
    "text": "シナリオ2では、子供は走っているが、今度は左に向かって走っている。"
  },
  {
    "start": 1581294,
    "end": 1587364,
    "text": "つまり、方向は変えるが、速度は明らかに伸びない。"
  },
  {
    "start": 1587494,
    "end": 1588272,
    "text": "収縮する。"
  },
  {
    "start": 1588368,
    "end": 1589008,
    "text": "なぜですか？"
  },
  {
    "start": 1589176,
    "end": 1598264,
    "text": "というのも、ここでは10人から20人になると速度が大幅に上がるからだ。"
  },
  {
    "start": 1598344,
    "end": 1606264,
    "text": "一方、ここでは、彼らが走っていた方向とは違う方向に子供を押したため、速度が落ちた。"
  },
  {
    "start": 1606384,
    "end": 1614916,
    "text": "言い換えれば、ここでのインパルス（運動量）は速度と同じ方向だったが、ここでは正反対の方向だった。"
  },
  {
    "start": 1614980,
    "end": 1623004,
    "text": "物理学がどのように機能するかはご存知の通りなので、目新しいことではないが、これがトレーニングにどのように使われるかをお見せしよう。"
  },
  {
    "start": 1623084,
    "end": 1625980,
    "text": "では、シナリオ1を見て、ちょっとしたプロットを作ってみよう。"
  },
  {
    "start": 1626172,
    "end": 1630004,
    "text": "横軸はニューラルネットワークの反復回数である。"
  },
  {
    "start": 1630084,
    "end": 1638332,
    "text": "そのため、ニューラルネットワークの学習は何度も繰り返され、そのたびにある状態の確率が変化する。"
  },
  {
    "start": 1638468,
    "end": 1647592,
    "text": "つまり、モデルをトレーニングするにつれて、ある状態の確率が上昇してきたとしよう。"
  },
  {
    "start": 1647648,
    "end": 1651456,
    "text": "つまり、確率を上げる必要がある。"
  },
  {
    "start": 1651640,
    "end": 1661200,
    "text": "子供が走っているのと同じ方向に押されると速度が増すのと同じようにね。"
  },
  {
    "start": 1661272,
    "end": 1666142,
    "text": "もう一つのシナリオは、ある状態の確率が、反復練習を通じて減少していったとしよう。"
  },
  {
    "start": 1666168,
    "end": 1670450,
    "text": "ニューラルネットワークの、しかし、これはかなり良い状態であることがわかった。"
  },
  {
    "start": 1670562,
    "end": 1673174,
    "text": "プラスになった。"
  },
  {
    "start": 1674034,
    "end": 1684842,
    "text": "つまり、この行動の確率を上げる必要があり、そのために確率は上がるが、それほど大きくはならない。"
  },
  {
    "start": 1684898,
    "end": 1688098,
    "text": "つまり、左翼には衝動があった。"
  },
  {
    "start": 1688186,
    "end": 1694454,
    "text": "確率が上がれば上がるほど、利得を得ることになる。"
  },
  {
    "start": 1694534,
    "end": 1704334,
    "text": "右の場合、インパルスは下降しており、ゲインはそれを反対方向に押す力なので、それほど増加しない。"
  },
  {
    "start": 1704414,
    "end": 1710654,
    "text": "他にも2つのシナリオがある。何が起こっているか想像がつくと思うので、手短に説明しよう。"
  },
  {
    "start": 1710774,
    "end": 1716174,
    "text": "確率が上昇しているときにゲインがマイナスになれば、今度は確率が低下する。"
  },
  {
    "start": 1716334,
    "end": 1723306,
    "text": "一方、確率が減少し、利得が負であれば、確率はより速く減少する。"
  },
  {
    "start": 1723410,
    "end": 1730322,
    "text": "左のケースでは、インパルスが上がってゲインが下がるので、それほど減らないからだ。"
  },
  {
    "start": 1730498,
    "end": 1738066,
    "text": "一方、シナリオ4では、インパルスは下がり、ゲインは同じ方向に向かうので、より大きく減少する。"
  },
  {
    "start": 1738130,
    "end": 1746886,
    "text": "シナリオ3と4は数字的には忘れてしまうが、もしよければビデオを一時停止して、実際に考えてみてほしい。"
  },
  {
    "start": 1746950,
    "end": 1753246,
    "text": "確率を一定の割合で増減させるというのは、具体的にどういうことですか？"
  },
  {
    "start": 1753390,
    "end": 1761486,
    "text": "さて、ニューラルネットワークをトレーニングしていて、反復100になったとしよう。"
  },
  {
    "start": 1761590,
    "end": 1768354,
    "text": "この方針では、左の状態と右の状態の間を行き来する確率は0.4である。"
  },
  {
    "start": 1768774,
    "end": 1771210,
    "text": "これはシナリオ2でも起こることだ。"
  },
  {
    "start": 1771282,
    "end": 1775402,
    "text": "では、仮に10の利得を見つけたとしよう。"
  },
  {
    "start": 1775538,
    "end": 1782850,
    "text": "これはニューラルネットワークが状態の値を10だけ過小評価したことを意味し、右のシナリオでも同じことが起こる。"
  },
  {
    "start": 1782922,
    "end": 1785858,
    "text": "この確率を高めなければならないのは明らかだ。"
  },
  {
    "start": 1785946,
    "end": 1791986,
    "text": "これはかなり良い状態なので、pが0.4に等しくなるように増やしたい。"
  },
  {
    "start": 1792130,
    "end": 1798480,
    "text": "イテレーション101では、しかしそれよりもはるかに高い確率になるだろう。"
  },
  {
    "start": 1798672,
    "end": 1800176,
    "text": "高さはどれくらいになりそうですか？"
  },
  {
    "start": 1800320,
    "end": 1804696,
    "text": "まあ、それはこの確率が持っている衝動による。"
  },
  {
    "start": 1804800,
    "end": 1814128,
    "text": "イテレーション99に戻って、この2つの状態を行き来する確率や方針を調べます。"
  },
  {
    "start": 1814216,
    "end": 1819288,
    "text": "シナリオ1で、反復99が0.2の確率を示したとしよう。"
  },
  {
    "start": 1819456,
    "end": 1821960,
    "text": "一方、シナリオ2の確率はこうだった。"
  },
  {
    "start": 1821992,
    "end": 1824748,
    "text": ".8 では、シナリオ1ではどうなるのか？"
  },
  {
    "start": 1824796,
    "end": 1826916,
    "text": "まあ、確率は0.2から0.3に上がった。"
  },
  {
    "start": 1826940,
    "end": 1836684,
    "text": "0.4はインパルスを意味し、このインパルスは確率が高まっていることを示すので、0.8としよう。"
  },
  {
    "start": 1836804,
    "end": 1840180,
    "text": "そうすれば、プラスになり、確率はもっと大きくなる。"
  },
  {
    "start": 1840252,
    "end": 1845164,
    "text": "しかし、シナリオ2では、確率は0.4から上昇するが、それほど大きくはならない。"
  },
  {
    "start": 1845204,
    "end": 1846884,
    "text": "0.5まで伸びる"
  },
  {
    "start": 1847044,
    "end": 1847760,
    "text": "なぜですか？"
  },
  {
    "start": 1847932,
    "end": 1849536,
    "text": "減少しているからだ。"
  },
  {
    "start": 1849720,
    "end": 1855404,
    "text": "確率が低下していることを知らせるインパルスがあり、それからまた適用する。"
  },
  {
    "start": 1855864,
    "end": 1863824,
    "text": "つまり、子供が一方向に走っているときに、私たちが別の方向に押しやってしまうから、それほど伸びないということだ。"
  },
  {
    "start": 1863904,
    "end": 1867392,
    "text": "ここでは確率が減少しているので、それを増やすように指示する。"
  },
  {
    "start": 1867488,
    "end": 1869720,
    "text": "だから、勢いというのはそういう意味なんだ。"
  },
  {
    "start": 1869792,
    "end": 1873032,
    "text": "しかし、実際の計算式を見てみよう。"
  },
  {
    "start": 1873088,
    "end": 1883170,
    "text": "ゲームゲインを使ってバリュー・ニューラルネットワークを訓練したように、サロゲート目的関数と呼ばれるものを使ってポリシー・ニューラルネットワークを訓練する。"
  },
  {
    "start": 1883242,
    "end": 1886314,
    "text": "ここでインパルスをエンコードする方法は、分割することだ。"
  },
  {
    "start": 1886394,
    "end": 1896834,
    "text": "反復回数100.4の確率を反復回数99の確率で割り、ゲインをかけると20になる。"
  },
  {
    "start": 1896954,
    "end": 1899338,
    "text": "右でも同じことができる。"
  },
  {
    "start": 1899426,
    "end": 1906224,
    "text": "0.4を0.8で割ってゲインをかけると5となる。"
  },
  {
    "start": 1906524,
    "end": 1912164,
    "text": "この20が大きいのは、0.4を小さな数字で割っているからだ。"
  },
  {
    "start": 1912284,
    "end": 1913580,
    "text": "0.2で。"
  },
  {
    "start": 1913692,
    "end": 1919516,
    "text": "継続時間99の確率が小さければ小さいほど、サロゲートの目的関数は高くなる。"
  },
  {
    "start": 1919580,
    "end": 1921876,
    "text": "これで5人だ。"
  },
  {
    "start": 1921940,
    "end": 1923068,
    "text": "少ない数字だ。"
  },
  {
    "start": 1923156,
    "end": 1924380,
    "text": "なぜ少ないのですか？"
  },
  {
    "start": 1924492,
    "end": 1934128,
    "text": "今、私たちはそれを大きな数字で割っているのだから。そうすることで、反復記号99は、それが小さな数字から大きな数字になった場合にどうすべきかを正確に教えてくれた。"
  },
  {
    "start": 1934176,
    "end": 1939160,
    "text": "小さな数字で割ると、左の20のような大きな数字になる。"
  },
  {
    "start": 1939312,
    "end": 1946752,
    "text": "もし大きな数字で割っているのなら、それは大きな数字から小さな数字になるため、より小さな代理目的関数になる。"
  },
  {
    "start": 1946808,
    "end": 1952764,
    "text": "この代理目的関数には、その勢いが組み込まれている。"
  },
  {
    "start": 1952824,
    "end": 1955644,
    "text": "数式がお好きなら、ここに数式がある。"
  },
  {
    "start": 1955684,
    "end": 1959772,
    "text": "不細工に見えるが、実際はこの商にゲインをかけたものを正確にエンコードしている。"
  },
  {
    "start": 1959828,
    "end": 1973332,
    "text": "PIはポリシー、θはパラメータ、aはアクション、sは状態であるため、分子は反復100における確率である。"
  },
  {
    "start": 1973428,
    "end": 1975980,
    "text": "分母は同じものである。"
  },
  {
    "start": 1976052,
    "end": 1979800,
    "text": "だから我々はPIオールドと呼んでいる。"
  },
  {
    "start": 1979912,
    "end": 1984720,
    "text": "このtのaは、ステップ100でのゲインである。"
  },
  {
    "start": 1984912,
    "end": 1993896,
    "text": "このアイデアは、この全体、20は損失であり、ポリシー・ニューラル・ネットワークの損失であるということだ。"
  },
  {
    "start": 1993960,
    "end": 1999568,
    "text": "この損失をできるだけ小さくするようにニューラルネットワークを訓練することである。"
  },
  {
    "start": 1999696,
    "end": 2004430,
    "text": "つまり、損失を使って政策ニューラルネットワークを訓練するのだ。"
  },
  {
    "start": 2004552,
    "end": 2015374,
    "text": "この損失を小さくすることができれば、政策ニューラル・ネットワークが実際の政策を実にうまく近似している地点に到達したことになる。"
  },
  {
    "start": 2015754,
    "end": 2018334,
    "text": "これがポリシー・ニューラル・ネットワークを訓練する方法だ。"
  },
  {
    "start": 2022994,
    "end": 2025442,
    "text": "まだ全部は話していない。"
  },
  {
    "start": 2025498,
    "end": 2026778,
    "text": "ちょっとした問題がある。"
  },
  {
    "start": 2026946,
    "end": 2033740,
    "text": "ある方針が与えられたとき、ある行動の問題確率が0.4だとしよう。"
  },
  {
    "start": 2033812,
    "end": 2040404,
    "text": "前の反復では0.001だったので、それが急激に増えたとしよう。"
  },
  {
    "start": 2040484,
    "end": 2042444,
    "text": "その比率は4000だ。"
  },
  {
    "start": 2042604,
    "end": 2046224,
    "text": "これにゲインをかけても、まだ大きな数字になる。"
  },
  {
    "start": 2046804,
    "end": 2051636,
    "text": "ニューラルネットワークに非常に大きな数字を入れるのはあまりよくない。"
  },
  {
    "start": 2051660,
    "end": 2053500,
    "text": "ちょっとカオスになりそうだ。"
  },
  {
    "start": 2053572,
    "end": 2062181,
    "text": "実際には、これらの代用損失関数が極端に大きくなったり、極端に小さくなったりしないようにしたい。"
  },
  {
    "start": 2062317,
    "end": 2064397,
    "text": "私たちはその中間を望んでいる。"
  },
  {
    "start": 2064445,
    "end": 2065213,
    "text": "どうする？"
  },
  {
    "start": 2065293,
    "end": 2066821,
    "text": "じゃあ、クリップで留めておこう。"
  },
  {
    "start": 2066877,
    "end": 2067789,
    "text": "その方法をお見せしよう。"
  },
  {
    "start": 2067901,
    "end": 2074813,
    "text": "分子が現在の確率、分母が以前の確率であることを思い出してほしい。"
  },
  {
    "start": 2074893,
    "end": 2079357,
    "text": "これは、ニューラルネットワークのトレーニングの前の反復における確率を意味する。"
  },
  {
    "start": 2079445,
    "end": 2082910,
    "text": "さて、確率はゼロから1の間である。"
  },
  {
    "start": 2083022,
    "end": 2086554,
    "text": "この2つの比率がどの程度になるか見てみよう。"
  },
  {
    "start": 2087054,
    "end": 2097158,
    "text": "というのも、原理的に確率はあまり変わらないからだ。"
  },
  {
    "start": 2097286,
    "end": 2102234,
    "text": "そこで、半径を0.3にするとしよう。"
  },
  {
    "start": 2102814,
    "end": 2111372,
    "text": "最小値は0.7、最大値は1.3である。"
  },
  {
    "start": 2111468,
    "end": 2121524,
    "text": "つまり、この比率がたまたま0.7と1.3の間、たとえば0.9になった場合は、そのままにするということだ。"
  },
  {
    "start": 2121604,
    "end": 2125636,
    "text": "さて、この比率が私たちが望む区間に入らなかったとしよう。"
  },
  {
    "start": 2125820,
    "end": 2128020,
    "text": "0.7より小さかったら？"
  },
  {
    "start": 2128172,
    "end": 2132444,
    "text": "我々はそれを受け入れず、0.7に変更した。"
  },
  {
    "start": 2132564,
    "end": 2136072,
    "text": "つまり、比率が小さすぎる場合は0.7にする。"
  },
  {
    "start": 2136168,
    "end": 2137632,
    "text": "大きすぎるとどうなる？"
  },
  {
    "start": 2137688,
    "end": 2143440,
    "text": "まあ、1.3より大きい場合は受け入れず、ずっと1.3にする。"
  },
  {
    "start": 2143552,
    "end": 2146724,
    "text": "大きすぎる場合は1.3にする。"
  },
  {
    "start": 2147104,
    "end": 2152312,
    "text": "これがクリッピングの意味であり、このパラメータを選択することができる。"
  },
  {
    "start": 2152368,
    "end": 2154320,
    "text": "この場合は0.3だった。"
  },
  {
    "start": 2154472,
    "end": 2156280,
    "text": "好きなように選択できる。"
  },
  {
    "start": 2156352,
    "end": 2161194,
    "text": "これがサロゲート目的関数が切り取られたときの様子である。"
  },
  {
    "start": 2161274,
    "end": 2175090,
    "text": "この確率をlポリシー・クリップと呼ぶことにする。この確率に再びゲインを掛け、クリッピングにゲインを掛けたものと比較し、この2つの最小値を取る。"
  },
  {
    "start": 2175162,
    "end": 2176786,
    "text": "では、クリップは何ですか？"
  },
  {
    "start": 2176890,
    "end": 2185682,
    "text": "イプシロンは前に話した0.3という値だ。"
  },
  {
    "start": 2185778,
    "end": 2200842,
    "text": "この切り取られた関数で、大きすぎたり小さすぎたりしないことを確認し、実際にポリシー・ニューラル・ネットワークのトレーニングを更新するための適切な関数があることを確認する。"
  },
  {
    "start": 2200938,
    "end": 2209494,
    "text": "というのも、この切り取られた代理目的関数は、ポリシー・ニューラル・ネットワークの訓練に使うものだからだ。"
  },
  {
    "start": 2214154,
    "end": 2214954,
    "text": "それだけだ。"
  },
  {
    "start": 2214994,
    "end": 2216690,
    "text": "それが近接政策の最適化だ。"
  },
  {
    "start": 2216802,
    "end": 2225964,
    "text": "つまり、要約すると、値を近似するニューラルネットワークと、この関数で与えられるゲインでそれを訓練する方法があるということです。"
  },
  {
    "start": 2226264,
    "end": 2236832,
    "text": "そしてポリシー・ニューラル・ネットワークがあり、これはポリシーを近似し、サロゲート目的関数を用いて訓練されます。"
  },
  {
    "start": 2236888,
    "end": 2241376,
    "text": "さて、近接政策最適化については以上である。"
  },
  {
    "start": 2241440,
    "end": 2251536,
    "text": "次回は、大規模な言語モデルを学習する方法である、人間のフィードバックによる強化学習を行うためのPPOの使い方を紹介するのでお楽しみに。"
  },
  {
    "start": 2251680,
    "end": 2259328,
    "text": "前回同様、深層強化学習の基本を詳しく復習したい方は、こちらの前回のビデオをご覧ください。"
  },
  {
    "start": 2259416,
    "end": 2260728,
    "text": "それらはすべてコメントにリンクされている。"
  },
  {
    "start": 2260816,
    "end": 2261712,
    "text": "以上です。"
  },
  {
    "start": 2261768,
    "end": 2263304,
    "text": "ご清聴ありがとうございました。"
  },
  {
    "start": 2263384,
    "end": 2269644,
    "text": "いつものように、このビデオを楽しんでいただけたなら、「いいね！」やコメントを押していただくか、お友達とシェアしてください。"
  },
  {
    "start": 2270024,
    "end": 2274748,
    "text": "特に、私に教えてほしい他のトピックがある場合は。"
  },
  {
    "start": 2274916,
    "end": 2276924,
    "text": "そこから多くのアイデアを得ている。"
  },
  {
    "start": 2277044,
    "end": 2279692,
    "text": "ツイートするならセラノ・アカデミーだ。"
  },
  {
    "start": 2279748,
    "end": 2287660,
    "text": "あるいは、私のページ、ソラノ・アカデミーをご覧いただければ、これらのビデオやブログ記事、その他たくさんの資料があります。"
  },
  {
    "start": 2287732,
    "end": 2298324,
    "text": "最後に、私の機械学習をロックする本をチェックしたい方は、下のリンクをクリックしてください。"
  },
  {
    "start": 2298404,
    "end": 2300964,
    "text": "また次のビデオでお会いしましょう。"
  }
]