[
  {
    "start": 250,
    "end": 4654,
    "text": "これは、我々がここに持っている大規模なテキスト埋め込みベンチマークのリーダーボードである。"
  },
  {
    "start": 4692,
    "end": 6670,
    "text": "これはハグフェイスが主催している。"
  },
  {
    "start": 6820,
    "end": 10622,
    "text": "基本的には、ここからすべての異なる埋め込みモデルをベンチマークする。"
  },
  {
    "start": 10676,
    "end": 16718,
    "text": "長い間、私のお気に入りのひとつが、このインストラクターのXL埋め込みであることがわかる。"
  },
  {
    "start": 16804,
    "end": 19694,
    "text": "OpenAIのエンベッディングもあります。"
  },
  {
    "start": 19812,
    "end": 26338,
    "text": "ちょっと手短に、なぜオープンAIエンベッディングを使うべきでないと私が考えるのか、タイムアウトをとって皆さんにお話ししようと思います。"
  },
  {
    "start": 26514,
    "end": 31042,
    "text": "エンベデッドとエンベデッドモデルについては、多くの混乱がある。"
  },
  {
    "start": 31186,
    "end": 36482,
    "text": "確かに、同じ埋め込みモデルのプロバイダーを使う必要はない。"
  },
  {
    "start": 36546,
    "end": 44566,
    "text": "もしOpenAIのGPT 4やGPT 3.5などを使っているなら、OpenAIのエンベッディングを使わなければならないということではありません。"
  },
  {
    "start": 44678,
    "end": 51978,
    "text": "同じような流れで、リャマのモデルのいくつかをエンベデッドモデルに変えようとしているプロジェクトもある。"
  },
  {
    "start": 52074,
    "end": 70366,
    "text": "エンベッディング・モデルは、エンベッディング・モデルであるためにトレーニングされています。つまり、エンベッディング・モデルは、多くの事前トレーニングを受けている一方で、通常、テキストを取り出して類似点などを見つけることができるようにチューニングされているのです。"
  },
  {
    "start": 70478,
    "end": 78566,
    "text": "最近見たように、たくさんの人がラマ2埋め込みモデルについて話している。"
  },
  {
    "start": 78668,
    "end": 80662,
    "text": "重要なのはそこではないだろう。"
  },
  {
    "start": 80796,
    "end": 84898,
    "text": "なぜOpenAIを使わないことを勧めるのか？"
  },
  {
    "start": 84994,
    "end": 91066,
    "text": "まず、OpenAIのモデルを見ると、オープンソースのモデルよりかなり遅れている。"
  },
  {
    "start": 91168,
    "end": 92746,
    "text": "物事は前進した。"
  },
  {
    "start": 92928,
    "end": 94442,
    "text": "それが主な理由ではない。"
  },
  {
    "start": 94496,
    "end": 106794,
    "text": "GTEベースやBGEスモールなど、非常に小型のモデルも含めて、他のモデルを手に入れることができるようになったのは、確かに良い理由のひとつだ。"
  },
  {
    "start": 106922,
    "end": 115754,
    "text": "これらは十分に小さいので、おそらくCPUでも動作させることができ、GPUなどのためにハードコアになることをあまり心配する必要もない。"
  },
  {
    "start": 115882,
    "end": 122814,
    "text": "しかし、このモデルを使いたくない大きな理由のひとつは、特定のプロバイダーに固定されたくないということだ。"
  },
  {
    "start": 122942,
    "end": 125310,
    "text": "ここにある他のモデルはすべてオープンソースだ。"
  },
  {
    "start": 125390,
    "end": 130322,
    "text": "ダウンロードもできるし、どんなプラットフォームでも、どんなハードウェアでも使える。"
  },
  {
    "start": 130466,
    "end": 143402,
    "text": "OpenAIのエンベッディングを使用し、将来使用する予定の大規模なコーパスデータをエンベッディングした瞬間に、将来そのデータを使用しなければならないことに気づくでしょう。"
  },
  {
    "start": 143536,
    "end": 148218,
    "text": "そうでなければ、またゼロからすべてを埋め込み直すことになる。"
  },
  {
    "start": 148384,
    "end": 150522,
    "text": "もちろん、それはできることだ。"
  },
  {
    "start": 150576,
    "end": 154606,
    "text": "このようなエンベデッドにもお金を払わなければならないとなると、コストがかかる。"
  },
  {
    "start": 154788,
    "end": 168014,
    "text": "もうひとつの問題は、おそらくそう遠くない将来、OpenAIはこのモデルを廃止するだろうということだ。"
  },
  {
    "start": 168142,
    "end": 174370,
    "text": "このモデルが非推奨になった場合、おそらく両方が使えるようになる一定の期間が重なったりするだろう。"
  },
  {
    "start": 174520,
    "end": 178606,
    "text": "もう一度、すべてをゼロから埋め込む必要がある。"
  },
  {
    "start": 178718,
    "end": 182562,
    "text": "OpenAIのエンベデッドは有用だと思う。"
  },
  {
    "start": 182626,
    "end": 187158,
    "text": "何かを素早くテストしたい場合、APIを使いたい場合、何かにpingを打ちたい場合。"
  },
  {
    "start": 187324,
    "end": 188182,
    "text": "わかったよ。"
  },
  {
    "start": 188236,
    "end": 192886,
    "text": "その場合は、テストに使ったり、アイデアを試すのに使ったり、いろいろな使い方ができる。"
  },
  {
    "start": 192998,
    "end": 199094,
    "text": "長期的に見れば、もしあなたが大きなプロジェクトを立ち上げようとしているのなら、オープンソースのエンベデッドソフトウェアを本当に使いたいと思いますか？"
  },
  {
    "start": 199222,
    "end": 202202,
    "text": "さて、BGEの埋め込みに戻ろう。"
  },
  {
    "start": 202346,
    "end": 211162,
    "text": "ここ数日で、このリーダーボードに新しいリーダーが誕生した。"
  },
  {
    "start": 211226,
    "end": 222770,
    "text": "このビデオでは、BGEのエンベッディングを見て、それをlangchainと一緒に使って、クロマ・ベクター・ストアの検索QAをやってみる。"
  },
  {
    "start": 222840,
    "end": 224802,
    "text": "私たちは、このすべてをまとめて見るつもりだ。"
  },
  {
    "start": 224936,
    "end": 226278,
    "text": "さあ、飛び込もう。"
  },
  {
    "start": 226364,
    "end": 228034,
    "text": "BGEのエンベッディングとは？"
  },
  {
    "start": 228082,
    "end": 238630,
    "text": "これは北京AIアカデミーから発表されたもので、基本的には彼らが発表した埋め込みモデルの新しいセットなんだ。"
  },
  {
    "start": 239210,
    "end": 248374,
    "text": "彼らのGitHubを見るとわかるが、このエンベッディングには英語と中国語のエンベッディングがある。"
  },
  {
    "start": 248502,
    "end": 255022,
    "text": "もうひとつ興味深いのは、まだリリースされていない多言語エンベッディングにも取り組んでいることだ。"
  },
  {
    "start": 255156,
    "end": 260958,
    "text": "これらはここ数日で、MTEBのリーダーボードに急上昇した。"
  },
  {
    "start": 261124,
    "end": 264850,
    "text": "私が気に入っているのは、このサイズだ。"
  },
  {
    "start": 265000,
    "end": 276558,
    "text": "今まで使っていたインストラクターのXLエンベッディングは、サイズが5GB弱。"
  },
  {
    "start": 276654,
    "end": 283442,
    "text": "私たちが持っているBGEのものは、スコアがはるかに良いだけでなく、それに比べて小さい。"
  },
  {
    "start": 283506,
    "end": 295798,
    "text": "今日使うのはBGEのベースとなる英語モデルで、インストラクターのエクセルで使っていたものの10分の1くらいの大きさです。"
  },
  {
    "start": 295894,
    "end": 302874,
    "text": "1ギガバイト強のラージモデルを選ぶこともできるが、その場合は埋め込み寸法も大きくなる。"
  },
  {
    "start": 302992,
    "end": 308958,
    "text": "おそらく、多くのものにとって、768次元のエンベッディングが極めて標準的であることがわかるだろう。"
  },
  {
    "start": 309044,
    "end": 314254,
    "text": "これはクロマとラングチェインで使うものだ。"
  },
  {
    "start": 314292,
    "end": 319618,
    "text": "以前使っていたインストラクターのXLエンベッディングと比較してどうですか？"
  },
  {
    "start": 319784,
    "end": 328482,
    "text": "前回のビデオで説明したことを復習しておくと、基本的にはtogether APIでホストされているllamaの270,000,000,000モデルを使用している。"
  },
  {
    "start": 328546,
    "end": 331734,
    "text": "今まで使っていたものを変えるつもりはない。"
  },
  {
    "start": 331772,
    "end": 337286,
    "text": "以前のエンベッディングは、このハグする顔のインストラクターのエンベッディングを使っていた。"
  },
  {
    "start": 337388,
    "end": 340898,
    "text": "これは、これらのエンベッディングの抱きつき顔の実装に過ぎない。"
  },
  {
    "start": 340994,
    "end": 344154,
    "text": "言ったように、このエンベデッドは長い間私のお気に入りだった。"
  },
  {
    "start": 344272,
    "end": 349082,
    "text": "唯一の難点は、これが5ギガバイトモデルだということだ。"
  },
  {
    "start": 349216,
    "end": 356782,
    "text": "実際にデータベースを使ってエンベッディングを作成するとなると、T4ではかなりの時間がかかる。"
  },
  {
    "start": 356916,
    "end": 357262,
    "text": "そうだね。"
  },
  {
    "start": 357316,
    "end": 363774,
    "text": "実際にエンベッディングを作っているテキストが1000個ほどあるので、しばらく座って待っていることになる。"
  },
  {
    "start": 363892,
    "end": 367538,
    "text": "だから、そこに座って待っているとかなり時間がかかる。"
  },
  {
    "start": 367624,
    "end": 374146,
    "text": "これと今日私が行った新しいものを比較すると、コードは基本的にすべて同じである。"
  },
  {
    "start": 374248,
    "end": 378562,
    "text": "この中で変更したのは、実は埋め込みだけなんだ。"
  },
  {
    "start": 378626,
    "end": 378854,
    "text": "そうだね。"
  },
  {
    "start": 378892,
    "end": 385714,
    "text": "Hfインストラクターのエクセル埋め込みを使うのではなく、このBGE埋め込みを使っている。"
  },
  {
    "start": 385842,
    "end": 391994,
    "text": "さて、先ほども言ったように、僕が使うモデルはこのBgEベース・エンだ。"
  },
  {
    "start": 392032,
    "end": 395462,
    "text": "英語の場合は、基本的に余弦類似度を使う。"
  },
  {
    "start": 395526,
    "end": 401114,
    "text": "埋め込みをtrueとして正規化し、このように設定すればいい。"
  },
  {
    "start": 401232,
    "end": 422722,
    "text": "インストラクターのXLでは処理に数分かかっていたのが、ここでは1000個のエンベッディングを35秒で処理できる。"
  },
  {
    "start": 422856,
    "end": 425986,
    "text": "あとは基本的に、以前と同じようにレトリーバーを作る。"
  },
  {
    "start": 426088,
    "end": 427646,
    "text": "残りのコードは以前と同じだ。"
  },
  {
    "start": 427688,
    "end": 430134,
    "text": "まだ、これから得られるソースがある。"
  },
  {
    "start": 430172,
    "end": 435574,
    "text": "基本的に、ソース・ドキュメントはtrueと等しい値を返す。"
  },
  {
    "start": 435692,
    "end": 436326,
    "text": "私たちはそれを見ている。"
  },
  {
    "start": 436348,
    "end": 438886,
    "text": "案の定、本当に重要なのはここを見下ろすことだ。"
  },
  {
    "start": 438988,
    "end": 443594,
    "text": "案の定、それぞれに適したアウトプットを見つけることだ。"
  },
  {
    "start": 443712,
    "end": 449450,
    "text": "さて、言語モデル自体が、実は他のものも書いているというのは興味深い。"
  },
  {
    "start": 449520,
    "end": 451066,
    "text": "ビデオを作ろうかな。"
  },
  {
    "start": 451168,
    "end": 456890,
    "text": "これでできることのひとつは、言語モデルに2回目のパスをして出力をクリーンアップすることだ。"
  },
  {
    "start": 456970,
    "end": 460842,
    "text": "おそらく、私たちはこれだけを望んでいるのだろう。"
  },
  {
    "start": 460906,
    "end": 463390,
    "text": "不正解が入るのは避けたい。"
  },
  {
    "start": 463540,
    "end": 467010,
    "text": "他の多くの選手にとっては、これでいい。"
  },
  {
    "start": 467160,
    "end": 479658,
    "text": "ここでも、言語モデルは少し奇妙な出力を出している。つまり、最初に正しい答えを出しているのだが、そこでは基本的に、2048から4096のトークンに拡張されたことを伝えている。"
  },
  {
    "start": 479774,
    "end": 483382,
    "text": "となると、ここで起こっているのはこのような奇妙なことだ。"
  },
  {
    "start": 483516,
    "end": 485286,
    "text": "これこそ、私たちが一掃したいものだ。"
  },
  {
    "start": 485308,
    "end": 489850,
    "text": "他の選手と同じようにすれば、ほとんどの選手は問題ない。"
  },
  {
    "start": 490000,
    "end": 492442,
    "text": "ラマ2は何枚のトークンで調教されたのですか？"
  },
  {
    "start": 492576,
    "end": 494502,
    "text": "2兆個のトークンでトレーニング。"
  },
  {
    "start": 494646,
    "end": 496458,
    "text": "それはすべて問題ない。"
  },
  {
    "start": 496624,
    "end": 500666,
    "text": "ラマ3世はいつ来るのか？"
  },
  {
    "start": 500848,
    "end": 506302,
    "text": "ここで興味深いのは、基本的にラマ3のリリースに関してメタからの公式発表はないと書かれていることだ。"
  },
  {
    "start": 506436,
    "end": 510542,
    "text": "現行バージョンのラマ2は2023年7月にリリースされた。"
  },
  {
    "start": 510676,
    "end": 513822,
    "text": "今、他の答えがいくつか出てきているのは興味深い。"
  },
  {
    "start": 513876,
    "end": 527246,
    "text": "ここで使っているシステムプロンプトとかのせいだと思うんだけど、基本的なQAリトリーバルを使っているから、ラマ2に最適化してシステムプロンプトをもっと使うことができるんだ。"
  },
  {
    "start": 527288,
    "end": 535670,
    "text": "これをまとめ、他のものを調べれば、以前見たのと同じような答え、同じようなアウトプットが見つかるはずだ。"
  },
  {
    "start": 535820,
    "end": 542714,
    "text": "ここでの大きな勝利は、必ずしも品質が大幅に向上したことではない。"
  },
  {
    "start": 542832,
    "end": 549898,
    "text": "より大きなモデルにすれば、より質の高い検索ができるかもしれないが、実際、以前は質が悪かったわけではない。"
  },
  {
    "start": 549984,
    "end": 553166,
    "text": "このようなことはたくさんある。"
  },
  {
    "start": 553268,
    "end": 562062,
    "text": "ここでの大きな勝利は、5ギガバイトではなく、この実際のモデルで438メガバイトしかないことだ。"
  },
  {
    "start": 562196,
    "end": 569246,
    "text": "基本的には、推論時間が短縮され、GPUに必要なVRAMなどのRAMの量も少なくて済むということだ。"
  },
  {
    "start": 569358,
    "end": 578134,
    "text": "CPUを使えば推論だってできるだろうし、そのために時間がかかることもないだろう。"
  },
  {
    "start": 578332,
    "end": 586930,
    "text": "とにかく、これが新しい一般的な埋め込み、北京AIアカデミーのBGE埋め込みです。"
  },
  {
    "start": 587090,
    "end": 588678,
    "text": "ぜひチェックしてほしい。"
  },
  {
    "start": 588764,
    "end": 601422,
    "text": "もし、あなたが多言語用のエンベッディングに興味があるのであれば、彼らがいつ多言語モデルをリリースするのか、とても注意して見ていた方がいいと思います。"
  },
  {
    "start": 601556,
    "end": 608206,
    "text": "今後、多言語モデルのアウトプットの結果が向上するかもしれない。"
  },
  {
    "start": 608308,
    "end": 609886,
    "text": "とにかく、これらをチェックしてほしい。"
  },
  {
    "start": 609988,
    "end": 613262,
    "text": "ご質問があれば、いつものように下のコメント欄に書き込んでください。"
  },
  {
    "start": 613396,
    "end": 618974,
    "text": "もしこのようなビデオが役に立つと思われたら、クリックして購読してください。"
  },
  {
    "start": 619092,
    "end": 620240,
    "text": "とりあえず、さようなら。"
  }
]