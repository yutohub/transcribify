[
  {
    "start": 720,
    "end": 1862,
    "text": "地域の皆さん、こんにちは。"
  },
  {
    "start": 2038,
    "end": 10794,
    "text": "人間のフィードバックによる強化学習が、間違いなくチャットGPTの成功のカギを握っていたことは周知の通りだ。"
  },
  {
    "start": 11294,
    "end": 16542,
    "text": "チャットGPTがリリースされて半年が経とうとしている。"
  },
  {
    "start": 16638,
    "end": 17510,
    "text": "どう思う？"
  },
  {
    "start": 17542,
    "end": 20014,
    "text": "強化学習を廃止するのか？"
  },
  {
    "start": 20174,
    "end": 21326,
    "text": "つまらないよ。"
  },
  {
    "start": 21510,
    "end": 22950,
    "text": "私たちは新しいものを手に入れた。"
  },
  {
    "start": 23142,
    "end": 30374,
    "text": "ダイレクト・プリファレンス・オプティマイゼーション（直接嗜好最適化）DPOの新しい技術を見てみよう。"
  },
  {
    "start": 31034,
    "end": 31986,
    "text": "さあ、始めよう。"
  },
  {
    "start": 32130,
    "end": 36534,
    "text": "人間のフィードバックから強化学習を行う。"
  },
  {
    "start": 36994,
    "end": 41210,
    "text": "複雑で不安定な手術だったことを覚えているだろう。"
  },
  {
    "start": 41282,
    "end": 57694,
    "text": "まず人間の嗜好を反映した報酬モデルを当てはめ、次に強化学習を使って教師なし言語モデルを微調整し、元のモデルから大きく逸脱することなく推定報酬を最大化する。"
  },
  {
    "start": 58194,
    "end": 61202,
    "text": "もう少し構造的なものがお好きなら。"
  },
  {
    "start": 61258,
    "end": 65386,
    "text": "技術的な面では、3段階のパイプラインがあった。"
  },
  {
    "start": 65490,
    "end": 73026,
    "text": "それぞれの、あるいは複数のダウンストリームタスクについて、高品質なデータセットを用いて、スーパーバイズド・ファインチューニングを行った。"
  },
  {
    "start": 73170,
    "end": 75774,
    "text": "対話指導、落下要約。"
  },
  {
    "start": 76234,
    "end": 80734,
    "text": "そして、SFTモデルには、答えのペアを生成するプロンプトが表示される。"
  },
  {
    "start": 81034,
    "end": 94106,
    "text": "これらのペアは人間のラベラーに提示され、ラベラーは1つの答えに好意を示した。"
  },
  {
    "start": 94210,
    "end": 97334,
    "text": "最後に、強化学習の最適化を行った。"
  },
  {
    "start": 98034,
    "end": 103882,
    "text": "この強化学習フェーズでは、学習した報酬関数を使って言語モデルにフィードバックを書き込む。"
  },
  {
    "start": 103978,
    "end": 114024,
    "text": "特に、我々は最適化問題を定式化し、2017年に我々が開発したPPO近接政策最適化を用いて最大化を行った。"
  },
  {
    "start": 114684,
    "end": 119464,
    "text": "これが人間のフィードバックによる古き良き強化学習だった。"
  },
  {
    "start": 120244,
    "end": 129264,
    "text": "さて、2023年5月29日、スタンフォードは再び直接選好の最適化に打って出た。"
  },
  {
    "start": 131244,
    "end": 132992,
    "text": "今はどうですか？"
  },
  {
    "start": 133048,
    "end": 148844,
    "text": "DPOは、明示的な報酬推定と強化学習という2つのステップを回避し、単一の最尤目的語を用いて方針を学習することができる。"
  },
  {
    "start": 149944,
    "end": 159364,
    "text": "人間のフィードバックによる先行強化学習とは異なり、報酬を学習し、それを強化学習によって最適化する。"
  },
  {
    "start": 159914,
    "end": 174734,
    "text": "スタンフォード大学によるこの新しいDPOアプローチは、報酬モデリングのステップを完全に回避し、嗜好データを用いて言語モデルを直接最適化する。"
  },
  {
    "start": 175514,
    "end": 176774,
    "text": "こんなことが可能なのか？"
  },
  {
    "start": 177794,
    "end": 182466,
    "text": "今、この美しいアーカイブ・プレプリントの著者たちが読んでいる。"
  },
  {
    "start": 182490,
    "end": 183890,
    "text": "とてもお勧めできる。"
  },
  {
    "start": 183962,
    "end": 190436,
    "text": "少し複雑な面もあり、特に別館では数学的証明が多い。"
  },
  {
    "start": 190540,
    "end": 193324,
    "text": "週末が必要なのかもしれない。"
  },
  {
    "start": 193484,
    "end": 196324,
    "text": "特に、週末を利用して理解しようと思う。"
  },
  {
    "start": 196444,
    "end": 198764,
    "text": "スタンフォードの論文は非常に興味深い。"
  },
  {
    "start": 198924,
    "end": 207036,
    "text": "重要な洞察は、報酬関数から最適なポリシーへの分析的マッピングを活用することである。"
  },
  {
    "start": 207220,
    "end": 214784,
    "text": "これにより、著者は報酬関数に対する損失関数をポリシーに対する損失関数に変換することができる。"
  },
  {
    "start": 216254,
    "end": 224794,
    "text": "彼らは、この変数を変えるアプローチによって、報酬のモデル化のステップを省略することができると主張する。"
  },
  {
    "start": 225454,
    "end": 233074,
    "text": "要するに、ポリシー・ネットワークは言語モデルと報酬そのものを表しているのだ。"
  },
  {
    "start": 233934,
    "end": 237942,
    "text": "そう簡単には理解できないが、理解しなければならない。"
  },
  {
    "start": 238078,
    "end": 241968,
    "text": "損失関数の勾配はそれほど複雑ではない。"
  },
  {
    "start": 242086,
    "end": 245904,
    "text": "次のスライドで、これを実装するPythonコードをお見せしよう。"
  },
  {
    "start": 246524,
    "end": 257184,
    "text": "ここでは、Dpoのメカニズム的な理解のために、我々の損失関数lの勾配を、我々のDpo法の勾配とする。"
  },
  {
    "start": 261324,
    "end": 265796,
    "text": "直感的に、よし、原文で行こう、と書いてくれるのがいい。"
  },
  {
    "start": 265860,
    "end": 277186,
    "text": "直観的には、損失関数の勾配は、好ましい完成の可能性を高め、好ましくない完成の可能性を下げる。"
  },
  {
    "start": 277250,
    "end": 289174,
    "text": "もちろん、重要なのは、暗黙の報酬モデルがどれだけ高い評価を下すかによって、例題が重み付けされることだ。"
  },
  {
    "start": 291394,
    "end": 293374,
    "text": "興味深い公式だ。"
  },
  {
    "start": 293954,
    "end": 299050,
    "text": "今は、本当にステップを踏むには、もう少し時間をかける必要があると思う。"
  },
  {
    "start": 299202,
    "end": 307818,
    "text": "本書の別冊でお伝えしたように、数学的な説明は数ページにわたっている。"
  },
  {
    "start": 307906,
    "end": 310242,
    "text": "それを理解するのに時間がかかる。"
  },
  {
    "start": 310418,
    "end": 316098,
    "text": "それを要約し、人間の嗜好から今を学ぶ。"
  },
  {
    "start": 316146,
    "end": 320334,
    "text": "私たちは、このフレームワークが意志決定者を育成するためのパワフルで拡張性のあるフレームワークであることを知っている。"
  },
  {
    "start": 320834,
    "end": 330802,
    "text": "この新しいDpoを使えば、強化学習なしで嗜好から言語モデルを学習するためのシンプルな学習パラダイムを手に入れることができる。"
  },
  {
    "start": 330898,
    "end": 334614,
    "text": "こんなことができるなんてすごいことだと思う。"
  },
  {
    "start": 335034,
    "end": 335938,
    "text": "よりも。"
  },
  {
    "start": 335986,
    "end": 336810,
    "text": "はい、はい、はい。"
  },
  {
    "start": 336882,
    "end": 337894,
    "text": "マッピングだ。"
  },
  {
    "start": 338674,
    "end": 345922,
    "text": "単純なクロスエントロピー損失で、人間の嗜好を直接満足させる。"
  },
  {
    "start": 346098,
    "end": 359966,
    "text": "単純な損失関数かどうかはわからないが、強化学習なしで、事実上ハイパーパラメーターのチューニングなしで、単一の、例えば単一のクロスロード・エントロピー損失関数で大丈夫だ。"
  },
  {
    "start": 360030,
    "end": 373454,
    "text": "著者らはプレプリントで、新しいDPOが、PPOを含む既存の人間のフィードバックによる強化学習アルゴリズムと同等かそれ以上の性能を発揮することを発表した。"
  },
  {
    "start": 373614,
    "end": 375118,
    "text": "これは驚きだ。"
  },
  {
    "start": 375246,
    "end": 377078,
    "text": "これは本当に素晴らしいことだ。"
  },
  {
    "start": 377206,
    "end": 384420,
    "text": "彼らは、この新しい方法論が、人間の嗜好からllmsを訓練する障壁を減らすと主張した。"
  },
  {
    "start": 384492,
    "end": 386636,
    "text": "これがここで開く。"
  },
  {
    "start": 386820,
    "end": 388304,
    "text": "驚くべき可能性だ。"
  },
  {
    "start": 389444,
    "end": 390188,
    "text": "しかしだ。"
  },
  {
    "start": 390356,
    "end": 391460,
    "text": "つまり、スタンフォードなんだ。"
  },
  {
    "start": 391532,
    "end": 404644,
    "text": "想像してみてほしいが、彼らが評価できたのは、60億の自由学習可能なパラメーターまでで、将来的には、現在のDPOを最先端のモデルにスケールアップさせることになるだろうという。"
  },
  {
    "start": 404804,
    "end": 411918,
    "text": "1750億の自由パラメータ、4億から5億の事前学習可能パラメータ。"
  },
  {
    "start": 412046,
    "end": 417354,
    "text": "まだまだ続くが、非常に興味深いプレプリントだ。"
  },
  {
    "start": 418134,
    "end": 424062,
    "text": "ああ、言っただろう、DPO損失関数のためのパタッチコードはむしろ単純なんだ。"
  },
  {
    "start": 424118,
    "end": 425014,
    "text": "これだ。"
  },
  {
    "start": 425174,
    "end": 426014,
    "text": "それだけだ。"
  },
  {
    "start": 426134,
    "end": 427422,
    "text": "パラメータも教えてくれる。"
  },
  {
    "start": 427478,
    "end": 428326,
    "text": "はい、はい、はい。"
  },
  {
    "start": 428390,
    "end": 430234,
    "text": "これらはすべて文献に載っている。"
  },
  {
    "start": 430854,
    "end": 447554,
    "text": "そして、2つの方法論、古典的なPPOであるPPOと、現在のDPOの間の人間による比較もここで行われ、プロンプトが表示され、DPOによる回答とPPOによる回答が表示され、そして人間による判定がここで行われる。"
  },
  {
    "start": 447594,
    "end": 450618,
    "text": "これはほんの一例に過ぎない。"
  },
  {
    "start": 450786,
    "end": 460794,
    "text": "人間の評価者による判定は、DPOによる要約の方が、イエス、イエス、イエスを作ることに焦点を当てることで、より効果的に投稿の主旨を捉えているというものだった。"
  },
  {
    "start": 460834,
    "end": 462618,
    "text": "自分自身を見ることができる。"
  },
  {
    "start": 462706,
    "end": 464178,
    "text": "別の例を挙げよう。"
  },
  {
    "start": 464346,
    "end": 472434,
    "text": "強化学習なしのPPOとDPOの直接的な成績がご覧いただけるだろう。"
  },
  {
    "start": 473494,
    "end": 476134,
    "text": "素晴らしいことだ。"
  },
  {
    "start": 476294,
    "end": 481478,
    "text": "もちろん、今は社内に競争相手がいると思う。"
  },
  {
    "start": 481646,
    "end": 487782,
    "text": "GPTの4番、スタンフォードのこのアーカイブのプレプリントを見てくれ。"
  },
  {
    "start": 487958,
    "end": 500624,
    "text": "次の3ページで、コーラーAIと呼ばれるGPT4のプラグインによって生成された要約をお見せしよう。"
  },
  {
    "start": 501164,
    "end": 503356,
    "text": "これが1ページ目だ。"
  },
  {
    "start": 503420,
    "end": 511504,
    "text": "4Kで収録されているので、一時停止して1ページ目、2ページ目、3ページ目を読むことも問題ない。"
  },
  {
    "start": 512804,
    "end": 514252,
    "text": "実に興味深い。"
  },
  {
    "start": 514308,
    "end": 520300,
    "text": "数学的には少し難しいし、自分で経験しなければならない。"
  },
  {
    "start": 520372,
    "end": 523641,
    "text": "本当に理解するためには、週末を過ごさなければならないと思う。"
  },
  {
    "start": 523787,
    "end": 541873,
    "text": "もしこれがうまくいけば、スタンフォード大学の著者が言うように、これが本当に効果的であれば、これは人間の嗜好に基づいた新しい大規模言語モデルを訓練するための重要な前進となる。"
  },
  {
    "start": 542293,
    "end": 543533,
    "text": "今日はここまでだった。"
  },
  {
    "start": 543653,
    "end": 544933,
    "text": "楽しんでいただけたなら幸いだ。"
  },
  {
    "start": 544973,
    "end": 548253,
    "text": "参考になったし、次のビデオで君に会えることを期待しているよ。"
  }
]