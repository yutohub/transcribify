[
  {
    "start": 26850,
    "end": 29110,
    "text": "さて、皆さん、今回もご参加いただきありがとうございます。"
  },
  {
    "start": 29180,
    "end": 33842,
    "text": "最後のセッション、最初のスピーカーはシシール・パティルです。"
  },
  {
    "start": 33906,
    "end": 38390,
    "text": "彼は巨大なAPIでllmsを接続するゴリラについて教えてくれるだろう。"
  },
  {
    "start": 41130,
    "end": 42600,
    "text": "それでは皆さん、こんばんは。"
  },
  {
    "start": 43370,
    "end": 46162,
    "text": "今日はゴリラについて話そう。"
  },
  {
    "start": 46226,
    "end": 55978,
    "text": "LLMを微調整するのはいつなのか、検索を使うのはいつなのか。"
  },
  {
    "start": 56074,
    "end": 59658,
    "text": "第二に、この聴衆にとってより興味深いのは、幻覚をどうやって測定するかということだ。"
  },
  {
    "start": 59834,
    "end": 63838,
    "text": "この2つは、今日のLLMにおけるオープンで興味深い問題だと思う。"
  },
  {
    "start": 64004,
    "end": 69822,
    "text": "簡単な自己紹介をすると、私はバークレー校の博士課程の学生で、バークレー校のAI研究とスカイラブで共同研究をしている。"
  },
  {
    "start": 69886,
    "end": 76420,
    "text": "私の仕事の多くはシステムと機械学習の交差点にあり、それが今日お見せするいくつかの仕事に影響を与え、インスピレーションを与えている。"
  },
  {
    "start": 77990,
    "end": 80342,
    "text": "今日の世界を見てみよう。"
  },
  {
    "start": 80396,
    "end": 80662,
    "text": "そうだね。"
  },
  {
    "start": 80716,
    "end": 84578,
    "text": "今日、あなたがしているllmとの付き合い方は、llmの数が多すぎるということだ。"
  },
  {
    "start": 84674,
    "end": 95046,
    "text": "ユーザーであるあなたは、LLMにプロンプトを要求し、応答を得ることになる。"
  },
  {
    "start": 95158,
    "end": 100938,
    "text": "そして、その手応えを生かして、世界の他の地域に何らかの効果をもたらすことができるかもしれない。"
  },
  {
    "start": 101024,
    "end": 108430,
    "text": "これは、ラテックスに証明を書いたり、コマンドをコピーしてターミナルに貼り付けて実行したりといったことである。"
  },
  {
    "start": 109010,
    "end": 112478,
    "text": "基本的に、この行為と対応の責任はあなたにある。"
  },
  {
    "start": 112564,
    "end": 123890,
    "text": "ラング、チェイン、ミン、GPTなど、LLMを実践しようとする皆さんの中には使ったことのあるツールもあるかもしれませんが、基本的にはLLMの世界の中心にいるのはあなたたちだと私たちは考えています。"
  },
  {
    "start": 123960,
    "end": 126370,
    "text": "私たちが本当にやりたいのは、この両方を反転させることだ。"
  },
  {
    "start": 126520,
    "end": 134630,
    "text": "ユーザーであるあなたが、LLMにプロンプトを導入しようとしている。"
  },
  {
    "start": 134780,
    "end": 137974,
    "text": "今、LLMは世界中を相手に行動しようとしている。"
  },
  {
    "start": 138092,
    "end": 143994,
    "text": "これは、単に状態を読むという意味でも、世界の状態を変えるようなアクションを起こすという意味でもあり得る。"
  },
  {
    "start": 144192,
    "end": 150522,
    "text": "そして、世界からのフィードバックを受け取り、それをLMが処理し、あなたに返す。"
  },
  {
    "start": 150576,
    "end": 155486,
    "text": "起きたことを受け入れるか、起きたことをやり直すか、あるいは今やったことを取り消すか。"
  },
  {
    "start": 155588,
    "end": 163902,
    "text": "根本的には、人間をその中心に置き換えることで、ある意味、人間をLLMと相互作用させ、LLMが世界の他の部分と相互作用するようにすることだ。"
  },
  {
    "start": 164036,
    "end": 170610,
    "text": "それがビジョンであり、願望であり、それを具体的な例で示すことである。"
  },
  {
    "start": 170760,
    "end": 177026,
    "text": "皆さんの多くは、llmsをトレーニングしたり、微調整したり、あるいは遊んでみたことがあるかもしれない。"
  },
  {
    "start": 177128,
    "end": 182550,
    "text": "仮にあなたが、東アジアで800台のGPUが欲しいと言ったとしよう。"
  },
  {
    "start": 182700,
    "end": 193378,
    "text": "さて、私たちが欲しいのはLLMであり、この場合、UCバークレーで訓練したLLMであるゴリラが、先に進み、この東側が800GPUの容量かどうかを確認する。"
  },
  {
    "start": 193474,
    "end": 201834,
    "text": "そうであれば、それをあなたに返し、そうでなければ、返して、私は東の私たちではなく、西の私たちだったかもしれないと言う。"
  },
  {
    "start": 201952,
    "end": 211322,
    "text": "LLMと話し、その返答を受け、他の世界に対して何らかのアクションを起こすのではなく、LLMと話すだけでいいという、ハイレベルなアイデアだ。"
  },
  {
    "start": 211386,
    "end": 214142,
    "text": "今、LLMは世界に対してあるアクションを起こそうとしている。"
  },
  {
    "start": 214276,
    "end": 215840,
    "text": "それがここでの目標だ。"
  },
  {
    "start": 217410,
    "end": 221906,
    "text": "だから今日、私が世界と話すと言ったら、それは何を意味するのか？"
  },
  {
    "start": 222088,
    "end": 226434,
    "text": "私たちがLLMについて考える方法は、道具として考えることであり、道具は互いに話し合う必要がある。"
  },
  {
    "start": 226552,
    "end": 230738,
    "text": "コンピュータ・サイエンスでは、ツール同士が互いに会話する方法は、明確に定義されたAPIコールを通じて行われる。"
  },
  {
    "start": 230834,
    "end": 232290,
    "text": "だから、それをターゲットにしている。"
  },
  {
    "start": 232370,
    "end": 233634,
    "text": "私たちはLLMを教えようとしている。"
  },
  {
    "start": 233682,
    "end": 239286,
    "text": "どのようにAPIコールを記述し、呼び出せば、他の世界でもこのような効果をもたらすことができるのか。"
  },
  {
    "start": 239388,
    "end": 245500,
    "text": "まず始めに、私たちには非常に人気のあるAPIがいくつかありますが、それについて話すよりも、簡単なデモをお見せしましょう。"
  },
  {
    "start": 247630,
    "end": 252094,
    "text": "ちなみに、これはすべてオープンソースで、ここからアクセスできる。"
  },
  {
    "start": 252212,
    "end": 256702,
    "text": "バークレーに小さなボックスがあって、そこに接続しているんだ。"
  },
  {
    "start": 256756,
    "end": 258990,
    "text": "英語から中国語に翻訳したい。"
  },
  {
    "start": 260290,
    "end": 271182,
    "text": "バークレーのインスタンスに接続するだけだ。"
  },
  {
    "start": 271246,
    "end": 277406,
    "text": "では、それを英語から中国語に移したいかというと、そうではない。"
  },
  {
    "start": 277518,
    "end": 279602,
    "text": "これは今日のLLMにできないことではない。"
  },
  {
    "start": 279656,
    "end": 284118,
    "text": "例えば、Chat GPTやオープンソースのllmsにこれを頼むと、おそらく何かを吐き出すことができるだろう。"
  },
  {
    "start": 284284,
    "end": 289190,
    "text": "要するに、あなたはLLMが知っていることを扱っているのであって、LLMが情報を増やす方法はないということだ。"
  },
  {
    "start": 289260,
    "end": 292426,
    "text": "プレトレーニングのセットには基本的に制限がある。"
  },
  {
    "start": 292608,
    "end": 310526,
    "text": "その代わり、LLMにAPIの呼び出し方を教えるとしたら、この場合、あまり明確ではありませんが、パイプライン翻訳の英語から中国語へのこのAPIを呼び出して、コードの束も吐き出すつもりです。"
  },
  {
    "start": 310628,
    "end": 313454,
    "text": "できることは、このコードをデプロイすることだ。"
  },
  {
    "start": 313492,
    "end": 318018,
    "text": "これは、サブプロセスを開くか、このコードを実行するだけである。"
  },
  {
    "start": 318104,
    "end": 320562,
    "text": "最後に見えるのは、実は最終結果なのだ。"
  },
  {
    "start": 320696,
    "end": 323938,
    "text": "これは時間の都合上、単なるおもちゃの例である。"
  },
  {
    "start": 324104,
    "end": 331474,
    "text": "このようなことができるのは、基本的にトレーニングレシピのおかげだ。"
  },
  {
    "start": 331522,
    "end": 334934,
    "text": "これを可能にするために、私たちは2つの重要な問題に取り組まなければならなかった。"
  },
  {
    "start": 335132,
    "end": 342630,
    "text": "今日は少し趣向を変えて、私たちがこの2つにどのように取り組んでいるのか、興味深い、そして根本的に未解決の問題だと私たちが考えていることについてお話ししたいと思います。"
  },
  {
    "start": 342780,
    "end": 345462,
    "text": "最初の質問は、微調整と検索をどのようにミックスするかということだ。"
  },
  {
    "start": 345526,
    "end": 346186,
    "text": "そうだね。"
  },
  {
    "start": 346368,
    "end": 348742,
    "text": "微調整は、事前に訓練されたLLMを受けるところだ。"
  },
  {
    "start": 348806,
    "end": 351942,
    "text": "ファンデーション・モデルやフロンティア・モデルと呼ばれることもある。"
  },
  {
    "start": 352006,
    "end": 359914,
    "text": "RLHFやインストラクター・チューニングなど、特定のタスクのために微調整したり、特定の目標のために微調整したりすることも含まれます。"
  },
  {
    "start": 359962,
    "end": 371490,
    "text": "このように、私たちが知っていたこと、あるいは少なくとも仮説として真実であってほしかったことは、ファインチューニングではモデルの挙動を補強するのに対し、検索ではモデルに新しい知識を導入するということだった。"
  },
  {
    "start": 371640,
    "end": 376134,
    "text": "さて、ゴリラで実証したことは、これはあまり真実ではないということだ。"
  },
  {
    "start": 376172,
    "end": 381990,
    "text": "ファイン・チューニングは実際、行動と知識の両方を取り入れるのに極めて効果的である。"
  },
  {
    "start": 382330,
    "end": 384386,
    "text": "ただし、レトリーバーは不正確だ。"
  },
  {
    "start": 384498,
    "end": 384918,
    "text": "そうだね。"
  },
  {
    "start": 385004,
    "end": 391066,
    "text": "今日では、何億というスケールのベクトル・データベースがあり、類似性検索を行う複数の方法がある。"
  },
  {
    "start": 391168,
    "end": 394326,
    "text": "リトリーバーは基本的に正確ではない。"
  },
  {
    "start": 394438,
    "end": 398006,
    "text": "最高クラスのレトリーバーでも精度は70％程度。"
  },
  {
    "start": 398118,
    "end": 400394,
    "text": "つまり、3分の1は間違っている。"
  },
  {
    "start": 400512,
    "end": 406078,
    "text": "もし、モデルに間違った情報を与えて、そのモデルにこう言わせたとしたら、「ゴミが入ればゴミが出る」という問いに答えられるだろうか？"
  },
  {
    "start": 406164,
    "end": 406510,
    "text": "そうだろう？"
  },
  {
    "start": 406580,
    "end": 408240,
    "text": "これをどう克服するのか？"
  },
  {
    "start": 409410,
    "end": 414018,
    "text": "これに答えるために、私たちは検索を意識したトレーニングであるラットと呼ばれるものを導入している。"
  },
  {
    "start": 414184,
    "end": 415780,
    "text": "アイデアは簡単だ。"
  },
  {
    "start": 416150,
    "end": 423538,
    "text": "我々はモデルを微調整し、検索されたコンテキストを使用するか無視するかを教えるつもりなんだ。"
  },
  {
    "start": 423704,
    "end": 432402,
    "text": "トレーニングの過程で、私はモデルに正しい情報と間違った情報の例を与え、この1ビット・イン・1ビットの高度な判断をするよう求めるつもりだ。"
  },
  {
    "start": 432466,
    "end": 436050,
    "text": "これは正しい検索コンテキストなのか、それとも間違った検索コンテキストなのか？"
  },
  {
    "start": 436210,
    "end": 444886,
    "text": "もしそれが正しければ、それを使い、もし間違っていれば、それを捨てて、事前トレーニングで得た知識を使うということだね？"
  },
  {
    "start": 445068,
    "end": 446262,
    "text": "例を挙げよう。"
  },
  {
    "start": 446316,
    "end": 448470,
    "text": "これはかなり忙しいスライドだが、順を追って説明しよう。"
  },
  {
    "start": 448540,
    "end": 456330,
    "text": "ユーザープロンプトは、「私はUberのエンジニアで、現場の画像から歩行者や車などを分類できるAPIを見つける必要がある。"
  },
  {
    "start": 456490,
    "end": 460126,
    "text": "さて、ほとんどの人は、これが単なる古典的な画像分類作業であることを知っているだろうか？"
  },
  {
    "start": 460148,
    "end": 460880,
    "text": "あるいは、異議を唱える。"
  },
  {
    "start": 462050,
    "end": 467778,
    "text": "私はレトリーバーを使ったが、これはあなたの好きなレトリーバーでもいい。"
  },
  {
    "start": 467864,
    "end": 470366,
    "text": "レトリーバーは言った。"
  },
  {
    "start": 470478,
    "end": 473634,
    "text": "この質問に答えるために、このリファレンスAPIを使ってみてはどうだろう？"
  },
  {
    "start": 473832,
    "end": 478950,
    "text": "私たちが彼らに最初に教えるのは、この質問に答えることだ。"
  },
  {
    "start": 479290,
    "end": 484594,
    "text": "今回は、歩行者やキャストなどを分類する必要があるAPIのために、物体検出を行っている。"
  },
  {
    "start": 484642,
    "end": 485798,
    "text": "これはおそらく正しい。"
  },
  {
    "start": 485884,
    "end": 487282,
    "text": "それがモデルの学習だ。"
  },
  {
    "start": 487426,
    "end": 493530,
    "text": "そうすると、モデルは先に進み、あなたが何をする必要があるかという回答と、それを少し解釈しやすくするための説明を与えてくれる。"
  },
  {
    "start": 495150,
    "end": 502170,
    "text": "Uberのエンジニアで、歩行者を分類できるAPIを探す必要がある。"
  },
  {
    "start": 502510,
    "end": 508270,
    "text": "こちらを見ていただきたいのですが、私が本当にPytorch運賃シーケンスと呼んでいるものは、実際には翻訳APIなのです。"
  },
  {
    "start": 508850,
    "end": 519038,
    "text": "このシナリオでは、分類タスクを実行するために自然言語翻訳APIを呼び出したとすると、ゴリラ・モデルはそれが実際には無関係であることを学習する。"
  },
  {
    "start": 519214,
    "end": 526900,
    "text": "この情報は捨てて、事前トレーニングで得た過去の情報を頼りに、質問に答えてください。"
  },
  {
    "start": 528790,
    "end": 540806,
    "text": "最後にもっと質問を受け付けてもいいのですが、私たちがかなり重要だと考えているのは、特に、より長いコンテクストレンズを使ってモデルを補強し始めたときに、検索が正確だったか不正確だったかを把握する方法が必要だということです。"
  },
  {
    "start": 540838,
    "end": 543500,
    "text": "もし不正確だったとしたら、どう説明するのですか？"
  },
  {
    "start": 545470,
    "end": 548122,
    "text": "さて、話は第2部に移る。"
  },
  {
    "start": 548256,
    "end": 551450,
    "text": "この話を始めるにあたって、簡単な質問をさせてほしい。"
  },
  {
    "start": 551600,
    "end": 553790,
    "text": "GPT4はどのくらい幻覚を見るのか？"
  },
  {
    "start": 555330,
    "end": 563482,
    "text": "鼻水を垂らしている人も見かけますが、多くの場合、得られる反応は、多いか少ないか、あるいは他のモデルと比較するかというようなもので、基本的には目的です。"
  },
  {
    "start": 563546,
    "end": 568250,
    "text": "GPTの4人がどれだけ幻覚を見ているのか、正確に数字で表すことはできないのでは？"
  },
  {
    "start": 568330,
    "end": 570670,
    "text": "プロンプトの配布は必要ないのですか？"
  },
  {
    "start": 571410,
    "end": 571918,
    "text": "その通りだ。"
  },
  {
    "start": 572004,
    "end": 575398,
    "text": "もし私がプロンプトの配給をするとしたら、あなたはどうする？"
  },
  {
    "start": 575484,
    "end": 588842,
    "text": "そうだね、いい質問だね。"
  },
  {
    "start": 588896,
    "end": 593020,
    "text": "これに答えれば、その答えの一端がわかるかもしれない。"
  },
  {
    "start": 593870,
    "end": 598954,
    "text": "技術的には、LLMが生み出すすべてのアウトプットは同じプロセスである。"
  },
  {
    "start": 599072,
    "end": 600454,
    "text": "いつも幻覚を見ているんだ。"
  },
  {
    "start": 600502,
    "end": 603950,
    "text": "ただ、アウトプットが事実上正しいこともあれば、そうでないこともある。"
  },
  {
    "start": 604020,
    "end": 604206,
    "text": "そうだろう？"
  },
  {
    "start": 604228,
    "end": 605230,
    "text": "どうやって測るんだ？"
  },
  {
    "start": 605300,
    "end": 606560,
    "text": "それがあなたの問題だ。"
  },
  {
    "start": 607250,
    "end": 617886,
    "text": "今日はその質問に答えようと思う。APIを使うことで得られる利点のひとつは、以前プログラミング言語で使われていた、抽象構文木マッチングというシンプルなトリックを引き出せることだ。"
  },
  {
    "start": 617998,
    "end": 619460,
    "text": "これはどういう仕組みなのか？"
  },
  {
    "start": 619830,
    "end": 622350,
    "text": "あなたが言ったように、LNMは出力を出そうとしている。"
  },
  {
    "start": 622430,
    "end": 626878,
    "text": "この場合、呼び出すAPIはtorch hublot pytorch vision densenetである。"
  },
  {
    "start": 626894,
    "end": 628370,
    "text": "1対1のプレトレーニングは、2対2に等しい。"
  },
  {
    "start": 628440,
    "end": 630398,
    "text": "これはAPIである。"
  },
  {
    "start": 630494,
    "end": 635814,
    "text": "ここで機械学習の例を挙げているのは、私たちが論文を書こうとしているからであり、コミュニティにとってはこれが効果的だからである。"
  },
  {
    "start": 635852,
    "end": 637394,
    "text": "これを安静と考えるかもしれない。"
  },
  {
    "start": 637442,
    "end": 638550,
    "text": "APIコール。"
  },
  {
    "start": 639050,
    "end": 639510,
    "text": "素晴らしい。"
  },
  {
    "start": 639580,
    "end": 649034,
    "text": "では、時間がないと言われたので、スピードをスライドさせますが、まず最初に、ここから小さなシンタックスツリーを作り、それから簡単な質問をします。"
  },
  {
    "start": 649152,
    "end": 652662,
    "text": "APIは極めて決定論的なもので、それがドメインから得られるメリットだ。"
  },
  {
    "start": 652726,
    "end": 657774,
    "text": "私はデータセットから、サブツリーではなく、ツリー全体を構築することができる。"
  },
  {
    "start": 657892,
    "end": 661406,
    "text": "素朴な疑問なのだが、サブツリーの一致は適切なのだろうか？"
  },
  {
    "start": 661588,
    "end": 666838,
    "text": "適切なサブツリーの一致があれば、これがあなたの指摘に正確かどうかは別として、少なくとも幻覚ではない。"
  },
  {
    "start": 666954,
    "end": 671134,
    "text": "ユーバーの場合、訳をつけるとすれば、存在するのは幻覚ではない。"
  },
  {
    "start": 671182,
    "end": 672414,
    "text": "それは間違った仕事かもしれない。"
  },
  {
    "start": 672462,
    "end": 676290,
    "text": "精度は悪いかもしれないが、少なくとも幻覚ではない。"
  },
  {
    "start": 676710,
    "end": 682102,
    "text": "色が正確かどうかはわからないが、これはマッチしている。"
  },
  {
    "start": 682156,
    "end": 686550,
    "text": "というのも、pythonのオプション引数だからです。"
  },
  {
    "start": 686890,
    "end": 691210,
    "text": "そして、このサブツリーのマッチングを行い、よし、これは幻覚ではない。"
  },
  {
    "start": 693070,
    "end": 697066,
    "text": "これはもちろん、私たちがうまくやれるということを示すための、選りすぐりの例である。"
  },
  {
    "start": 697168,
    "end": 698806,
    "text": "GBD 4は幻覚を見る傾向がある。"
  },
  {
    "start": 698838,
    "end": 702442,
    "text": "クラウドは間違っているかもしれないが、実際のデータセットではどうなのか？"
  },
  {
    "start": 702496,
    "end": 709690,
    "text": "X軸は幻覚、Y軸は正確さ。"
  },
  {
    "start": 709770,
    "end": 712302,
    "text": "幻覚が少ない方が良く、精度が高い方が良い。"
  },
  {
    "start": 712436,
    "end": 718462,
    "text": "ご覧の通り、ゴーラは多くのクローズドソースやオープンソースのモデルよりもはるかに良い結果を出すことができる。"
  },
  {
    "start": 718606,
    "end": 725346,
    "text": "だけでなく、たとえ最高のレトリーバーであっても、神がオラクル・レトリーバーである優秀なレトリーバーを与えたとしても、ここにたどり着くことはできない。"
  },
  {
    "start": 725368,
    "end": 726702,
    "text": "ちなみにこれは最先端の技術だ。"
  },
  {
    "start": 726776,
    "end": 734520,
    "text": "このシナリオでも、私たちのトレーニングのレシピを使えば、クローズドソースのモデルほどではないにせよ、より良い結果を出せることがおわかりいただけるでしょう。"
  },
  {
    "start": 735690,
    "end": 739618,
    "text": "なるほど、これは単なる学術的なプロジェクトではない。"
  },
  {
    "start": 739804,
    "end": 742358,
    "text": "私たちは、これをオープンソースの取り組みに閉じようとしている。"
  },
  {
    "start": 742454,
    "end": 748140,
    "text": "そのために、私たちは何度もハッカー・ニュースのトップに立った。"
  },
  {
    "start": 748990,
    "end": 754906,
    "text": "Discordのコミュニティでは多くの人が試しているし、GitHubでも良い交流がある。"
  },
  {
    "start": 754928,
    "end": 758206,
    "text": "皆さんもぜひ行ってみて、試してみて、もしうまくいかなかったら教えてください。"
  },
  {
    "start": 758228,
    "end": 760430,
    "text": "うまくいったら使ってください。"
  },
  {
    "start": 760580,
    "end": 766782,
    "text": "それだけでなく、国際的にも国内的にも、多くの一般紙や学術誌の取材を受けた。"
  },
  {
    "start": 766916,
    "end": 775346,
    "text": "この2ヵ月間で30万件以上のイノベーションがあった。"
  },
  {
    "start": 775448,
    "end": 778574,
    "text": "これらはすべて、口コミによる有機的な成長だ。"
  },
  {
    "start": 778702,
    "end": 781158,
    "text": "これが私たちの努力だ。"
  },
  {
    "start": 781244,
    "end": 782898,
    "text": "ゴリラLLMと呼ばれている。"
  },
  {
    "start": 782994,
    "end": 786194,
    "text": "コード、モデル、すべてがオープンソースで、Apache 2.0ライセンスです。"
  },
  {
    "start": 786242,
    "end": 788790,
    "text": "義務なしに商業的に使用することもできる。"
  },
  {
    "start": 789610,
    "end": 792042,
    "text": "これをもって、私は高いレベルで結論を出したいと思う。"
  },
  {
    "start": 792096,
    "end": 797418,
    "text": "要約すると、私たちが行っているのは、検索を意識したトレーニングと呼ばれるコンセプトを導入し、トレーニングを行うことです。"
  },
  {
    "start": 797584,
    "end": 800534,
    "text": "リトリーバルは正確ではないという事実を認識しておくこと。"
  },
  {
    "start": 800662,
    "end": 807242,
    "text": "第二に、我々の課題を考えると、モデルがどれだけ幻覚を見るかについて正確な数字を出すことができるという利点もある。"
  },
  {
    "start": 807306,
    "end": 809358,
    "text": "幻覚というのは、作り話という意味だ。"
  },
  {
    "start": 809444,
    "end": 810320,
    "text": "ありがとう。"
  },
  {
    "start": 814390,
    "end": 815458,
    "text": "ありがとう。"
  },
  {
    "start": 815544,
    "end": 818740,
    "text": "次の発言者が発言する間、1つだけ質問を受け付けます。"
  },
  {
    "start": 822470,
    "end": 825186,
    "text": "そのためのトレーニングデータはどのように作成しているのですか？"
  },
  {
    "start": 825288,
    "end": 826420,
    "text": "ああ、いい質問だね。"
  },
  {
    "start": 829130,
    "end": 833570,
    "text": "その多くは、基本的にAPIをスクレイピングすることに依存する。"
  },
  {
    "start": 833650,
    "end": 838738,
    "text": "人々が遭遇した多くのライセンス問題とは異なり、APIは配布されることを意図している。"
  },
  {
    "start": 838834,
    "end": 842726,
    "text": "もし私がストライプで、支払いAPIを持っているなら、それを隠すインセンティブはない。"
  },
  {
    "start": 842748,
    "end": 845818,
    "text": "例えば、APIを提供したいのは、あなたがストライプを使うと、私に報酬が入るからです。"
  },
  {
    "start": 845904,
    "end": 847654,
    "text": "APIは分散するものだ。"
  },
  {
    "start": 847702,
    "end": 853542,
    "text": "そのため、チュートリアルやサンプル、ワーグルファイルなどが用意されている。"
  },
  {
    "start": 853606,
    "end": 858560,
    "text": "そして、それをすべてかき集め、自己指導のもと、トレーニングデータセットを作成する。"
  },
  {
    "start": 860850,
    "end": 861886,
    "text": "ありがとうございました。"
  },
  {
    "start": 861908,
    "end": 869840,
    "text": "質問があればいつでも聞くから、電源を切ってくれ。"
  },
  {
    "start": 874210,
    "end": 893610,
    "text": "次回のスピーカーはネー・ヨン・リーさんで、小型変圧器への算数の教え方についてお話しいただきます。"
  },
  {
    "start": 895470,
    "end": 895882,
    "text": "こんにちは。"
  },
  {
    "start": 895936,
    "end": 902874,
    "text": "小型変圧器の模型に算数を教えるという、私たちの最近の研究を紹介できることをとても嬉しく思う。"
  },
  {
    "start": 903002,
    "end": 914130,
    "text": "この研究は、ウィスコンシン大学マディソン校とプリンストン大学の素晴らしい共同研究者、カルティク・カンガル、ジェイソン、ディミトリスとの共同研究である。"
  },
  {
    "start": 916310,
    "end": 925590,
    "text": "そもそも私たちは、膨大なデータを使って大規模な言語モデルを訓練するときに出てくる、この創発的な能力に特に関心がある。"
  },
  {
    "start": 925740,
    "end": 938890,
    "text": "というのも、これらのタスクは、次の単語予測の目的でモデルを訓練する訓練目的には特にエンコードされていなかったからだ。"
  },
  {
    "start": 939550,
    "end": 950762,
    "text": "ヘミングウェイ風のテキスト生成や、さまざまなタスクなど、多くの成功例を見てきた。"
  },
  {
    "start": 950826,
    "end": 957610,
    "text": "このモデルは特に算数的な課題を得意としているわけではないことがわかる。"
  },
  {
    "start": 957690,
    "end": 970862,
    "text": "例えば、月曜日にイェジンが例を挙げてくれたように、掛け算は苦手だが、大きな数の足し算も苦手だ。"
  },
  {
    "start": 970926,
    "end": 990570,
    "text": "私たちは、デコードされたモデルが実際にどのように足し算を学習するのか、また、テキストデータ上で左から右へとモデルを訓練することから次の単語を予測することが、足し算を訓練する最も効果的な方法なのかどうかを理解することに興味があります。"
  },
  {
    "start": 992030,
    "end": 1003680,
    "text": "先行研究では、モデル・サイズとデータ・サイズというスケール尺度に関する創発特性に取り組んできた。"
  },
  {
    "start": 1004210,
    "end": 1013006,
    "text": "データが多すぎるし、規模が大きすぎるからだ。"
  },
  {
    "start": 1013108,
    "end": 1024690,
    "text": "私たちの解決策は、分析できる最も単純な設定を選ぶことです。"
  },
  {
    "start": 1025370,
    "end": 1032902,
    "text": "私たちは、小さなnanogtモデルに足し算を教えることにしました。"
  },
  {
    "start": 1033036,
    "end": 1043740,
    "text": "私たちの発見は、トレーニングデータセットのデータをどのようにサンプリングするか、そしてデータをどのようにフォーマットするかは、モデルが学習する上で非常に重要であるということです。"
  },
  {
    "start": 1044590,
    "end": 1050054,
    "text": "私たちの実験設定を簡単に紹介しよう。"
  },
  {
    "start": 1050182,
    "end": 1059950,
    "text": "私たちはnanoGptモデルを使用しているが、これは私たちが使用しているGPTモデルの非常に小さなバージョンで、1000万個のパラメーターしか持っていない。"
  },
  {
    "start": 1060290,
    "end": 1075934,
    "text": "他のモデルとの違いは、他の多くの研究が事前に訓練されたGPTモデルを使用しているのに対し、我々は特に文字レベルのトークナイザーを使用し、ゼロからモデルを訓練していることです。"
  },
  {
    "start": 1076072,
    "end": 1079922,
    "text": "課題は主に3桁の足し算である。"
  },
  {
    "start": 1079986,
    "end": 1085974,
    "text": "を他の算数タスクに拡張し、注目する長さを拡張する。"
  },
  {
    "start": 1086092,
    "end": 1093446,
    "text": "目標は、サンプリング、フォーマット、プロンプトの重要性を評価することである。"
  },
  {
    "start": 1093478,
    "end": 1097770,
    "text": "この講演では、主にフォーマットの側面に焦点を当てたいと思います。"
  },
  {
    "start": 1099470,
    "end": 1101822,
    "text": "トレーニングはどのように行われるのか？"
  },
  {
    "start": 1101876,
    "end": 1115170,
    "text": "モデルに足し算を教えるには、足し算の例をたくさん与え、高い確率で正しい答えを出力するようにモデルを訓練する。"
  },
  {
    "start": 1115990,
    "end": 1123230,
    "text": "算数でこんなことをするのは、ちょっと変な感じがする。"
  },
  {
    "start": 1123390,
    "end": 1128066,
    "text": "n桁の足し算を考えてみよう。"
  },
  {
    "start": 1128178,
    "end": 1151040,
    "text": "左側には従来の足し算の書き方があり、右側には単純に出力を反転させる方法がある。従来のデータでは最上位桁から最下位桁へと出力し、右側では最下位桁が最初に来るように反転させるのだ。"
  },
  {
    "start": 1151650,
    "end": 1160862,
    "text": "従来のスタイルでモデルを訓練すると、モデルは最上位の桁を最初に出力するように訓練される。"
  },
  {
    "start": 1160996,
    "end": 1179430,
    "text": "この場合、キャリーオンは最下位桁から最上位桁へと伝搬するため、両オペランドに現れる2つのn個の数すべてを使用しなければならないマッピングを見つけるモデルが必要となる。"
  },
  {
    "start": 1179930,
    "end": 1193562,
    "text": "しかし、単純に出力を逆にすれば、訓練データは足し算の逆バージョンで構成されるようになり、逆の方法で出力するようにモデルを訓練することになる。"
  },
  {
    "start": 1193616,
    "end": 1210510,
    "text": "そうすれば、このモデルは、対応する桁の位置の2桁だけをマッピングするマッピングを見つけるだけで、正しく足し算を実行できるようになる。"
  },
  {
    "start": 1211330,
    "end": 1220302,
    "text": "言い換えれば、モデルは今、逆バージョンの出力で最も単純な関数を学習する必要がある。"
  },
  {
    "start": 1220366,
    "end": 1235350,
    "text": "そうすれば、逆に出力するようにモデルを訓練すれば、より少ない訓練例数でより高い精度が得られることを示している。"
  },
  {
    "start": 1236570,
    "end": 1240250,
    "text": "私たち人間の足し算のやり方を考えてみよう。"
  },
  {
    "start": 1240830,
    "end": 1254734,
    "text": "右から左に向かって足し算をし、中間ステップでキャリーオンを書き出し、ステップ・バイ・ステップでそれを行う。"
  },
  {
    "start": 1254932,
    "end": 1263422,
    "text": "これに基づき、私たちは思考の連鎖スタイルのデータフォーマットを考えることができる。"
  },
  {
    "start": 1263556,
    "end": 1287722,
    "text": "思考の連鎖のデータ形式は、実は他の論文から引用したものだが、このような方法でゼロからモデルを訓練して出力したところ、逆スクラッチパッド、簡略化スクラッチパッド、詳細スクラッチパッド、これらの修正された形式はすべて、ある時点で100％の精度に達することが確認された。"
  },
  {
    "start": 1287776,
    "end": 1299660,
    "text": "より多くの情報を与えることで、モデルはより少ない学習例で優れた性能を発揮できるようになった。"
  },
  {
    "start": 1300850,
    "end": 1312320,
    "text": "というのも、思考訓練スタイルのトレーニングデータは、ステップ・バイ・ステップで行うことで、より簡単ではるかに小さなタスクを学習するようモデルに教えるからだ。"
  },
  {
    "start": 1315750,
    "end": 1326882,
    "text": "逆順に合計を出力するように訓練したら、どうなるかについて話してきた。"
  },
  {
    "start": 1327016,
    "end": 1334814,
    "text": "左右の合計を実際に出力させるために、さらに少しトレーニングすることはできますか？"
  },
  {
    "start": 1334952,
    "end": 1337854,
    "text": "うん、とても興味深い視点だね。"
  },
  {
    "start": 1337922,
    "end": 1346438,
    "text": "それが望ましいことだからだ。"
  },
  {
    "start": 1346534,
    "end": 1350780,
    "text": "左から右へ、よりうまくやってもらいたい。"
  },
  {
    "start": 1351230,
    "end": 1357774,
    "text": "ご質問は、まず右から左へのトレーニングを行い、さらに左から右へのトレーニングをもう少し行うかどうかということだと思います。"
  },
  {
    "start": 1357812,
    "end": 1370260,
    "text": "それが役に立つかどうか、まだ厳密な答えを持っているわけではないが、我々はこれを持つように微調整を設定している。"
  },
  {
    "start": 1370790,
    "end": 1378022,
    "text": "この3桁の足し算を右から左に訓練し、さらに4桁の足し算を左から右に訓練する。"
  },
  {
    "start": 1378076,
    "end": 1380918,
    "text": "あまり役に立たなかったようだ。"
  },
  {
    "start": 1381004,
    "end": 1385240,
    "text": "非常に興味深い方向性だと思う。"
  },
  {
    "start": 1387450,
    "end": 1402618,
    "text": "データフォーマットについて話してきたが、さらに興味深いのは、モデルが少数の例で足し算を全く知らなかった状態から、突然足し算のやり方を完全に理解できるようになることだ。"
  },
  {
    "start": 1402714,
    "end": 1414786,
    "text": "なぜこのようなことが起こるのか、ひとつの疑問が浮かび上がったが、マトリックス補完との関係から、その理由がある程度説明できる。"
  },
  {
    "start": 1414888,
    "end": 1429800,
    "text": "各要素が行インデックスと列インデックスの和である左の足し算表を考えると、この足し算表を埋めるのは、次のようになる。"
  },
  {
    "start": 1431290,
    "end": 1438950,
    "text": "この足し算チャート自体がランク2の行列であり、このチャートを埋めることが低ランクの行列補完を行うことになる。"
  },
  {
    "start": 1444590,
    "end": 1460670,
    "text": "マトリックス補完理論によれば、アウトプットを明らかにすれば、マトリックスを完璧に補完できるようになる。"
  },
  {
    "start": 1461650,
    "end": 1476370,
    "text": "これは、足し算を知らなかったモデルが、十分なデータサンプルがあれば突然足し算ができるようになる理由を部分的に説明するものである。"
  },
  {
    "start": 1476450,
    "end": 1481254,
    "text": "もう時間がないんだ。"
  },
  {
    "start": 1481292,
    "end": 1492774,
    "text": "行列の補完について簡単に説明すると、行列の行や列が空白の場合、行列を補完することができません。"
  },
  {
    "start": 1492822,
    "end": 1511034,
    "text": "私たちのGPTモデルは、1003桁の数字のうち500桁の数字を学習例のどこからでも除外したとしても、隠れていた数字を足し合わせることがほぼ完全に可能です。"
  },
  {
    "start": 1511082,
    "end": 1525954,
    "text": "また、5という数字がデータセットの最下位桁のどこかに隠されていたとしても、他の桁の位置にある5から推測することができるため、足し算をすることができる。"
  },
  {
    "start": 1526082,
    "end": 1530854,
    "text": "これが、マトリックス補完と我々のナノグプト・モデルの違いのひとつだ。"
  },
  {
    "start": 1530972,
    "end": 1538018,
    "text": "また、このモデルが本当に足し算を理解しているのかという疑問も強調したい。"
  },
  {
    "start": 1538114,
    "end": 1552106,
    "text": "残念ながら、1桁と3桁だけでモデルを訓練しても、2桁や4桁の足し算ができないのです。"
  },
  {
    "start": 1552298,
    "end": 1562960,
    "text": "この詳細なスクラッチパッドのデータで訓練しても、ランダムな桁から脱落し始め、間違ってしまうのだ。"
  },
  {
    "start": 1565490,
    "end": 1575958,
    "text": "答えはノーだ。正しい数字に対応するc桁の数字を擬似的にアルゴリズムとして学習しているだけなのだ。"
  },
  {
    "start": 1576124,
    "end": 1578680,
    "text": "正解だ。"
  },
  {
    "start": 1579290,
    "end": 1589450,
    "text": "他の算数タスクで何が起こるか、あるいはデータフュージョンにテキストがある場合に何が起こるか、プロンプティングなどについては、私たちの論文にもっと多くのことが書かれている。"
  },
  {
    "start": 1589520,
    "end": 1599962,
    "text": "もしご興味があれば、私たちの論文をご覧ください。重要なポイントは、データのフォーマットとサンプリングの方法が非常に重要であるということです。"
  },
  {
    "start": 1600096,
    "end": 1611626,
    "text": "また、なぜこのモデルが突然足し算を学習できるようになったのかを理解するために、低ランクの行列補完と関連づけることもできる。"
  },
  {
    "start": 1611738,
    "end": 1615090,
    "text": "また、レンズの一般化はまだ難しい。"
  },
  {
    "start": 1616390,
    "end": 1617300,
    "text": "ありがとう。"
  },
  {
    "start": 1621190,
    "end": 1622082,
    "text": "ありがとう。"
  },
  {
    "start": 1622216,
    "end": 1624180,
    "text": "いくつか質問の時間があります。"
  },
  {
    "start": 1627590,
    "end": 1633910,
    "text": "出力を反転させたのだから、入力も反転させる実験をしただろうと期待していたんだ。"
  },
  {
    "start": 1634060,
    "end": 1635030,
    "text": "試してみた？"
  },
  {
    "start": 1635100,
    "end": 1635958,
    "text": "そう、その通りだ。"
  },
  {
    "start": 1636044,
    "end": 1639478,
    "text": "結果はほとんど同じだった。"
  },
  {
    "start": 1639644,
    "end": 1657360,
    "text": "というのも、結果を処理するためには、人間は結果を元に戻さなければならないからだ。"
  },
  {
    "start": 1661250,
    "end": 1662560,
    "text": "他に質問は？"
  },
  {
    "start": 1667490,
    "end": 1670320,
    "text": "そうでなければ、もう一度スピーカーに感謝しよう。"
  },
  {
    "start": 1674610,
    "end": 1706542,
    "text": "素晴らしいよ。"
  },
  {
    "start": 1706596,
    "end": 1711530,
    "text": "次のスピーカーはエリック・ウォレスで、暗記と大規模な言語モデルについて話してくれる。"
  },
  {
    "start": 1711610,
    "end": 1711950,
    "text": "素晴らしい。"
  },
  {
    "start": 1712020,
    "end": 1712414,
    "text": "ありがとう。"
  },
  {
    "start": 1712452,
    "end": 1715374,
    "text": "みんな、最終日の最後のトークに付き合ってくれてありがとう。"
  },
  {
    "start": 1715412,
    "end": 1716730,
    "text": "正義を貫けることを願っている。"
  },
  {
    "start": 1716810,
    "end": 1722766,
    "text": "この講演で私がしようと思っているのは、大規模な言語モデルは事前学習データからテキストの断片を記憶するということを納得してもらうことだ。"
  },
  {
    "start": 1722868,
    "end": 1725538,
    "text": "そのために、この話を3つの使い方に分けてお話しします。"
  },
  {
    "start": 1725634,
    "end": 1728934,
    "text": "まず、暗記とは何を意味するのか、そしてそれをどのように測定できるのかを定義する。"
  },
  {
    "start": 1729052,
    "end": 1733778,
    "text": "次に、言語モデルから実際に記憶されたデータを抽出する方法のアルゴリズムを紹介する。"
  },
  {
    "start": 1733874,
    "end": 1737960,
    "text": "そして3つ目は、それがどの程度うまくいっているのか、そして今後どのような方向に進んでいくのかを分析することだ。"
  },
  {
    "start": 1738890,
    "end": 1742194,
    "text": "まず、私が世代別暗記と呼んでいるこの言葉を定義しようと思う。"
  },
  {
    "start": 1742322,
    "end": 1743862,
    "text": "ここでの考え方はとてもシンプルだ。"
  },
  {
    "start": 1743996,
    "end": 1748838,
    "text": "事前に学習させた言語モデルを、例えばGPD 3とか、そんな感じで用意するんだ。"
  },
  {
    "start": 1748924,
    "end": 1751902,
    "text": "この講演では、完全にブラックボックスだと仮定する。"
  },
  {
    "start": 1752036,
    "end": 1754750,
    "text": "何がトレーニングされているのか、どんな詳細があるのかわからない。"
  },
  {
    "start": 1754820,
    "end": 1758510,
    "text": "私にできるのは、プロンプトをいくつか入れて、モデルからサンプルを返してもらうことだけだ。"
  },
  {
    "start": 1758660,
    "end": 1764730,
    "text": "例えば、この場合はハリー・ポッターの冒頭のようなものだ。"
  },
  {
    "start": 1764810,
    "end": 1768434,
    "text": "モデルから生成して、何が出てくるか見てみるよ。"
  },
  {
    "start": 1768552,
    "end": 1776274,
    "text": "この場合、これをGPD 3に入れると、ハリー・ポッターの最初のページがすべてそのまま再生され、長時間続けることができる。"
  },
  {
    "start": 1776392,
    "end": 1788070,
    "text": "つまり、このようなケースで考えられるのは、基本的に、モデルはこの特定の接尾辞について可能性が非常に高いので、モデルからの貪欲な解読や生成のような単純なもので、このサンプルを再生成することになる、ということだ。"
  },
  {
    "start": 1789050,
    "end": 1790578,
    "text": "これはなぜ良いのか、悪いのか？"
  },
  {
    "start": 1790684,
    "end": 1797830,
    "text": "私は基本的に、暗記はコンテクストによって、本当に良いものにも、本当に悪いものにもなり得るということを主張したい。"
  },
  {
    "start": 1797990,
    "end": 1808286,
    "text": "例えば、質問応答や読解のような場合、言語モデルが事前学習データからすべての情報を吸収してくれるという事実が、みんな大好きなんだ。"
  },
  {
    "start": 1808388,
    "end": 1812986,
    "text": "修正第14条とは何なのか？"
  },
  {
    "start": 1813018,
    "end": 1814106,
    "text": "差分プライバシーとは何か？"
  },
  {
    "start": 1814138,
    "end": 1816010,
    "text": "その利点は何ですか？"
  },
  {
    "start": 1816100,
    "end": 1821858,
    "text": "このモデルは、トレーニングセットから正確に情報を呼び出すことで、本当に素晴らしいことができる。"
  },
  {
    "start": 1821944,
    "end": 1823890,
    "text": "というわけで、これは大まかにはプラス材料として捉えられている。"
  },
  {
    "start": 1824550,
    "end": 1831990,
    "text": "同時に、ちょっとプライバシーに配慮した帽子をかぶれば、エリックの社会保障番号をモデルから生成するようなプロンプトを入れることもできる。"
  },
  {
    "start": 1832060,
    "end": 1838360,
    "text": "データセットの中に個人情報があれば、モデルから逐語的な情報を完璧に再生成してくれるかもしれない。"
  },
  {
    "start": 1839370,
    "end": 1845578,
    "text": "もちろん、あなたのデータが個人的なものであったり、機密性の高いものであったりする場合は、例えば、健康記録、電子メール、チャットログなど、このようなものをトレーニングしているとします。"
  },
  {
    "start": 1845664,
    "end": 1848810,
    "text": "このような世代は、破滅的なプライバシーの漏洩となるだろう。"
  },
  {
    "start": 1849630,
    "end": 1857710,
    "text": "さらに、ワークショップを通して見てきたように、たとえデータが非公開であっても、モデルがそれを記憶することに多くの問題があるケースもたくさんある。"
  },
  {
    "start": 1857780,
    "end": 1862778,
    "text": "水曜日のパムは、著作権データの問題について話した。"
  },
  {
    "start": 1862874,
    "end": 1868814,
    "text": "また、例えばウェブ上に存在する、モデルには記憶させたくないような厄介なデータのケースも見てきた。"
  },
  {
    "start": 1868942,
    "end": 1873220,
    "text": "たとえば、医学研究のリークが誤ってオンラインに掲載されたケースである。"
  },
  {
    "start": 1873830,
    "end": 1883842,
    "text": "また、ワークショップを通して共通して言えることは、言語モデルは本当にウェブから何かを記憶し、それを使ってタスクを解決しているのか、それとも新しい推論をしているのか、ということだ。"
  },
  {
    "start": 1883906,
    "end": 1887602,
    "text": "だから、モデルに記憶させたくないのは、アカデミックなテストデータセットだ。"
  },
  {
    "start": 1887666,
    "end": 1893510,
    "text": "モデルを評価するために使用するデータセットの多くは、偶然オンラインに置かれ、トレーニングセットによってかき集められる。"
  },
  {
    "start": 1893590,
    "end": 1901930,
    "text": "だから、パブリックドメインのデータやインターネットで公開されているものであっても、モデルに暗記させたくないんだ。"
  },
  {
    "start": 1902430,
    "end": 1906240,
    "text": "ここでの収穫は、基本的に暗記は良くも悪くもなるということだ。"
  },
  {
    "start": 1907810,
    "end": 1912350,
    "text": "次にお話しするのは、基本的に、記憶されたデータを実際にどのように抽出するかの具体的なアルゴリズムである。"
  },
  {
    "start": 1912420,
    "end": 1921940,
    "text": "ここでは、ある言語モデル（GPT-3とする）が与えられていて、そのモデルを操作して、学習データから例と思われるリストを抽出できるようにしたい。"
  },
  {
    "start": 1922950,
    "end": 1925250,
    "text": "一般的に、この方法は非常に簡単だ。"
  },
  {
    "start": 1925400,
    "end": 1929650,
    "text": "まず、モデルを作成し、そこから大量のサンプルを生成します。"
  },
  {
    "start": 1929720,
    "end": 1933990,
    "text": "例えば、GPD3を使って、モデルから1億個のサンプルを生成する。"
  },
  {
    "start": 1934060,
    "end": 1936278,
    "text": "だから今、この大量のテキストがある。"
  },
  {
    "start": 1936444,
    "end": 1944218,
    "text": "そうなると、主な技術的な問題は、どうやってその大きなコーパスをふるいにかけ、実際にトレーニングデータからだと思われるケースを特定できるか、ということになる。"
  },
  {
    "start": 1944304,
    "end": 1947658,
    "text": "つまり、基本的にはフラグを立てたり、トレーニングデータと思われるサンプルを見つけたりする。"
  },
  {
    "start": 1947744,
    "end": 1950986,
    "text": "というわけで、プライバシーに詳しい人なら、これはメンバーシップの推論問題である。"
  },
  {
    "start": 1951088,
    "end": 1952362,
    "text": "これはテキストの一部である。"
  },
  {
    "start": 1952416,
    "end": 1954310,
    "text": "この場合は、サンプルの大きなプールだ。"
  },
  {
    "start": 1954390,
    "end": 1956558,
    "text": "それはトレーニングセットのものですか、そうではありませんか？"
  },
  {
    "start": 1956724,
    "end": 1964350,
    "text": "そこで次に紹介するのは、基本的に言語モデルについて具体的に、最新のメンバーシップ推論手法とはどのようなものなのか、そしてそれを実際にどのように実行するのか、ということだ。"
  },
  {
    "start": 1965330,
    "end": 1967150,
    "text": "この考え方は実にシンプルだ。"
  },
  {
    "start": 1967300,
    "end": 1970610,
    "text": "基本的に想像できるのは、モデルにサンプルのセットを入れるということだ。"
  },
  {
    "start": 1970680,
    "end": 1977010,
    "text": "例えば、エリックの社会保障番号があり、ジョン・ドウがガンであるとしよう。"
  },
  {
    "start": 1977160,
    "end": 1983010,
    "text": "会員制推論の最も単純な基本線は、「このサンプルは本当に尤度が高い」と言うことだ。"
  },
  {
    "start": 1983090,
    "end": 1989174,
    "text": "言語モデルはトレーニングセットをオーバーフィット（ある程度）させるので、尤度が高いのはトレーニングセットのものだからだ。"
  },
  {
    "start": 1989212,
    "end": 1996634,
    "text": "従って、最も単純なメンバーシップ推論攻撃は、モデルの尤度やperplexityなどに基づいて閾値を設定するだけである。"
  },
  {
    "start": 1996832,
    "end": 2003302,
    "text": "こうすることの問題点は、尤度の高いものがトレーニングセットのものである可能性もあるが、単なる簡単なサンプルである可能性もあるということだ。"
  },
  {
    "start": 2003366,
    "end": 2007338,
    "text": "一般的な、言語化しやすいテキストのようなもので、ごく基本的な英語だ。"
  },
  {
    "start": 2007434,
    "end": 2009966,
    "text": "言語モデルの下では、その可能性も高くなるだろう。"
  },
  {
    "start": 2010068,
    "end": 2013870,
    "text": "つまり、サンプルの難易度を調整するのだ。"
  },
  {
    "start": 2014850,
    "end": 2018322,
    "text": "それを可能にする方法は、非常に単純だが、第二言語モデルを導入することだ。"
  },
  {
    "start": 2018376,
    "end": 2021154,
    "text": "これは言語モデルなので、ホワイトボックスで表します。"
  },
  {
    "start": 2021192,
    "end": 2022206,
    "text": "私たちは自分自身を鍛える。"
  },
  {
    "start": 2022318,
    "end": 2025938,
    "text": "詳細がわかっていることについては、どんな面にもテコ入れはしない。"
  },
  {
    "start": 2026024,
    "end": 2029862,
    "text": "実際には、ハグフェイスからモデルをダウンロードして、利用可能なものを使うだけだ。"
  },
  {
    "start": 2029996,
    "end": 2033682,
    "text": "同じサンプルを使って、尤度を比較する。"
  },
  {
    "start": 2033746,
    "end": 2038406,
    "text": "例えば、この最初のサンプルは、どちらのモデルも本当に尤度の高いテキストだと考えた。"
  },
  {
    "start": 2038508,
    "end": 2043430,
    "text": "だからこの場合、トレーニングデータではなく、単なる簡単なサンプルだと思うだろう。"
  },
  {
    "start": 2043590,
    "end": 2047658,
    "text": "この場合、ターゲットモデルはこの参照モデルよりもはるかに高い尤度を割り当てた。"
  },
  {
    "start": 2047744,
    "end": 2050714,
    "text": "ということは、このサンプルはおそらくトレーニングデータからのものだろう。"
  },
  {
    "start": 2050832,
    "end": 2064730,
    "text": "基本的に私たちが行うのは、膨大なサンプルプール（仮に1億サンプルとする）を、ターゲットモデルと参照モデルの尤度の比率でフィルタリングまたはソートし、その指標の下で高得点のサンプルを検査することです。"
  },
  {
    "start": 2064810,
    "end": 2069330,
    "text": "プライバシーの漏洩や著作権侵害のような事例があればいいのだが。"
  },
  {
    "start": 2069990,
    "end": 2075890,
    "text": "実際には、次のスライドでお見せするように、基本的に言語モデルはあまりオーバーフィットしないからです。"
  },
  {
    "start": 2075960,
    "end": 2080318,
    "text": "火曜のサーシャの話の中で、私たちは1つのエポックのようなトレーニングしかしていないという事実について話していたと思う。"
  },
  {
    "start": 2080424,
    "end": 2083974,
    "text": "その結果、テストとのギャップが小さくなった。"
  },
  {
    "start": 2084092,
    "end": 2091162,
    "text": "そのため、実際には、このようなメンバーシップ推論メトリクスを実行すると、多くの記憶されたコンテンツを特定するのは本当に難しいということが起こります。"
  },
  {
    "start": 2091296,
    "end": 2096774,
    "text": "その代わりに、精度は高いが再現率が極端に低い分類器を見つけることになる。"
  },
  {
    "start": 2096902,
    "end": 2100790,
    "text": "私たちが確実に識別できるのは、ごく少数のサンプルだけです。"
  },
  {
    "start": 2100870,
    "end": 2104240,
    "text": "そのセットのサンプルは、トレーニングセットのものであるという高い信頼性がある。"
  },
  {
    "start": 2106290,
    "end": 2108590,
    "text": "具体的には、以下のような例がある。"
  },
  {
    "start": 2108660,
    "end": 2111562,
    "text": "我々はこれをいくつかの異なるモデルに適用した。"
  },
  {
    "start": 2111626,
    "end": 2120526,
    "text": "これはGPD3の例で、最初のスライドで示したように、たとえば『ハリー・ポッター』の全章を再生させることができる。"
  },
  {
    "start": 2120718,
    "end": 2133590,
    "text": "codexやGitHub copilotのようなオープンソース、あるいはオープンソースではないがプロプライエタリなコード・アシスタント・モデルの場合、非許諾ライセンスの関数やコードファイル全体を生成させることができる。"
  },
  {
    "start": 2134170,
    "end": 2140642,
    "text": "興味深いことに、これとまったく同じ手法を、細部を少し変えるだけで、たとえば画像生成にも応用できる。"
  },
  {
    "start": 2140786,
    "end": 2146986,
    "text": "この場合、安定した拡散を実行しており、上部にモデルのトレーニングセットからの一連の画像を表示しています。"
  },
  {
    "start": 2147088,
    "end": 2153510,
    "text": "一番下には、テスト時にモデルによって生成され、メンバーシップ推論の指標で採点された一連の画像を示している。"
  },
  {
    "start": 2153590,
    "end": 2159200,
    "text": "画像の場合、モデルから逐語的に生成することができる。"
  },
  {
    "start": 2160370,
    "end": 2171300,
    "text": "最後に、この種のモデルでは、プライバシー侵害のようなものも発生します。例えば、特定の個人のトレーニング画像に非常によく似た画像を再生成するようにモデルを促すことができます。"
  },
  {
    "start": 2173110,
    "end": 2177694,
    "text": "これらは、言語モデルと画像モデルの両方に関する定性的な結果である。"
  },
  {
    "start": 2177822,
    "end": 2187800,
    "text": "最後に、なぜこのようなことが起きているのか、そしてモデルが拡大・発展していくにつれてどのような方向に向かっていくのかについて、5分ほどお話ししたいと思います。"
  },
  {
    "start": 2188330,
    "end": 2192042,
    "text": "私が伝えたい一つの大きなポイントは、モデルをスケールアップし始めるということだ。"
  },
  {
    "start": 2192096,
    "end": 2199770,
    "text": "例えば、小さな言語モデルの時代から大きな言語モデルの時代になると、暗記する量が膨大に増える。"
  },
  {
    "start": 2200190,
    "end": 2207774,
    "text": "具体的には、例えばこのプロットでは、X軸に言語モデルのモデル・パラメーターの量をスケーリングしています。"
  },
  {
    "start": 2207892,
    "end": 2210698,
    "text": "Y軸に暗記度をプロットしている。"
  },
  {
    "start": 2210794,
    "end": 2224420,
    "text": "何をプロットしているのかの詳細は省くが、ここで重要なのは、単純にモデルの規模を拡大することで精度が向上し、全体的な結果が改善されると同時に、例えばプライバシーの漏洩も同程度増加するということだ。"
  },
  {
    "start": 2225350,
    "end": 2237080,
    "text": "ここから得られることは、基本的に、新しいハードウェアやデータなど、パフォーマンスやスケーリングを続けるにつれて、このような暗記の問題は時間とともに悪化していくということです。"
  },
  {
    "start": 2238170,
    "end": 2246890,
    "text": "質問に対する回答という観点からこのことを考えると、逆の話になる。つまり、ただ座って待って、スケーリングを見ていれば、人々は自分たちの仕事をするようになり、質問に対する回答はもっと良くなるだろう、ということだ。"
  },
  {
    "start": 2246960,
    "end": 2251260,
    "text": "プライバシーの観点から考えれば、事態は時間とともに悪化している。"
  },
  {
    "start": 2253070,
    "end": 2255566,
    "text": "データについて考えてみても、同じような話が当てはまる。"
  },
  {
    "start": 2255668,
    "end": 2262554,
    "text": "例えば、私たちがたくさん見てきたことのひとつに、大きな言語モデルの事前学習データセットはかなり異質だということがあります。"
  },
  {
    "start": 2262602,
    "end": 2264474,
    "text": "いろいろなソースが混ざっているんだ。"
  },
  {
    "start": 2264602,
    "end": 2269554,
    "text": "よく出くわすのは、何度も重複しているサンプルがあるということだ。"
  },
  {
    "start": 2269672,
    "end": 2272910,
    "text": "例えば、何度も言及されている名言があるかもしれない。"
  },
  {
    "start": 2272990,
    "end": 2275906,
    "text": "もしかしたら、同じようなトピックに言及したニュース記事がたくさんあるかもしれない。"
  },
  {
    "start": 2276008,
    "end": 2282870,
    "text": "私たちが発見したことのひとつは、データ内にわずかな重複があるだけで、大量の暗記量が増えるということだ。"
  },
  {
    "start": 2283210,
    "end": 2286082,
    "text": "例えば、ここにあるのは基本的にX軸のものだ。"
  },
  {
    "start": 2286146,
    "end": 2288418,
    "text": "特定のngramについてプロットしています。"
  },
  {
    "start": 2288514,
    "end": 2290550,
    "text": "データには何回出てきますか？"
  },
  {
    "start": 2290700,
    "end": 2293606,
    "text": "例えば、ここにあるのはNgramがシングルトンである場合だ。"
  },
  {
    "start": 2293638,
    "end": 2294746,
    "text": "データ上では唯一無二だ。"
  },
  {
    "start": 2294848,
    "end": 2298220,
    "text": "こちらでは、Ngramが100回オーダーで表示されるケースを紹介する。"
  },
  {
    "start": 2298590,
    "end": 2304150,
    "text": "興味深いことに、このY軸には世代ベースの暗記度をプロットしている。"
  },
  {
    "start": 2304310,
    "end": 2312702,
    "text": "重複の数を例えば100倍に増やすだけで、世代に基づく暗記量は、例えば10,000倍、10,0000倍に増やすことができる。"
  },
  {
    "start": 2312836,
    "end": 2323490,
    "text": "つまり、あるテキストが10回、100回と複製されるだけで、プライバシー漏えいが膨大な量になるような、超直線的な効果があるのだ。"
  },
  {
    "start": 2324150,
    "end": 2333106,
    "text": "ここから得られる良いニュースは、データを重複排除し、収集の仕方に細心の注意を払えば、実際にプライバシーが漏れる量を大幅に減らすことができるということだ。"
  },
  {
    "start": 2333298,
    "end": 2340150,
    "text": "悪いニュースとしては、例えば、質問に対する回答を気にするのであれば、モデルはこのような珍しい質問に対しては本当に不利になるということだ。"
  },
  {
    "start": 2340220,
    "end": 2343398,
    "text": "難解な知識について尋ねると、モデルはひどいことになる。"
  },
  {
    "start": 2343494,
    "end": 2347686,
    "text": "一般的なことを聞けば、その手の例には不釣り合いなほど詳しい。"
  },
  {
    "start": 2347798,
    "end": 2351606,
    "text": "そこで本稿では、プライバシーの観点から、このアイデアについて述べる。"
  },
  {
    "start": 2351638,
    "end": 2359840,
    "text": "この論文では、質問回答の観点から、難解なロングテールの知識に関する質問を評価する場合について述べている。"
  },
  {
    "start": 2362770,
    "end": 2368020,
    "text": "最後に、2日前にアーカイブに載せたばかりの論文を紹介したい。"
  },
  {
    "start": 2368870,
    "end": 2377140,
    "text": "ここでの核となるアイデアは、基本的に、モデルの知識的側面とベースとなる言語モデルそのものを切り離すことはできないか、ということだ。"
  },
  {
    "start": 2377590,
    "end": 2386854,
    "text": "そこで、私たちが「リーガルリスクのサイロ化」と呼んでいるこのアイデアは、基本的には、パブリックドメインでリスクの低いデータだけを集めたらどうだろう、というものだ。"
  },
  {
    "start": 2386892,
    "end": 2390566,
    "text": "オープンソースのコードとか、古い本とか、そういうものだ。"
  },
  {
    "start": 2390668,
    "end": 2403178,
    "text": "そのデータは言語モデルを適合させるためだけに使用する。そして、例えば、著作権に関するものやプライベートなものなど、厄介でリスクの高いデータはすべて、テスト時にモデルが検索して読み込めるような大きなデータベースに入れる。"
  },
  {
    "start": 2403264,
    "end": 2415034,
    "text": "例えば、著作権に関するニュース記事が入っているかもしれないが、それをモデルのパラメータに当てはめるために使うのではなく、テスト時に検索し、それらの文書を読み込んで、それを使って予測を更新することができる。"
  },
  {
    "start": 2415162,
    "end": 2419250,
    "text": "したがって、これら2つのソースの種類は、トレーニング時ではなく、テスト時にのみマージされる。"
  },
  {
    "start": 2419400,
    "end": 2430150,
    "text": "この機能でできることは、自分のモデルがどの文書を予測に使っているかを、どの文書を検索しているかを見るだけで簡単に属性化できることです。"
  },
  {
    "start": 2430220,
    "end": 2434966,
    "text": "例えば、自分のデータが含まれることに不満がある場合は、簡単にオプトアウトできる。"
  },
  {
    "start": 2435068,
    "end": 2439442,
    "text": "もし人々が、自分たちのデータがモデルの予測に使用されることに対して、補償や帰属を望むなら。"
  },
  {
    "start": 2439506,
    "end": 2441574,
    "text": "検索エンジンからそれを取り出すのは簡単だ。"
  },
  {
    "start": 2441692,
    "end": 2451194,
    "text": "結局のところ、すべての厄介で複雑な機械学習は、このようなパブリックドメインのデータに対してのみ行われているのだ。"
  },
  {
    "start": 2451312,
    "end": 2457374,
    "text": "とにかく、これまで話してきたことをまとめると、こういうことになる。"
  },
  {
    "start": 2457492,
    "end": 2460206,
    "text": "基本的に、言語モデルはテキストを記憶するという考え方だ。"
  },
  {
    "start": 2460308,
    "end": 2462026,
    "text": "これは質問に答える場合には良いことだ。"
  },
  {
    "start": 2462058,
    "end": 2464058,
    "text": "これは、プライバシーや著作権の場合には良くない。"
  },
  {
    "start": 2464154,
    "end": 2468690,
    "text": "特にモデルの規模が大きくなり、より強力になると、この問題はしばしば悪化する。"
  },
  {
    "start": 2469670,
    "end": 2484070,
    "text": "第2に、冒頭で指摘したように、またワークショップを通して実際に見てきたように、プライバシー、法的、倫理的、どのような観点から見ても、機械学習システムの訓練や構築に使用するデータの重要な側面であることは間違いない。"
  },
  {
    "start": 2484410,
    "end": 2490866,
    "text": "最後に、この講演で重要な新事実だと思うのは、正確さとプライバシーの間にはトレードオフの関係があるということだ。"
  },
  {
    "start": 2490898,
    "end": 2496986,
    "text": "多くの場合、多くのアプリケーションでは、モデルが学習データからできるだけ多くのことを記憶し、吸収することを重視する。"
  },
  {
    "start": 2497088,
    "end": 2500938,
    "text": "プライバシーの観点からは、学習データからできる限り記憶し、記憶しないようにしたい。"
  },
  {
    "start": 2501024,
    "end": 2509200,
    "text": "だから、サイロの例で示したように、新しいアルゴリズムであれ、新しいモデルデザインであれ、実践の中でどうやってそれらのバランスをとり、調整するのか。"
  },
  {
    "start": 2510290,
    "end": 2510654,
    "text": "クールだ。"
  },
  {
    "start": 2510692,
    "end": 2511230,
    "text": "それがすべてだ。"
  },
  {
    "start": 2511300,
    "end": 2512080,
    "text": "ありがとう。"
  },
  {
    "start": 2516870,
    "end": 2517618,
    "text": "ありがとう。"
  },
  {
    "start": 2517704,
    "end": 2519700,
    "text": "質問の時間は1つか2つだ。"
  },
  {
    "start": 2523270,
    "end": 2527010,
    "text": "私がシャットアウトするよりも、早くシャットアウトできるだろう。"
  },
  {
    "start": 2527350,
    "end": 2530818,
    "text": "温度を上げると、この現象はどのくらい起こるのですか？"
  },
  {
    "start": 2530914,
    "end": 2536822,
    "text": "これはハリー・ポッターを温度ゼロで再現するようなものなのか、それとも温度を上げればそうなるのか？"
  },
  {
    "start": 2536876,
    "end": 2537046,
    "text": "そうだね。"
  },
  {
    "start": 2537068,
    "end": 2542758,
    "text": "一般的に暗記の例では、プロンプトから最も可能性の高い完了を解読したい。"
  },
  {
    "start": 2542854,
    "end": 2549046,
    "text": "だから、サンプリングの方法をコントロールできないと、暗記を引き出すのが難しくなる。"
  },
  {
    "start": 2549158,
    "end": 2557390,
    "text": "これらの場合、貪欲な復号化、あるいはビームサーチ復号化、あるいはある種の深さ優先探索を行い、最も尤度の高いシーケンスを見つけると仮定する。"
  },
  {
    "start": 2561490,
    "end": 2567246,
    "text": "複数のエポックでトレーニングした場合、重複したデータと同じような効果があると思いますか？"
  },
  {
    "start": 2567348,
    "end": 2570254,
    "text": "ええ、私たちは重複排除に関する論文でそれを実際に見てきました。"
  },
  {
    "start": 2570302,
    "end": 2573838,
    "text": "基本的には、2回目のエポックを行った瞬間にデータを2倍にするようなものだと考えればいいと思う。"
  },
  {
    "start": 2573854,
    "end": 2581880,
    "text": "同じようなことで、複数のエポックを行うことは、質問に対する回答パフォーマンスには大いに役立ちますが、プライバシーを大きく損なうことになります。"
  },
  {
    "start": 2584330,
    "end": 2585560,
    "text": "他に質問は？"
  },
  {
    "start": 2587690,
    "end": 2590070,
    "text": "そうでなければ、もう一度エリックに感謝しよう。"
  },
  {
    "start": 2590140,
    "end": 2594470,
    "text": "最後にもう一人、アニルさんです。"
  },
  {
    "start": 2624890,
    "end": 2625640,
    "text": "それだ。"
  },
  {
    "start": 2637550,
    "end": 2637962,
    "text": "素晴らしい。"
  },
  {
    "start": 2638016,
    "end": 2644930,
    "text": "最後のスピーカーはアニル・アナンタ・スワミで、チャット、GPT、鉱山理論について話してくれる。"
  },
  {
    "start": 2645110,
    "end": 2646654,
    "text": "ありがとうございました。"
  },
  {
    "start": 2646692,
    "end": 2648254,
    "text": "これにはとても驚いた。"
  },
  {
    "start": 2648292,
    "end": 2650718,
    "text": "40分前に決めたばかりだ。"
  },
  {
    "start": 2650804,
    "end": 2652414,
    "text": "エリックの話が最後だった。"
  },
  {
    "start": 2652452,
    "end": 2656740,
    "text": "これが最後の最後の話し合いであり、これ以上はないと確信している。"
  },
  {
    "start": 2657510,
    "end": 2676726,
    "text": "私はただ、llmsが持つ非常に興味深い能力、特に最終バージョンについて少しお話ししたかっただけです。"
  },
  {
    "start": 2676828,
    "end": 2682890,
    "text": "彼らは推論ができるのか、それとも確率的なオウム返しなのか。"
  },
  {
    "start": 2683040,
    "end": 2689434,
    "text": "このようなLMSでできることのひとつは、LMSが心の理論と呼ばれる特性を持っているかどうかを確認することだ。"
  },
  {
    "start": 2689472,
    "end": 2690700,
    "text": "心の理論。"
  },
  {
    "start": 2691630,
    "end": 2697518,
    "text": "これが何かはご存じかもしれないし、そうでないかもしれない。"
  },
  {
    "start": 2697604,
    "end": 2702802,
    "text": "他人の心の中で起こっていることを推し量ることは、私たち誰もが持っている能力だ。"
  },
  {
    "start": 2702856,
    "end": 2715442,
    "text": "もしあなたが誰かを見ていて、その人の顔や態度から行動のヒントを得ているなら、その人が何を考えているのかを推測することができるだろう。"
  },
  {
    "start": 2715496,
    "end": 2718806,
    "text": "あなたは、彼らの心の中で何が起こっているのかについて理論を持っている。"
  },
  {
    "start": 2718908,
    "end": 2732122,
    "text": "例えば、私が話している最中に誰かを見ていて、その人が私の言っていることを理解していないような表情をした場合、私は基本的にその人が私の言っていることを理解していないと推測することができる。"
  },
  {
    "start": 2732176,
    "end": 2732780,
    "text": "そうだね。"
  },
  {
    "start": 2733710,
    "end": 2741580,
    "text": "1980年代だったと思うが、サリー・アン・テストという非常に簡単なテストがある。"
  },
  {
    "start": 2743170,
    "end": 2744122,
    "text": "これがセットアップだ。"
  },
  {
    "start": 2744186,
    "end": 2750398,
    "text": "サリーとアンが一緒にいる部屋がある。"
  },
  {
    "start": 2750484,
    "end": 2753886,
    "text": "それぞれ蓋つきのバスケットを持っている。"
  },
  {
    "start": 2753918,
    "end": 2759234,
    "text": "サリーはバスケットにビー玉を隠し、蓋を閉めて部屋を出る。"
  },
  {
    "start": 2759432,
    "end": 2768530,
    "text": "サリーが部屋の外にいるとき、アンはサリーのバスケットからビー玉を取り出し、自分のバスケットに移してふたを閉める。"
  },
  {
    "start": 2768610,
    "end": 2772278,
    "text": "基本的に、外からは大理石がどこにあるかわからない。"
  },
  {
    "start": 2772364,
    "end": 2779082,
    "text": "さて、サリーが再び部屋に戻り、このシナリオが子供たちに演じられるとしよう。"
  },
  {
    "start": 2779136,
    "end": 2783450,
    "text": "サリーはどこでビー玉を探すのだろう？"
  },
  {
    "start": 2783790,
    "end": 2801242,
    "text": "子どもが何歳かにもよるが、「サリーはアンのかごの中のビー玉を探すよ」と言うかもしれない。なぜなら、子どもはこのシナリオの展開を見てきて、ビー玉がアンのかごの中にあることを知っているからだ。"
  },
  {
    "start": 2801306,
    "end": 2805134,
    "text": "すると子供は、ああ、ビー玉が蟻のかごの中にあることは知っているよ、と言うだろう。"
  },
  {
    "start": 2805182,
    "end": 2809230,
    "text": "サリーもそのバスケットの中にビー玉を探す。"
  },
  {
    "start": 2809310,
    "end": 2817542,
    "text": "子どもの発達には段階があって、「実は、ビー玉がアリのカゴの中にあることは知っているんだけど、サリーは知らないんだ。"
  },
  {
    "start": 2817596,
    "end": 2820886,
    "text": "サリーは大理石を置き忘れた場所を実際に探す。"
  },
  {
    "start": 2820988,
    "end": 2831994,
    "text": "ということは、この子は今、自分の頭の中で起こっていることだけでなく、サリーの頭の中で起こっていることもモデル化する能力を持っているということだ。"
  },
  {
    "start": 2832032,
    "end": 2840154,
    "text": "今、子どもの中で発達しているこの能力は、心の理論と呼ばれる能力である。"
  },
  {
    "start": 2840272,
    "end": 2862530,
    "text": "神経系で発達する自己のモデル、神経系が身体や外界をどのようにモデル化するか、神経系が他の神経系で起こっていることをどのようにモデル化するかという意味で。"
  },
  {
    "start": 2862870,
    "end": 2869330,
    "text": "そこで出てきた疑問は、このような大規模なLLMに今、どれだけの能力があるのかということだった。"
  },
  {
    "start": 2869410,
    "end": 2869846,
    "text": "そうだろう？"
  },
  {
    "start": 2869948,
    "end": 2880182,
    "text": "GPTのようなLLMにクエリーを出そうとするときの問題は、もちろん、文献でこのような例をたくさん見ていることだろう。"
  },
  {
    "start": 2880246,
    "end": 2890122,
    "text": "本当に問題なのは、この質問の言い回しを考え出そうとしていることと、その質問の特定のバージョンを見ていないことを望んでいることだ。"
  },
  {
    "start": 2890176,
    "end": 2897086,
    "text": "CHaD GPTアリスとボブは一緒の部屋にいる。"
  },
  {
    "start": 2897188,
    "end": 2899034,
    "text": "引き出し付きのテーブルがある。"
  },
  {
    "start": 2899162,
    "end": 2902826,
    "text": "アリスは老眼鏡をはずし、左側の引き出しに入れておく。"
  },
  {
    "start": 2902938,
    "end": 2907826,
    "text": "アリスが間違った眼鏡をかけて本を読むと、翌日ひどい頭痛に襲われる。"
  },
  {
    "start": 2907928,
    "end": 2908834,
    "text": "このことを覚えておいてほしい。"
  },
  {
    "start": 2908872,
    "end": 2910706,
    "text": "チャドGPTにこう言っているんだ。"
  },
  {
    "start": 2910808,
    "end": 2917766,
    "text": "なぜこのことを心に留めておいたのかはわからないが、これは最近llmsでやっていることの一つに過ぎない。"
  },
  {
    "start": 2917948,
    "end": 2919666,
    "text": "質問は続く。"
  },
  {
    "start": 2919858,
    "end": 2921478,
    "text": "さて、アリスは部屋を出る。"
  },
  {
    "start": 2921564,
    "end": 2926002,
    "text": "外に出ると、ボブは眼鏡を外して右側の引き出しにしまう。"
  },
  {
    "start": 2926146,
    "end": 2931740,
    "text": "左側の引き出しに、アリセとそっくりだが度数が違うメガネを入れる。"
  },
  {
    "start": 2932430,
    "end": 2937158,
    "text": "アリスの眼鏡を取り替えたことがわからないように、彼は両方の引き出しを閉める。"
  },
  {
    "start": 2937334,
    "end": 2940886,
    "text": "アリスは戻ってきて眼鏡を取り、本を読み始める。"
  },
  {
    "start": 2941078,
    "end": 2943086,
    "text": "翌日、彼女はどう感じるだろうか？"
  },
  {
    "start": 2943268,
    "end": 2944510,
    "text": "よく考えて答えなさい。"
  },
  {
    "start": 2944580,
    "end": 2963860,
    "text": "これはチャットJPTのクエリです。ここでは、LLMがトレーニング・コーパスでこのクエリを見ていないことを期待して、何重もの間接的なレイヤーを導入しようとしています。"
  },
  {
    "start": 2965290,
    "end": 2968582,
    "text": "マイケルは「そうだ、君に言っておいた」と言う。"
  },
  {
    "start": 2968636,
    "end": 2970920,
    "text": "スティーブンはイエスと言う。"
  },
  {
    "start": 2971290,
    "end": 2974758,
    "text": "チャドGPのANSwerだ。"
  },
  {
    "start": 2974844,
    "end": 2984438,
    "text": "ボブはアリスの老眼鏡を、見た目は同じだが度数が間違っているものとすり替えたので、アリスは読書を始めるとき、知らず知らずのうちに間違った眼鏡を使うことになる。"
  },
  {
    "start": 2984534,
    "end": 2989210,
    "text": "前述のように、アリスが間違った眼鏡をかけて本を読むと、翌日ひどい頭痛に襲われる。"
  },
  {
    "start": 2989280,
    "end": 2993950,
    "text": "そのため、アリスは間違った眼鏡を使った結果、翌日ひどい頭痛に襲われることになる。"
  },
  {
    "start": 2994850,
    "end": 2999258,
    "text": "斜体と下線を引いた言葉に注目してほしい。"
  },
  {
    "start": 2999274,
    "end": 3001262,
    "text": "それを強調するために僕がやっているんだ。"
  },
  {
    "start": 3001316,
    "end": 3007118,
    "text": "という言葉は、知らず知らずのうちに他人の心の状態を表している。"
  },
  {
    "start": 3007214,
    "end": 3016142,
    "text": "この言葉は、アリスの心の中で起こっていることについての推論を示唆しているようなものだが、私はさらに問い合わせることにした。"
  },
  {
    "start": 3016206,
    "end": 3019362,
    "text": "なぜアリスは知らず知らずのうちに間違った眼鏡を使うのか？"
  },
  {
    "start": 3019426,
    "end": 3021814,
    "text": "もう一度言うが、これはかなりすごいことだ。"
  },
  {
    "start": 3022012,
    "end": 3029062,
    "text": "ボブが彼女の老眼鏡を、彼女の老眼鏡とそっくりだが度数が間違っている別の眼鏡とすり替えたため、アリスは知らず知らずのうちに間違った眼鏡を使うことになる。"
  },
  {
    "start": 3029196,
    "end": 3036490,
    "text": "部屋に戻ったアリスはスイッチに気づかず、左側の引き出しから取り出した眼鏡を自分のものだと思い込む。"
  },
  {
    "start": 3036640,
    "end": 3038982,
    "text": "どちらのメガネも同じに見えるからだ。"
  },
  {
    "start": 3039046,
    "end": 3044106,
    "text": "彼女は、自分が使っているメガネが最初に引き出しに入れたものとは違うことに気づく由もない。"
  },
  {
    "start": 3044138,
    "end": 3047454,
    "text": "その結果、彼女は知らず知らずのうちに不鮮明な眼鏡を使っている。"
  },
  {
    "start": 3047492,
    "end": 3050302,
    "text": "もう一度、斜体で書かれた言葉を見てほしい。"
  },
  {
    "start": 3050356,
    "end": 3059454,
    "text": "これらはすべて、他のエージェント（この場合はアリス）の内容をモデル化する能力を指す言葉だ。"
  },
  {
    "start": 3059502,
    "end": 3073750,
    "text": "今、私にとっては、この5日間、llnsは確率的なオウムなのか、それとも推論する能力があるのか、あるいは新奇な出力を生成するのか、等々について皆で議論した。"
  },
  {
    "start": 3074410,
    "end": 3084810,
    "text": "このようなアウトプットに直面したとき、彼らがやっていることは自分が見たものをオウム返ししているだけだと主張するのは、本当にとても難しいことだ。"
  },
  {
    "start": 3084880,
    "end": 3095518,
    "text": "つまり、chatgptが見たことのないようなクエリであることを期待して、懸命にクエリを考えてきたんだ。"
  },
  {
    "start": 3095604,
    "end": 3108158,
    "text": "注意しなければならないのは、その1ヵ月後にchatgPDに出された同じクエリは、ある重要な情報を間違えていて、その結果、回答全体がバラバラになってしまったことだ。"
  },
  {
    "start": 3108334,
    "end": 3117890,
    "text": "それでも完璧とは言えないが、少なくとも私にとっては、このような現象が起きているという事実は、これらのモデルの内部でいったい何が起きているのかという疑問を投げかけている。"
  },
  {
    "start": 3118390,
    "end": 3120066,
    "text": "この言葉を残しておきたい。"
  },
  {
    "start": 3120088,
    "end": 3122402,
    "text": "これらのモデルの内部で何が起こっているのか、私は答えを持っていない。"
  },
  {
    "start": 3122466,
    "end": 3130538,
    "text": "この5日間で私たちが学んだことのひとつは、私たち全員がそれを解明しようとしているということであり、理論家たちはその疑問に答えようと懸命に努力しているということだ。"
  },
  {
    "start": 3130624,
    "end": 3139902,
    "text": "経験主義者たちはそれに翻弄され、これらのことを検証する方法を考え出そうとしている。"
  },
  {
    "start": 3139956,
    "end": 3151130,
    "text": "llmは、ロボットのセットアップにある種の頭脳を提供するために使われるようなものだと想像できるだろう。"
  },
  {
    "start": 3151290,
    "end": 3168502,
    "text": "LLMベースのエージェントは、外の情景を調べ、そこから学び、何かを推測することができる。"
  },
  {
    "start": 3168636,
    "end": 3174866,
    "text": "このワークショップ全体を終わらせる方法として、この言葉を残しておきたい。"
  },
  {
    "start": 3174898,
    "end": 3175734,
    "text": "ありがとうございました。"
  },
  {
    "start": 3175772,
    "end": 3179162,
    "text": "5日間、ここでみんなの話を聞けてよかった。"
  },
  {
    "start": 3179296,
    "end": 3180220,
    "text": "ありがとう。"
  },
  {
    "start": 3184030,
    "end": 3184490,
    "text": "素晴らしい。"
  },
  {
    "start": 3184560,
    "end": 3186902,
    "text": "あなたの視点を共有してくれて本当にありがとう。"
  },
  {
    "start": 3187046,
    "end": 3188860,
    "text": "質問の時間がある。"
  },
  {
    "start": 3198850,
    "end": 3200330,
    "text": "プロンプトに戻っていただけますか？"
  },
  {
    "start": 3200410,
    "end": 3203380,
    "text": "実は、その中に私が言及したかった部分があるんだ。"
  },
  {
    "start": 3206950,
    "end": 3212274,
    "text": "ああ、実際、君のプロンプトにはいくつかのパートがある。"
  },
  {
    "start": 3212392,
    "end": 3212834,
    "text": "これか？"
  },
  {
    "start": 3212872,
    "end": 3213460,
    "text": "そうだね。"
  },
  {
    "start": 3213910,
    "end": 3214660,
    "text": "そうだね。"
  },
  {
    "start": 3218170,
    "end": 3224002,
    "text": "基本的に眼鏡は持っていくように作戦を指示する？"
  },
  {
    "start": 3224146,
    "end": 3224742,
    "text": "そうだね。"
  },
  {
    "start": 3224876,
    "end": 3229560,
    "text": "なぜチャットGPTの4人は、彼女が自分のメガネを正しく見つけることができないのか？"
  },
  {
    "start": 3231690,
    "end": 3233114,
    "text": "なぜそのように推測するのか？"
  },
  {
    "start": 3233152,
    "end": 3235606,
    "text": "そうすると彼女はメガネを見つける。"
  },
  {
    "start": 3235638,
    "end": 3236700,
    "text": "誰がどうやって気にするんだ？"
  },
  {
    "start": 3238510,
    "end": 3244042,
    "text": "さて、ここでの仮定は、アリスがメガネを置き忘れた場所を探すということだ。"
  },
  {
    "start": 3244176,
    "end": 3249294,
    "text": "ええ、でもあなたが聞いたのはメガネのことで、メガネをどこに置いたかじゃないでしょう？"
  },
  {
    "start": 3249492,
    "end": 3255438,
    "text": "ちょっと気になったんだけど、もしこの問題を、彼女が出発したと思っている場所だけに変えたらどうなるんだろう？"
  },
  {
    "start": 3255524,
    "end": 3256654,
    "text": "ああ、やってみる価値はあるよ。"
  },
  {
    "start": 3256692,
    "end": 3258914,
    "text": "そのすべてのバリエーション、これが私の置いた問題である。"
  },
  {
    "start": 3258952,
    "end": 3264500,
    "text": "あなたの言う通り、これらはすべて、LLMが探索空間のどこかに行くための合図だと思う。"
  },
  {
    "start": 3267370,
    "end": 3271480,
    "text": "答えを出すには、それで十分だと思う。"
  },
  {
    "start": 3271850,
    "end": 3275334,
    "text": "答えを出すのに十分なヒントを与えているのだ。"
  },
  {
    "start": 3275372,
    "end": 3276790,
    "text": "推論にしか見えない。"
  },
  {
    "start": 3277450,
    "end": 3280046,
    "text": "それはある意味、本当に理性的なのか、ということだと思う。"
  },
  {
    "start": 3280098,
    "end": 3284614,
    "text": "現時点では、推論だと断言することはできないと思う。"
  },
  {
    "start": 3284662,
    "end": 3295420,
    "text": "質問の中に、クエリの中に、十分なヒントがあり、オートコンプリートだけで十分な理由ある答えのように見えるものを得ることができるのです。"
  },
  {
    "start": 3298110,
    "end": 3302300,
    "text": "その他の質問"
  },
  {
    "start": 3316410,
    "end": 3320562,
    "text": "まあ、アリスが戻ってきて眼鏡を取ったという意味ではね。"
  },
  {
    "start": 3320706,
    "end": 3329622,
    "text": "アリスが戻って来て、彼女が知っていると思ったものを手に入れた。"
  },
  {
    "start": 3329676,
    "end": 3334102,
    "text": "混乱するのは簡単だ。"
  },
  {
    "start": 3334166,
    "end": 3334490,
    "text": "そうだね。"
  },
  {
    "start": 3334560,
    "end": 3339722,
    "text": "文字通りに受け取るなら、アリスが戻ってきて眼鏡を取ったから問題はないと言うだろう。"
  },
  {
    "start": 3339856,
    "end": 3345520,
    "text": "もっと人間的な人なら、よし、彼女は戻って来て、自分の考えたものを取ったんだ、と思うだろう。"
  },
  {
    "start": 3346690,
    "end": 3355994,
    "text": "彼女が戻って来て眼鏡を取ったが、それは彼女が考えていたことであって、彼女の眼鏡が何であったかということではない。"
  },
  {
    "start": 3356042,
    "end": 3367190,
    "text": "つまり、私はお二人の意見に賛成だし、そこに問題があると思う。私たちはGPTの頭の中に入って、GPTが何をしているのかを理解しようとしているが、現時点ではその手がかりがないのだ。"
  },
  {
    "start": 3367260,
    "end": 3373554,
    "text": "曖昧な部分が多いので、GPDに何らかの能力があると考えることもできる。"
  },
  {
    "start": 3373682,
    "end": 3388540,
    "text": "ところで、エリックが話していたと思うが、これは創発的な性質で、GPT-2やGPT3.5以降にはなかったという意味での創発に過ぎない。"
  },
  {
    "start": 3388910,
    "end": 3404538,
    "text": "しかし、私自身はこの能力をあらゆる種類のクエリーでテストしてみた。"
  },
  {
    "start": 3404564,
    "end": 3420466,
    "text": "というのも、これはまだ推論とは言えない、非常に洗練されたパターンマッチングをしているのだ、ということを教えてくれるからだ。"
  },
  {
    "start": 3420498,
    "end": 3424680,
    "text": "そして、それが失敗したときには、理性的な答えでないことが明白になる。"
  },
  {
    "start": 3429130,
    "end": 3437690,
    "text": "ひとつ気になるのは、あなたの講演の文脈で、人間にこのことについて話してもらうというのは、明らかにサリーアンテストのことだということです。"
  },
  {
    "start": 3438270,
    "end": 3443322,
    "text": "ということは、ある意味、チャットGPTがサリーアント・テストとして認識しているのか気になるところだ。"
  },
  {
    "start": 3443376,
    "end": 3444262,
    "text": "ごめん、理解できなかった。"
  },
  {
    "start": 3444336,
    "end": 3450826,
    "text": "ある意味、チャットGPTはサリーの同族体のようなものを認識しているのか気になるところだ。"
  },
  {
    "start": 3451018,
    "end": 3453790,
    "text": "そうだと思うよ、完全にね。"
  },
  {
    "start": 3453860,
    "end": 3461506,
    "text": "これをテストしている私たちの課題は、このような質問をせずに済む方法を考え出すことだと思う。"
  },
  {
    "start": 3461688,
    "end": 3465178,
    "text": "サリー・アン・テンプレートなしでこの質問をする方法がまだわかっていない。"
  },
  {
    "start": 3465294,
    "end": 3467350,
    "text": "アリスとボブを変えただけだ。"
  },
  {
    "start": 3467770,
    "end": 3469074,
    "text": "それは些細なことだ。"
  },
  {
    "start": 3469202,
    "end": 3477810,
    "text": "サリー・アンのテスト例を十分に見てきているので、その比喩で動いているのは間違いない。"
  },
  {
    "start": 3477980,
    "end": 3478602,
    "text": "そうだね。"
  },
  {
    "start": 3478736,
    "end": 3487462,
    "text": "ということは、容器に物を入れることを伴わない心の理論のテストを構築し始めるには、興味深い道かもしれない。"
  },
  {
    "start": 3487606,
    "end": 3488700,
    "text": "私はそうしてきた。"
  },
  {
    "start": 3489070,
    "end": 3496238,
    "text": "私はスティーブンに、人形の中に人形が入っているロシアの人形、マトルーシュカ人形だと思う、と言ったんだ。"
  },
  {
    "start": 3496324,
    "end": 3502910,
    "text": "サリーとアンは、入れ子になった一連のプレーを見ている。"
  },
  {
    "start": 3503810,
    "end": 3510114,
    "text": "サリーはそれを組み立て、アンはそれを見ている。"
  },
  {
    "start": 3510152,
    "end": 3516770,
    "text": "アンが部屋を出ている間に、サリーはそれを分解し、一番奥のものを取り出して元に戻す。"
  },
  {
    "start": 3516840,
    "end": 3519026,
    "text": "そこへアンが戻ってくる。"
  },
  {
    "start": 3519048,
    "end": 3522722,
    "text": "問題は、アンが何体の人形を考えているかということだ。"
  },
  {
    "start": 3522776,
    "end": 3530250,
    "text": "しかし、理想を言えば、サランから離れることだ。"
  },
  {
    "start": 3530320,
    "end": 3538380,
    "text": "その答えはわからないが、そのテンプレートから離れたクエリーを考え、それがうまくいくかどうかを確かめたい。"
  },
  {
    "start": 3539150,
    "end": 3542410,
    "text": "おそらく、ある程度はうまくいくのではないかと私は思っている。"
  },
  {
    "start": 3544430,
    "end": 3545660,
    "text": "ありがとうございました。"
  },
  {
    "start": 3551150,
    "end": 3551400,
    "text": "ありがとう。"
  }
]