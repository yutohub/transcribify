[
  {
    "start": 250,
    "end": 1662,
    "text": "やあ、みんな。"
  },
  {
    "start": 1716,
    "end": 5130,
    "text": "今日は、人間のフィードバックからの強化学習とPPOについてお話します。"
  },
  {
    "start": 5210,
    "end": 13114,
    "text": "つまり、人間のフィードバックからの強化学習は、言語モデルの振る舞いを、言語モデルが出力したいものに合わせるために使われるテクニックなのだ。"
  },
  {
    "start": 13162,
    "end": 19770,
    "text": "例えば、言語モデルには呪いの言葉を使わせたくないし、言語モデルにはユーザーに対して無礼な振る舞いをさせたくない。"
  },
  {
    "start": 19850,
    "end": 22026,
    "text": "ある種のアラインメントが必要だ。"
  },
  {
    "start": 22058,
    "end": 30040,
    "text": "人間のフィードバックからの強化学習は最も有名なテクニックのひとつだが、DPOのような新しいテクニックも登場している。"
  },
  {
    "start": 30410,
    "end": 34722,
    "text": "人間のフィードバックによる強化学習は、チャットGPTを生み出した方法でもある。"
  },
  {
    "start": 34786,
    "end": 45594,
    "text": "今日のトピックでは、まず言語モデルについて、その使い方や仕組みについて少し紹介する。"
  },
  {
    "start": 45712,
    "end": 54426,
    "text": "その後、AIのアライメントというトピックについて、なぜそれが重要なのかについてお話しし、その後、人間のフィードバックからの強化学習について深く掘り下げていきます。"
  },
  {
    "start": 54458,
    "end": 58110,
    "text": "特に、まず強化学習とは何かを紹介する。"
  },
  {
    "start": 58180,
    "end": 61262,
    "text": "続いて、強化学習のすべての設定について説明する。"
  },
  {
    "start": 61316,
    "end": 63434,
    "text": "報酬モデル、軌道とは何か？"
  },
  {
    "start": 63562,
    "end": 68654,
    "text": "特に、政策勾配最適化を見て、アルゴリズムを導出する。"
  },
  {
    "start": 68782,
    "end": 70546,
    "text": "また、その問題点も見えてくるだろう。"
  },
  {
    "start": 70568,
    "end": 75742,
    "text": "つまり、分散を減らす方法、高度な推定、政策学習の重要なサンプリング、などなど。"
  },
  {
    "start": 75886,
    "end": 80514,
    "text": "今日のビデオの目的は、実はPPOの損失を導き出すことだ。"
  },
  {
    "start": 80562,
    "end": 83094,
    "text": "フォーミュラをただ投げつけたくはない。"
  },
  {
    "start": 83132,
    "end": 90806,
    "text": "実際にPPOのすべてのアルゴリズムをステップ・バイ・ステップで導き出し、それに至る歴史もすべてお見せしたい。"
  },
  {
    "start": 90828,
    "end": 96186,
    "text": "PPOが数学的見地から解決しようとしていた問題は何だったのか。"
  },
  {
    "start": 96368,
    "end": 104510,
    "text": "ビデオの最後の部分では、PPOを使った人間のフィードバックからの強化学習の実際の実装のコードを見ていきます。"
  },
  {
    "start": 104850,
    "end": 109374,
    "text": "実は一行一行コーディングするつもりはない。"
  },
  {
    "start": 109412,
    "end": 111294,
    "text": "実際にコードを一行ずつ説明しよう。"
  },
  {
    "start": 111332,
    "end": 115854,
    "text": "特に、ハグリング・フェイス・チームが行った実装を紹介しよう。"
  },
  {
    "start": 115892,
    "end": 128340,
    "text": "ここでは、人間のフィジオックから強化学習を利用するための抱きつき顔ライブラリの使い方は紹介しないが、抱きつき顔ライブラリのコードの中に入り、抱きつき顔チームによってどのように実装されたかを見てみよう。"
  },
  {
    "start": 128650,
    "end": 132306,
    "text": "そうすることで、学んだ理論を実践に結びつけることができる。"
  },
  {
    "start": 132418,
    "end": 137478,
    "text": "さて、抱きつきフェイスチームが書いたコードは、ちょっとわかりにくく複雑だ。"
  },
  {
    "start": 137564,
    "end": 148090,
    "text": "いくつかの部分を削除し、また、私自身のコメントで、この方法では理解しにくかった部分をコメントした。"
  },
  {
    "start": 148240,
    "end": 151146,
    "text": "さて、このビデオを見る前にいくつかの前提条件がある。"
  },
  {
    "start": 151248,
    "end": 154922,
    "text": "まず、確率や統計の概念をある程度持っていてほしい。"
  },
  {
    "start": 154986,
    "end": 155502,
    "text": "あまりない。"
  },
  {
    "start": 155556,
    "end": 157550,
    "text": "少なくとも、何が期待されているかはわかっているはずだ。"
  },
  {
    "start": 158770,
    "end": 169810,
    "text": "もちろん、ディープラーニングの知識、例えば勾配降下法、損失関数とは何か、勾配降下法ではある種の勾配を計算すること、などを知っておく必要がある。"
  },
  {
    "start": 170790,
    "end": 175026,
    "text": "強化学習の基本的な知識は持っておく必要がある。"
  },
  {
    "start": 175048,
    "end": 179090,
    "text": "少なくとも、あなたはエージェント、状態、環境、報酬が何であるかを知っている。"
  },
  {
    "start": 179850,
    "end": 184422,
    "text": "このビデオで重要な点は、トランス・モデルを多用することだ。"
  },
  {
    "start": 184476,
    "end": 194460,
    "text": "このビデオを理解するための鍵となる自己の注意や因果の仮面の概念に馴染みのない方は、変圧器に関する前回のビデオをご覧になることをお勧めする。"
  },
  {
    "start": 195310,
    "end": 198822,
    "text": "このビデオの目的は、理論と実践を結びつけることだ。"
  },
  {
    "start": 198886,
    "end": 204746,
    "text": "複雑な数式に対しては、常に直感のようなものを与えるようにするつもりだ。"
  },
  {
    "start": 204938,
    "end": 208490,
    "text": "最初はすべてを理解できなくても心配しないでほしい。"
  },
  {
    "start": 208570,
    "end": 208862,
    "text": "なぜですか？"
  },
  {
    "start": 208916,
    "end": 214206,
    "text": "というのも、後でコードをお見せすることになるので、最初に多くの理論をお話しすることになるからです。"
  },
  {
    "start": 214308,
    "end": 217822,
    "text": "理論的な知識なしにコードを示すことはできない。"
  },
  {
    "start": 217886,
    "end": 229746,
    "text": "コードを見るときには、一行ずつ理論に戻るので、この知識の実践的な面と理論的な面を組み合わせることができる。"
  },
  {
    "start": 229858,
    "end": 231750,
    "text": "さあ、旅を始めよう。"
  },
  {
    "start": 232170,
    "end": 234662,
    "text": "では、言語モデルとは何か？"
  },
  {
    "start": 234716,
    "end": 240458,
    "text": "まず言語モデルとは、単語列に確率を割り当てる確率論的モデルである。"
  },
  {
    "start": 240544,
    "end": 247478,
    "text": "特に言語モデルは、入力シーケンスが与えられたときに、次のトークンの確率を計算することができる。"
  },
  {
    "start": 247654,
    "end": 255802,
    "text": "特に、例えば、上海は中国の都市であるというプロンプトがあった場合、次の単語が中国である確率はどのくらいだろうか？"
  },
  {
    "start": 255866,
    "end": 259594,
    "text": "あるいは、次の単語が北京、猫、ピザである確率は？"
  },
  {
    "start": 259642,
    "end": 263550,
    "text": "これは言語モデルがモデル化している確率の一種である。"
  },
  {
    "start": 263970,
    "end": 271698,
    "text": "さて、私の言語モデルのトラクテーションでは、常に単純化している。つまり、それぞれの単語はトークンであり、それぞれのトークンは単語であるということだ。"
  },
  {
    "start": 271784,
    "end": 275634,
    "text": "これは使用するトークナイザーに依存するため、必ずしもそうなるとは限らない。"
  },
  {
    "start": 275752,
    "end": 279182,
    "text": "実際、ほとんどの場合はこうではない。"
  },
  {
    "start": 279336,
    "end": 285480,
    "text": "簡単のため、ビデオの残りの部分では、各単語はトークンであり、各トークンは単語であると考えることにする。"
  },
  {
    "start": 286010,
    "end": 290454,
    "text": "さて、言語モデルを使ってテキストを生成するにはどうすればいいのだろうか、と疑問に思うかもしれない。"
  },
  {
    "start": 290572,
    "end": 297414,
    "text": "つまり、例えば「上海はどこですか？"
  },
  {
    "start": 297462,
    "end": 299862,
    "text": "次に、言語モデルに次のトークンは何かと尋ねる。"
  },
  {
    "start": 299926,
    "end": 303782,
    "text": "例えば、貪欲に、最も確率の高いトークンを選択する。"
  },
  {
    "start": 303926,
    "end": 306378,
    "text": "例えば、shanghaiという単語を選ぶ。"
  },
  {
    "start": 306474,
    "end": 307962,
    "text": "そしてこの言葉、シャンハイを取り上げる。"
  },
  {
    "start": 308026,
    "end": 309546,
    "text": "レーザーを使わせてくれ。"
  },
  {
    "start": 309658,
    "end": 313834,
    "text": "それを入力に戻し、言語モデルに「次のトークンは何か？"
  },
  {
    "start": 313882,
    "end": 317274,
    "text": "言語モデルは次のトークンの確率を教えてくれる。"
  },
  {
    "start": 317322,
    "end": 319102,
    "text": "より確率の高い方を選ぶ。"
  },
  {
    "start": 319166,
    "end": 323054,
    "text": "仮にisという単語だとすると、それを入力に戻す。"
  },
  {
    "start": 323102,
    "end": 325582,
    "text": "次のトークンは何か？"
  },
  {
    "start": 325646,
    "end": 339254,
    "text": "次のトークンが入力されたとすると、私たちはそれを受け取り、入力に戻し、私たちが生成したトークンの数に達するまで、あるいは答えが完了したと信じるまで、言語モデルに次のトークンは何か、などと再び尋ねる。"
  },
  {
    "start": 339372,
    "end": 346538,
    "text": "この場合、例えば、答えが上海にある、中国にある、言語モデルによって生成された答えであることがわかるので、止めることができます。"
  },
  {
    "start": 346624,
    "end": 350374,
    "text": "これは、言語モデルを使ってテキストを生成する反復プロセスである。"
  },
  {
    "start": 350432,
    "end": 353280,
    "text": "すべての言語モデルは、実際にはこのように動作する。"
  },
  {
    "start": 354450,
    "end": 357710,
    "text": "さて、AIアライメントの話題とは？"
  },
  {
    "start": 358050,
    "end": 369998,
    "text": "言語モデルは通常、膨大な量のデータで事前に訓練されている。つまり、何十億ものウェブページ、ウィキペディア全体、何千冊もの書籍で事前に訓練されている。"
  },
  {
    "start": 370174,
    "end": 379734,
    "text": "これにより、言語モデルは多くの知識を得ることができ、そこからプロンプトを合理的な方法で完了することを学習することができる。"
  },
  {
    "start": 379852,
    "end": 384294,
    "text": "しかし、これは言語モデルに特定の振る舞いを教えるものではない。"
  },
  {
    "start": 384332,
    "end": 386434,
    "text": "例えば、プレトレーニングだけでもいい。"
  },
  {
    "start": 386482,
    "end": 393900,
    "text": "攻撃的な言葉を使うなとか、人種差別的な表現を使うなとか、呪いの言葉を使うなとか、そういう言語モデルは教えない。"
  },
  {
    "start": 394430,
    "end": 402634,
    "text": "そのためには、また、たとえば、ユーザーに親しみやすいチャットアシスタントを作るためには、ある種のアライメントを行う必要がある。"
  },
  {
    "start": 402762,
    "end": 408682,
    "text": "AIアライメントのテーマは、モデルの振る舞いをある望ましい振る舞いに合わせることである。"
  },
  {
    "start": 408826,
    "end": 410798,
    "text": "強化学習について話そう。"
  },
  {
    "start": 410884,
    "end": 424414,
    "text": "つまり、強化学習は人工知能の一分野であり、知的エージェントが環境から受け取る報酬を最大化するために、環境内で行動をとるように訓練することに関係する。"
  },
  {
    "start": 424542,
    "end": 426418,
    "text": "具体的な例を挙げよう。"
  },
  {
    "start": 426584,
    "end": 430182,
    "text": "とてもシンプルな世界に住む猫がいるとしよう。"
  },
  {
    "start": 430236,
    "end": 437046,
    "text": "そこがたくさんのグリッドで構成された部屋で、この猫があるマスから別のマスへと移動できるとする。"
  },
  {
    "start": 437228,
    "end": 447100,
    "text": "さて、この場合、エージェントは猫であり、このエージェントは、例えばこのエージェントの位置を記述する状態を持っている。"
  },
  {
    "start": 447790,
    "end": 451846,
    "text": "この場合、猫の状態は2つの変数で記述できる。"
  },
  {
    "start": 451878,
    "end": 457118,
    "text": "ひとつはこの猫の位置のx座標、もうひとつはy座標である。"
  },
  {
    "start": 457284,
    "end": 466180,
    "text": "その状態に基づいて、猫はいくつかの行動を選択することができる。例えば、下に移動する、左に移動する、右に移動する、上に移動する、などである。"
  },
  {
    "start": 466630,
    "end": 469982,
    "text": "その状態に基づいて、猫はいくつかのアクションを起こすことができる。"
  },
  {
    "start": 470046,
    "end": 474798,
    "text": "猫が何か行動を起こすたびに、環境から何らかの報酬を受け取る。"
  },
  {
    "start": 474894,
    "end": 481590,
    "text": "確実に新しいポジションに移動すると同時に、環境から何らかの報酬を受け取ることになる。"
  },
  {
    "start": 482010,
    "end": 484838,
    "text": "報酬はこの報酬モデルに従っている。"
  },
  {
    "start": 484924,
    "end": 488966,
    "text": "もし猫が空のセルに移動すれば、報酬はゼロとなる。"
  },
  {
    "start": 489068,
    "end": 495270,
    "text": "例えば、ほうきのところに移動したら、うちの猫はほうきを怖がるので、報酬はマイナス1。"
  },
  {
    "start": 495430,
    "end": 506030,
    "text": "一連の状態や行動の後、どうにかして猫がバスタブにたどり着いたら、マイナス10のご褒美をあげる。"
  },
  {
    "start": 506180,
    "end": 512320,
    "text": "しかし、猫がどうにかして肉にたどり着けば、プラス100の大きな報酬を受け取ることができる。"
  },
  {
    "start": 513090,
    "end": 514834,
    "text": "猫はどう動くべきか？"
  },
  {
    "start": 514952,
    "end": 522434,
    "text": "さて、現在の状態から次の行動を起こす確率を示す方針がある。"
  },
  {
    "start": 522552,
    "end": 524930,
    "text": "各ポジションについて、ポリシーが記述している。"
  },
  {
    "start": 525000,
    "end": 532406,
    "text": "猫の各状態について、猫がどのような確率で上下左右に動くべきか。"
  },
  {
    "start": 532508,
    "end": 542966,
    "text": "その場合、エージェントはランダムに行動を選択することもできるし、例えば貪欲な戦略である最も確率の高い行動を選択することもできる。"
  },
  {
    "start": 543078,
    "end": 547430,
    "text": "さて、強化学習の目的は確率を学習することである。"
  },
  {
    "start": 547510,
    "end": 567700,
    "text": "つまり、エージェントがこの方針に従って行動したときに、期待リターンが最大になるような方針を最適化すること、つまり、非常に高い確率でミートまで持っていけるような方針を持つことが、この場合の期待リターンを最大化する一つの方法なのです。"
  },
  {
    "start": 568070,
    "end": 573042,
    "text": "さて、あなたは不思議に思うかもしれないが、よし、猫は強化学習エージェントとして見ることができる。"
  },
  {
    "start": 573096,
    "end": 578114,
    "text": "強化学習の設定は、猫と肉とこれらすべての報酬にとって理にかなっている。"
  },
  {
    "start": 578242,
    "end": 582738,
    "text": "強化学習と言語モデルの関係は？"
  },
  {
    "start": 582834,
    "end": 584502,
    "text": "これを明確にしよう。"
  },
  {
    "start": 584636,
    "end": 589734,
    "text": "言語モデルをポリシーそのものと考えることができる。"
  },
  {
    "start": 589852,
    "end": 598250,
    "text": "つまり、前に見たように、ポリシーとは、ある状態が与えられたとき、その状態で取るべき行動の確率はどれくらいかを教えてくれるものなのだ。"
  },
  {
    "start": 598400,
    "end": 605850,
    "text": "言語モデルの場合、プロンプトが与えられたとき、次のトークンの確率はどのくらいか？"
  },
  {
    "start": 605930,
    "end": 615006,
    "text": "プロンプトを状態、次のトークンを言語モデルが選択できるアクションと考えることができる。"
  },
  {
    "start": 615108,
    "end": 619038,
    "text": "なぜなら、ネクストトークンをサンプリングするたびに、それをプロンプトに戻しているからだ。"
  },
  {
    "start": 619134,
    "end": 622798,
    "text": "そうすれば、言語モデルに、次の次のトークンは何か、などと尋ねることができる。"
  },
  {
    "start": 622894,
    "end": 641590,
    "text": "つまり、状態とはプロンプトであり、アクションとは言語モデルがある戦略に従って選択する次のトークンである。"
  },
  {
    "start": 641750,
    "end": 644890,
    "text": "ここに欠けているのは、報酬モデルだけだ。"
  },
  {
    "start": 644960,
    "end": 649158,
    "text": "言語モデルが良い反応をした場合、どのように報いることができるだろうか？"
  },
  {
    "start": 649254,
    "end": 654618,
    "text": "言語モデルの反応が悪かった場合、どのようにペナルティを科すのか？"
  },
  {
    "start": 654794,
    "end": 659358,
    "text": "これは、我々が構築しなければならない報酬モデルを通じて行われる。"
  },
  {
    "start": 659444,
    "end": 660480,
    "text": "どうやるか見てみよう。"
  },
  {
    "start": 661010,
    "end": 668338,
    "text": "さて、強化学習エージェントになる言語モデルの報酬モデルを作りたいとしよう。"
  },
  {
    "start": 668504,
    "end": 679558,
    "text": "さて、質問に対して特定の答えを生成したモデルに報酬を与えるために、モデルによって生成された質問と答えのこのようなデータセットを作成することができる。"
  },
  {
    "start": 679724,
    "end": 682434,
    "text": "例えば、モデルに「上海はどこですか？"
  },
  {
    "start": 682482,
    "end": 685746,
    "text": "モデル言語モデルは、よし、上海は中国の都市だ、と言うことができる。"
  },
  {
    "start": 685858,
    "end": 688870,
    "text": "私たちはこの答えに報いるべきだ。"
  },
  {
    "start": 688940,
    "end": 691078,
    "text": "この答えがいかに優れているか。"
  },
  {
    "start": 691244,
    "end": 697594,
    "text": "さて、私の場合、高い報酬を与えたいと思うのは、答えが短く、要点がまとまっていると思うからだ。"
  },
  {
    "start": 697712,
    "end": 705070,
    "text": "この答えでは短すぎると思う人もいるだろうから、もう少し長い答えを好む人もいるだろう。"
  },
  {
    "start": 705220,
    "end": 707374,
    "text": "この場合、例えば、2プラス2とは何か？"
  },
  {
    "start": 707412,
    "end": 710526,
    "text": "この言語モデルが4という単語しか言わないとする。"
  },
  {
    "start": 710708,
    "end": 715646,
    "text": "さて、私の場合、この答えは短すぎると思うので、もう少し詳しく書いてもいいと思う。"
  },
  {
    "start": 715678,
    "end": 720722,
    "text": "この答えで十分だと思う人もいるだろう。"
  },
  {
    "start": 720856,
    "end": 724562,
    "text": "さて、このアンサー、あるいはこの答えにはどんな報酬を与えるべきだろうか？"
  },
  {
    "start": 724616,
    "end": 729958,
    "text": "このように、万人に受け入れられる数字を出すのは容易ではない。"
  },
  {
    "start": 730124,
    "end": 734514,
    "text": "私たち人間は、合意するための共通の土台を見つけるのが得意ではない。"
  },
  {
    "start": 734562,
    "end": 736978,
    "text": "残念ながら、私たちは比較するのが得意だ。"
  },
  {
    "start": 737074,
    "end": 742410,
    "text": "この事実を利用して、報酬モデルをトレーニングするためのデータセットを作成する。"
  },
  {
    "start": 742560,
    "end": 749242,
    "text": "一つの答えを生成する代わりに、同じ言語モデルを使って複数の答えを生成できるとしたら？"
  },
  {
    "start": 749376,
    "end": 752250,
    "text": "これは例えば、高温を使うことで可能だ。"
  },
  {
    "start": 752410,
    "end": 761738,
    "text": "そこで、この分野の専門家であるラベラーに、どの答えがいいかを選んでもらえばいい。"
  },
  {
    "start": 761914,
    "end": 773140,
    "text": "この嗜好のデータセットがあれば、各質問と回答に対して数値報酬を生成するモデルを作成することができる。"
  },
  {
    "start": 773590,
    "end": 777506,
    "text": "まず、質問のデータセットを作成します。"
  },
  {
    "start": 777608,
    "end": 783314,
    "text": "そして言語モデルに、同じ質問に対して複数の答えを生成するように依頼する。"
  },
  {
    "start": 783442,
    "end": 786482,
    "text": "そして、どちらの答えが好きかを選んでもらう。"
  },
  {
    "start": 786626,
    "end": 791290,
    "text": "さて、我々の目標は、報酬モデルとして機能するニューラルネットワークを作成することである。"
  },
  {
    "start": 791360,
    "end": 803078,
    "text": "質問と答えが与えられると、選ばれた答えが高い報酬を得るように数値を生成するモデル。"
  },
  {
    "start": 803174,
    "end": 808554,
    "text": "選ばれなかった答えは、私たちが好まないもので、報酬は低いはずだ。"
  },
  {
    "start": 808682,
    "end": 810400,
    "text": "その方法を見てみよう。"
  },
  {
    "start": 810770,
    "end": 814794,
    "text": "実際には、事前に学習させた言語モデルを使う。"
  },
  {
    "start": 814852,
    "end": 821250,
    "text": "例えば、事前に訓練されたラマを使い、言語モデルに質問と答えを与えます。"
  },
  {
    "start": 821320,
    "end": 826446,
    "text": "この入力トークンは、質問と答えを連結したものです。"
  },
  {
    "start": 826478,
    "end": 827010,
    "text": "一緒にね。"
  },
  {
    "start": 827160,
    "end": 829778,
    "text": "それを入力として言語モデルに与える。"
  },
  {
    "start": 829874,
    "end": 831910,
    "text": "言語モデル、それはトランスフォーマーモデルだ。"
  },
  {
    "start": 831980,
    "end": 834722,
    "text": "それはいくつかの出力埋め込みを生成する。"
  },
  {
    "start": 834786,
    "end": 836518,
    "text": "これらは隠れた状態と呼ばれる。"
  },
  {
    "start": 836684,
    "end": 840854,
    "text": "ご存知のように、入力はトークンであり、エンベッディングに変換される。"
  },
  {
    "start": 840982,
    "end": 842374,
    "text": "次に位置エンコード。"
  },
  {
    "start": 842422,
    "end": 844310,
    "text": "そしてそれをトランスレイヤーに送る。"
  },
  {
    "start": 844390,
    "end": 849340,
    "text": "変換層は、実際にいくつかの埋め込みを出力する。これは隠れ状態と呼ばれる。"
  },
  {
    "start": 849710,
    "end": 858426,
    "text": "通常、テキストを生成する場合、最後に隠れた状態を取り出し、それを線形層に送り、線形層がそれを語彙に投影する。"
  },
  {
    "start": 858458,
    "end": 861546,
    "text": "次にソフトマークを使い、次のトークンを選択する。"
  },
  {
    "start": 861658,
    "end": 875134,
    "text": "というのも、ここではテキストを生成するのではなく、単に数値の報酬を生成したいだけだからだ。"
  },
  {
    "start": 875182,
    "end": 894060,
    "text": "入力として入力エンベッディングを受け取り、出力として1つの値のみを生成するようにする。"
  },
  {
    "start": 894670,
    "end": 897562,
    "text": "もちろん、この種のモデルは、これがモデルのアーキテクチャだ。"
  },
  {
    "start": 897616,
    "end": 899002,
    "text": "それを訓練する必要もある。"
  },
  {
    "start": 899056,
    "end": 907690,
    "text": "また、このモデルには、選ばれた答えには高い報酬を、選ばれなかった答えには低い報酬を与えなければならないことを伝える必要がある。"
  },
  {
    "start": 907770,
    "end": 911534,
    "text": "では、このモデルを訓練するのに使う損失関数は何だろう。"
  },
  {
    "start": 911732,
    "end": 914882,
    "text": "使用する損失関数はこれだ。"
  },
  {
    "start": 914936,
    "end": 924900,
    "text": "良い答えに割り当てられた報酬のシグモイドの対数から、悪い答えに割り当てられた報酬を引いたものであることがわかるだろう。"
  },
  {
    "start": 925450,
    "end": 929462,
    "text": "さて、ここでこの損失関数を分析してみよう。"
  },
  {
    "start": 929516,
    "end": 931046,
    "text": "だから、ペンを。"
  },
  {
    "start": 931148,
    "end": 933938,
    "text": "なるほど、2つの可能性がある。"
  },
  {
    "start": 934034,
    "end": 942970,
    "text": "この差はマイナスであるか、プラスであるかのどちらかである。"
  },
  {
    "start": 943040,
    "end": 944346,
    "text": "どうやって鍛えるのか？"
  },
  {
    "start": 944368,
    "end": 949254,
    "text": "まず基本的に、私たちのデータセットは質問と可能な答えで構成されているからだ。"
  },
  {
    "start": 949302,
    "end": 950970,
    "text": "考えられる答えが2つしかないとする。"
  },
  {
    "start": 951040,
    "end": 953694,
    "text": "一方は良い方で、一方は悪い方だ。"
  },
  {
    "start": 953892,
    "end": 963950,
    "text": "私たちはそれぞれの質問の答えを受け取り、その答えを連結してモデルに送り、私たちは本物のモデルが何らかの報酬を生成する。"
  },
  {
    "start": 964370,
    "end": 971950,
    "text": "良い質問にも良い答えにも、そして悪い答えにもそうする。"
  },
  {
    "start": 972030,
    "end": 974834,
    "text": "これが善良な者への報酬だとしよう。"
  },
  {
    "start": 974872,
    "end": 980440,
    "text": "良いほうを書こう、悪いほうにはご褒美をあげよう。"
  },
  {
    "start": 981050,
    "end": 988054,
    "text": "さて、このモデルは良い方に高い報酬を、悪い方に低い報酬を割り当てた。"
  },
  {
    "start": 988092,
    "end": 991450,
    "text": "この違いはプラスに働くだろう。"
  },
  {
    "start": 991600,
    "end": 993642,
    "text": "この場合、損失はこのようになる。"
  },
  {
    "start": 993696,
    "end": 1002854,
    "text": "良い答えに与えられる報酬が悪い答えに与えられる報酬よりも高い場合、この差はプラスになる。"
  },
  {
    "start": 1002982,
    "end": 1004750,
    "text": "シグモイド関数を見てみよう。"
  },
  {
    "start": 1004820,
    "end": 1007162,
    "text": "入力がプラスの場合、どのような挙動を示すのか？"
  },
  {
    "start": 1007226,
    "end": 1013810,
    "text": "入力が正のとき、シグモイドは0.5と1の間の出力値を与える。"
  },
  {
    "start": 1013960,
    "end": 1022206,
    "text": "ログが入力を受けると、この値は0.5から1の間になる。"
  },
  {
    "start": 1022238,
    "end": 1025806,
    "text": "なぜなら、ここでは括弧があると考えることができるからだ。"
  },
  {
    "start": 1025918,
    "end": 1035640,
    "text": "ログが0.5から1の間の入力を見ると、ゼロからマイナス1の間の多かれ少なかれ負の数を生成する。"
  },
  {
    "start": 1036090,
    "end": 1042042,
    "text": "ここでマイナス記号を付けると、0と1の間の正の数になる。"
  },
  {
    "start": 1042096,
    "end": 1049340,
    "text": "この場合、損失はゼロと1の間の数値になるため、小さくなる。"
  },
  {
    "start": 1050350,
    "end": 1054622,
    "text": "もしかしたら2つか3つかもしれないけど、まあ、ログのグラフ次第だね。"
  },
  {
    "start": 1054676,
    "end": 1059760,
    "text": "0.5の正確な値は覚えていない。"
  },
  {
    "start": 1060610,
    "end": 1068334,
    "text": "しかし、モデルが悪い反応に高得点を与え、良い反応に低得点を与えたかどうかを見てみよう。"
  },
  {
    "start": 1068382,
    "end": 1069780,
    "text": "もう一度始めよう。"
  },
  {
    "start": 1070390,
    "end": 1080710,
    "text": "さて、これが悪い反応、そしてこれが良い反応だ。"
  },
  {
    "start": 1082010,
    "end": 1086822,
    "text": "では、この値がこの値より小さければどうなるか？"
  },
  {
    "start": 1086876,
    "end": 1088870,
    "text": "この差はマイナスになる。"
  },
  {
    "start": 1089030,
    "end": 1098300,
    "text": "シグモイドが負の入力を受けると、ゼロと0.5の間の出力を返す。"
  },
  {
    "start": 1098750,
    "end": 1110926,
    "text": "ゼロと0.5の間の入力を見たとき、ログは、多かれ少なかれ、ここではマイナス無限大と多かれ少なかれ1の間の負の数を返す。"
  },
  {
    "start": 1111108,
    "end": 1117806,
    "text": "ここでマイナス記号があるため、そのマイナス範囲では非常に大きな数字になる。"
  },
  {
    "start": 1117918,
    "end": 1120338,
    "text": "この場合の損失は大きい。"
  },
  {
    "start": 1120424,
    "end": 1129190,
    "text": "大敗した。"
  },
  {
    "start": 1129610,
    "end": 1142682,
    "text": "さて、おわかりのように、報酬モデルが良い答えに高い報酬を与え、悪い答えに悪い低い得点を与えている場合、損失は小さい。"
  },
  {
    "start": 1142816,
    "end": 1152480,
    "text": "しかし、報酬モデルが悪い答えに高い報酬を与え、良い答えに低い得点を与える場合、損失は非常に大きくなる。"
  },
  {
    "start": 1153730,
    "end": 1168654,
    "text": "モデルが損失を最小化する唯一の方法であるため、勝ちの反応には常に高い報酬を与え、負けの反応には低い報酬を与えるよう強制されるということは、モデルにとってどのような意味を持つのだろうか。"
  },
  {
    "start": 1168702,
    "end": 1171774,
    "text": "学習中のモデルの目標は常に損失を最小化することだからだ。"
  },
  {
    "start": 1171822,
    "end": 1179800,
    "text": "モデルは、選ばれた答えに高い報酬を与え、選ばれなかった答えや悪い答えには低い報酬を与えることを余儀なくされる。"
  },
  {
    "start": 1182090,
    "end": 1188310,
    "text": "抱きしめる段階では、この報酬モデルは報酬トレーナークラスに実装される。"
  },
  {
    "start": 1188460,
    "end": 1200442,
    "text": "独自の報酬モデルを訓練したい場合は、この報酬トレーナー・クラスを使う必要があり、同期分類の自動モデルを入力として受け取る。"
  },
  {
    "start": 1200496,
    "end": 1202250,
    "text": "トランスフォーマーモデルだ。"
  },
  {
    "start": 1202400,
    "end": 1210138,
    "text": "語彙に投影する線形層の代わりに、報酬を与える1つの出力特徴のみを持つ線形層を持つ。"
  },
  {
    "start": 1210314,
    "end": 1219586,
    "text": "ハギング・フェイス・ライブラリでこれがどのように実装されているかコードを見ると、まず選ばれた答えに対する報酬を生成していることがわかる。"
  },
  {
    "start": 1219688,
    "end": 1231254,
    "text": "正解の場合は報酬を、不正解の場合は報酬を、つまり不合格の場合は「不合格」と呼ばれる。"
  },
  {
    "start": 1231292,
    "end": 1238986,
    "text": "つまり、選ばれたほうに与えられる報酬の対数シグモイドから、拒否されたほうに与えられる報酬を引いたものである。"
  },
  {
    "start": 1239168,
    "end": 1241674,
    "text": "軌道の話をしよう。"
  },
  {
    "start": 1241872,
    "end": 1254846,
    "text": "さて、前にも述べたように、強化学習では、エージェントがその方針に従って行動したときに、エージェントの期待収益が最大になるような方針を選択すること、あるいは方針を最適化することが目標となる。"
  },
  {
    "start": 1254948,
    "end": 1267790,
    "text": "より正式には、エージェントがこの政策PIに従って行動したときに最大の期待報酬を与える政策PIを選択したい、と書くことができる。"
  },
  {
    "start": 1267950,
    "end": 1270098,
    "text": "さて、期待リターンはいくらか？"
  },
  {
    "start": 1270264,
    "end": 1279160,
    "text": "ポリシーの期待リターンは、エージェントがこのポリシーを使用したときにとりうるすべての軌道に対する期待リターンである。"
  },
  {
    "start": 1279530,
    "end": 1283778,
    "text": "これは、すべての可能な軌道における期待リターンである。"
  },
  {
    "start": 1283874,
    "end": 1286914,
    "text": "ご存知のように、期待値は積分として書くこともできる。"
  },
  {
    "start": 1286962,
    "end": 1295610,
    "text": "これは、特定の軌道がこのポリシーを使用する確率に、その特定の軌道のリターンを掛けたものである。"
  },
  {
    "start": 1295950,
    "end": 1298474,
    "text": "さて、まず軌道とは何か？"
  },
  {
    "start": 1298512,
    "end": 1301014,
    "text": "後ほど、軌道の確率とは何かを見てみよう。"
  },
  {
    "start": 1301062,
    "end": 1312398,
    "text": "つまり、軌跡とは一連の状態と行動のことであり、猫の場合、軌跡とは猫が通ることのできる道だと考えることができる。"
  },
  {
    "start": 1312564,
    "end": 1315694,
    "text": "それぞれの軌跡が最大長を持つとする。"
  },
  {
    "start": 1315742,
    "end": 1323006,
    "text": "エージェントがゴールに到着するまでに10以上のステップを踏ませたくない。"
  },
  {
    "start": 1323118,
    "end": 1332038,
    "text": "さて、猫は肉に行くことができる。例えば、ここのパーツを使ってもいいし、ここのパスを選んでもいいし、ここのパーツを使ってもいい。"
  },
  {
    "start": 1332124,
    "end": 1340742,
    "text": "例えば、前進して後戻りして止まる。すでに10ステップを使ったからだ。"
  },
  {
    "start": 1340806,
    "end": 1342678,
    "text": "道はたくさんある。"
  },
  {
    "start": 1342774,
    "end": 1354430,
    "text": "私たちが求めているのは、期待リターンを最大化する政策を見つけることです。"
  },
  {
    "start": 1355490,
    "end": 1363674,
    "text": "ここで、猫の次の状態も確率的であるとモデル化する。"
  },
  {
    "start": 1363722,
    "end": 1367022,
    "text": "そこでまず、これらの状態と行動とは何かを紹介しよう。"
  },
  {
    "start": 1367086,
    "end": 1368500,
    "text": "では、例を挙げよう。"
  },
  {
    "start": 1368870,
    "end": 1374530,
    "text": "猫がある状態sゼロからスタートしているとする。"
  },
  {
    "start": 1374680,
    "end": 1379222,
    "text": "方針は、その状態から次にとるべき行動を教えてくれる。"
  },
  {
    "start": 1379276,
    "end": 1383542,
    "text": "猫は政策に、次に取るべき行動は何かを問うだろう。"
  },
  {
    "start": 1383676,
    "end": 1389554,
    "text": "政策が確率的であるため、政策は次の行動の確率を教えてくれる。"
  },
  {
    "start": 1389682,
    "end": 1397110,
    "text": "つまり、言語モデルの場合と同じように、プロンプトが与えられたら、次のトークンの確率を選択する。"
  },
  {
    "start": 1397270,
    "end": 1401198,
    "text": "そこで、方針が「猫は下へ下がるべきだ」と指示したとしよう。"
  },
  {
    "start": 1401284,
    "end": 1408858,
    "text": "例えば、非常に高い確率で下に動くか、低い確率で右に動くはずである。"
  },
  {
    "start": 1408954,
    "end": 1414718,
    "text": "もっと低い確率で左に動くはずだし、もっと低い確率で上に動くはずだ。"
  },
  {
    "start": 1414894,
    "end": 1417122,
    "text": "仮に下降を選択したとしよう。"
  },
  {
    "start": 1417256,
    "end": 1422562,
    "text": "その結果、新たな国家が誕生する。"
  },
  {
    "start": 1422616,
    "end": 1422882,
    "text": "なぜですか？"
  },
  {
    "start": 1422936,
    "end": 1431942,
    "text": "というのも、私たちは猫が酔っ払っていることをモデルにしているからだ。つまり、猫は下に移動したがっているが、常に下に移動するとは限らない。"
  },
  {
    "start": 1432076,
    "end": 1434946,
    "text": "これがなぜ役に立つかは、後で説明する。"
  },
  {
    "start": 1435058,
    "end": 1436406,
    "text": "別のケースも考えられる。"
  },
  {
    "start": 1436428,
    "end": 1446890,
    "text": "例えば、ロボットがいて、ロボットは下に移動したいが、ロボットの車輪が壊れているので、ロボットは実際には下に移動せず、同じ状態のままだとする。"
  },
  {
    "start": 1446960,
    "end": 1457566,
    "text": "我々は常に、次の状態は決定論的に決定されるのではなく、確率論的に決定されるものとしてモデル化する。"
  },
  {
    "start": 1457748,
    "end": 1460942,
    "text": "そこで、ダウンというアクションを選択したとしよう。"
  },
  {
    "start": 1461076,
    "end": 1466726,
    "text": "猫は、ある確率分布に従った新しい状態、s1に到着するかもしれない。"
  },
  {
    "start": 1466858,
    "end": 1470114,
    "text": "では、次に私が取るべき行動は何か？"
  },
  {
    "start": 1470152,
    "end": 1480498,
    "text": "政策としては、非常に高い確率で右に動くべきであり、低い確率で下に動くべきであり、さらに低い確率で左に動くべきであり、等々。"
  },
  {
    "start": 1480594,
    "end": 1491210,
    "text": "つまり、おわかりのように、私たちは軌跡を作成しているのだ。軌跡とは、猫が特定の軌跡の中でどのように動くかを定義する、一連の状態とアクションのことである。"
  },
  {
    "start": 1493150,
    "end": 1497750,
    "text": "さて、では軌道の確率はどれくらいだろう？"
  },
  {
    "start": 1497830,
    "end": 1505934,
    "text": "軌跡の確率は、ここにあるように、特定の行動を選択したという事実は、私たちが置かれていた状態だけに依存する。"
  },
  {
    "start": 1506052,
    "end": 1512426,
    "text": "私たちがこのような状態になったのは、私たちがどのような状態にあり、どのような行動を選択したかにかかっている。"
  },
  {
    "start": 1512538,
    "end": 1518562,
    "text": "それなら、私たちがここでこの行動を選んだのは、私たちが置かれているこの状態によるものだ。"
  },
  {
    "start": 1518616,
    "end": 1524306,
    "text": "なぜなら、ポリシーは状態を入力とし、取るべき行動の確率を与えるだけだからだ。"
  },
  {
    "start": 1524488,
    "end": 1532850,
    "text": "これらの事象は互いに独立しているので、それらを掛け合わせて軌道の確率を求めることができる。"
  },
  {
    "start": 1532930,
    "end": 1537478,
    "text": "軌道の確率は、特定の出発点から出発する確率である。"
  },
  {
    "start": 1537564,
    "end": 1542154,
    "text": "この状態から、ここでゼロになり、その後、歩みを進めるごとにゼロになる。"
  },
  {
    "start": 1542192,
    "end": 1565620,
    "text": "この特定の軌跡の各行動状態について、ステップtの時点でこの状態にあり、ステップtの時点で行動80を選択したとすると、その状態で行動を選択し、新しい状態に到達する確率がある。"
  },
  {
    "start": 1566470,
    "end": 1573550,
    "text": "もうひとつ考えておきたいのは、軌道の報酬をどうやって計算するかということだ。"
  },
  {
    "start": 1573710,
    "end": 1579954,
    "text": "軌跡の報酬を計算する非常に簡単な方法は、その軌跡に沿って得られる報酬をすべて合計することである。"
  },
  {
    "start": 1580002,
    "end": 1583474,
    "text": "例えば、猫が肉にたどり着くまでの軌跡を想像してみよう。"
  },
  {
    "start": 1583602,
    "end": 1594294,
    "text": "ここでは報酬はゼロだから、ゼロ、ゼロと言えるが、肉に到達すると突然プラス100になる。"
  },
  {
    "start": 1594422,
    "end": 1600566,
    "text": "例えば、猫がこの道を通った場合、「よし、猫はほうきを怖がっているからマイナス1点だ。"
  },
  {
    "start": 1600598,
    "end": 1608362,
    "text": "しかし、実はこの方法は、軌跡の報酬を計算する方法ではない。"
  },
  {
    "start": 1608426,
    "end": 1615402,
    "text": "つまり、将来の報酬よりも即時の報酬を優先する。"
  },
  {
    "start": 1615546,
    "end": 1619682,
    "text": "その理由を直感的に理解してもらうために、まずお金の話をしよう。"
  },
  {
    "start": 1619736,
    "end": 1625954,
    "text": "つまり、私が今日あなたに1万ドルを渡すと、あなたはそれを1年後に受け取るよりも今日受け取る方を好む。"
  },
  {
    "start": 1625992,
    "end": 1626242,
    "text": "なぜですか？"
  },
  {
    "start": 1626296,
    "end": 1628942,
    "text": "万ドルを銀行に預けることができるからだ。"
  },
  {
    "start": 1629006,
    "end": 1630534,
    "text": "多少の関心は持たれるだろう。"
  },
  {
    "start": 1630652,
    "end": 1634006,
    "text": "年末には10,000ドルを超えることになる。"
  },
  {
    "start": 1634188,
    "end": 1637782,
    "text": "強化学習の場合、これは別のケースでも役に立つ。"
  },
  {
    "start": 1637836,
    "end": 1644358,
    "text": "例えば、猫がミートまで10歩、あるいは20歩しか歩けないとする。"
  },
  {
    "start": 1644454,
    "end": 1649242,
    "text": "だから、猫がミートに到着するひとつの方法は、このように直接肉に向かうことだ。"
  },
  {
    "start": 1649376,
    "end": 1650870,
    "text": "これはひとつの軌跡だ。"
  },
  {
    "start": 1650950,
    "end": 1653482,
    "text": "猫にとって別の方法はこうだ。"
  },
  {
    "start": 1653536,
    "end": 1657950,
    "text": "例えば、ここへ行って、ここへ行って、ここへ行って、ここへ行って、ここへ行って。"
  },
  {
    "start": 1658100,
    "end": 1664414,
    "text": "この場合、私たちは猫がこのような遠回りをするのではなく、直接肉に向かうことを好む。"
  },
  {
    "start": 1664462,
    "end": 1664722,
    "text": "なぜですか？"
  },
  {
    "start": 1664776,
    "end": 1668382,
    "text": "次の状態を確率的にモデル化したからだ。"
  },
  {
    "start": 1668526,
    "end": 1676660,
    "text": "ルートが長ければ長いほど、これらの障害に行き当たる確率は高くなる。"
  },
  {
    "start": 1677830,
    "end": 1681734,
    "text": "この場合、ルートは短い方がいい。"
  },
  {
    "start": 1681932,
    "end": 1692326,
    "text": "これは数学的な観点からも好都合である。"
  },
  {
    "start": 1692358,
    "end": 1704858,
    "text": "さて、無限級数は扱わないが、この級数の要素がどんどん小さくなっていけば、この級数は収束する可能性があるので役に立つ。"
  },
  {
    "start": 1704954,
    "end": 1710366,
    "text": "割引の場合の報酬の計算方法について、実際の例を挙げてみよう。"
  },
  {
    "start": 1710548,
    "end": 1716894,
    "text": "つまり、猫がここから出発して、この道をたどったとする。"
  },
  {
    "start": 1717022,
    "end": 1721762,
    "text": "この軌道の報酬を計算するには、次のようにする。"
  },
  {
    "start": 1721816,
    "end": 1729126,
    "text": "箒に到着するステップゼロの報酬にガンマの1乗を掛けたものである。"
  },
  {
    "start": 1729228,
    "end": 1732918,
    "text": "はガンマにマイナス1を掛けたものになる。"
  },
  {
    "start": 1733084,
    "end": 1738230,
    "text": "それなら、これらの報酬はすべて、合計されることはないだろう。"
  },
  {
    "start": 1738300,
    "end": 1751420,
    "text": "最後に、タイムスタンプ1234-5678で報酬がプラス100となり、ガンマの8乗に100を掛けたものになる。"
  },
  {
    "start": 1751790,
    "end": 1753994,
    "text": "ガンマが選ばれる。"
  },
  {
    "start": 1754042,
    "end": 1754590,
    "text": "通常はそうではない。"
  },
  {
    "start": 1754660,
    "end": 1757566,
    "text": "それは常にゼロと1の間にあるものだ。"
  },
  {
    "start": 1757668,
    "end": 1759550,
    "text": "は1より小さい数である。"
  },
  {
    "start": 1759620,
    "end": 1769266,
    "text": "つまり、この報酬はガンマの8乗で減衰するので、報酬に到達するまでの時間が長ければ長いほど小さくなる。"
  },
  {
    "start": 1769368,
    "end": 1772370,
    "text": "これが報酬割引の直感である。"
  },
  {
    "start": 1774310,
    "end": 1778690,
    "text": "さて、不思議に思うかもしれないが、猫の場合、軌道は理にかなっている。"
  },
  {
    "start": 1778760,
    "end": 1782226,
    "text": "私は、猫が肉にたどり着くために何らかの道をたどるのがわかる。"
  },
  {
    "start": 1782258,
    "end": 1784870,
    "text": "肉に辿り着くまでには、さまざまな経路がある。"
  },
  {
    "start": 1785370,
    "end": 1787942,
    "text": "猫の場合の軌道はわかっている。"
  },
  {
    "start": 1787996,
    "end": 1790938,
    "text": "言語モデルの場合の軌跡は？"
  },
  {
    "start": 1791104,
    "end": 1796778,
    "text": "さて、前に見たように、私たちには言語モデルそのものであるポリシーがある。"
  },
  {
    "start": 1796864,
    "end": 1800646,
    "text": "というのも、方針は、その状態が与えられたときに、次に何をすべきかを教えてくれるからだ。"
  },
  {
    "start": 1800678,
    "end": 1805390,
    "text": "言語モデルの場合、言語モデル自体がポリシーであることがわかる。"
  },
  {
    "start": 1805540,
    "end": 1820740,
    "text": "このポリシーを最適化し、前に見た嗜好のデータセットを使って、前に構築した報酬モデルに従って累積報酬を最大化するように次のトークンを選択したい。"
  },
  {
    "start": 1821190,
    "end": 1825790,
    "text": "また、言語モデルの場合、軌跡は一連の状態と行動である。"
  },
  {
    "start": 1825950,
    "end": 1826882,
    "text": "州とは？"
  },
  {
    "start": 1826936,
    "end": 1830290,
    "text": "言語モデルの場合、プロンプトは何ですか、アクションは何ですか？"
  },
  {
    "start": 1830370,
    "end": 1831858,
    "text": "次のトークンだ。"
  },
  {
    "start": 1831954,
    "end": 1835222,
    "text": "言語モデルについてこんな質問があったとしよう。"
  },
  {
    "start": 1835276,
    "end": 1836626,
    "text": "上海はどこ？"
  },
  {
    "start": 1836738,
    "end": 1839750,
    "text": "もちろん、言語モデルには、次のトークンは何か？"
  },
  {
    "start": 1840170,
    "end": 1842126,
    "text": "これが最初のプロンプトとなる。"
  },
  {
    "start": 1842178,
    "end": 1844122,
    "text": "言語モデルの初期状態。"
  },
  {
    "start": 1844256,
    "end": 1846374,
    "text": "次のトークンは何か？"
  },
  {
    "start": 1846422,
    "end": 1855806,
    "text": "そしてそれを言語モデルにフィードバックし、言語モデルの新しい状態にする。"
  },
  {
    "start": 1855988,
    "end": 1858954,
    "text": "次のトークンは何か？"
  },
  {
    "start": 1859082,
    "end": 1864382,
    "text": "例えば、isという単語になり、これが再び言語モデルの入力になる。"
  },
  {
    "start": 1864436,
    "end": 1869038,
    "text": "そして、言語モデルに、次のトークンは何か？"
  },
  {
    "start": 1869134,
    "end": 1875954,
    "text": "例えば、inというトークンを選び、これらのトークンを連結したものが言語モデルの新しい状態になる。"
  },
  {
    "start": 1875992,
    "end": 1881110,
    "text": "次のトークンは何か、などなど、答えが生成されるまで、言語モデルにもう一度尋ねる。"
  },
  {
    "start": 1881180,
    "end": 1889270,
    "text": "言語モデルの場合もそうですが、一連のプロンプトと選択したトークンの軌跡があります。"
  },
  {
    "start": 1889690,
    "end": 1901680,
    "text": "さて、我々の目標は言語モデルを最適化することなので、過去に構築した報酬モデルに従って累積報酬を最大化するような方針があるとしよう。"
  },
  {
    "start": 1902130,
    "end": 1905662,
    "text": "さて、より正式には、我々の目標はこうだ。"
  },
  {
    "start": 1905716,
    "end": 1914106,
    "text": "ここで最大化したいのは、言語モデルが生成しうるすべての可能な軌道に対する期待リターンである。"
  },
  {
    "start": 1914218,
    "end": 1919410,
    "text": "軌跡はプロンプトとネクストトークンの連続である。"
  },
  {
    "start": 1920390,
    "end": 1923646,
    "text": "さて、確率的勾配降下法を使う場合だ。"
  },
  {
    "start": 1923678,
    "end": 1930034,
    "text": "例えば、ニューラルネットワークを最適化しようとする場合、確率的勾配降下法を使う。"
  },
  {
    "start": 1930152,
    "end": 1940890,
    "text": "モデルのパラメータに対する損失関数の勾配を計算し、この勾配の方向に逆らうようにモデルのパラメータを変更する。"
  },
  {
    "start": 1941790,
    "end": 1949770,
    "text": "この損失関数を最小化するために、モデルのパラメータを最適化するために、勾配の方向に対して少しずつステップを踏んでいく。"
  },
  {
    "start": 1949920,
    "end": 1952970,
    "text": "私たちの場合、損失関数を最小化したいわけではない。"
  },
  {
    "start": 1953040,
    "end": 1956254,
    "text": "ここにある関数を最大化したい。"
  },
  {
    "start": 1956452,
    "end": 1961082,
    "text": "これは、最大化したい目的関数と考えることもできる。"
  },
  {
    "start": 1961146,
    "end": 1965134,
    "text": "勾配降下法を使う代わりに、勾配上昇法を使う。"
  },
  {
    "start": 1965262,
    "end": 1970340,
    "text": "両者の唯一の違いは、ここにマイナス記号がない代わりにプラス記号があることだ。"
  },
  {
    "start": 1970790,
    "end": 1976210,
    "text": "さて、このアルゴリズムは政策勾配最適化と呼ばれる。"
  },
  {
    "start": 1976550,
    "end": 1983030,
    "text": "ポイントは、目的関数のこの関数の勾配を計算する必要があるということだ。"
  },
  {
    "start": 1983100,
    "end": 1986902,
    "text": "モデルのパラメータに対する勾配は？"
  },
  {
    "start": 1986956,
    "end": 1997002,
    "text": "では、私たちの言語モデルは、モデルのパラメータに関して、すべての可能な軌道にわたって期待リターンの勾配は何ですか？"
  },
  {
    "start": 1997056,
    "end": 2009614,
    "text": "この勾配の式を見つけ、それを計算し、勾配上昇を使ってモデルのパラメータを最適化できるようにする必要がある。"
  },
  {
    "start": 2009812,
    "end": 2016180,
    "text": "では、この目的関数の勾配の式を導く方法を見てみよう。"
  },
  {
    "start": 2016630,
    "end": 2021870,
    "text": "さて、目的関数の勾配はこの期待値の勾配である。"
  },
  {
    "start": 2021950,
    "end": 2031154,
    "text": "これは、すべての可能な軌道に対する期待値に、特定の軌道に対するリターンを掛けたものである。"
  },
  {
    "start": 2031282,
    "end": 2033954,
    "text": "ご存知のように、期待値も積分である。"
  },
  {
    "start": 2034002,
    "end": 2043610,
    "text": "これは、特定の軌道をたどる確率の積分の勾配に、この軌道上のリターンを掛けたものとして書くことができる。"
  },
  {
    "start": 2044430,
    "end": 2054234,
    "text": "高校で習ったように、和の勾配は勾配の和に等しい。"
  },
  {
    "start": 2054362,
    "end": 2059994,
    "text": "関数の和の導関数は、導関数の和に等しい。"
  },
  {
    "start": 2060122,
    "end": 2065938,
    "text": "このグラデーション記号を内側に持ってくれば、次のように書くことができる。"
  },
  {
    "start": 2066104,
    "end": 2071214,
    "text": "では、対数微分のトリックを使ってこの式を展開してみよう。"
  },
  {
    "start": 2071262,
    "end": 2075910,
    "text": "タウのpは、セータをこの式に当てはめたものである。"
  },
  {
    "start": 2076060,
    "end": 2077880,
    "text": "その仕組みをお見せしよう。"
  },
  {
    "start": 2079210,
    "end": 2080680,
    "text": "ペンを使おう。"
  },
  {
    "start": 2081530,
    "end": 2082280,
    "text": "オーケー。"
  },
  {
    "start": 2082970,
    "end": 2100006,
    "text": "また、微積分学で、ある関数の対数関数（この場合はタウのp）のθに対する勾配が、θが与えられたときに等しいことを思い出すかもしれない。"
  },
  {
    "start": 2100048,
    "end": 2115090,
    "text": "対数関数の導関数の勾配は、θが与えられたタウの関数pに、対数の内側にある関数のθに関する勾配を掛けたものである。"
  },
  {
    "start": 2115160,
    "end": 2119358,
    "text": "つまり、シータが与えられたときのタウのpである。"
  },
  {
    "start": 2119534,
    "end": 2131026,
    "text": "この項を左辺に持っていき、ここに掛け合わせれば、この式はこの式にこの式を掛けたものに等しくなる。"
  },
  {
    "start": 2131058,
    "end": 2132790,
    "text": "これはまさに、ここにあるものだ。"
  },
  {
    "start": 2132860,
    "end": 2136722,
    "text": "という式に置き換えることができる。"
  },
  {
    "start": 2136786,
    "end": 2143180,
    "text": "この式をこの式に当てはめると、下式のようになる。"
  },
  {
    "start": 2143710,
    "end": 2153310,
    "text": "さて、この積分は、この量のすべての可能な軌道に対する期待値として書き戻すことができる。"
  },
  {
    "start": 2153380,
    "end": 2160510,
    "text": "さて、確率はこの項だけなので、期待値として書き戻すことができる。"
  },
  {
    "start": 2160930,
    "end": 2163626,
    "text": "さて、ここでこの用語を拡大する必要がある。"
  },
  {
    "start": 2163668,
    "end": 2165922,
    "text": "ログの勾配は？"
  },
  {
    "start": 2166056,
    "end": 2168594,
    "text": "この表現がここにある。"
  },
  {
    "start": 2168632,
    "end": 2175506,
    "text": "モデルのパラメータが与えられた場合、特定の軌道の確率の対数の勾配はどのようになるのか？"
  },
  {
    "start": 2175608,
    "end": 2176870,
    "text": "拡大しよう。"
  },
  {
    "start": 2177020,
    "end": 2186242,
    "text": "ある軌跡の確率は、その軌跡に含まれるすべての状態アクションの確率の積に過ぎないことは前に見た。"
  },
  {
    "start": 2186306,
    "end": 2209390,
    "text": "特定の状態から出発する確率と、特定の行動をとる確率を掛け合わせたもので、今いる状態が、タイムステップtの状態から出発し、タイムステップtで行動をとったとして、新しい状態に行き着く確率を掛け合わせたものである。"
  },
  {
    "start": 2209730,
    "end": 2216878,
    "text": "この式に対数を適用すると、積は和になる。"
  },
  {
    "start": 2217054,
    "end": 2219234,
    "text": "実際にやってみよう。"
  },
  {
    "start": 2219352,
    "end": 2234802,
    "text": "それでは、PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI PI"
  },
  {
    "start": 2234866,
    "end": 2238040,
    "text": "ここでシータを忘れてしまったが、それは問題ではない。"
  },
  {
    "start": 2238510,
    "end": 2243046,
    "text": "この式のすべての対数と等しい。"
  },
  {
    "start": 2243078,
    "end": 2244842,
    "text": "これは一連の製品のログである。"
  },
  {
    "start": 2244896,
    "end": 2292190,
    "text": "の対数pゼロに和を足したもの、つまりStのpの対数に1を足したものと書くことができる。Stにいるのはプラス80であってプラス80ではないし、80の行動をとったのはプラス80であり、Stにいると仮定したときの方針に従ってとった行動の対数である。"
  },
  {
    "start": 2293120,
    "end": 2296712,
    "text": "さて、今度はこの式の勾配をとる。"
  },
  {
    "start": 2296776,
    "end": 2303680,
    "text": "ここでわかるように、θに依存する項はないので、これも削除できる。"
  },
  {
    "start": 2303750,
    "end": 2311084,
    "text": "この場合、この式にはθに依存する表現はない。"
  },
  {
    "start": 2311132,
    "end": 2321152,
    "text": "なぜなら、導出される変数を持たないものの導出は定数だからである。"
  },
  {
    "start": 2321216,
    "end": 2323430,
    "text": "を削除することができる。"
  },
  {
    "start": 2323800,
    "end": 2331624,
    "text": "というのも、ここにあるように、シータを含むのはこの項だけだからである。"
  },
  {
    "start": 2331742,
    "end": 2334504,
    "text": "最後の表現はこれだ。"
  },
  {
    "start": 2334542,
    "end": 2337240,
    "text": "このまとめ、削除させてください。"
  },
  {
    "start": 2339180,
    "end": 2350408,
    "text": "なぜ目的関数の勾配が必要かというと、勾配上昇を実行したいからである。"
  },
  {
    "start": 2350584,
    "end": 2356400,
    "text": "さて、ここでひとつわかることは、可能なすべての軌道に対する期待値を持っているということだ。"
  },
  {
    "start": 2357140,
    "end": 2370016,
    "text": "さて、すべての可能な軌跡について計算するということは、猫の場合、例えば10歩の長さの猫がとりうるすべての軌跡についてこの勾配を計算する必要があるということだ。"
  },
  {
    "start": 2370048,
    "end": 2380248,
    "text": "長さが10しかない軌道をモデル化したいのであれば、長さが10の猫がとりうるすべての経路を計算する必要があり、それは膨大な数になる可能性がある。"
  },
  {
    "start": 2380334,
    "end": 2386440,
    "text": "言語モデルの場合はさらに大きくなる。通常、100サイズの軌跡を生成することを想像してほしい。"
  },
  {
    "start": 2386510,
    "end": 2394972,
    "text": "つまり、言語モデルを用いて、100個のトークンを生成することができるテキストには、どのようなものがあるのだろうか？"
  },
  {
    "start": 2395106,
    "end": 2401224,
    "text": "それぞれについて、報酬と対数行動確率を計算する必要がある。"
  },
  {
    "start": 2401352,
    "end": 2409840,
    "text": "さて、おわかりのように、問題はこの期待値が多くの項にわたっていることで、計算するのは困難である。"
  },
  {
    "start": 2409910,
    "end": 2416212,
    "text": "この式を計算するためには、言語モデルのために大量のテキストを生成する必要があるからだ。"
  },
  {
    "start": 2416346,
    "end": 2424260,
    "text": "そこで、この期待値を計算する一つの方法は、標本平均で近似することである。"
  },
  {
    "start": 2424410,
    "end": 2430096,
    "text": "我々は常に標本平均で期待値を近似することができる。"
  },
  {
    "start": 2430138,
    "end": 2435588,
    "text": "可能な軌道すべてについて計算する代わりに、いくつかの軌道について計算することができる。"
  },
  {
    "start": 2435684,
    "end": 2446984,
    "text": "つまり、ネコの場合、ネコを捕まえて、あるステップ数のポリシーを使って動くように要求し、1つの軌跡を生成するということだ。"
  },
  {
    "start": 2447032,
    "end": 2450296,
    "text": "何度もやることで、いくつかの軌道が生まれる。"
  },
  {
    "start": 2450408,
    "end": 2455424,
    "text": "言語モデルの場合、何らかのプロンプトがあり、言語モデルにテキストを生成するよう依頼する。"
  },
  {
    "start": 2455542,
    "end": 2459980,
    "text": "その後、異なる温度と異なるサンプリング戦略で何度も行う。"
  },
  {
    "start": 2460060,
    "end": 2466556,
    "text": "例えば、ランダムにサンプリングすることで、グリッド戦略の代わりにトップPを使うことができ、多くのテキストを生成することができる。"
  },
  {
    "start": 2466668,
    "end": 2468848,
    "text": "それぞれのテキストは軌跡を表している。"
  },
  {
    "start": 2468944,
    "end": 2474196,
    "text": "言語モデルが生成しうるすべてのテキストに対して行う必要はなく、一部のテキストに対して行えばよい。"
  },
  {
    "start": 2474298,
    "end": 2476848,
    "text": "つまり、いくつかの軌道を生成することになる。"
  },
  {
    "start": 2476944,
    "end": 2482468,
    "text": "ここでこの式を計算できるのは、言語モデルが生成するいくつかの軌跡のみである。"
  },
  {
    "start": 2482564,
    "end": 2487496,
    "text": "これでこの勾配の近似値が得られる。"
  },
  {
    "start": 2487678,
    "end": 2496270,
    "text": "この勾配を得たら、サンプリングした軌跡上でそれを評価し、勾配上昇を実行することができる。"
  },
  {
    "start": 2496880,
    "end": 2499116,
    "text": "つまり、現実的にはこうなる。"
  },
  {
    "start": 2499218,
    "end": 2512930,
    "text": "猫の場合、ある種のニューラルネットワークがポリシーを定義している。つまり、猫の状態、つまり猫の位置から、猫が次にとるべき行動の確率を割り出しているのだ。"
  },
  {
    "start": 2513300,
    "end": 2518320,
    "text": "最適化されていないこのポリシーを使って、いくつかの軌道を生成することができる。"
  },
  {
    "start": 2518400,
    "end": 2520452,
    "text": "例えば、ここからスタートする。"
  },
  {
    "start": 2520506,
    "end": 2522324,
    "text": "どこに行くべきか？"
  },
  {
    "start": 2522362,
    "end": 2528872,
    "text": "例えば、グリッド戦略を使って下に移動したり、トップPを使ったり。"
  },
  {
    "start": 2528926,
    "end": 2536772,
    "text": "この場合も、トップpを使って、ネットワークが生成した確率からランダムに行動をサンプリングすることができる。"
  },
  {
    "start": 2536916,
    "end": 2541320,
    "text": "猫が倒れたとしたら、どこに行くべきか？"
  },
  {
    "start": 2541390,
    "end": 2545528,
    "text": "方針は、オーケー、右へ、下へ、右へ、下へ、エトセトラ、エトセトラと言うかもしれない。"
  },
  {
    "start": 2545544,
    "end": 2547016,
    "text": "我々は1つの軌道を生成する。"
  },
  {
    "start": 2547128,
    "end": 2553090,
    "text": "ポリシーによって生成された確率に従って、常にランダムにサンプリングすることによって、何度もそれを行う。"
  },
  {
    "start": 2554100,
    "end": 2557292,
    "text": "それぞれの状態アクションに対して、多くの軌道を生成する。"
  },
  {
    "start": 2557356,
    "end": 2563836,
    "text": "この場合、各状態のアクションで蓄積される報酬もわかっているので、評価することができる。"
  },
  {
    "start": 2563868,
    "end": 2565460,
    "text": "報酬を計算する。"
  },
  {
    "start": 2565800,
    "end": 2576070,
    "text": "各状態について、その行動を取る確率が何％であったかを知っていて、それを選択するのだから。"
  },
  {
    "start": 2577160,
    "end": 2581252,
    "text": "これらの対数確率の勾配も計算する必要がある。"
  },
  {
    "start": 2581316,
    "end": 2585524,
    "text": "これは、後方へのロストを実行する際に、Pytorchによって自動的に行われる。"
  },
  {
    "start": 2585572,
    "end": 2588664,
    "text": "Pytorchは実際に勾配を計算してくれる。"
  },
  {
    "start": 2588862,
    "end": 2591832,
    "text": "他のすべての可能性のある軌道についてもそうする。"
  },
  {
    "start": 2591976,
    "end": 2606000,
    "text": "勾配上昇を実行し、勾配に向かうステップを使用してモデルのパラメータを最適化します。"
  },
  {
    "start": 2606500,
    "end": 2612720,
    "text": "もう一度、軌道を集める必要がある。"
  },
  {
    "start": 2614660,
    "end": 2617360,
    "text": "対数確率の勾配を評価する。"
  },
  {
    "start": 2617440,
    "end": 2624532,
    "text": "私たちは勾配のある上り坂を走るので、勾配のある方向に向かって少しずつ一歩を踏み出し、またそれを繰り返す。"
  },
  {
    "start": 2624586,
    "end": 2627092,
    "text": "もう一度行って、いくつかの軌道を集める。"
  },
  {
    "start": 2627236,
    "end": 2641304,
    "text": "ここでこの式を評価し、パラメータに対するポリシーの勾配を計算し、再び勾配上昇を実行することで、勾配の方向に向かって少しずつステップを踏んでいく。"
  },
  {
    "start": 2641432,
    "end": 2649020,
    "text": "これは文献的には強化学習アルゴリズムとして知られており、言語モデルの最適化にも利用できる。"
  },
  {
    "start": 2649170,
    "end": 2654268,
    "text": "言語モデルの場合は、軌跡も生成しなければならない。"
  },
  {
    "start": 2654444,
    "end": 2665684,
    "text": "軌跡を生成する一つの方法は、例えば、報酬モデルのために以前に構築した質問と回答のデータベースを使用することである。"
  },
  {
    "start": 2665802,
    "end": 2680264,
    "text": "私たちは言語モデルに、例えばトップpストラテジーを使って、それぞれの質問に対して何らかの答えを生成するように依頼します。"
  },
  {
    "start": 2680462,
    "end": 2702136,
    "text": "言語モデルの生成プロセスは、状態、つまり次のトークンであるプロンプトとアクションで構成される反復プロセスであるため、これは一連の軌跡となり、言語モデルが次のトークンに対する確率のリストを生成するため、ログ確率を持つ軌跡のリストとなる。"
  },
  {
    "start": 2702248,
    "end": 2712080,
    "text": "というのも、lost backwardを実行すると、Pytorchが勾配を計算してくれるからだ。"
  },
  {
    "start": 2712980,
    "end": 2714984,
    "text": "実際にはどうすればいいのか？"
  },
  {
    "start": 2715052,
    "end": 2719124,
    "text": "では、この項を計算してみよう。"
  },
  {
    "start": 2719162,
    "end": 2728932,
    "text": "つまり、特定のプロンプトが与えられたときに、次のトークンが出現する確率はどれくらいか？"
  },
  {
    "start": 2729076,
    "end": 2732948,
    "text": "言語モデルが次のような応答を生成したとしよう。"
  },
  {
    "start": 2733044,
    "end": 2735444,
    "text": "上海はどこですか？"
  },
  {
    "start": 2735492,
    "end": 2738640,
    "text": "言語モデルは上海が中国にあると言った。"
  },
  {
    "start": 2738820,
    "end": 2741116,
    "text": "我々の言語モデルはトランスフォーマーモデルである。"
  },
  {
    "start": 2741218,
    "end": 2748632,
    "text": "は変換層であり、与えられた埋込みの入力シーケンスを生成する。"
  },
  {
    "start": 2748696,
    "end": 2755840,
    "text": "各入力トークンに対して1つずつ、隠れ状態と呼ばれる埋め込みの出力シーケンスを生成する。"
  },
  {
    "start": 2756420,
    "end": 2765584,
    "text": "ご存知のように、言語モデルをテキスト生成に使用する場合、各ポジションのロジットを計算できる線形レイヤーがあります。"
  },
  {
    "start": 2765702,
    "end": 2771536,
    "text": "次のトークンが何であるかを理解したいので、通常は最後のトークンのロジットだけを計算する。"
  },
  {
    "start": 2771648,
    "end": 2774740,
    "text": "実際に、各ポジションのロジットを計算することができる。"
  },
  {
    "start": 2774890,
    "end": 2784452,
    "text": "例えば、この位置のロジットも計算することができ、この位置のロジットは、この入力が与えられたときに、次に最も可能性の高いトークンが何かを示す。"
  },
  {
    "start": 2784516,
    "end": 2786452,
    "text": "上海はどこ？"
  },
  {
    "start": 2786596,
    "end": 2787320,
    "text": "クエスチョンマーク？"
  },
  {
    "start": 2787390,
    "end": 2788570,
    "text": "上海はそうだ。"
  },
  {
    "start": 2788940,
    "end": 2794488,
    "text": "これは、私たちが自己注意メカニズムの間に適用している因果関係のマスクのせいである。"
  },
  {
    "start": 2794584,
    "end": 2799544,
    "text": "それぞれの隠された状態は、実際には現在のトークンに関する情報をカプセル化している。"
  },
  {
    "start": 2799592,
    "end": 2804008,
    "text": "この場合、トークンはisであり、それ以前のすべてのトークンも同様である。"
  },
  {
    "start": 2804104,
    "end": 2809008,
    "text": "これは、トレーニング中に使用されるトランスフォーマーモデルの言語のプロパティである。"
  },
  {
    "start": 2809174,
    "end": 2814752,
    "text": "トレーニング中、ご存知のように、言語モデルの出力を一歩一歩計算することはありません。"
  },
  {
    "start": 2814806,
    "end": 2820580,
    "text": "入力文と、入力文をシフトした出力文を与えるだけだ。"
  },
  {
    "start": 2823160,
    "end": 2827556,
    "text": "フォワードパスをしてから、フォワードパス1回だけでログを計算する。"
  },
  {
    "start": 2827658,
    "end": 2839524,
    "text": "同じメカニズムを使って、この軌跡の各状態とアクションの対数確率を計算することができる。"
  },
  {
    "start": 2839652,
    "end": 2845150,
    "text": "これで、この位置、この位置、この位置、この位置のロジックを計算することができる。"
  },
  {
    "start": 2845840,
    "end": 2851784,
    "text": "そして通常、ソフトマックスを適用して、次のトークンの確率を理解する。"
  },
  {
    "start": 2851832,
    "end": 2857280,
    "text": "この場合、対数確率が欲しいので、各ポジションに対数ソフトマックスを適用できる。"
  },
  {
    "start": 2857430,
    "end": 2864784,
    "text": "これは、現在のトークンと比較して、前のトークンのみが与えられた場合の次のトークンの対数確率を与える。"
  },
  {
    "start": 2864822,
    "end": 2873060,
    "text": "入力が上海のクエスチョンマークがどこにあるかだけだとすると、次のトークンの対数確率が得られる。"
  },
  {
    "start": 2873560,
    "end": 2876384,
    "text": "もちろん、すべての対数確率が欲しいわけではない。"
  },
  {
    "start": 2876432,
    "end": 2881492,
    "text": "私たちが欲しいのは、この軌跡の中で実際に選ばれたトークンの対数確率だけである。"
  },
  {
    "start": 2881556,
    "end": 2887016,
    "text": "この特別なポジションに選ばれた実際のトークンとは？"
  },
  {
    "start": 2887198,
    "end": 2889576,
    "text": "私たちはそれを知っている。"
  },
  {
    "start": 2889678,
    "end": 2893820,
    "text": "という単語に対応する対数確率のみを選択する。"
  },
  {
    "start": 2893970,
    "end": 2897512,
    "text": "これで軌跡全体の対数確率が返される。"
  },
  {
    "start": 2897576,
    "end": 2902280,
    "text": "上海という単語を選択する対数確率がわかったからだ。"
  },
  {
    "start": 2902360,
    "end": 2917804,
    "text": "where is shanghaiという入力があれば、isという単語を選択する対数確率がある。また、where is Shanghaiという入力があれば、inという単語を選択する対数確率がある。"
  },
  {
    "start": 2917852,
    "end": 2918360,
    "text": "などなど。"
  },
  {
    "start": 2918460,
    "end": 2927796,
    "text": "これで、この軌跡における各状態アクションの各位置の対数確率が得られた。"
  },
  {
    "start": 2927988,
    "end": 2943292,
    "text": "そして、それぞれの勾配に報酬モデルから得られる報酬を掛け合わせる。"
  },
  {
    "start": 2943426,
    "end": 2952568,
    "text": "そして、この式を計算し、勾配上昇を実行して、この近似勾配に基づいてポリシーを最適化することができる。"
  },
  {
    "start": 2952744,
    "end": 2956412,
    "text": "軌跡に対する報酬の計算方法を見てみよう。"
  },
  {
    "start": 2956556,
    "end": 2959088,
    "text": "報酬の計算も同様のプロセスだ。"
  },
  {
    "start": 2959174,
    "end": 2967072,
    "text": "前にも見たように、報酬モデルがある。これは、線形レイヤーを上に持つ変換モデルで、1つの出力特徴のみを持つ。"
  },
  {
    "start": 2967136,
    "end": 2969252,
    "text": "私たちの文章が同じであることを想像してほしい。"
  },
  {
    "start": 2969306,
    "end": 2971104,
    "text": "上海はどこ？"
  },
  {
    "start": 2971152,
    "end": 2972192,
    "text": "上海は中国にある。"
  },
  {
    "start": 2972256,
    "end": 2975556,
    "text": "これが言語モデルによって生成された軌跡である。"
  },
  {
    "start": 2975738,
    "end": 2977984,
    "text": "今度はそれを報酬モデルに与える。"
  },
  {
    "start": 2978042,
    "end": 2989924,
    "text": "報酬モデルは変換モデルであるため、いくつかの隠れ状態を生成する。この軌跡にあるアクションに対応するすべての位置に線形レイヤーを適用する。"
  },
  {
    "start": 2989972,
    "end": 2992172,
    "text": "最初のアクションは、この言葉の選択である。"
  },
  {
    "start": 2992226,
    "end": 2994920,
    "text": "2つ目のアクションがこれ、3つ目がこれ、4つ目がこれだ。"
  },
  {
    "start": 2995080,
    "end": 2997884,
    "text": "各時間ステップの報酬を生成することができる。"
  },
  {
    "start": 2998002,
    "end": 3002904,
    "text": "これらの報酬を合計すれば、軌跡の総報酬が算出される。"
  },
  {
    "start": 3002952,
    "end": 3007552,
    "text": "あるいは、割引後の報酬を合計することもできる。"
  },
  {
    "start": 3007606,
    "end": 3011344,
    "text": "例えば、計算してみよう。"
  },
  {
    "start": 3011382,
    "end": 3028368,
    "text": "ステップ0の時の報酬にガンマを乗じたものにステップ1の時の報酬にガンマの2乗を乗じたもの、ステップ2の時の報酬にガンマの3乗を乗じたものにステップ3の時の報酬を乗じたもの、などなど。"
  },
  {
    "start": 3028384,
    "end": 3032196,
    "text": "これで、それぞれの軌道に対する報酬の計算方法もわかった。"
  },
  {
    "start": 3032388,
    "end": 3036292,
    "text": "これでこの式の評価方法がわかった。"
  },
  {
    "start": 3036356,
    "end": 3037368,
    "text": "こちらをご覧いただきたい。"
  },
  {
    "start": 3037454,
    "end": 3042030,
    "text": "これで、言語モデルを最適化するために勾配上昇を実行する方法もわかった。"
  },
  {
    "start": 3042480,
    "end": 3055676,
    "text": "前に説明したアルゴリズムは、勾配政策最適化と呼ばれるもので、非常に小さな問題には問題なく機能するが、大きな問題、例えば言語モデリングなどには問題があり、完璧ではない。"
  },
  {
    "start": 3055868,
    "end": 3057760,
    "text": "問題は非常に単純だ。"
  },
  {
    "start": 3057830,
    "end": 3059788,
    "text": "問題は、私たちが近似値を出していることだ。"
  },
  {
    "start": 3059884,
    "end": 3062176,
    "text": "ここに何か書こう。"
  },
  {
    "start": 3062278,
    "end": 3082996,
    "text": "つまり、前に見たように、我々の目的関数はθのjであり、我々の方針と期待値に従ってサンプリングされたすべての可能な軌道に対する期待値であり、それぞれの軌道に沿って報酬がある。"
  },
  {
    "start": 3083108,
    "end": 3087752,
    "text": "は、期待値を標本平均で近似している。"
  },
  {
    "start": 3087806,
    "end": 3092072,
    "text": "この式は、すべての可能な軌道について計算するわけではない。"
  },
  {
    "start": 3092136,
    "end": 3095288,
    "text": "一部の軌道でのみ計算する。"
  },
  {
    "start": 3095464,
    "end": 3097724,
    "text": "さて、これはフェアだ。"
  },
  {
    "start": 3097842,
    "end": 3105040,
    "text": "つまり、得られる結果は、平均して真の期待値に収束する近似値になるということだ。"
  },
  {
    "start": 3105380,
    "end": 3111228,
    "text": "これは、長期的には真の期待値に収束するが、分散が大きいことを意味する。"
  },
  {
    "start": 3111404,
    "end": 3116052,
    "text": "では、その意味を直感的に理解してもらうために、もっと簡単な話をしよう。"
  },
  {
    "start": 3116186,
    "end": 3121824,
    "text": "例えば、私がアメリカの人口の平均年齢を計算するように頼んだとしよう。"
  },
  {
    "start": 3121952,
    "end": 3125830,
    "text": "現在、アメリカの人口は3億3,000万人である。"
  },
  {
    "start": 3126280,
    "end": 3141332,
    "text": "平均年齢を計算するということは、すべての人を訪ね、誕生日を聞いて年齢を計算し、集めた年齢をすべて合計して人数で割れば、アメリカ人の本当の平均年齢がわかるということである。"
  },
  {
    "start": 3141396,
    "end": 3147310,
    "text": "もちろん、おわかりのように、3億3,000万人にインタビューする必要があるので、これを計算するのは簡単ではない。"
  },
  {
    "start": 3147920,
    "end": 3152716,
    "text": "もう一つのアイデアは、すべてのアメリカ人のところに行くわけじゃない。"
  },
  {
    "start": 3152818,
    "end": 3162508,
    "text": "私は一部のアメリカ人だけを訪ね、彼らの平均年齢を計算する。"
  },
  {
    "start": 3162684,
    "end": 3172608,
    "text": "というのも、1人にしかインタビューしなければ、母集団全体を代表しているとは限らないからだ。"
  },
  {
    "start": 3172704,
    "end": 3176576,
    "text": "たとえ10人にインタビューしたとしても、それが全人口を代表しているとは限らない。"
  },
  {
    "start": 3176688,
    "end": 3178904,
    "text": "面接する人数は多ければ多いほどいい。"
  },
  {
    "start": 3179022,
    "end": 3184280,
    "text": "これは実は中心極限定理によって統計的に証明されている結果である。"
  },
  {
    "start": 3185180,
    "end": 3188852,
    "text": "では、この推定量の分散について説明しよう。"
  },
  {
    "start": 3188916,
    "end": 3192616,
    "text": "そこで、アメリカ人の平均年齢を算出したい。"
  },
  {
    "start": 3192728,
    "end": 3206732,
    "text": "仮に、アメリカ人の平均年齢が40歳とか45歳とか、標本平均を使って近似値を出したとしよう。"
  },
  {
    "start": 3206796,
    "end": 3208544,
    "text": "平均年齢は？"
  },
  {
    "start": 3208662,
    "end": 3211888,
    "text": "無作為に何人かをサンプリングして、その年齢を聞く必要がある。"
  },
  {
    "start": 3211974,
    "end": 3216932,
    "text": "仮に、時間がないので1人しかインタビューしないとしよう。"
  },
  {
    "start": 3217066,
    "end": 3225044,
    "text": "運悪く、その人がたまたま幼稚園の生徒だったとしよう。"
  },
  {
    "start": 3225162,
    "end": 3230120,
    "text": "となると、母集団の真の平均から大きくかけ離れた結果が得られることになる。"
  },
  {
    "start": 3230460,
    "end": 3237700,
    "text": "その一方で、無作為に何人かに尋ねてみることもできる。"
  },
  {
    "start": 3237780,
    "end": 3244600,
    "text": "例えば、80歳というような非常に高い数字が出るだろうが、これも真の人口を代表するものではない。"
  },
  {
    "start": 3244760,
    "end": 3252060,
    "text": "サンプルが小さければ小さいほど、真の平均値から大きくかけ離れた値を得るのは不運である。"
  },
  {
    "start": 3252210,
    "end": 3254928,
    "text": "一つの方法は、サンプル数を増やすことである。"
  },
  {
    "start": 3255014,
    "end": 3258464,
    "text": "1000人に平均年齢を聞くとしたら？"
  },
  {
    "start": 3258502,
    "end": 3272308,
    "text": "おそらく、この40歳に近いものが出てくるだろう。運悪く6人しか出なかったり、たまたま全員が幼稚園児だったり、定年退職して老人ホームに入ったりするようなことはありえないからだ。"
  },
  {
    "start": 3272474,
    "end": 3277880,
    "text": "これは標本平均で推定を近似するときにも起こる。"
  },
  {
    "start": 3277950,
    "end": 3285224,
    "text": "ここで、この近似の質は、いくつの軌道を選ぶかに依存する。"
  },
  {
    "start": 3285342,
    "end": 3296380,
    "text": "というのも、これらの軌跡を計算するために、言語モデルの推論を何度も実行する必要があるからだ。"
  },
  {
    "start": 3299760,
    "end": 3314812,
    "text": "問題は、軌跡の数を簡単に増やすことができないことだ。しかし、この分散を減らす方法を見つける必要がある。なぜなら、この分散は、勾配上昇を実行するために使用する勾配の方向を教えてくれるからだ。"
  },
  {
    "start": 3314956,
    "end": 3317532,
    "text": "グラデーションの真の方向を見つけたい。"
  },
  {
    "start": 3317596,
    "end": 3320212,
    "text": "本当のグラデーションの方向はこちらだと想像してほしい。"
  },
  {
    "start": 3320266,
    "end": 3330048,
    "text": "分散が大きいということは、この近似が、勾配が実際にはこの方向に向いている、あるいはこの方向に向いている、あるいはこの方向に向いていると教えてくれることがあるということだ。"
  },
  {
    "start": 3330144,
    "end": 3336756,
    "text": "分散を減らせば、より真の勾配の方向に近いことがわかるだろう。"
  },
  {
    "start": 3336788,
    "end": 3345864,
    "text": "というのも、勾配の真の方向に従って動いているからである。"
  },
  {
    "start": 3345912,
    "end": 3349580,
    "text": "これが、この推定量の分散を小さくしたい理由である。"
  },
  {
    "start": 3349920,
    "end": 3357170,
    "text": "それでは、標本サイズを大きくすることなく、この推定量の分散を小さくするためにどのようなテクニックがあるのか見てみましょう。"
  },
  {
    "start": 3361540,
    "end": 3370004,
    "text": "まず最初に気づくべきことは、標本平均を使って近似した期待値があるということだ。"
  },
  {
    "start": 3370042,
    "end": 3379292,
    "text": "それぞれの対数確率は、軌道全体の報酬と掛け合わされている。"
  },
  {
    "start": 3379456,
    "end": 3388484,
    "text": "さて、まず注意しなければならないのは、それぞれの行動は、前のステップで受け取った報酬を変えることはできないということだ。"
  },
  {
    "start": 3388532,
    "end": 3392100,
    "text": "つまり、一連のステートとアクションがあるとしよう。"
  },
  {
    "start": 3392180,
    "end": 3410588,
    "text": "例えば、私たちはゼロの状態からスタートし、アクション1を起こし、それがゼロのアクションにつながり、それが1の状態につながり、それが2の状態につながり、それが2のアクションにつながり......といった具合だ。"
  },
  {
    "start": 3410684,
    "end": 3420820,
    "text": "各状態の行動に対して、私たちは報酬を受け取る。私たちが行動を起こすと、例えば猫は新しいセルに移動するか、同じセルに留まり、何らかの報酬を受け取るからだ。"
  },
  {
    "start": 3421240,
    "end": 3423392,
    "text": "また、この一戦のために報酬を用意する。"
  },
  {
    "start": 3423456,
    "end": 3426948,
    "text": "1つ目の報酬、そして2つ目の報酬だ。"
  },
  {
    "start": 3427114,
    "end": 3434728,
    "text": "さて、私たちがこの行動をとるとき、たとえば2番目の行動をとるとき、過去にすでに受け取った報酬を変えることはできない。"
  },
  {
    "start": 3434894,
    "end": 3444472,
    "text": "このタウの報酬項を乗算するとき、この合計で考えているアクションの前に来たすべての報酬は考慮しない。"
  },
  {
    "start": 3444616,
    "end": 3457520,
    "text": "ゼロから始まる軌跡の報酬を計算する代わりに、アクションのタイムスタンプから始まる報酬を計算することで、アクションの対数確率を考慮することができる。"
  },
  {
    "start": 3458500,
    "end": 3463276,
    "text": "ここでのこの言葉は、「行くための報酬」として知られている。"
  },
  {
    "start": 3463388,
    "end": 3471780,
    "text": "この状態からスタートして、この行動をとり、残りの軌跡は方針に従って行動する。"
  },
  {
    "start": 3472280,
    "end": 3474004,
    "text": "なぜこんなことをしたいのか？"
  },
  {
    "start": 3474122,
    "end": 3483460,
    "text": "なぜなら、おわかりのように、この式は真の期待値の近似値だからだ。"
  },
  {
    "start": 3483540,
    "end": 3489560,
    "text": "ノイズが少なくなるので、用語は少なければ少ないほどいい。"
  },
  {
    "start": 3490220,
    "end": 3490872,
    "text": "なぜですか？"
  },
  {
    "start": 3491006,
    "end": 3505840,
    "text": "というのも、まず第一に、私たちが知っているように、それぞれの行動は過去に受け取った報酬を変えることはできない。"
  },
  {
    "start": 3505990,
    "end": 3516908,
    "text": "したがって、これらを考慮しなければ、この近似にノイズが加わり、真の勾配から離れた方向に勾配を送ることを避けることができる。"
  },
  {
    "start": 3517084,
    "end": 3531480,
    "text": "この式からいくつかの項を取り除くことができれば、勾配をこの期待値によって与えられるであろう真の勾配とはかけ離れた方向に送るノイズを導入する可能性が減るので、より良い。"
  },
  {
    "start": 3532060,
    "end": 3547832,
    "text": "まず、すべての軌跡に対して報酬を計算するのではなく、軌跡の終点に到達するまでの各状態アクションに対して、その状態アクションから始まる報酬のみを計算する。"
  },
  {
    "start": 3547896,
    "end": 3558080,
    "text": "このtは、大きなtで、大文字のtは、ここで考えている現在の状態アクションの時点から軌跡の終わりまでを示している。"
  },
  {
    "start": 3558900,
    "end": 3564796,
    "text": "さて、これは推定量の分散を減らす一つの方法である。"
  },
  {
    "start": 3564988,
    "end": 3578640,
    "text": "もうひとつの方法は、ベースラインを導入することだ。強化学習の研究で、ここに定数を導入すると分散が小さくなることが証明されている。"
  },
  {
    "start": 3578720,
    "end": 3583992,
    "text": "定数である必要はないが、状態によって変化するものでもある。"
  },
  {
    "start": 3584046,
    "end": 3590180,
    "text": "また、軌跡の報酬を計算する状態の関数である可能性もある。"
  },
  {
    "start": 3590260,
    "end": 3596604,
    "text": "各対数確率に対して、ここでは行くべき報酬を示す項を掛ける。"
  },
  {
    "start": 3596642,
    "end": 3601896,
    "text": "この状態アクションから軌跡の終わりまでの報酬からベースラインを引いたもの。"
  },
  {
    "start": 3602008,
    "end": 3607104,
    "text": "それは一定である必要はなく、州の関数であることもある。"
  },
  {
    "start": 3607302,
    "end": 3612128,
    "text": "私たちが選ぶ関数はバリュー関数と呼ばれる。"
  },
  {
    "start": 3612294,
    "end": 3617380,
    "text": "このベースラインには多くのものがあるが、私たちが選ぶのはバリュー・ファンクションである。"
  },
  {
    "start": 3617450,
    "end": 3622948,
    "text": "価値関数は、ある政策に従ったsを示す。"
  },
  {
    "start": 3623034,
    "end": 3633140,
    "text": "PIは、sからスタートし、残りの軌跡を方針に従って行動した場合に期待される報酬がいくらか教えてくれる。"
  },
  {
    "start": 3633220,
    "end": 3634696,
    "text": "これが価値関数だ。"
  },
  {
    "start": 3634878,
    "end": 3636760,
    "text": "いくつか例を挙げよう。"
  },
  {
    "start": 3637660,
    "end": 3645260,
    "text": "この特定のセル、このセルの価値関数が高いことを期待している。"
  },
  {
    "start": 3645330,
    "end": 3645660,
    "text": "なぜですか？"
  },
  {
    "start": 3645730,
    "end": 3652344,
    "text": "なぜなら、猫が行動を起こし、下に移動し、直接肉に向かう可能性が非常に高いからだ。"
  },
  {
    "start": 3652472,
    "end": 3663228,
    "text": "言語モデルの場合、これはプロンプトであり、次のトークンの確率を生成するために言語モデルに与える一連のトークンだからだ。"
  },
  {
    "start": 3663404,
    "end": 3665520,
    "text": "この状態であることはとてもいいことだ。"
  },
  {
    "start": 3665590,
    "end": 3665920,
    "text": "なぜですか？"
  },
  {
    "start": 3665990,
    "end": 3674336,
    "text": "というのも、次のトークンは、上海はどこにあるのかという問いに実際に答えられるような形で生成される可能性が非常に高いからだ。"
  },
  {
    "start": 3674448,
    "end": 3694604,
    "text": "モデルがすでにこの2つのトークンを生成している場合、たとえば、上海は、次のトークンはinという単語になり、次の次のトークンはchinaという単語になる可能性が非常に高い。"
  },
  {
    "start": 3694722,
    "end": 3702920,
    "text": "一方、例えば猫と一緒にいる場合、この状態はベッドのスタブに移動することにつながる。"
  },
  {
    "start": 3703000,
    "end": 3711340,
    "text": "というのも、ここからベッドの半券に行き着く可能性が低くなるからだ。"
  },
  {
    "start": 3711420,
    "end": 3717120,
    "text": "もしかしたら、私たちはベッドの半身に近づくかもしれないが、ベッドの浴槽に直接たどり着くことはない。"
  },
  {
    "start": 3717190,
    "end": 3719452,
    "text": "この状態の価値を下げることになる。"
  },
  {
    "start": 3719606,
    "end": 3722340,
    "text": "言語モデルの悪い値とは？"
  },
  {
    "start": 3722410,
    "end": 3725780,
    "text": "例えば、このプロンプトの場合。"
  },
  {
    "start": 3725850,
    "end": 3732964,
    "text": "という質問に対して、言語モデルはどういうわけか、チョコレートマフィンという2つの単語を生成した。"
  },
  {
    "start": 3733092,
    "end": 3736308,
    "text": "さて、言語モデルに次のトークンを生成するように頼むとしよう。"
  },
  {
    "start": 3736404,
    "end": 3742436,
    "text": "このプロンプトを見ると、上海はどこにあるのかという実際の反応とはかけ離れたものになるだろう。"
  },
  {
    "start": 3742468,
    "end": 3744824,
    "text": "上海が中国にあるとは教えてくれない。"
  },
  {
    "start": 3744952,
    "end": 3758288,
    "text": "というのも、報酬モデルによれば、報酬が低くなるような悪い反応を起こしてしまう可能性があるからだ。"
  },
  {
    "start": 3758374,
    "end": 3761712,
    "text": "これが価値関数、バリュー・ファンクションの意味である。"
  },
  {
    "start": 3761766,
    "end": 3769510,
    "text": "この状態からスタートし、方針に従って行動した場合、どの程度のリターンが期待できるのか教えてほしい。"
  },
  {
    "start": 3770840,
    "end": 3773910,
    "text": "さて、この価値関数をどのように推定するか？"
  },
  {
    "start": 3774280,
    "end": 3787000,
    "text": "報酬モデルでやったのと同じように、ニューラルネットワークを生成し、その上にこの価値関数を推定できる線形レイヤーを追加すればいい。"
  },
  {
    "start": 3787150,
    "end": 3793492,
    "text": "通常、実用的に行われるのは、最適化しようとしているのと同じ言語モデルを使うことだ。"
  },
  {
    "start": 3793556,
    "end": 3795692,
    "text": "その上にもうひとつリニアレイヤーを追加する。"
  },
  {
    "start": 3795746,
    "end": 3806008,
    "text": "ボキャブラリーに投影するものとは別に、もう1つ値を推定できるものを追加し、トランスフォーマー・レイヤーのパラメーターを共有する。"
  },
  {
    "start": 3806104,
    "end": 3811324,
    "text": "言語モデリングと値の推定では、2つの違いは線形レイヤーのみである。"
  },
  {
    "start": 3811372,
    "end": 3818764,
    "text": "ひとつはトークンを語彙に投影するために使われ、もうひとつはプロンプトである状態の値を推定するために使われる。"
  },
  {
    "start": 3818812,
    "end": 3819624,
    "text": "ブジだ。"
  },
  {
    "start": 3819772,
    "end": 3825664,
    "text": "私たちの言語モデルがプロンプトに対してこのような応答を生成したと仮定する。"
  },
  {
    "start": 3825712,
    "end": 3826528,
    "text": "上海はどこ？"
  },
  {
    "start": 3826544,
    "end": 3833752,
    "text": "言語モデルが上海は中国にあると言ったので、それを政策モデルに送る。"
  },
  {
    "start": 3833806,
    "end": 3837850,
    "text": "最適化しようとしている言語モデルをポリシーと呼ぶ。"
  },
  {
    "start": 3838940,
    "end": 3843180,
    "text": "それぞれのトークンに対応する隠しステートを生成する。"
  },
  {
    "start": 3843520,
    "end": 3857504,
    "text": "そして、各隠れ状態を語彙に投影するように語彙の線形層を使用する代わりに、各状態の値を推定するために使用される1つの出力特徴のみを持つ別の線形層を使用する。"
  },
  {
    "start": 3857622,
    "end": 3870230,
    "text": "この線形層によって生成された値を使用することで、この状態、この状態、この状態、そしてシーケンス全体の値を推定することができる。"
  },
  {
    "start": 3871560,
    "end": 3874724,
    "text": "さて、それは以前にも見たことがある。"
  },
  {
    "start": 3874842,
    "end": 3882392,
    "text": "分散を減らすために、まず、軌跡全体の報酬を行きに対する報酬に変換した。"
  },
  {
    "start": 3882446,
    "end": 3889450,
    "text": "つまり、tゼロからではなく、ここで考えているアクション状態と等しいtからスタートするものだ。"
  },
  {
    "start": 3889820,
    "end": 3898264,
    "text": "また、状態に依存するベースラインを導入しても、近似値は変わらないこともわかった。"
  },
  {
    "start": 3898312,
    "end": 3921492,
    "text": "つまり、例えばアメリカ人の平均年齢を計算する場合、年齢が非常に低くなったり、年齢が非常に高くなったりする可能性を減らすことができます。"
  },
  {
    "start": 3921546,
    "end": 3926240,
    "text": "そうすれば、よりアメリカの人口の平均年齢に近いものが得られるだろう。"
  },
  {
    "start": 3926400,
    "end": 3931072,
    "text": "さて、この機能、つまり「行く報酬」は強化の文献にある。"
  },
  {
    "start": 3931136,
    "end": 3932776,
    "text": "q関数とも呼ばれる。"
  },
  {
    "start": 3932878,
    "end": 3943384,
    "text": "q関数は、この状態からスタートしてこの行動をとった場合、残りの軌跡をその方針に従って行動した場合の将来の期待報酬がいくらになるかを教えてくれる。"
  },
  {
    "start": 3943512,
    "end": 3950696,
    "text": "q関数は、私がこの状態からスタートしてこの行動をとった場合の期待報酬を教えてくれる。"
  },
  {
    "start": 3950728,
    "end": 3956480,
    "text": "すぐに報酬を得て、残りの軌道はその方針に従って行動する。"
  },
  {
    "start": 3958500,
    "end": 3965984,
    "text": "を単純化すると、ステップtの時点における状態とアクションのqとなる。"
  },
  {
    "start": 3966022,
    "end": 3970756,
    "text": "ここでは、tからタイムステップtにおける状態の値を引いたものを忘れている。"
  },
  {
    "start": 3970938,
    "end": 3974630,
    "text": "この2つの違いはアドバンテージ機能として知られている。"
  },
  {
    "start": 3975160,
    "end": 3978992,
    "text": "さて、多くの用語や専門用語を紹介していることは承知している。"
  },
  {
    "start": 3979136,
    "end": 3982360,
    "text": "後で意味がわかるから我慢してくれ。"
  },
  {
    "start": 3982510,
    "end": 3985912,
    "text": "これで、すべての用語を覚える必要はなくなった。"
  },
  {
    "start": 3985966,
    "end": 3988436,
    "text": "私はこれらのコンセプトを何度も繰り返す。"
  },
  {
    "start": 3988628,
    "end": 4013868,
    "text": "そして、すべての軌跡に対して報酬を計算する代わりに、行動値を考慮する時間ステップから報酬だけを計算することができることがわかりました。"
  },
  {
    "start": 4014044,
    "end": 4019990,
    "text": "この2つの違いは、強化学習の文献ではアドバンテージ関数と呼ばれている。"
  },
  {
    "start": 4020440,
    "end": 4027604,
    "text": "アドバンテージ関数の式を見ると、まずこの2つの項を分析してみよう。"
  },
  {
    "start": 4027802,
    "end": 4028550,
    "text": "ペン"
  },
  {
    "start": 4029160,
    "end": 4033368,
    "text": "さて、q関数は期待リターンを教えてくれる。"
  },
  {
    "start": 4033454,
    "end": 4037688,
    "text": "時間ステップtで状態sからスタートする場合、アクションaを取る。"
  },
  {
    "start": 4037774,
    "end": 4046060,
    "text": "ここでもt、t、アクションt、t、そしてここでもtとtを忘れていた。"
  },
  {
    "start": 4046130,
    "end": 4046844,
    "text": "いいかい？"
  },
  {
    "start": 4047042,
    "end": 4057344,
    "text": "q関数は、私が状態tからスタートし、行動aをとり、その後、方針に従って行動した場合、期待リターンはいくらになるかを教えてくれる。"
  },
  {
    "start": 4057462,
    "end": 4066450,
    "text": "一方、価値関数は、私が状態sから出発し、方針に従って行動した場合、期待リターンはいくらかということを教えてくれる。"
  },
  {
    "start": 4067060,
    "end": 4071600,
    "text": "さて、この場合、例えばペンを使ってみよう。"
  },
  {
    "start": 4071670,
    "end": 4082840,
    "text": "この場合、この状態で、左へ行くより下へ行く方がいい。"
  },
  {
    "start": 4082910,
    "end": 4104332,
    "text": "この2つの項の差であるアドバンテージ項は、状態sで取ることができる平均的な行動と比較して、この特定の行動がどれだけ優れているかを教えてくれる。"
  },
  {
    "start": 4104386,
    "end": 4110380,
    "text": "この状態では、他のアクションのアドバンテージ関数よりも高くなる。"
  },
  {
    "start": 4110540,
    "end": 4119750,
    "text": "アドバンテージ関数は、私たちが考えているこのアクションが、この状態で持っている他のアクションと比べて平均よりどれだけ優れているかを教えてくれる。"
  },
  {
    "start": 4120360,
    "end": 4133960,
    "text": "この式全体を解釈するならば、各対数確率について、つまり特定の状態における各行動について、その優位性を掛け合わせたいということをモデルに伝えていることになる。"
  },
  {
    "start": 4135020,
    "end": 4145000,
    "text": "これは勾配なので、勾配上昇を使ってパラメーターを最適化する必要がある方向を示している。"
  },
  {
    "start": 4145080,
    "end": 4170512,
    "text": "基本的に、われわれがやっていることは、われわれの政策を強制的に押し上げることである。つまり、高い優位性を持つ行動、つまり平均よりも良いリターンをもたらす行動の可能性あるいは対数確率を高め、各状態において平均よりも低いリターンをもたらす行動の対数確率を押し下げることである。"
  },
  {
    "start": 4170656,
    "end": 4174128,
    "text": "つまり、例えば言語モデリングについて話そう。"
  },
  {
    "start": 4174224,
    "end": 4176944,
    "text": "上海はどこですか？"
  },
  {
    "start": 4176992,
    "end": 4180660,
    "text": "上海はどこ？"
  },
  {
    "start": 4182700,
    "end": 4183850,
    "text": "何が良いのか？"
  },
  {
    "start": 4184220,
    "end": 4188360,
    "text": "クエスチョンマーク、取るべき良い行動とは？"
  },
  {
    "start": 4188430,
    "end": 4191896,
    "text": "次に選ぶべきトークンは？"
  },
  {
    "start": 4192078,
    "end": 4204800,
    "text": "まあ、チョコレートから始めると、平均より悪い報酬が得られることは分かっている。"
  },
  {
    "start": 4204950,
    "end": 4216812,
    "text": "なぜなら、次のトークンは中国にある上海だからである。"
  },
  {
    "start": 4216876,
    "end": 4222596,
    "text": "その結果、我々の報酬モデルによって良い報酬が与えられる。"
  },
  {
    "start": 4222698,
    "end": 4228960,
    "text": "このプロンプトが表示されると、モデルは上海という単語を選択する可能性が高くなる。"
  },
  {
    "start": 4229040,
    "end": 4232252,
    "text": "このアドバンテージをどう解釈するかである。"
  },
  {
    "start": 4232416,
    "end": 4253708,
    "text": "基本的に、われわれがやろうとしていることは、与えられた状態に対して、報酬モデルに従って平均より良い報酬をもたらす行動の対数確率を押し上げ、報酬モデルに従って平均より低い報酬をもたらす行動の対数確率を押し下げようとしているのだ。"
  },
  {
    "start": 4253874,
    "end": 4257104,
    "text": "では、このアドバンテージの見積もり方を見てみよう。"
  },
  {
    "start": 4257222,
    "end": 4261116,
    "text": "まず、アドバンテージの表現についてもう一度書いておこう。"
  },
  {
    "start": 4261148,
    "end": 4262192,
    "text": "ペンを使おう。"
  },
  {
    "start": 4262246,
    "end": 4267280,
    "text": "先ほど見たように、タイムステップtにおけるアドバンテージ項である。"
  },
  {
    "start": 4267350,
    "end": 4286490,
    "text": "状態sから開始し、ステップtのアクションをとることは、ステップtの時点のq関数、ステップtの時点のアクションaからステップtの時点の値を引いたものに等しい。"
  },
  {
    "start": 4288220,
    "end": 4289592,
    "text": "q関数とは何ですか？"
  },
  {
    "start": 4289646,
    "end": 4317484,
    "text": "q関数は、状態sからスタートして、方針に従って行動aをとり、残りの軌跡を方針に従って行動した場合、期待リターンはいくらになるかを教えてくれる。一方、価値関数は、状態sからスタートして、方針に従って行動した場合、期待リターンはいくらになるかを教えてくれる。つまり、軌跡を想像してほしい。"
  },
  {
    "start": 4317612,
    "end": 4320520,
    "text": "軌跡とは、状態と方向性のリストのことだ。"
  },
  {
    "start": 4320620,
    "end": 4328916,
    "text": "ゼロの状態、ゼロの行動があり、これには何らかの報酬が伴う。"
  },
  {
    "start": 4329018,
    "end": 4333832,
    "text": "これは、私たちを状態1へと導くものであり、その中で私たちはおそらくアクション1を起こすだろう。"
  },
  {
    "start": 4333966,
    "end": 4337608,
    "text": "これには何らかの報酬が伴う。"
  },
  {
    "start": 4337774,
    "end": 4339592,
    "text": "これで別の州に行くことになる。"
  },
  {
    "start": 4339646,
    "end": 4355164,
    "text": "例えば、状態2、アクション、これは我々がアクション2を取ることであり、これには何らかの報酬が関連付けられ、これが報酬2である。次に状態3、これは我々がアクション3を取ることであり、これには何らかの報酬が関連付けられ、これが報酬3である。"
  },
  {
    "start": 4355202,
    "end": 4356556,
    "text": "エトセトラ、エトセトラ、エトセトラ、エトセトラ。"
  },
  {
    "start": 4356588,
    "end": 4362268,
    "text": "残りの軌道については、このアドバンテージ項をどのように推定できるかを理解しよう。"
  },
  {
    "start": 4362364,
    "end": 4373536,
    "text": "価値関数を推定するために、最適化しようとしている言語モデルであるポリシー・ネットワークの上に線形ヘッドであるネブラル・ネットワークを構築することができることも、前に見た。"
  },
  {
    "start": 4373728,
    "end": 4386760,
    "text": "線形ネットワーク、つまり隠れ状態を語彙に投影する線形層を使う代わりに、特定の状態の値関数を推定できる出力特徴を1つだけ持つ別の特別な線形層を使うことができる。"
  },
  {
    "start": 4386910,
    "end": 4394204,
    "text": "後ほど、このバリュー・ヘッドを訓練するために、どの損失関数を使う必要があるかについても説明する。"
  },
  {
    "start": 4394402,
    "end": 4397528,
    "text": "では、このアドバンテージの推定に集中しよう。"
  },
  {
    "start": 4397624,
    "end": 4399320,
    "text": "さて、軌道があるとしよう。"
  },
  {
    "start": 4399400,
    "end": 4402252,
    "text": "このアドバンテージは次のように見積もることができる。"
  },
  {
    "start": 4402306,
    "end": 4407136,
    "text": "ご存知のように、アドバンテージ項はq関数を示している。"
  },
  {
    "start": 4407318,
    "end": 4417648,
    "text": "これは、タイムステップtにおける状態sとアクションaが与えられたときのq関数で、次のように計算できる。"
  },
  {
    "start": 4417664,
    "end": 4431720,
    "text": "もし状態sからスタートすれば、いくらかの報酬を受け取ることになる。"
  },
  {
    "start": 4431790,
    "end": 4462370,
    "text": "ステップtで状態sからスタートし、この状態で行動tをとり、その後ポリシーに従って行動する場合、軌跡について持っているこれらの項をすべて合計するか、あるいは、状態ゼロからスタートして行動ゼロをとれば、すぐに報酬が得られるだろう、それはこの報酬と、残りの報酬を価値関数で近似したものである。"
  },
  {
    "start": 4462740,
    "end": 4471844,
    "text": "あるいは、今すぐ削除することもできる。"
  },
  {
    "start": 4471882,
    "end": 4481316,
    "text": "あるいは、アドバンテージの項、つまり、私が時間ステップtで状態sからスタートし、行動を起こした場合、tは次のように近似することもできる。"
  },
  {
    "start": 4481348,
    "end": 4488964,
    "text": "すぐに得られる報酬と、次の状態に移行するための報酬、そして残りの軌跡がある。"
  },
  {
    "start": 4489012,
    "end": 4493560,
    "text": "タイムステップtに2を加えた値関数で近似する。"
  },
  {
    "start": 4493630,
    "end": 4494830,
    "text": "の2人だ。"
  },
  {
    "start": 4495280,
    "end": 4497532,
    "text": "これこそ、私たちがここでやっていることだ。"
  },
  {
    "start": 4497666,
    "end": 4501884,
    "text": "また、ここで見たガンマ・パラメーターで割り引いている。"
  },
  {
    "start": 4501922,
    "end": 4504616,
    "text": "将来の報酬を割り引きたい。"
  },
  {
    "start": 4504808,
    "end": 4511570,
    "text": "このマイナスVは、アドバンテージ・タームの公式がこのマイナス値関数を持っているからにほかならない。"
  },
  {
    "start": 4512020,
    "end": 4520244,
    "text": "3項でも4項でも5項でも、好きなようにやって、あとはバリュー・ファンクションだけでカットできる。"
  },
  {
    "start": 4520442,
    "end": 4522404,
    "text": "さて、なぜこんなことをしたいのか？"
  },
  {
    "start": 4522522,
    "end": 4524070,
    "text": "いくつか削除させてください。"
  },
  {
    "start": 4524840,
    "end": 4527092,
    "text": "よし、早々にやめよう。"
  },
  {
    "start": 4527146,
    "end": 4534680,
    "text": "というのは、軌道の大部分を値関数で近似しているからである。"
  },
  {
    "start": 4534750,
    "end": 4548888,
    "text": "これは高いバイアスを示すことを意味し、値関数で軌道の大部分を近似しているため、このアドバンテージの推定値はあまり正しくない。"
  },
  {
    "start": 4549064,
    "end": 4561328,
    "text": "あるいは、この近似を改善するために、実際に得られた軌跡からさらに報酬を導入し、軌跡のほんの一部だけを価値関数で近似することもできる。"
  },
  {
    "start": 4561494,
    "end": 4570790,
    "text": "あるいは、得られた報酬で軌道のすべてを近似し、値頭では近似を使わないこともできる。"
  },
  {
    "start": 4571480,
    "end": 4576240,
    "text": "より多くの用語を使えば、分散が大きくなる。"
  },
  {
    "start": 4576400,
    "end": 4582392,
    "text": "もし使用する項数が少なければ、近似が多くなるため、バイアスが高くなる。"
  },
  {
    "start": 4582526,
    "end": 4600988,
    "text": "そこで、このバイアス分散の問題を解決するために、一般化されたアドバンテージ推定を使うことができます。これは基本的に、この項、この項、この項のすべての加重和をとり、それぞれに減衰パラメータλを掛けたものです。"
  },
  {
    "start": 4601154,
    "end": 4611810,
    "text": "つまり、基本的には、各時間ステップtにおけるアドバンテージを、時間ステップt＋1における将来のアドバンテージから計算することができる再帰式となる。"
  },
  {
    "start": 4612180,
    "end": 4613836,
    "text": "この公式を使ってみよう。"
  },
  {
    "start": 4613868,
    "end": 4617580,
    "text": "例えば、一連の状態と行動である軌跡があるとする。"
  },
  {
    "start": 4617660,
    "end": 4622804,
    "text": "ゼロの状態でゼロのアクションをすれば、報酬はゼロになる。"
  },
  {
    "start": 4622922,
    "end": 4630052,
    "text": "その結果、別の状態s1が発生し、そこでは行動1を起こし、何らかの報酬1を得ることになる。"
  },
  {
    "start": 4630186,
    "end": 4637588,
    "text": "その結果、新たな状態「2」が生まれ、そこで私たちは「2」の行動をとり、それが「3」の状態へとつながり、そこで私たちは「3」の行動をとる、エトセトラ、エトセトラ、エトセトラ。"
  },
  {
    "start": 4637684,
    "end": 4643950,
    "text": "これには3つ、これには2つ、これには3つの報酬が与えられる。"
  },
  {
    "start": 4644400,
    "end": 4646264,
    "text": "アドバンテージを計算してみよう。"
  },
  {
    "start": 4646312,
    "end": 4660912,
    "text": "例えば、タイムステップ3でのアドバンテージは、軌道の最後の項であるため、タイムステップtでのデルタに、タイムステップ4でのガンマにラムダを掛けたものに等しい。"
  },
  {
    "start": 4660966,
    "end": 4664540,
    "text": "は時間ステップ4を持っていないので、この項は存在しない。"
  },
  {
    "start": 4664620,
    "end": 4675940,
    "text": "デルタ3は、タイムステップtのリターンにタイムステップ4の値関数を乗じたガンマを足したものに等しい。"
  },
  {
    "start": 4676090,
    "end": 4685016,
    "text": "なぜなら、4から3の値を引いた状態は存在しないからである。"
  },
  {
    "start": 4685198,
    "end": 4692380,
    "text": "これはステップ3でのアドバンテージの推定を物語っている。"
  },
  {
    "start": 4692450,
    "end": 4709650,
    "text": "そして、これを用いて時間ステップ2でのアドバンテージ推定値（a 2）を計算すると、ガンマデルタ2＋ラムダガンマラムダa 3に等しくなる。"
  },
  {
    "start": 4710340,
    "end": 4711456,
    "text": "デルタ2とは何か？"
  },
  {
    "start": 4711478,
    "end": 4726624,
    "text": "デルタ2は、ステップ2で得た報酬にガンマを足したものに、状態3の値から状態2の値を引いたものを掛けたものである。"
  },
  {
    "start": 4726672,
    "end": 4730560,
    "text": "各項のアドバンテージ推定を再帰的に計算することができる。"
  },
  {
    "start": 4730640,
    "end": 4732644,
    "text": "なぜアドバンテージを計算する必要があるのか？"
  },
  {
    "start": 4732692,
    "end": 4741480,
    "text": "というのも、この利点はグラディエントの計算式にあり、グラディエント・アセントを実行する必要があるからだ。"
  },
  {
    "start": 4742620,
    "end": 4744964,
    "text": "多くのコンセプトを紹介したことは承知している。"
  },
  {
    "start": 4745012,
    "end": 4746412,
    "text": "バリュー関数を紹介した。"
  },
  {
    "start": 4746466,
    "end": 4749230,
    "text": "q関数とadvantage関数を紹介した。"
  },
  {
    "start": 4749600,
    "end": 4757932,
    "text": "また、なぜ私たちがこのような計算をしているのか、その理由もよくわからないかもしれない。"
  },
  {
    "start": 4758066,
    "end": 4759276,
    "text": "ご容赦いただきたい。"
  },
  {
    "start": 4759298,
    "end": 4765772,
    "text": "さて、覚えておかなければならないことがたくさんあるのは承知しているが、コードを見るときには、これらのスライドをすべて見返すことにしよう。"
  },
  {
    "start": 4765836,
    "end": 4766610,
    "text": "今のところはね。"
  },
  {
    "start": 4768660,
    "end": 4774180,
    "text": "これらの公式を作ったのは、後で戻ったときに、より意味がわかるようにするためだ。"
  },
  {
    "start": 4774330,
    "end": 4777012,
    "text": "また、今後、もし望むなら、このビデオを見直してほしい。"
  },
  {
    "start": 4777066,
    "end": 4790010,
    "text": "数式を理解するためにコードを見る必要はない。このビデオを一度理解すれば、あとは興味のある部分を見直すだけで、より明確になるからだ。"
  },
  {
    "start": 4791180,
    "end": 4794652,
    "text": "では、言語モデルにはどんな利点があるのか見てみよう。"
  },
  {
    "start": 4794706,
    "end": 4804976,
    "text": "さっきの例と同じように、各対数確率にアドバンテージ関数を掛け合わせた勾配の式がある。"
  },
  {
    "start": 4805078,
    "end": 4807410,
    "text": "また、ここでTを忘れていた。"
  },
  {
    "start": 4807860,
    "end": 4809376,
    "text": "Tを忘れた。"
  },
  {
    "start": 4809398,
    "end": 4811040,
    "text": "後でスライドを修正します。"
  },
  {
    "start": 4811620,
    "end": 4817024,
    "text": "さて、前にも見たように、上海はどこにあるのか？"
  },
  {
    "start": 4817072,
    "end": 4829380,
    "text": "言語モデルがshanghaiという単語を選択すると、おそらくこれは次のトークンを生成するために言語モデルに供給される新しい状態になる。"
  },
  {
    "start": 4830680,
    "end": 4850508,
    "text": "上海という最初の選択肢は良い答えにつながります。なぜなら、次のトークンは、例えば、上海は中国にあるというフレーズになるように選択される可能性が高いからです。"
  },
  {
    "start": 4850594,
    "end": 4856604,
    "text": "私たちの報酬モデルは、このような回答に対して良い報酬を与える。"
  },
  {
    "start": 4856722,
    "end": 4864192,
    "text": "というのも、この状態は、報酬モデルによって良い報酬が得られる将来の状態につながるからである。"
  },
  {
    "start": 4864326,
    "end": 4879876,
    "text": "しかし、この質問の後に、言語モデルがたまたまchocolateという単語をlexトークンとして選択した場合、この新しい状態は、私たちが見つけようとしている答えにあまり近くない新しいトークンが選択されることにつながります。"
  },
  {
    "start": 4880058,
    "end": 4886376,
    "text": "これでは反応が悪いので、報酬モデルからの報酬は低くなる。"
  },
  {
    "start": 4886478,
    "end": 4897608,
    "text": "言語モデルの場合、shanghaiという単語の対数確率を押し上げようとしている。"
  },
  {
    "start": 4897704,
    "end": 4905260,
    "text": "が上海であるとき、チョコレートという単語の対数確率を押し下げるか？"
  },
  {
    "start": 4905340,
    "end": 4911548,
    "text": "なぜなら、チョコレートという言葉を選ぶメリットよりも、上海を選ぶメリットの方が高いからだ。"
  },
  {
    "start": 4911644,
    "end": 4919520,
    "text": "このプロンプトを考えると、言語モデルのアドバンテージ推定をどう解釈するかということになる。"
  },
  {
    "start": 4919680,
    "end": 4925300,
    "text": "政策勾配最適化のもう一つの問題は、サンプリングを行っていることだ。"
  },
  {
    "start": 4925370,
    "end": 4929376,
    "text": "ご存知のように、政策勾配最適化ではアルゴリズムはこうだ。"
  },
  {
    "start": 4929418,
    "end": 4930584,
    "text": "我々は言語モデルを持っている。"
  },
  {
    "start": 4930622,
    "end": 4933432,
    "text": "この言語モデルからいくつかの軌跡をサンプリングする。"
  },
  {
    "start": 4933566,
    "end": 4937492,
    "text": "これらの軌道に関連する報酬を計算する。"
  },
  {
    "start": 4937556,
    "end": 4941540,
    "text": "これらの軌道に関連する利点を計算する。"
  },
  {
    "start": 4941700,
    "end": 4945336,
    "text": "これらの軌跡に関連する対数確率を計算する。"
  },
  {
    "start": 4945448,
    "end": 4960064,
    "text": "そして、このすべての情報を使って、勾配の方向、つまりモデルのパラメータに対する期待報酬の勾配を計算することができる。"
  },
  {
    "start": 4960182,
    "end": 4966080,
    "text": "そして、勾配の方向に従ってモデルのパラメータを最適化するために勾配上昇を実行することができる。"
  },
  {
    "start": 4966900,
    "end": 4971212,
    "text": "これは成分降下でも使われるプロセスだ。"
  },
  {
    "start": 4971276,
    "end": 4973668,
    "text": "成分降下法を使えば、損失関数ができる。"
  },
  {
    "start": 4973754,
    "end": 4982164,
    "text": "モデルのパラメータに対する損失関数の勾配を計算し、勾配の方向に従ってモデルの準備パラメータを最適化する。"
  },
  {
    "start": 4982292,
    "end": 4984648,
    "text": "このプロセスを何度も何度も繰り返す。"
  },
  {
    "start": 4984734,
    "end": 4985032,
    "text": "なぜですか？"
  },
  {
    "start": 4985086,
    "end": 4992968,
    "text": "なぜなら、学習率アルファに従って、勾配の方向に対して少しずつステップを踏んでいくからだ。"
  },
  {
    "start": 4993064,
    "end": 5002920,
    "text": "さて、問題は、この勾配上昇の各ステップで、言語モデルから軌跡をサンプリングしていることだ。"
  },
  {
    "start": 5003000,
    "end": 5013500,
    "text": "この最適化プロセスの各ステップでは、多くの軌道をサンプリングし、多くの利点を計算し、多くの報酬を計算し、多くの対数確率を計算する必要がある。"
  },
  {
    "start": 5013660,
    "end": 5020672,
    "text": "なぜなら、勾配降下法では小さなステップしか踏まないからだ。"
  },
  {
    "start": 5020736,
    "end": 5033556,
    "text": "このような小さなステップのひとつひとつに多くの計算が必要であり、多くの異なるモデルでこれらすべての前進ステップを実行することはできないため、計算はほとんど不可能に近い。"
  },
  {
    "start": 5033588,
    "end": 5036948,
    "text": "価値、利点、報酬などを計算する。"
  },
  {
    "start": 5037044,
    "end": 5038712,
    "text": "もっといい方法を見つける必要がある。"
  },
  {
    "start": 5038846,
    "end": 5046860,
    "text": "覚えているように、私たちが見つけたこの勾配の公式は、期待値の近似である。"
  },
  {
    "start": 5047360,
    "end": 5051880,
    "text": "確率論には、重要なサンプリングというものがある。"
  },
  {
    "start": 5051960,
    "end": 5064160,
    "text": "そのため、ある分布に関する期待値を評価する際には、前の分布とは異なる別の分布に関する期待値を計算する。"
  },
  {
    "start": 5064310,
    "end": 5071476,
    "text": "修正する限り、期待値内の関数にここで追加の項を掛ける。"
  },
  {
    "start": 5071578,
    "end": 5074452,
    "text": "では、その意味を理解しよう。"
  },
  {
    "start": 5074586,
    "end": 5077200,
    "text": "この期待値を計算するとしよう。"
  },
  {
    "start": 5077280,
    "end": 5093720,
    "text": "言語モデルの最適化、あるいは勾配ポリシーの最適化の場合、θでパラメータ化されたポリシーに従って、可能なすべての軌道に対する勾配を計算していることを思い出してほしい。"
  },
  {
    "start": 5093880,
    "end": 5094620,
    "text": "何の？"
  },
  {
    "start": 5094690,
    "end": 5100764,
    "text": "ここではそれぞれの軌道の報酬について。"
  },
  {
    "start": 5100882,
    "end": 5110556,
    "text": "この場合、xは、ポリシーθPIθからサンプリングされた軌道であると考えることができる。"
  },
  {
    "start": 5110668,
    "end": 5114012,
    "text": "これは各シータの報酬となり得る。"
  },
  {
    "start": 5114156,
    "end": 5133912,
    "text": "さて、ご存知のように、期待値は、期待値の各項目の確率にxの関数fを掛けたものの積分として書くことができる。"
  },
  {
    "start": 5133966,
    "end": 5139428,
    "text": "掛け算の結果を変えることなく、掛け算に常に1を掛けることができる。"
  },
  {
    "start": 5139524,
    "end": 5144156,
    "text": "この分数の上下に同じ量、つまり数字の1を掛けているのである。"
  },
  {
    "start": 5144178,
    "end": 5145390,
    "text": "私たちならできる。"
  },
  {
    "start": 5146160,
    "end": 5155872,
    "text": "そして、基本的にxのpをxのqで割るように項を並べ替えることができる。"
  },
  {
    "start": 5155926,
    "end": 5158188,
    "text": "ここでの用語はディストリビューションである。"
  },
  {
    "start": 5158284,
    "end": 5162400,
    "text": "これは別の分布であり、別の分布の確率密度関数なのだ。"
  },
  {
    "start": 5162980,
    "end": 5168660,
    "text": "であれば、この積分を期待値に戻すことができる。"
  },
  {
    "start": 5168810,
    "end": 5182228,
    "text": "ここで、期待値を分布qからのサンプルとして書き、xのfにこの追加項を掛けた関数に関して計算することができる。"
  },
  {
    "start": 5182404,
    "end": 5198620,
    "text": "これは、ここで初期期待値を計算するために、期待値を計算したい分布からサンプリングする代わりに、各項目にこの追加係数を掛ける限り、別の分布からサンプリングできることを意味する。"
  },
  {
    "start": 5198770,
    "end": 5209820,
    "text": "勾配ポリシーの最適化の式でも同じことができる。ここでは、あるポリシーからサンプリングしている。"
  },
  {
    "start": 5209980,
    "end": 5218592,
    "text": "重要なサンプリングを使って、別のネヴァラルネットワークからサンプリングすることで、これを修正することができる。"
  },
  {
    "start": 5218656,
    "end": 5229364,
    "text": "しかし、サンプリングの軌跡は、いくつかの質問からテキストを生成することを意味するからだ。"
  },
  {
    "start": 5229482,
    "end": 5234884,
    "text": "実際には、ネブラルネットワークと各アイテムからサンプリングしているんだ。"
  },
  {
    "start": 5234932,
    "end": 5245452,
    "text": "このアドバンテージの各項は、最適化しようとしているネットワークに従った確率によってのみ乗算されるのではなく、xのqによっても除算される。"
  },
  {
    "start": 5245506,
    "end": 5261010,
    "text": "サンプリングしている分布の対数確率を、サンプリングしている分布をオフラインPI、最適化しようとしている分布をオンラインPIと呼ぶことにする。"
  },
  {
    "start": 5261700,
    "end": 5265824,
    "text": "どのように機能するか、図解で例を挙げよう。"
  },
  {
    "start": 5265942,
    "end": 5276790,
    "text": "とりあえず、重要なサンプリングを使えば、別のネットワークを最適化しながら、別のネットワークからサンプリングしてこの期待値を計算できることを覚えておいてほしい。"
  },
  {
    "start": 5277240,
    "end": 5278292,
    "text": "こんな感じだ。"
  },
  {
    "start": 5278346,
    "end": 5281844,
    "text": "これは強化学習の文献ではオフポリシー学習と呼ばれている。"
  },
  {
    "start": 5281972,
    "end": 5292136,
    "text": "言語モデルがあり、それをオフライン・データと呼ばれるパラメータでパラメータ化し、オフライン・ポリシーと呼ぶとする。"
  },
  {
    "start": 5292318,
    "end": 5294052,
    "text": "いくつかの軌道をサンプリングする。"
  },
  {
    "start": 5294116,
    "end": 5294836,
    "text": "どういう意味ですか？"
  },
  {
    "start": 5294878,
    "end": 5300664,
    "text": "例えば、「上海はどこですか？"
  },
  {
    "start": 5300712,
    "end": 5306352,
    "text": "例えば、高温を使って多くの答えを生成するように言語モデルに依頼する。"
  },
  {
    "start": 5306486,
    "end": 5310524,
    "text": "そして、生成された軌道に対する報酬を計算する。"
  },
  {
    "start": 5310572,
    "end": 5313996,
    "text": "すべてのステート・アクション・ペアについて利点を計算する。"
  },
  {
    "start": 5314028,
    "end": 5324484,
    "text": "これらの状態アクション・ペアの対数確率を計算し、オンライン・ポリシーと呼ばれる別のモデルを最適化する。"
  },
  {
    "start": 5324602,
    "end": 5333944,
    "text": "オフライン・ポリシーからサンプリングしたすべての軌道を、データベースかメモリに保存し、そこに保管する。"
  },
  {
    "start": 5334062,
    "end": 5344748,
    "text": "そして、このデータベースかメモリーから軌道のミニバッチを取り出し、この式を計算する。"
  },
  {
    "start": 5344834,
    "end": 5349180,
    "text": "オンライン・モデルに従って対数確率を計算することができる。"
  },
  {
    "start": 5349250,
    "end": 5360252,
    "text": "このメモリからサンプリングされた軌跡に対して、オンライン・ポリシーに従って再びアドバンテージ項を計算することもできる。"
  },
  {
    "start": 5360396,
    "end": 5363744,
    "text": "また、計算することもできる。後ほど、その方法をコードで紹介しよう。"
  },
  {
    "start": 5363862,
    "end": 5367520,
    "text": "また、オンライン・ポリシーに従って、有利な期間を計算することもできる。"
  },
  {
    "start": 5367590,
    "end": 5371008,
    "text": "また、オンライン・ポリシーに従って報酬を計算することもできる。"
  },
  {
    "start": 5371184,
    "end": 5387700,
    "text": "そして、この式だけに基づいて勾配上昇を実行し、このオンライン・ポリシーを最適化する。それを数エポック、つまり軌跡の大きなメモリからサンプリングする数回のミニバッチで実行する。"
  },
  {
    "start": 5387860,
    "end": 5397128,
    "text": "しばらくすると、オンライン・ポリシーのパラメータとオフライン・ポリシーのパラメータを等しく設定し、ループを再開する。"
  },
  {
    "start": 5397224,
    "end": 5401688,
    "text": "いくつかの軌跡をサンプリングし、それをメモリに保存する。"
  },
  {
    "start": 5401864,
    "end": 5405452,
    "text": "その後、いくつかのエポックについて、いくつかの軌道をサンプリングする。"
  },
  {
    "start": 5405516,
    "end": 5409490,
    "text": "ここから、オンライン・ポリシーに関する対数確率を計算する。"
  },
  {
    "start": 5410020,
    "end": 5416060,
    "text": "この式は、勾配上昇法で最適化するために必要である。"
  },
  {
    "start": 5416140,
    "end": 5420310,
    "text": "そしてしばらくして、オフライン・ポリシーをオンライン・ポリシーと同じに設定する。"
  },
  {
    "start": 5420760,
    "end": 5429312,
    "text": "今、この2つは異なるネットワーク、ニューラルネットワークのように見えるが、実は同じネボラネットワークで、最初にサンプリングしたのはネブラルネットワークだ。"
  },
  {
    "start": 5429376,
    "end": 5437160,
    "text": "サンプリングした軌跡を記憶しておき、その軌跡からニューラルネットワークを最適化する。"
  },
  {
    "start": 5437740,
    "end": 5440472,
    "text": "しばらくしたら、またこの作業をする。"
  },
  {
    "start": 5440606,
    "end": 5445260,
    "text": "これは視覚化するのが容易でないことは承知しているので、後でコードで見てみよう。"
  },
  {
    "start": 5445330,
    "end": 5458444,
    "text": "重要なのは、最適化しようとしているネットワークから、最適化しようとしているポリシーを毎回サンプリングすることなく、勾配上昇を複数回実行する方法を見つけたということだ。"
  },
  {
    "start": 5458572,
    "end": 5469376,
    "text": "一度サンプリングして、その軌跡をメモリに保存し、いくつかのステップでネットワークを最適化し、いくつかのサブステップで最適化した後、新しい軌跡をサンプリングすることができる。"
  },
  {
    "start": 5469488,
    "end": 5472848,
    "text": "登り勾配の一歩一歩にそれをする必要はない。"
  },
  {
    "start": 5472944,
    "end": 5481690,
    "text": "これによって、この政策勾配アルゴリズムの計算が扱いやすくなった。そうでなければ、遅すぎて実行できなかったからだ。"
  },
  {
    "start": 5483100,
    "end": 5485592,
    "text": "これがコード上のやり方だ。"
  },
  {
    "start": 5485646,
    "end": 5490856,
    "text": "このオフライン・ポリシーを実行するためのコードも作成した。"
  },
  {
    "start": 5490958,
    "end": 5494190,
    "text": "トレーニングしたいモデルがあるとする。"
  },
  {
    "start": 5496000,
    "end": 5500076,
    "text": "よし、これを使おう。"
  },
  {
    "start": 5500178,
    "end": 5502812,
    "text": "よし、とりあえず凍結モデルは無視してくれ。"
  },
  {
    "start": 5502866,
    "end": 5503884,
    "text": "私たちはそれを使わない。"
  },
  {
    "start": 5503922,
    "end": 5508316,
    "text": "勾配上昇を使って訓練したいニューラルネットワークがある。"
  },
  {
    "start": 5508348,
    "end": 5512076,
    "text": "勾配上昇を使って最適化したいポリシーがある。"
  },
  {
    "start": 5512268,
    "end": 5525232,
    "text": "この方針からいくつかの軌道をサンプリングし、各軌道について対数確率、報酬、利点、KLダイバージェンスなどを計算してメモリに保存する。"
  },
  {
    "start": 5525376,
    "end": 5527664,
    "text": "後ほど、なぜKLダイバージェンスが必要なのかを説明する。"
  },
  {
    "start": 5527712,
    "end": 5529270,
    "text": "とりあえず、無視してください。"
  },
  {
    "start": 5531340,
    "end": 5535816,
    "text": "そして、私たちが見たこれらの軌道からいくつかのミニバッチをサンプリングする。"
  },
  {
    "start": 5535918,
    "end": 5542216,
    "text": "損失を計算したPPOアルゴリズムを実行する。"
  },
  {
    "start": 5542398,
    "end": 5549004,
    "text": "ロス・バックワードを使って勾配を計算し、オプティマイザ・ステップを実行するが、再度サンプリングする必要はない。"
  },
  {
    "start": 5549042,
    "end": 5552776,
    "text": "すでに保存した軌跡から別のサンプルを取るだけだ。"
  },
  {
    "start": 5552888,
    "end": 5559692,
    "text": "指定したステップ数に達するまで、また別のステップの勾配上昇を行い、さらに、etc...を繰り返す。"
  },
  {
    "start": 5559836,
    "end": 5569068,
    "text": "何ステップかモデルを最適化した後、新しい軌道をサンプリングし、何ステップも最適化のループを繰り返すことができる。"
  },
  {
    "start": 5569164,
    "end": 5573280,
    "text": "勾配上昇のステップごとに新しい軌道をサンプリングする必要はない。"
  },
  {
    "start": 5573360,
    "end": 5577540,
    "text": "一度サンプリングし、何段階もの勾配上昇を行い、そしてまたサンプリングする。"
  },
  {
    "start": 5577610,
    "end": 5580020,
    "text": "何段階もの勾配上昇を行い、そしてまたサンプリングする。"
  },
  {
    "start": 5580090,
    "end": 5582356,
    "text": "これにより、トレーニングのスピードが格段に速くなる。"
  },
  {
    "start": 5582548,
    "end": 5583556,
    "text": "わかった、約束するよ。"
  },
  {
    "start": 5583588,
    "end": 5586104,
    "text": "これが、これから見る最後の数式群である。"
  },
  {
    "start": 5586142,
    "end": 5588360,
    "text": "これでようやくPPOの負けが確定したわけだ。"
  },
  {
    "start": 5589580,
    "end": 5591464,
    "text": "それを理解しよう。"
  },
  {
    "start": 5591502,
    "end": 5599132,
    "text": "というわけで、これまで見てきたことを踏まえれば、まず最初に見るべきことは、この用語は以前見たものとまったく同じだということだ。"
  },
  {
    "start": 5599266,
    "end": 5607760,
    "text": "つまり、最適化しようとしているポリシーによる対数確率を、サンプリングしたポリシーの対数確率で割ったものがある。"
  },
  {
    "start": 5607830,
    "end": 5612076,
    "text": "ここのオフライン・ポリシーは、なぜこんなに醜いのか分からない。"
  },
  {
    "start": 5612268,
    "end": 5617200,
    "text": "の対数確率があり、これをオンライン・ポリシーと呼ぶ。"
  },
  {
    "start": 5617270,
    "end": 5619360,
    "text": "私たちが最適化しようとしている政策。"
  },
  {
    "start": 5619440,
    "end": 5620870,
    "text": "だから、オンラインと呼ぼう。"
  },
  {
    "start": 5622600,
    "end": 5626292,
    "text": "これは、サンプリングしたポリシーに従った対数確率である。"
  },
  {
    "start": 5626346,
    "end": 5628704,
    "text": "そこで、この政策からいくつかの軌道をサンプリングしてみた。"
  },
  {
    "start": 5628842,
    "end": 5630490,
    "text": "これがオフラインのポリシーだ。"
  },
  {
    "start": 5633020,
    "end": 5638040,
    "text": "となると、それぞれのアクション・ステート・ペアに乗算されるこのアドバンテージ項がある。"
  },
  {
    "start": 5638380,
    "end": 5643612,
    "text": "ここでは、この式と別の式の最小値を計算している。"
  },
  {
    "start": 5643666,
    "end": 5648108,
    "text": "なぜ対数確率を切り取ったのか？"
  },
  {
    "start": 5648274,
    "end": 5650172,
    "text": "まず、クリップ機能とは何ですか？"
  },
  {
    "start": 5650226,
    "end": 5661036,
    "text": "クリップ関数は、この式が1＋イプシロンより大きければ、1＋イプシロンにクリップする。"
  },
  {
    "start": 5661148,
    "end": 5667620,
    "text": "この式が1マイナスεより小さい場合は、1マイナスεに切り取られる。"
  },
  {
    "start": 5668520,
    "end": 5669780,
    "text": "なぜこれを望むのか？"
  },
  {
    "start": 5669850,
    "end": 5677510,
    "text": "まず、この言葉を解釈してみよう。"
  },
  {
    "start": 5679400,
    "end": 5681892,
    "text": "2つの対数確率の比。"
  },
  {
    "start": 5682036,
    "end": 5689780,
    "text": "サンプリングするポリシーがあり、最適化するポリシーがある。"
  },
  {
    "start": 5689940,
    "end": 5705890,
    "text": "つまり、最適化しているポリシーの対数確率が、サンプリングしたものに比べて、特定のアクションに対してはるかに高い場合、将来そのアクションを選択する可能性を高めようとしていることを意味する。"
  },
  {
    "start": 5706740,
    "end": 5715744,
    "text": "この増加はあまり大きくしたくないので、この値で最大にクリップしたい。"
  },
  {
    "start": 5715942,
    "end": 5729152,
    "text": "一方、ある行動の可能性を以前より減らそうとする場合、あまり大きく減らしたいとは思わない。"
  },
  {
    "start": 5729306,
    "end": 5734772,
    "text": "つまり、最適化のステップでは、行動確率を動かしていることになる。"
  },
  {
    "start": 5734836,
    "end": 5744204,
    "text": "特定のプロンプトが与えられたときに特定のトークンを選択する確率は、連続的に変化させるが、あまり変化させたくない。"
  },
  {
    "start": 5744322,
    "end": 5746760,
    "text": "小さな一歩を踏み出したい。"
  },
  {
    "start": 5746920,
    "end": 5747436,
    "text": "なぜですか？"
  },
  {
    "start": 5747538,
    "end": 5762048,
    "text": "というのも、あまりに動かしすぎると、モデルが他の選択肢を十分に検討しない可能性があるからだ。"
  },
  {
    "start": 5762134,
    "end": 5766112,
    "text": "モデルは、実際にその特定のアクションを最適化しすぎている可能性がある。"
  },
  {
    "start": 5766166,
    "end": 5770448,
    "text": "常にそのアクションを避けるかもしれないし、常にそのアクションを使うかもしれない。"
  },
  {
    "start": 5770544,
    "end": 5773172,
    "text": "この場合は、少しずつやっていきたい。"
  },
  {
    "start": 5773226,
    "end": 5782756,
    "text": "モデルには、特定のアクションを少しずつ増やしたり、特定のアクションの対数確率を少しずつ減らしたりしてもらいたい。"
  },
  {
    "start": 5782948,
    "end": 5784324,
    "text": "なぜ行動の話をするのか？"
  },
  {
    "start": 5784372,
    "end": 5792724,
    "text": "なぜなら、私たちは言語モデルについて話しているのだから、プロンプトが与えられたときに特定のトークンを選択する確率を増減させたいのだ。"
  },
  {
    "start": 5792852,
    "end": 5795660,
    "text": "この確率をあまり変えたくない。"
  },
  {
    "start": 5795730,
    "end": 5798572,
    "text": "そのため、ここでは最低ラインを設けている。"
  },
  {
    "start": 5798626,
    "end": 5803132,
    "text": "できる限り悲観的なアップデートをしたい。"
  },
  {
    "start": 5803186,
    "end": 5804792,
    "text": "楽観的になりすぎないようにしたい。"
  },
  {
    "start": 5804856,
    "end": 5807964,
    "text": "モデルには最も楽観的なステップを踏ませたくない。"
  },
  {
    "start": 5808002,
    "end": 5813504,
    "text": "もしモデルがこのトークンを常に選択できると確信しているのであれば、私たちはモデルが確信することを望まない。"
  },
  {
    "start": 5813542,
    "end": 5818340,
    "text": "私たちは、モデルがより良い選択だと思うことに向かって、少しずつステップを踏んでもらいたい。"
  },
  {
    "start": 5819320,
    "end": 5825156,
    "text": "前に紹介したもうひとつのヘッドは、価値関数を計算するためのヘッドだ。"
  },
  {
    "start": 5825258,
    "end": 5828564,
    "text": "それで、覚えているように、このバリュー・ファンクションも紹介した。"
  },
  {
    "start": 5828682,
    "end": 5840616,
    "text": "この価値関数は、状態の関数であり、その特定の状態から出発することによって受け取ることができる期待報酬が何であるかを示していると言う。"
  },
  {
    "start": 5840798,
    "end": 5847032,
    "text": "例えば、「上海はどこですか？"
  },
  {
    "start": 5847176,
    "end": 5849660,
    "text": "上海はどこ？"
  },
  {
    "start": 5852320,
    "end": 5860368,
    "text": "例えば、モデルが次のトークンとしてshanghaiという単語を選択した場合、この状態の値を期待する。"
  },
  {
    "start": 5860454,
    "end": 5864048,
    "text": "つまり、これは言語モデルにとって新たな入力となり、高い値を示すことになるからだ。"
  },
  {
    "start": 5864134,
    "end": 5864480,
    "text": "なぜですか？"
  },
  {
    "start": 5864550,
    "end": 5869972,
    "text": "なぜなら、その方が、私たちのモデルによって良い報酬を得られる良い答えになる可能性が高いからだ。"
  },
  {
    "start": 5870106,
    "end": 5875892,
    "text": "もちろん、この値関数をうまく近似するようにニューラルネットワークを訓練する必要もある。"
  },
  {
    "start": 5876026,
    "end": 5883508,
    "text": "PPOの損失は、価値関数推定器を訓練するために使います。"
  },
  {
    "start": 5883684,
    "end": 5890440,
    "text": "基本的には、特定の状態に基づく価値関数推定器を意味する。"
  },
  {
    "start": 5890510,
    "end": 5898888,
    "text": "これはモデルの出力であり、サンプリングした軌跡に基づくこの状態の実際の値と比較する。"
  },
  {
    "start": 5898984,
    "end": 5903560,
    "text": "軌跡があるから、それぞれの軌跡は状態アクションで構成される。"
  },
  {
    "start": 5903640,
    "end": 5905548,
    "text": "各状態の行動には何らかの報酬がある。"
  },
  {
    "start": 5905644,
    "end": 5912732,
    "text": "サンプリングした軌跡に従って、この状態の値を計算することができる。"
  },
  {
    "start": 5912796,
    "end": 5919510,
    "text": "我々は、ポリシーから実際にサンプリングされた軌道に従って、価値関数推定器を最適化したい。"
  },
  {
    "start": 5919960,
    "end": 5922784,
    "text": "ポリシーの最後の用語、PPOの損失。"
  },
  {
    "start": 5922832,
    "end": 5928228,
    "text": "まず、ここで説明した政策最適化用語がある。"
  },
  {
    "start": 5928314,
    "end": 5935960,
    "text": "そして、価値関数推定器の損失があり、さらにエントロピー損失という別の項がある。"
  },
  {
    "start": 5939340,
    "end": 5942620,
    "text": "これは、我々のモデルにより多くの選択肢を探らせるためだ。"
  },
  {
    "start": 5942770,
    "end": 5965360,
    "text": "私たちのモデルを想像してみてほしいが、もしこの用語がなければ、非常に有利な行動をより多く選択し、平均より低い有利な行動をより少なく選択するように、モデルは行動を最適化するだけかもしれない。"
  },
  {
    "start": 5965510,
    "end": 5970384,
    "text": "これは、トークンを選択する際にモデルを非常に厳格にすることになる。"
  },
  {
    "start": 5970432,
    "end": 5977984,
    "text": "モデルは常に良いアドバンテージになったトークンを選び、悪いアドバンテージになったトークンは決して選ばない。"
  },
  {
    "start": 5978112,
    "end": 5982436,
    "text": "これでは、モデルも他の選択肢を探らなくなる。"
  },
  {
    "start": 5982618,
    "end": 5987604,
    "text": "つまり、例えば、いくつかの軌道をサンプリングし、上海はどこにあるのか？"
  },
  {
    "start": 5987652,
    "end": 5991384,
    "text": "模範解答は常にshanghaiという単語を選択する。"
  },
  {
    "start": 5991502,
    "end": 5995180,
    "text": "他の選択肢を模索するようなモデルにしたい。"
  },
  {
    "start": 5995250,
    "end": 5999016,
    "text": "例えば、上海はどこですか？"
  },
  {
    "start": 5999208,
    "end": 6004940,
    "text": "もしかしたら、次の言葉はitになるかもしれない。"
  },
  {
    "start": 6006340,
    "end": 6013040,
    "text": "私たちはまた、モデルにこれらの選択肢をもっと探求する可能性を与えてほしい。"
  },
  {
    "start": 6013190,
    "end": 6019732,
    "text": "各状態のアクションのモデルに、他の選択肢も探索させたいからだ。"
  },
  {
    "start": 6019786,
    "end": 6022788,
    "text": "モデルに他の選択肢を探らせたい。"
  },
  {
    "start": 6022954,
    "end": 6033656,
    "text": "この目的関数を最大化するために、この損失も最小化したい。"
  },
  {
    "start": 6033678,
    "end": 6035288,
    "text": "その方法は後で説明する。"
  },
  {
    "start": 6035374,
    "end": 6042010,
    "text": "モデルがより多くのオプションを探索できるように、エントロピーを最大化したい。"
  },
  {
    "start": 6042380,
    "end": 6042696,
    "text": "なぜですか？"
  },
  {
    "start": 6042718,
    "end": 6050568,
    "text": "エントロピーを使えば、予測にどの程度の不確実性があるのかがわかるからだ。"
  },
  {
    "start": 6050664,
    "end": 6053080,
    "text": "モデルの不確実性を高めたい。"
  },
  {
    "start": 6053160,
    "end": 6053500,
    "text": "なぜですか？"
  },
  {
    "start": 6053570,
    "end": 6055932,
    "text": "なぜなら、それがモデルの探究心を高めるからだ。"
  },
  {
    "start": 6055986,
    "end": 6074224,
    "text": "次に、与えられたプロンプトに対するトークンですが、最後に考慮しなければならないのは、前に説明したPPOロスを使用してポリシーを最適化すると、モデルは常に良い報酬になるトークンやトークンのシーケンスを学習する可能性があるということです。"
  },
  {
    "start": 6074352,
    "end": 6079488,
    "text": "モデルは常に良い報酬を得るためにこれらのトークンを選ぶことができる。"
  },
  {
    "start": 6079664,
    "end": 6083156,
    "text": "これらのトークンは、私たち人間にとっては意味をなさないかもしれない。"
  },
  {
    "start": 6083188,
    "end": 6092200,
    "text": "例えば、我々のモデルが、我々のデータセットが、報酬モデルのデータセットが、モデルに礼儀正しさを強制したとしよう。"
  },
  {
    "start": 6092700,
    "end": 6100264,
    "text": "ありがとうございます、ありがとうございます、ありがとうございますという言葉を連発するのは、それがとても礼儀正しく、良い報酬につながることを知っているからだ。"
  },
  {
    "start": 6100392,
    "end": 6105656,
    "text": "上海はどこですか？"
  },
  {
    "start": 6105688,
    "end": 6109616,
    "text": "もしモデルが、ありがとう、ありがとう、ありがとうと言い続けてくれるなら。"
  },
  {
    "start": 6109638,
    "end": 6116588,
    "text": "そうすると、確かに報酬モデルはこの答えに良い報酬を与えるだろう。それは丁寧な答えだからだが、人間には意味がない。"
  },
  {
    "start": 6116684,
    "end": 6123460,
    "text": "モデルには、トレーニング中に見たデータに非常によく似た、意味のある出力を実際に生成してもらいたい。"
  },
  {
    "start": 6123610,
    "end": 6134108,
    "text": "だからこそ、良い報酬を得るためだけでなく、同時に生成された答えに非常に近い答えを生成するためにも、モデルを制約したいのだ。"
  },
  {
    "start": 6134224,
    "end": 6137448,
    "text": "訓練されていないモデルを見るだけで生成される。"
  },
  {
    "start": 6137534,
    "end": 6145496,
    "text": "そのため、最適化したいモデルのコピーをもう1つ作り、その重みを凍結する。"
  },
  {
    "start": 6145608,
    "end": 6147340,
    "text": "これが冷凍モデルだ。"
  },
  {
    "start": 6147490,
    "end": 6158624,
    "text": "軌跡の各ステップごとに報酬を生成するが、各ステップでの対数確率が凍結モデルからどれだけ変化するかによってペナルティを課す。"
  },
  {
    "start": 6158742,
    "end": 6165468,
    "text": "各隠れ状態に対して、1つの出力特徴のみを持つ、前に見た線形レイヤーを使用することで、報酬を生成することができる。"
  },
  {
    "start": 6165564,
    "end": 6172672,
    "text": "各隠れ状態の対数確率を計算すると同時に、もう1つの線形層を使って対数確率を計算する。"
  },
  {
    "start": 6172736,
    "end": 6177008,
    "text": "ロジットを生成するために、これもリニアレイヤーに送る。"
  },
  {
    "start": 6177184,
    "end": 6182900,
    "text": "これは対数を計算し、次に対数確率を計算する。"
  },
  {
    "start": 6186140,
    "end": 6191652,
    "text": "フローズンモデルも同じようにして、制約をかけ、報酬にペナルティを課す。"
  },
  {
    "start": 6191716,
    "end": 6204032,
    "text": "この時間ステップの報酬は、時間ステップゼロの報酬から凍結モデルの対数確率間のKLダイバージェンスを引いたものに等しい。"
  },
  {
    "start": 6204086,
    "end": 6212892,
    "text": "凍結モデルの対数確率と最適化するポリシーの対数確率。"
  },
  {
    "start": 6213036,
    "end": 6218800,
    "text": "私たちは、凍結されたモデルとあまりにも異なる答えを生成したモデルにペナルティを与えたい。"
  },
  {
    "start": 6218950,
    "end": 6235560,
    "text": "報酬が最大化されることを望むと同時に、私たちはモデルが何らかの出力を生成することで報酬を得ることだけをズルしてほしくない。"
  },
  {
    "start": 6236460,
    "end": 6243004,
    "text": "さて、このような説明や理論はもう見飽きただろうから、今すぐコードに飛び込もう。"
  },
  {
    "start": 6243202,
    "end": 6259800,
    "text": "さて、これから紹介するコードは、hugging faceのウェブサイトから引用したもので、基本的に強化学習のセットアップを訓練するものである。"
  },
  {
    "start": 6259880,
    "end": 6272324,
    "text": "テキストを生成している言語モデルがあるが、その言語モデルに、例えばレストランや映画など、特定のものについての肯定的なレビューを生成させたい。"
  },
  {
    "start": 6272522,
    "end": 6280496,
    "text": "私たちは、言語モデルが人間に理解できるものを生成するために、まだ類似していることを望みます。"
  },
  {
    "start": 6280528,
    "end": 6285064,
    "text": "同時に、言語モデルをポジティブにし、ポジティブなものを生成させる。"
  },
  {
    "start": 6285102,
    "end": 6289800,
    "text": "例えば、この映画がすごく好きだとか、このレストランがすごく好きだとか。"
  },
  {
    "start": 6290220,
    "end": 6292600,
    "text": "IMDbのデータセットを使用する。"
  },
  {
    "start": 6292670,
    "end": 6296284,
    "text": "抱擁顔のウェブサイト、IMDbのデータセットからわかるように。"
  },
  {
    "start": 6296322,
    "end": 6299128,
    "text": "テキストとレビューからなるデータセットだ。"
  },
  {
    "start": 6299224,
    "end": 6304540,
    "text": "各レビューについて、そのレビューが肯定的か否定的かを示す。"
  },
  {
    "start": 6305700,
    "end": 6313280,
    "text": "このIMDbのデータセットを使って、レビューにつけたい点数を理解しよう。"
  },
  {
    "start": 6313350,
    "end": 6319116,
    "text": "このデータセットによれば、レビューが肯定的であれば、高い報酬が与えられる。"
  },
  {
    "start": 6319228,
    "end": 6324640,
    "text": "生成されたテキストが否定的なレビューに似ている場合は、低い報酬が与えられます。"
  },
  {
    "start": 6324800,
    "end": 6333624,
    "text": "最初にすることは、最適化したいモデルを作成することだ。"
  },
  {
    "start": 6333662,
    "end": 6341736,
    "text": "すでにIMDbのデータセットで微調整されたGPT-2だと思う。"
  },
  {
    "start": 6341838,
    "end": 6342200,
    "text": "なぜですか？"
  },
  {
    "start": 6342270,
    "end": 6354812,
    "text": "重みを凍結したモデルが必要だからだ。重みを凍結しておき、最適化しようとしているモデルのレスポンスが凍結したモデルとどれだけ違うかを比較する必要がある。"
  },
  {
    "start": 6354866,
    "end": 6357568,
    "text": "アウトプットに大きな違いは求めないからだ。"
  },
  {
    "start": 6357654,
    "end": 6359628,
    "text": "ただ、少しでもポジティブであってほしい。"
  },
  {
    "start": 6359724,
    "end": 6364172,
    "text": "高い報酬を得るためにモデルがゴミを出力するようなことは避けたい。"
  },
  {
    "start": 6364236,
    "end": 6367540,
    "text": "実際にテキストを入手したい。"
  },
  {
    "start": 6367610,
    "end": 6368804,
    "text": "それは理にかなっている。"
  },
  {
    "start": 6369002,
    "end": 6371350,
    "text": "そのため、冷凍モデルも用意している。"
  },
  {
    "start": 6372520,
    "end": 6375408,
    "text": "そして、このPPOトレーナーをロードする。"
  },
  {
    "start": 6375424,
    "end": 6385348,
    "text": "ハグ顔のPPOトレーナーは、PPOアルゴリズムを使って人間のフィードバックから強化学習を実行するための訓練に使われるクラスである。"
  },
  {
    "start": 6385524,
    "end": 6388312,
    "text": "それではまず、報酬モデルとは何か。"
  },
  {
    "start": 6388366,
    "end": 6391236,
    "text": "報酬モデルは基本的に単なるセンチメント分析である。"
  },
  {
    "start": 6391348,
    "end": 6403564,
    "text": "このモデルを使用すると、この報酬モデルに入力した各テキストについて、IMDbデータセットによるとどの程度ポジティブであるかを示す数値が表示される。"
  },
  {
    "start": 6403602,
    "end": 6404652,
    "text": "こちらをご覧いただきたい。"
  },
  {
    "start": 6404786,
    "end": 6410704,
    "text": "保存しているテキストが肯定的なレビューか否定的なレビューかを教えてくれる。"
  },
  {
    "start": 6410742,
    "end": 6415264,
    "text": "例えば、この文章をここにあげれば、おそらく悪い評価だと教えてくれるだろう。"
  },
  {
    "start": 6415382,
    "end": 6416892,
    "text": "報酬は少ない。"
  },
  {
    "start": 6417036,
    "end": 6427424,
    "text": "この映画が本当に良かったという文章をここに書けば、ポジティブな報酬が得られ、この数字を報酬として使うことができる。"
  },
  {
    "start": 6427472,
    "end": 6430570,
    "text": "正のクラスに対応するスコア。"
  },
  {
    "start": 6432140,
    "end": 6437620,
    "text": "さて、PPOの最初のステップは軌道を生成することだ。"
  },
  {
    "start": 6437700,
    "end": 6447244,
    "text": "オフライン・ポリシーというモデルがあり、そこからいくつかの軌道をサンプリングする必要がある。"
  },
  {
    "start": 6447362,
    "end": 6450072,
    "text": "いくつかの軌道をサンプリングするというのはどういう意味か？"
  },
  {
    "start": 6450136,
    "end": 6461968,
    "text": "つまり、テキストを与えると、テキストを生成するための質問やプロンプトのようなものを使って、テキストを生成してくれるのだ。"
  },
  {
    "start": 6462054,
    "end": 6469236,
    "text": "ここでは、IMDbのデータセットから初期サンプルのテキストを使用します。"
  },
  {
    "start": 6469338,
    "end": 6473776,
    "text": "例えば、このデータセットは多くのレビューで構成されている。"
  },
  {
    "start": 6473808,
    "end": 6475232,
    "text": "ポジティブなものもあれば、ネガティブなものもある。"
  },
  {
    "start": 6475296,
    "end": 6482840,
    "text": "私たちは、レビューの最初の部分をランダムに取り出し、それをプロンプトとして残りのレビューを生成します。"
  },
  {
    "start": 6482910,
    "end": 6486532,
    "text": "そして、報酬モデルに、生成されたこのレビューを判断してもらう。"
  },
  {
    "start": 6486596,
    "end": 6488660,
    "text": "ポジティブでもネガティブでもポジティブだ。"
  },
  {
    "start": 6488740,
    "end": 6490580,
    "text": "そうすれば、高い報酬を得ることができる。"
  },
  {
    "start": 6490660,
    "end": 6493340,
    "text": "それがネガティブなものであれば、低い報酬しか得られない。"
  },
  {
    "start": 6497280,
    "end": 6503576,
    "text": "各レビューからいくつのトークンを取得する必要があるかランダムに選択するように、いくつかの長さを生成します。"
  },
  {
    "start": 6503698,
    "end": 6505308,
    "text": "ランダムに選ぶ。"
  },
  {
    "start": 6505484,
    "end": 6515740,
    "text": "私たちはデータセットからこれらのプロンプトを入手し、PPOモデルにこれらのプロンプトに対する質問の答えを生成するよう依頼する。"
  },
  {
    "start": 6515820,
    "end": 6522660,
    "text": "もランダムにサンプリングされる。"
  },
  {
    "start": 6523400,
    "end": 6525972,
    "text": "これが今のところ私たちの軌道だ。"
  },
  {
    "start": 6526026,
    "end": 6529990,
    "text": "これらはプロンプトと生成されたテキストの組み合わせに過ぎない。"
  },
  {
    "start": 6530440,
    "end": 6537800,
    "text": "対数確率を計算したわけでも、長所を計算したわけでも、報酬を計算したわけでもない。"
  },
  {
    "start": 6538300,
    "end": 6544910,
    "text": "さて、今はオフライン・ポリシーによって生成されたクエリとレスポンスだけを持っている。"
  },
  {
    "start": 6545520,
    "end": 6548780,
    "text": "オフライン・ポリシーとは、我々が訓練しようとしているモデルのことである。"
  },
  {
    "start": 6548850,
    "end": 6557708,
    "text": "この変数のモデルで、いくつかの回答が得られたので、報酬モデルにこれらの回答を判断してもらうことができる。"
  },
  {
    "start": 6557804,
    "end": 6568656,
    "text": "私たちは基本的にセンチメント分類を行うだけで、ポリシーによって与えられた回答を与え、センチメントパイプに質問します。"
  },
  {
    "start": 6568688,
    "end": 6574020,
    "text": "センチメント分析パイプは、このテキストを判断する報酬モデルとして機能する。"
  },
  {
    "start": 6574170,
    "end": 6577376,
    "text": "このレビューはどの程度好意的なものなのだろうか？"
  },
  {
    "start": 6577488,
    "end": 6583512,
    "text": "ここでは、生成されるポジティブ・クラスに関連するスコアを取る。"
  },
  {
    "start": 6583566,
    "end": 6588676,
    "text": "報酬として、その報酬をフルレスポンスに割り当てる。"
  },
  {
    "start": 6588708,
    "end": 6601020,
    "text": "各回答に対して1つの数値が得られ、この数値は実際にはロジットであるため、このセンチメント分析パイプラインによれば、ポジティブ・クラスに対応するスコアとなる。"
  },
  {
    "start": 6601520,
    "end": 6609660,
    "text": "これで、いくつかの軌跡、つまりいくつかの質問、つまり生成されたテキストと報酬とともにいくつかのプロンプトができた。"
  },
  {
    "start": 6609740,
    "end": 6615164,
    "text": "生成されたテキストそれぞれについて、PPSトレーニングのセットアップを実行することができる。"
  },
  {
    "start": 6615212,
    "end": 6618160,
    "text": "では、ライブラリーのコードの中に入ってみよう。"
  },
  {
    "start": 6618580,
    "end": 6630204,
    "text": "ここで、言語モデルに与えたプロンプト、生成されたレスポンス、各レスポンスに関連する報酬を与えます。"
  },
  {
    "start": 6630352,
    "end": 6633450,
    "text": "そして、このステップ関数を実行する。"
  },
  {
    "start": 6633820,
    "end": 6635800,
    "text": "さて、このステップ機能である。"
  },
  {
    "start": 6635870,
    "end": 6636152,
    "text": "オーケー。"
  },
  {
    "start": 6636206,
    "end": 6640120,
    "text": "まず、渡されたテンソルが正しいかどうかをチェックする。"
  },
  {
    "start": 6640190,
    "end": 6644200,
    "text": "データ型やテンソルの形状などなど。"
  },
  {
    "start": 6644360,
    "end": 6650536,
    "text": "そして、スコアは各回答に対して少なくとも1つのスコアであるため、スコアをテンソルに変換する。"
  },
  {
    "start": 6650568,
    "end": 6652552,
    "text": "テンソルに変換する。"
  },
  {
    "start": 6652696,
    "end": 6657592,
    "text": "私の説明に役立たないコードはコメントした。"
  },
  {
    "start": 6657656,
    "end": 6661104,
    "text": "だから、ハグの局面では多くの機能があるが、そのすべてを使うわけではない。"
  },
  {
    "start": 6661142,
    "end": 6666240,
    "text": "私はスライドで説明したように、バニラPPOの説明に集中したい。"
  },
  {
    "start": 6667460,
    "end": 6668112,
    "text": "オーケー。"
  },
  {
    "start": 6668246,
    "end": 6676336,
    "text": "最初にすべきことは、勾配を計算するために必要なアクションの対数確率をすべて計算することだ。"
  },
  {
    "start": 6676528,
    "end": 6679536,
    "text": "このファンクションでそれを行う。"
  },
  {
    "start": 6679578,
    "end": 6687400,
    "text": "つまり、回答、モデルによって生成されたテキスト、そして使用されたクエリが与えられた。"
  },
  {
    "start": 6687470,
    "end": 6693876,
    "text": "ここではクエリーとレスポンスと呼ばれているが、実際にはプロンプトと生成されたテキストであり、抱き合わせの段階である。"
  },
  {
    "start": 6693908,
    "end": 6696636,
    "text": "各ステップの対数確率を計算する。"
  },
  {
    "start": 6696738,
    "end": 6697836,
    "text": "どうやって計算するんだ？"
  },
  {
    "start": 6697858,
    "end": 6706732,
    "text": "さて、彼らはこの関数をバッチ・フォワード・パスと呼んでいる。"
  },
  {
    "start": 6706796,
    "end": 6722688,
    "text": "テキストが生成され、このテキストを生成するために使用されたプロンプトが生成され、これらの質問と回答のそれぞれをミニバッチに分け、それをモデル（スライドで見たモデル）に通します。"
  },
  {
    "start": 6722784,
    "end": 6724630,
    "text": "ここに戻ろう。"
  },
  {
    "start": 6728520,
    "end": 6738040,
    "text": "私たちは、テキストと質問に基づいて、各ポジションに対応する対数確率を計算できることを知っている。"
  },
  {
    "start": 6738110,
    "end": 6743044,
    "text": "質問と生成されたテキストの連結を作成することができます。"
  },
  {
    "start": 6743092,
    "end": 6744296,
    "text": "それをモデルに渡す。"
  },
  {
    "start": 6744398,
    "end": 6748440,
    "text": "このモデルは、トークンの各位置に対して1つずつ、いくつかのロジットを生成する。"
  },
  {
    "start": 6748600,
    "end": 6754216,
    "text": "次のトークンの対数確率を取るだけである。なぜなら、どのトークンが生成されたかはすでに分かっているからである。"
  },
  {
    "start": 6754328,
    "end": 6759708,
    "text": "この4つのトークンからなる特定のプロンプトでは、次のトークンは上海であることがわかる。"
  },
  {
    "start": 6759804,
    "end": 6763020,
    "text": "上海という単語に対応する対数確率だけを取る。"
  },
  {
    "start": 6763100,
    "end": 6765344,
    "text": "これがこのラインで行われていることだ。"
  },
  {
    "start": 6765382,
    "end": 6771744,
    "text": "言語モデルに、すべてのポジションに対応するロジットを生成するよう依頼する。"
  },
  {
    "start": 6771872,
    "end": 6775520,
    "text": "そして、これらのロジットから対数確率を計算した。"
  },
  {
    "start": 6775680,
    "end": 6776390,
    "text": "どうやって？"
  },
  {
    "start": 6777320,
    "end": 6782192,
    "text": "ここでは、私のスライドにあるように、対数ソフトマックスを計算した。"
  },
  {
    "start": 6782256,
    "end": 6785704,
    "text": "ご覧のように、私たちはここで対数ソフトマックスを計算した。"
  },
  {
    "start": 6785742,
    "end": 6793768,
    "text": "各ロジテックについて、各ポジションの対数確率である対数ソフトマックスを計算する。"
  },
  {
    "start": 6793854,
    "end": 6797544,
    "text": "我々は次のトークンに対応する位置にしか興味がない。"
  },
  {
    "start": 6797592,
    "end": 6800812,
    "text": "これは、ここにあるgather関数で行われる。"
  },
  {
    "start": 6800866,
    "end": 6807948,
    "text": "なぜなら、どのトークンが生成されたかはすでに分かっているからだ。"
  },
  {
    "start": 6808124,
    "end": 6818476,
    "text": "すべてのトークンの対数確率が必要なわけではないので、対数確率を保存することができる。"
  },
  {
    "start": 6818508,
    "end": 6822180,
    "text": "また、対数確率がどこから始まるかを追跡しておく必要がある。"
  },
  {
    "start": 6822250,
    "end": 6825380,
    "text": "私たちが考慮したいのは、その終着点である。"
  },
  {
    "start": 6825530,
    "end": 6826036,
    "text": "なぜですか？"
  },
  {
    "start": 6826138,
    "end": 6831632,
    "text": "というのも、スライドをご覧いただければお分かりのように、私たちの軌跡は、上海はどこにあるのか？"
  },
  {
    "start": 6831696,
    "end": 6835668,
    "text": "モデルは4つのトークンを生成したが、上海は中国にある。"
  },
  {
    "start": 6835764,
    "end": 6839012,
    "text": "私たちは皆、この軌跡に興味を持っている。"
  },
  {
    "start": 6839076,
    "end": 6842932,
    "text": "我々は皆、4つのトークンの対数確率に興味がある。"
  },
  {
    "start": 6843076,
    "end": 6845004,
    "text": "これこそ、私たちがここでやっていることだ。"
  },
  {
    "start": 6845042,
    "end": 6854072,
    "text": "対数確率を考慮する開始点、対数確率を考慮する終了トークン。"
  },
  {
    "start": 6854136,
    "end": 6861264,
    "text": "というのも、このモデルではすべてのポジションの対数確率が生成されるが、私たちが欲しいのはその一部だけだからだ。"
  },
  {
    "start": 6861302,
    "end": 6862336,
    "text": "これが我々の仕事だ。"
  },
  {
    "start": 6862358,
    "end": 6868852,
    "text": "というマスクを作成する。"
  },
  {
    "start": 6868986,
    "end": 6876310,
    "text": "この4つの確率、つまりモデルによって実際にどのトークンが生成されたかに応じて4つの5つの確率だけを考えることにする。"
  },
  {
    "start": 6877000,
    "end": 6880740,
    "text": "これで各行動の対数確率がわかった。"
  },
  {
    "start": 6880900,
    "end": 6885050,
    "text": "ステップ関数に戻ろう。"
  },
  {
    "start": 6889220,
    "end": 6895410,
    "text": "さて、それではオフラインの方針に従って対数確率を計算してみた。"
  },
  {
    "start": 6895780,
    "end": 6899604,
    "text": "なぜステップメソッドの中でやって、外ではやらないのか？"
  },
  {
    "start": 6899722,
    "end": 6903088,
    "text": "まあ、ハグ顔は使い勝手のいい図書館だからだ。"
  },
  {
    "start": 6903184,
    "end": 6909568,
    "text": "各アクションの対数確率を計算する負担をユーザーに与えたくないのだ。"
  },
  {
    "start": 6909664,
    "end": 6911284,
    "text": "図書館の中でやっているんだ。"
  },
  {
    "start": 6911332,
    "end": 6918730,
    "text": "このような場合、各プロンプトに対する回答を生成するようユーザーに求めるだけで、あとは残りの情報を計算してくれる。"
  },
  {
    "start": 6919740,
    "end": 6923964,
    "text": "ここで、参照モデルに対する対数確率も計算する必要がある。"
  },
  {
    "start": 6924002,
    "end": 6925148,
    "text": "凍結モデル"
  },
  {
    "start": 6925234,
    "end": 6925532,
    "text": "なぜですか？"
  },
  {
    "start": 6925586,
    "end": 6940816,
    "text": "なぜなら、各ポジションの報酬をペナルティー化するために使用されるKLダイバージェンスも計算する必要があるからです。なぜなら、凍結モデルとは大きく異なる対数確率を生成するモデルにペナルティーを与えたいからです。"
  },
  {
    "start": 6940918,
    "end": 6952176,
    "text": "そうでなければ、モデルは報酬ハッキングと呼ばれる、実際には良い報酬を与えるが、ユーザーにとって意味のないランダムなトークンを生成するだけになる。"
  },
  {
    "start": 6952368,
    "end": 6954816,
    "text": "も同じ方法で生成する必要がある。"
  },
  {
    "start": 6954848,
    "end": 6964760,
    "text": "このバジェットフォワードパスは、凍結モデルを使用して対数確率を生成し、報酬にペナルティを与えるKLダイバージェンスを計算するために使用されます。"
  },
  {
    "start": 6965340,
    "end": 6968388,
    "text": "次のステップは、実際に報酬を計算することだ。"
  },
  {
    "start": 6968484,
    "end": 6970180,
    "text": "どうやって報酬を計算するのか？"
  },
  {
    "start": 6970260,
    "end": 6985276,
    "text": "KLダイバージェンスを計算する必要があるので、最適化しようとしているモデルの対数確率と凍結されたモデルを使用して、すべての応答の対数確率があるので、どの対数確率を考慮する必要があるかを示すマスクがあります。"
  },
  {
    "start": 6985308,
    "end": 6990000,
    "text": "そのうちのいくつかは、私たちにとって興味深いものである。"
  },
  {
    "start": 6990340,
    "end": 6993680,
    "text": "報酬の計算方法を見てみよう。"
  },
  {
    "start": 6994580,
    "end": 6996732,
    "text": "報酬は次のように計算される。"
  },
  {
    "start": 6996796,
    "end": 7000080,
    "text": "対数確率の差であるKLペナルティを計算する。"
  },
  {
    "start": 7000160,
    "end": 7007450,
    "text": "ここにあるように、KLダイバージェンスは対数確率の差である。"
  },
  {
    "start": 7007900,
    "end": 7020540,
    "text": "これはKLダイバージェンスにある係数を掛けたもので、これがペナルティ係数です。"
  },
  {
    "start": 7020880,
    "end": 7023400,
    "text": "そしてスコアを合計する。"
  },
  {
    "start": 7023480,
    "end": 7031952,
    "text": "スコアは、報酬モデルによって各レスポンスに関連づけられた得点に過ぎないことは前述した。"
  },
  {
    "start": 7032006,
    "end": 7039500,
    "text": "私たちの報酬モデルは、各回答に対して1つの報酬、1つの数字を生成するセンチメント分類パイプラインに過ぎない。"
  },
  {
    "start": 7039580,
    "end": 7055952,
    "text": "つまり、生成された反応がどれだけポジティブか、あるいはどれだけネガティブかを示すのだ。"
  },
  {
    "start": 7056016,
    "end": 7059224,
    "text": "スライドでお見せしましょう。"
  },
  {
    "start": 7059342,
    "end": 7070876,
    "text": "各ステップごとに報酬を計算していましたが、実際には、センチメント分類モデルは最後のトークン、完全な回答、生成されたテキスト全体に対してのみ報酬を計算します。"
  },
  {
    "start": 7071058,
    "end": 7076696,
    "text": "基本的には私たちが作るのだが、もちろん軌道の報酬を計算する必要がある。"
  },
  {
    "start": 7076728,
    "end": 7079660,
    "text": "各州のアクションに対する報酬が必要なのだ。"
  },
  {
    "start": 7080160,
    "end": 7089612,
    "text": "なぜなら、凍結モデルと最適化しようとしているモデルの対数確率を知っているからである。"
  },
  {
    "start": 7089676,
    "end": 7093824,
    "text": "各ポジションにはKLのペナルティがあるが、最後の1つには報酬がある。"
  },
  {
    "start": 7093862,
    "end": 7095668,
    "text": "これこそ、私たちがここでやっていることだ。"
  },
  {
    "start": 7095754,
    "end": 7104448,
    "text": "各ポジションのKLペナルティの対数確率を計算するが、スコアは最後のトークンにのみ加算される。"
  },
  {
    "start": 7104544,
    "end": 7107284,
    "text": "この位置で"
  },
  {
    "start": 7107482,
    "end": 7119560,
    "text": "そして、アドバンテージを計算するとき、アドバンテージは最後から最初に計算するので、この報酬を前のステップに回すことになる。"
  },
  {
    "start": 7119710,
    "end": 7129208,
    "text": "各ポジションに関連する報酬を計算する方法を見つけました。各ポジションは、センチメント分類によっていくつかのスコアを与えられます。"
  },
  {
    "start": 7129384,
    "end": 7135250,
    "text": "これは最後のトークンにのみ与えられ、KLペナルティは各ポジションに与えられる。"
  },
  {
    "start": 7136500,
    "end": 7137970,
    "text": "戻ろう。"
  },
  {
    "start": 7140820,
    "end": 7143100,
    "text": "さて、報酬を計算した。"
  },
  {
    "start": 7143180,
    "end": 7145064,
    "text": "これでアドバンテージを計算できる。"
  },
  {
    "start": 7145212,
    "end": 7146848,
    "text": "では、その利点をどのように計算するか見てみよう。"
  },
  {
    "start": 7146944,
    "end": 7149360,
    "text": "アドバンテージを計算するには、その値が必要だ。"
  },
  {
    "start": 7149520,
    "end": 7155350,
    "text": "値が価値関数の推定値である場合の値とは何か。"
  },
  {
    "start": 7155720,
    "end": 7160232,
    "text": "前に見たように、この値は同じモデルを使って計算される。"
  },
  {
    "start": 7160286,
    "end": 7169624,
    "text": "これは線形レイヤで、特定の状態の値推定を行う。"
  },
  {
    "start": 7169742,
    "end": 7174220,
    "text": "スライドでお見せしましょう。"
  },
  {
    "start": 7174290,
    "end": 7176824,
    "text": "私たちは以前、政策ネットワークについて見た。"
  },
  {
    "start": 7176872,
    "end": 7186380,
    "text": "最適化しようとしているモデルには、軌道の各ステップの値推定を行う線形レイヤーも追加されている。"
  },
  {
    "start": 7186540,
    "end": 7190316,
    "text": "これは実は、対数確率を計算した時点ですでに出ている。"
  },
  {
    "start": 7190428,
    "end": 7197424,
    "text": "この関数はまた、軌跡の各ステップの値推定を持っていた値を返します。"
  },
  {
    "start": 7197552,
    "end": 7210376,
    "text": "そして、その推定値＋計算した報酬＋マスクを使えばいい。先ほどと同じ計算式でアドバンテージを計算するには、どの値を持っていて、どの値を持っていないかを知る必要があるからだ。"
  },
  {
    "start": 7210478,
    "end": 7216024,
    "text": "の公式から始める。"
  },
  {
    "start": 7216062,
    "end": 7224486,
    "text": "ここで公式に戻ろう。"
  },
  {
    "start": 7224668,
    "end": 7230874,
    "text": "デルタtを計算し、ステップtにおけるアドバンテージを計算する。"
  },
  {
    "start": 7230912,
    "end": 7242190,
    "text": "これは、タイムステップtの報酬にガンマを足したものに、タイムステップtに1を足したものを掛けたものである。"
  },
  {
    "start": 7242260,
    "end": 7242894,
    "text": "これがここだ。"
  },
  {
    "start": 7242932,
    "end": 7246714,
    "text": "将来の価値がなければゼロである。"
  },
  {
    "start": 7246762,
    "end": 7254002,
    "text": "そうでない場合は、タイムスタンプtの値に1を足した値からタイムステップtの値を引いた値となる。"
  },
  {
    "start": 7254136,
    "end": 7255474,
    "text": "こちらをご覧いただきたい。"
  },
  {
    "start": 7255672,
    "end": 7268934,
    "text": "これは、デルタにガンマとラムダを掛け合わせたものに、次の時間ステップでのgeを掛けたものである。"
  },
  {
    "start": 7268972,
    "end": 7276986,
    "text": "つまり、ステップt時点のデルタにガンマを掛け合わせたものにラムダを掛け合わせ、ステップt時点のアドバンテージ推定値に1を足したものとなる。"
  },
  {
    "start": 7277168,
    "end": 7284750,
    "text": "軌跡の最後の項目から軌跡の最初の項目まで行う。"
  },
  {
    "start": 7285250,
    "end": 7288314,
    "text": "だから、このforループを逆にするのだ。"
  },
  {
    "start": 7288442,
    "end": 7292762,
    "text": "そして、アドバンテージを逆に計算したため、それを元に戻す。"
  },
  {
    "start": 7292826,
    "end": 7301380,
    "text": "であるならば、計算された利点を逆にして、資本tからゼロではなく、ゼロから資本tへのステップt回とする。"
  },
  {
    "start": 7302230,
    "end": 7308210,
    "text": "次に、価値関数の最適化に使用するq値を計算する。"
  },
  {
    "start": 7308280,
    "end": 7312150,
    "text": "だから、ここにあるように、バリューヘッドを最適化する。"
  },
  {
    "start": 7312220,
    "end": 7322294,
    "text": "値推定では、値関数の推定が必要だが、サンプリングした軌跡に従う。"
  },
  {
    "start": 7322342,
    "end": 7332058,
    "text": "というのも、価値関数が教えてくれるからだ。"
  },
  {
    "start": 7332144,
    "end": 7336014,
    "text": "じゃあ、ここに書いてみよう。"
  },
  {
    "start": 7336132,
    "end": 7337678,
    "text": "そうでなければ、理解するのは容易ではない。"
  },
  {
    "start": 7337764,
    "end": 7345550,
    "text": "ここでの価値関数は、特定の状態の価値を教えてくれる。"
  },
  {
    "start": 7345620,
    "end": 7351860,
    "text": "ある状態からスタートすることで得られる期待リターンは何か？"
  },
  {
    "start": 7352310,
    "end": 7359858,
    "text": "も、サンプルの軌跡からq関数を用いて実際に近似することができる。"
  },
  {
    "start": 7359954,
    "end": 7360600,
    "text": "なぜですか？"
  },
  {
    "start": 7361610,
    "end": 7379674,
    "text": "というのも、価値関数とは、状態sから出発して行動aをとるという、取りうるすべての行動に対する期待収益だからである。"
  },
  {
    "start": 7379792,
    "end": 7397470,
    "text": "つまり、q関数は、状態sからスタートして行動aをとった場合の期待リターンを教えてくれる。"
  },
  {
    "start": 7397540,
    "end": 7432090,
    "text": "価値関数は、状態sからスタートして方針に従って反応した場合に得られる期待リターンを教えてくれます。これは基本的にはq関数に対する期待リターンとして計算することもできますが、取りうるすべての行動に対する期待リターン、つまり、状態sからスタートして取りうるすべての行動の中で何らかの行動を取ることによって得られる平均リターンはどれくらいか、と考えることもできます。"
  },
  {
    "start": 7432240,
    "end": 7435046,
    "text": "我々はすべての可能なアクションを持っているわけではない。"
  },
  {
    "start": 7435238,
    "end": 7442170,
    "text": "この期待値は、軌跡にある標本平均で近似することができる。"
  },
  {
    "start": 7442250,
    "end": 7445578,
    "text": "私たちの軌跡には、いくつかのアクション、ステートアクションがある。"
  },
  {
    "start": 7445674,
    "end": 7452526,
    "text": "私たちの軌道にあるQSAを使えば、この近似値を求めることができる。"
  },
  {
    "start": 7452718,
    "end": 7456610,
    "text": "QSAはどのように計算するのか？"
  },
  {
    "start": 7456950,
    "end": 7468598,
    "text": "覚えているように、アドバンテージの公式は、特定の時間ステップにおけるSAのアドバンテージは、SAのqからSのvを引いたものに等しい。"
  },
  {
    "start": 7468684,
    "end": 7477434,
    "text": "とすると、q SaはアドバンテージSaに値sを足したものに等しい。"
  },
  {
    "start": 7477632,
    "end": 7481018,
    "text": "これこそ、私たちがここでやっていることだ。"
  },
  {
    "start": 7481104,
    "end": 7486006,
    "text": "私たちはq関数を得るために、長所プラス値を計算していると言っているのです。"
  },
  {
    "start": 7486038,
    "end": 7491310,
    "text": "この用語は、後で説明するバリューヘッドの損失計算に使われる。"
  },
  {
    "start": 7491380,
    "end": 7494000,
    "text": "ここでやっているリターンを覚えておいてほしい。"
  },
  {
    "start": 7494690,
    "end": 7499630,
    "text": "さて、これで長所と値を計算した。"
  },
  {
    "start": 7499790,
    "end": 7502414,
    "text": "今はまだ第一段階だ。"
  },
  {
    "start": 7502462,
    "end": 7507730,
    "text": "最適化しようとしているモデルからいくつかの軌道をサンプリングした。"
  },
  {
    "start": 7508070,
    "end": 7510930,
    "text": "私たちは報酬を使って報酬を計算した。"
  },
  {
    "start": 7511910,
    "end": 7514822,
    "text": "各時間ステップの対数確率も計算した。"
  },
  {
    "start": 7514876,
    "end": 7517766,
    "text": "また、各時間ステップの長所も計算した。"
  },
  {
    "start": 7517868,
    "end": 7522520,
    "text": "各時間ステップのq値も計算した。"
  },
  {
    "start": 7523530,
    "end": 7531542,
    "text": "さて、pPUアルゴリズムの第2段階、つまり、これらの軌道からいくつかのミニバッチを取る段階に行こう。"
  },
  {
    "start": 7531686,
    "end": 7535994,
    "text": "推定された勾配に基づいてモデルを最適化する。"
  },
  {
    "start": 7536122,
    "end": 7548510,
    "text": "何度もステップを踏んで、また新しい軌跡をサンプリングし、ミニバッチをいくつかサンプリングし、損失に応じてモデルを最適化し、それを何度も繰り返し、また新しい軌跡をサンプリングする。"
  },
  {
    "start": 7548670,
    "end": 7551460,
    "text": "ステップ関数に戻ろう。"
  },
  {
    "start": 7554560,
    "end": 7561180,
    "text": "私たちはここにいる。"
  },
  {
    "start": 7562320,
    "end": 7566224,
    "text": "これで、サンプルと軌跡を使ってモデルを最適化できる。"
  },
  {
    "start": 7566342,
    "end": 7567264,
    "text": "どうする？"
  },
  {
    "start": 7567302,
    "end": 7568812,
    "text": "ミニバッチをいくつか試食。"
  },
  {
    "start": 7568876,
    "end": 7570924,
    "text": "これが今回試飲しているミニバッチだ。"
  },
  {
    "start": 7571052,
    "end": 7574528,
    "text": "ミニバッチを試食してみた。"
  },
  {
    "start": 7574694,
    "end": 7576864,
    "text": "それなら、私たちは何をすべきか。"
  },
  {
    "start": 7576902,
    "end": 7579552,
    "text": "まず最初に、私たちは選手たちの公式戦で見たとおりにする。"
  },
  {
    "start": 7579616,
    "end": 7594472,
    "text": "また、サンプリングしたモデル（PIの古いモデル）に応じた対数確率が必要であり、さらに、サンプリングしたミニバッチを使って最適化しようとしているモデルに応じた対数確率も必要である。"
  },
  {
    "start": 7594526,
    "end": 7599160,
    "text": "ある方針からサンプリングし、その方針から得られた軌跡が必要である。"
  },
  {
    "start": 7599230,
    "end": 7602940,
    "text": "また、オフライン・ポリシーであるこのポリシーからのログ確率でもある。"
  },
  {
    "start": 7603090,
    "end": 7604904,
    "text": "このサンプル軌道を使用する。"
  },
  {
    "start": 7604952,
    "end": 7609852,
    "text": "ミニバッチを取り、オンライン・ポリシーで勾配上昇を実行する。"
  },
  {
    "start": 7609906,
    "end": 7615532,
    "text": "また、最適化しようとしているオンライン・ポリシーに従ったログ確率も必要である。"
  },
  {
    "start": 7615676,
    "end": 7617808,
    "text": "これこそ、私たちがここでやっていることだ。"
  },
  {
    "start": 7617894,
    "end": 7620720,
    "text": "前に実行したこのメソッドをもう一度実行する。"
  },
  {
    "start": 7620790,
    "end": 7629824,
    "text": "そこで、フォワード・パスのバッチは、私たちが考えているミニバッチに従って、対数確率、対数、予測値を計算する。"
  },
  {
    "start": 7629952,
    "end": 7632992,
    "text": "そして、このミニバッチに従ってモデルを訓練する。"
  },
  {
    "start": 7633056,
    "end": 7634550,
    "text": "そのやり方を見てみよう。"
  },
  {
    "start": 7635320,
    "end": 7642084,
    "text": "まず必要なのは、スライドで見た計算式に従ってPPOの損失を算出することだ。"
  },
  {
    "start": 7642132,
    "end": 7643560,
    "text": "では、負けで行こう。"
  },
  {
    "start": 7644540,
    "end": 7647012,
    "text": "損失では、3つの損失を計算しなければならない。"
  },
  {
    "start": 7647076,
    "end": 7650780,
    "text": "1つ目はバリューヘッドの損失で、これはここにある損失だ。"
  },
  {
    "start": 7650850,
    "end": 7656296,
    "text": "実際にハグしている段階では、クリッピングされたロスも計算している。"
  },
  {
    "start": 7656328,
    "end": 7658284,
    "text": "とりあえず、切り取られたロスは考えないことにしよう。"
  },
  {
    "start": 7658322,
    "end": 7662440,
    "text": "単なる最適化だ。"
  },
  {
    "start": 7662520,
    "end": 7666880,
    "text": "バニラのVPOである必要はない。"
  },
  {
    "start": 7667030,
    "end": 7676116,
    "text": "つまり、モデルによって予測された値と、私たちが計算したリターンの合計を、私たちが前に見た値に加えて、長所としているのだ。"
  },
  {
    "start": 7676298,
    "end": 7683204,
    "text": "つまり、この計算式に従えば、これがバリューヘッドの損失となる。"
  },
  {
    "start": 7683322,
    "end": 7684244,
    "text": "見ての通りだ。"
  },
  {
    "start": 7684282,
    "end": 7691560,
    "text": "これは基本的に、我々の軌跡に従って推定されたq関数である。"
  },
  {
    "start": 7692460,
    "end": 7695064,
    "text": "これがバリューヘッドの損失である。"
  },
  {
    "start": 7695182,
    "end": 7702152,
    "text": "そして、PPOの損失は、アドバンテージの項に対数確率の比率を掛けたものに過ぎない。"
  },
  {
    "start": 7702216,
    "end": 7704040,
    "text": "対数確率の比は？"
  },
  {
    "start": 7704120,
    "end": 7705950,
    "text": "ここだよ。"
  },
  {
    "start": 7707200,
    "end": 7708680,
    "text": "対数確率。"
  },
  {
    "start": 7708840,
    "end": 7710988,
    "text": "さて、まずは公式を見てみよう。"
  },
  {
    "start": 7711074,
    "end": 7716940,
    "text": "さて、ここでわかるように、我々は2つの確率の比を持っているが、我々は対数確率を持っている。"
  },
  {
    "start": 7717020,
    "end": 7722290,
    "text": "ここで使ってみよう。"
  },
  {
    "start": 7723880,
    "end": 7724532,
    "text": "オーケー。"
  },
  {
    "start": 7724666,
    "end": 7734710,
    "text": "対数確率があるので、aの対数からbの対数を引いたものである。"
  },
  {
    "start": 7735580,
    "end": 7738250,
    "text": "ということは、私たちはこの指数を使っていることになる。"
  },
  {
    "start": 7740300,
    "end": 7752664,
    "text": "これは、aの対数をbで割った指数と等価であり、2つの確率のa÷bを行うことと同じである。"
  },
  {
    "start": 7752712,
    "end": 7758636,
    "text": "つまり、確率はないが対数確率はあるので、このように計算する。"
  },
  {
    "start": 7758658,
    "end": 7764944,
    "text": "まず、オンライン・モデルの対数確率からオフライン・モデルの対数確率を引く。"
  },
  {
    "start": 7765062,
    "end": 7770448,
    "text": "を適用すると、aをbで割った結果になる。"
  },
  {
    "start": 7770534,
    "end": 7774230,
    "text": "コードを見てみよう。"
  },
  {
    "start": 7775400,
    "end": 7782820,
    "text": "対数確率の差を計算し、指数に当てはめることで、この比率が算出される。"
  },
  {
    "start": 7783320,
    "end": 7788424,
    "text": "そして、この比率にアドバンテージの項を掛ける。"
  },
  {
    "start": 7788462,
    "end": 7791476,
    "text": "このアドバンテージを掛け合わせる必要がある。"
  },
  {
    "start": 7791668,
    "end": 7799916,
    "text": "ということは、この式のもう一方の部分も計算する必要がある。"
  },
  {
    "start": 7800018,
    "end": 7806380,
    "text": "この比率は、1マイナスεと1プラスεの間で切り取られる。"
  },
  {
    "start": 7806880,
    "end": 7808956,
    "text": "私たちはここでやっている。"
  },
  {
    "start": 7808978,
    "end": 7815760,
    "text": "アドバンテージに1マイナスイプシロンと1プラスイプシロンの間の比率を掛けたもの。"
  },
  {
    "start": 7817060,
    "end": 7819680,
    "text": "なぜここにマイナス記号があるのか？"
  },
  {
    "start": 7819750,
    "end": 7830832,
    "text": "PPOの目標は、この項を最大化することだが、我々はPytorchとPytorchのオプティマイザを使っている。"
  },
  {
    "start": 7830896,
    "end": 7836760,
    "text": "Pytorchは常に勾配降下を実行する。つまり、勾配上昇の逆だ。"
  },
  {
    "start": 7840220,
    "end": 7847460,
    "text": "基本的には、これを最大化する代わりに、ここに見られるようなマイナスの損失を最小化することができる。"
  },
  {
    "start": 7847550,
    "end": 7850300,
    "text": "これこそが、このマイナス記号がある理由だ。"
  },
  {
    "start": 7850370,
    "end": 7855112,
    "text": "つまり、ピトーチは常に最小化するので、これにマイナスをかければいい。"
  },
  {
    "start": 7855176,
    "end": 7862850,
    "text": "この項を最大化しているようなもので、ご覧のようにエントロピーはここで計算される。"
  },
  {
    "start": 7863300,
    "end": 7872700,
    "text": "PPOのバニラ・ロスには存在しないからだ。"
  },
  {
    "start": 7872860,
    "end": 7881092,
    "text": "PPOの損失は、保険の損失にバリュー・ヘッドを足したものに、ここにある係数を掛けたものとして計算される。"
  },
  {
    "start": 7881146,
    "end": 7885716,
    "text": "だから、ロス・ポリシーではエントロピーも計算するが、それは使わない。"
  },
  {
    "start": 7885738,
    "end": 7887108,
    "text": "正直なところ、理由はわからない。"
  },
  {
    "start": 7887194,
    "end": 7889092,
    "text": "彼らはここでエントロピーを計算する。"
  },
  {
    "start": 7889146,
    "end": 7902380,
    "text": "ご覧のように、彼らはロジックを使ってエントロピーを計算するのですが、スライドでお見せした式（実際のエントロピーの式）ではなく、対数和xと呼ばれる最適化されたバージョンを使っているのです。"
  },
  {
    "start": 7902530,
    "end": 7907404,
    "text": "どのように行われるのか、その由来を知りたい人のために、ここにいくつかの情報を載せておく。"
  },
  {
    "start": 7907442,
    "end": 7913120,
    "text": "基本的にウィキペディアによれば、対数和Xの凸共役は負のエントロピーである。"
  },
  {
    "start": 7916900,
    "end": 7921900,
    "text": "ここにはエントロピーの項もある。"
  },
  {
    "start": 7921980,
    "end": 7924848,
    "text": "最適化のステップに戻ろう。"
  },
  {
    "start": 7925014,
    "end": 7926550,
    "text": "私たちはここに行く。"
  },
  {
    "start": 7928520,
    "end": 7945636,
    "text": "つまり、最初に損失を計算し、その損失に対して逆伝播を実行する。"
  },
  {
    "start": 7945668,
    "end": 7947130,
    "text": "もう一度戻るよ。"
  },
  {
    "start": 7949360,
    "end": 7955260,
    "text": "1つのミニバッチでトレーニングし、その後、何度もミニバッチでトレーニングするんだ。"
  },
  {
    "start": 7955410,
    "end": 7958044,
    "text": "しばらくしてまた戻ってくる。"
  },
  {
    "start": 7958242,
    "end": 7964652,
    "text": "ここでもう一度、新しい軌道を生成するための手順を繰り返す。"
  },
  {
    "start": 7964796,
    "end": 7970348,
    "text": "ハグフェイス・ライブラリーは、もちろん報酬も計算する。"
  },
  {
    "start": 7970444,
    "end": 7975856,
    "text": "抱きつき顔ライブラリーは、これらの軌跡に従って対数確率を計算する。"
  },
  {
    "start": 7975968,
    "end": 7981632,
    "text": "これらの軌跡に従ってアドバンテージを推定し、これらの軌跡に従って価値を推定する。"
  },
  {
    "start": 7981776,
    "end": 7993336,
    "text": "次に、これらの軌跡からいくつかのミニバッチを繰り返しサンプリングし、これらのミニバッチに対してPPO損失に従って勾配上昇を何度も実行する。"
  },
  {
    "start": 7993438,
    "end": 7995364,
    "text": "そしてまたループを再開する。"
  },
  {
    "start": 7995492,
    "end": 8002780,
    "text": "これが、人間のフィードバックから強化学習を行うPPOアルゴリズムの実行方法である。"
  },
  {
    "start": 8003920,
    "end": 8008540,
    "text": "スライドに戻り、このビデオを見てくれてありがとう。"
  },
  {
    "start": 8008610,
    "end": 8010940,
    "text": "とても、とても厳しいものだったと思う。"
  },
  {
    "start": 8011700,
    "end": 8019810,
    "text": "私自身、迷うことなくこれらの部分をすべて説明するのは、最も難しいビデオのひとつでもある。"
  },
  {
    "start": 8020180,
    "end": 8026864,
    "text": "PPOと強化学習は非常に大きなトピックなので、私は多くの知識を提供した。"
  },
  {
    "start": 8026912,
    "end": 8029316,
    "text": "大学にはこのようなことを学ぶコースがある。"
  },
  {
    "start": 8029338,
    "end": 8034724,
    "text": "たった数時間で完全な理解を与えるのは容易ではない。"
  },
  {
    "start": 8034842,
    "end": 8042664,
    "text": "これは、私がゼロからコーディングするのをやめた理由のひとつでもある。"
  },
  {
    "start": 8042862,
    "end": 8050988,
    "text": "少なくとも、人間のフィードバックからの強化学習の各ステップがどのように行われるのか、深く理解していただけたと思う。"
  },
  {
    "start": 8051074,
    "end": 8062640,
    "text": "私がコメントしたコードを、不要な部分をすべて削除して、あるいはとにかく、PPOアルゴリズムに不要な部分を明示してコメントしたものを、皆さんと共有したいと思います。"
  },
  {
    "start": 8063620,
    "end": 8075340,
    "text": "このビデオを準備するのに1カ月以上かかったし、何度も録画しなければならなかった。"
  },
  {
    "start": 8075500,
    "end": 8077920,
    "text": "その後、私はそれらを修正しなければならなかった。"
  },
  {
    "start": 8078080,
    "end": 8082544,
    "text": "このビデオが役に立ったと思ったら、他の人にシェアしてくれるのが一番だ。"
  },
  {
    "start": 8082592,
    "end": 8086292,
    "text": "とても難しいと思うので、何度も見ることをお勧めする。"
  },
  {
    "start": 8086426,
    "end": 8091960,
    "text": "なぜなら、このビデオを初めて見た人は、ある程度の理解はできるだろうが、それほど深くは理解できないからだ。"
  },
  {
    "start": 8092300,
    "end": 8102548,
    "text": "2回目になると、より理解が深まり、もしかしたら強化学習やトランスフォーマーのコンセプトを復習して、より深く理解する必要があるかもしれないことに気づくだろう。"
  },
  {
    "start": 8102724,
    "end": 8108424,
    "text": "何度もご覧になることをお勧めします。分かりにくいところがあれば、コメントに残してください。"
  },
  {
    "start": 8108462,
    "end": 8110224,
    "text": "私はいつでもあなたを助けようとする。"
  },
  {
    "start": 8110342,
    "end": 8112350,
    "text": "ええ、良い一日を。"
  }
]