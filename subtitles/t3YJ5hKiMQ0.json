[
  {
    "start": 0,
    "end": 6012,
    "text": "皆さんこんにちは。今日は、私たちのお気に入りの文字レベル言語モデルであるMakeMoreの実装の続きです。"
  },
  {
    "start": 6012,
    "end": 12560,
    "text": "私の後ろの背景が違うことにお気づきでしょうか、それは京都にいるからで、すごいので、ここはホテルの一室なんです。"
  },
  {
    "start": 13440,
    "end": 20338,
    "text": "ここ数回の講義で、多層パーセプトロンの文字レベル言語モデルというアーキテクチャを構築してきました。"
  },
  {
    "start": 20338,
    "end": 30720,
    "text": "3つ前の文字を受け取り、10進数で構成された1つの隠れ層のニューロンを用いた非常に単純な多層パーセプトロンを用いて、連続する4番目の文字を予測しようとすることがわかります。"
  },
  {
    "start": 31440,
    "end": 34960,
    "text": "この講義では、このアーキテクチャをより複雑なものにしたいと思います。"
  },
  {
    "start": 35519,
    "end": 47437,
    "text": "特に、3文字だけでなく、もっと多くの文字を入力にしたいですし、さらに、1つの隠れ層にすべてを入力するのは、情報が多すぎてすぐにつぶれてしまうので避けたいです。"
  },
  {
    "start": 47437,
    "end": 54965,
    "text": "これらの情報を徐々に融合させて、次のキャラクターを推測するような、より深いモデルを作りたいと考えています。"
  },
  {
    "start": 54965,
    "end": 63489,
    "text": "このアーキテクチャをより複雑にしていくと、実はWaveNetに非常によく似たものができあがることがわかります。"
  },
  {
    "start": 63489,
    "end": 76632,
    "text": "WaveNetは2016年にDeepMindが発表したこの論文で、基本的には言語モデルでもあるのですが、文字レベルの配列や単語レベルの配列ではなく、音声の配列を予測しようとするものです。"
  },
  {
    "start": 76632,
    "end": 79825,
    "text": "基本的にモデリング設定は同じです。"
  },
  {
    "start": 79825,
    "end": 84832,
    "text": "これは自己回帰モデルであり、シーケンスの次の文字を予測しようとするものである。"
  },
  {
    "start": 84832,
    "end": 94185,
    "text": "このアーキテクチャは、ツリー状の構造で次の文字を予測するという、興味深い階層的なアプローチをとっています。"
  },
  {
    "start": 94185,
    "end": 99040,
    "text": "というアーキテクチャで、このビデオの中で実装していきます。"
  },
  {
    "start": 99040,
    "end": 100362,
    "text": "はじめましょう。"
  },
  {
    "start": 100362,
    "end": 105163,
    "text": "第5部のストーリーコードは、第3部で終わったところと非常に似ています。"
  },
  {
    "start": 105163,
    "end": 110160,
    "text": "パート4は、余談のようなものですが、手動バックプロパゲーションの練習だったことを思い出してください。"
  },
  {
    "start": 110160,
    "end": 115273,
    "text": "私たちはパート3に戻り、その一部をコピーペーストしています。これがパート5のスターターコードです。"
  },
  {
    "start": 115273,
    "end": 117155,
    "text": "それ以外はほとんど変えていません。"
  },
  {
    "start": 117155,
    "end": 120876,
    "text": "第3部まで読んでいただいた方には、見覚えのある内容も多いはずです。"
  },
  {
    "start": 120876,
    "end": 123974,
    "text": "特に、ごく簡単に言うと、輸入をやっています。"
  },
  {
    "start": 123974,
    "end": 133507,
    "text": "私たちは単語のデータセットを読み、単語のデータセットを個々の例に加工しているのですが、このデータ生成コードは何一つ変わっていません。"
  },
  {
    "start": 133507,
    "end": 136636,
    "text": "たくさんの事例があります。"
  },
  {
    "start": 136636,
    "end": 142982,
    "text": "特に、3人のキャラクターが4人目のキャラクターを予測しようとする例は182,000件もあるのです。"
  },
  {
    "start": 142982,
    "end": 149011,
    "text": "これらの単語を、3つの文字から4つ目の文字を予測する小さな問題に分割しました。"
  },
  {
    "start": 149011,
    "end": 153044,
    "text": "これがデータセットで、これがニューラルネットにやらせようとしていることです。"
  },
  {
    "start": 153044,
    "end": 160605,
    "text": "第3部では、このレイヤーモジュールを中心に、例えばクラスリニアなどのコードを開発するようになりました。"
  },
  {
    "start": 160605,
    "end": 170672,
    "text": "これは、これらのモジュールを積み木のように考え、レゴブロックのように積み上げてニューラルネットワークにするためです。"
  },
  {
    "start": 170672,
    "end": 175232,
    "text": "これらのレイヤーの間にデータを送り込み、グラフのように積み重ねることができるのです。"
  },
  {
    "start": 175232,
    "end": 183170,
    "text": "これらのレイヤーは、PyTorchにあるものと非常によく似たAPIとシグネチャーを持つように開発しました。"
  },
  {
    "start": 183170,
    "end": 188240,
    "text": "torch.nnには、実務で使うようなレイヤーの構成要素がすべて入っているんです。"
  },
  {
    "start": 188240,
    "end": 191440,
    "text": "APIを模倣して、これらすべてを開発していました。"
  },
  {
    "start": 191440,
    "end": 193462,
    "text": "例えば、リニアがあります。"
  },
  {
    "start": 193462,
    "end": 199095,
    "text": "torch.nn.linearも登場し、その署名は我々の署名と非常に似ています。"
  },
  {
    "start": 199095,
    "end": 202695,
    "text": "私が知る限りでは、機能も全く同じです。"
  },
  {
    "start": 202695,
    "end": 208988,
    "text": "リニアは、バッチノルム1d層と、以前開発した10h層がありました。"
  },
  {
    "start": 208988,
    "end": 213681,
    "text": "linearは、このモジュールのフォワードパスで行列の乗算を行うだけです。"
  },
  {
    "start": 213681,
    "end": 218560,
    "text": "バッチノームはもちろん、前回の講義で開発したこのクレイジーレイヤーです。"
  },
  {
    "start": 218560,
    "end": 221857,
    "text": "まあ、いろいろあるんですけどね。"
  },
  {
    "start": 221857,
    "end": 226907,
    "text": "1つ目は、バックプロパゲーションの外で学習させた実行平均と分散を持っていることです。"
  },
  {
    "start": 226907,
    "end": 234368,
    "text": "フォワードパスと呼ばれるこの層の内部で指数移動平均を使って学習させるのです。"
  },
  {
    "start": 234368,
    "end": 240480,
    "text": "それに加えて、訓練時間中はバッチノルムの挙動が異なるため、この訓練フラグが立っているのです。"
  },
  {
    "start": 240480,
    "end": 241426,
    "text": "評価時間です。"
  },
  {
    "start": 241426,
    "end": 247303,
    "text": "バッチノルムが正しい状態、つまり評価状態や訓練状態にあることに、突然注意しなければならなくなりました、"
  },
  {
    "start": 247303,
    "end": 253840,
    "text": "正しいモードにするのを忘れてバグを発生させることがあるものを把握することができるようになったからです。"
  },
  {
    "start": 254480,
    "end": 261224,
    "text": "最後に、バッチノルムは、バッチ内の例全体の統計量や活性度を結合していることを確認した。"
  },
  {
    "start": 261224,
    "end": 274829,
    "text": "今はバッチ要素間で計算を結合しており、前のビデオで見たように活性化統計を制御する目的で行われています。"
  },
  {
    "start": 274829,
    "end": 283021,
    "text": "少なくとも多くのバグにとっては、非常に奇妙なレイヤーです。例えば、トレーニングや評価フェーズなどを調整しなければならないからです。"
  },
  {
    "start": 283021,
    "end": 291634,
    "text": "例えば、平均と分散が落ち着き、実際に定常状態になるのを待つ必要があります。"
  },
  {
    "start": 291634,
    "end": 293815,
    "text": "ということを確認する必要があります。"
  },
  {
    "start": 293815,
    "end": 299397,
    "text": "基本的にこの層には国家があり、国家は有害です、普通は。"
  },
  {
    "start": 299397,
    "end": 302720,
    "text": "今度はジェネレーターのオブジェクトを出した。"
  },
  {
    "start": 302720,
    "end": 306373,
    "text": "以前は、これらのレイヤーの中にジェネレーターイコールGなどがありました。"
  },
  {
    "start": 306373,
    "end": 315840,
    "text": "トーチのRNGを初期化するのは、シンプルに一度だけ、グローバルに行うことにしました。"
  },
  {
    "start": 316800,
    "end": 319821,
    "text": "ここではニューラルネットワークの要素を構築し始めています。"
  },
  {
    "start": 319821,
    "end": 321661,
    "text": "これはとても見覚えがあるはずです。"
  },
  {
    "start": 321661,
    "end": 328655,
    "text": "エンベッディングテーブルCがあり、レイヤーのリストがあり、リニアでフィードからバッチまでノルマがありますね、"
  },
  {
    "start": 328655,
    "end": 336124,
    "text": "10Hにフィードし、その後、線形出力層とその重みはスケールダウンされるので、初期化時に自信を持って間違うことはありません。"
  },
  {
    "start": 336124,
    "end": 338710,
    "text": "これが約12,000個のパラメータであることがわかります。"
  },
  {
    "start": 338710,
    "end": 342516,
    "text": "パラメータにグラデーションが必要であることをPyTorchに伝えているのです。"
  },
  {
    "start": 342516,
    "end": 347405,
    "text": "最適化は、私が知る限り、同一であり、非常に、見覚えがあるはずです。"
  },
  {
    "start": 347405,
    "end": 348640,
    "text": "ここは何も変わりません。"
  },
  {
    "start": 349440,
    "end": 351711,
    "text": "損失関数が非常に狂っているように見えます。"
  },
  {
    "start": 351711,
    "end": 356658,
    "text": "これを修正すべきなのですが、それは32個のバッチ要素が少なすぎるからです。"
  },
  {
    "start": 356658,
    "end": 363911,
    "text": "これらのバッチのどれかで非常に幸運になったり不運になったりすることがあり、非常に厚い損失関数が生まれます。"
  },
  {
    "start": 363911,
    "end": 366043,
    "text": "近日中に修正する予定です。"
  },
  {
    "start": 366043,
    "end": 373607,
    "text": "学習したニューラルネットワークを評価する場合、バッチノルム層のため、すべての層が学習イコールfalseになるように設定することを忘れてはいけません。"
  },
  {
    "start": 373607,
    "end": 376751,
    "text": "今までのバッチノームレイヤーにしか関係ありません。"
  },
  {
    "start": 376751,
    "end": 378719,
    "text": "評価します。"
  },
  {
    "start": 378719,
    "end": 387162,
    "text": "現在、検証損失は2.10で、かなり良好ですが、まだ先があります。"
  },
  {
    "start": 387162,
    "end": 395143,
    "text": "2.10でも、モデルからサンプリングすると、トレーニングセットには存在しない、比較的名前に近い結果が得られることがわかります。"
  },
  {
    "start": 395143,
    "end": 400681,
    "text": "例えば、Ivan、Kilo、Pras、Alayaなどです。"
  },
  {
    "start": 400681,
    "end": 406880,
    "text": "確かに無理はないが、すごいとは言えないと思う。"
  },
  {
    "start": 406880,
    "end": 413343,
    "text": "この検証のロスをもっと低くして、より名前に近い良いサンプルを得ることは可能です。"
  },
  {
    "start": 413343,
    "end": 415893,
    "text": "このモデルを今すぐ改善しよう。"
  },
  {
    "start": 415893,
    "end": 421956,
    "text": "まずはこのグラフを修正しましょう！私の目には短剣が刺さっていて、もう我慢できませんから。"
  },
  {
    "start": 421956,
    "end": 425579,
    "text": "loss iは、思い起こせばPythonのfloatのリストである。"
  },
  {
    "start": 425579,
    "end": 429680,
    "text": "例えば、最初の10個の要素はこのような感じです。"
  },
  {
    "start": 430880,
    "end": 439156,
    "text": "基本的には、これらの値を平均化し、より代表的な値を得ることが必要です。"
  },
  {
    "start": 439156,
    "end": 441596,
    "text": "次のような方法があります。"
  },
  {
    "start": 441596,
    "end": 449262,
    "text": "PyTorchでは、例えば最初の10個の数字のテンソルを作ると、これは現在1次元の配列になります。"
  },
  {
    "start": 449262,
    "end": 452319,
    "text": "この配列を2次元として見ることができることを思い出してください。"
  },
  {
    "start": 452319,
    "end": 457194,
    "text": "例えば、2x5の配列として見ることができ、これは今、2x5の2次元テンソルです。"
  },
  {
    "start": 457194,
    "end": 465920,
    "text": "PyTorchが行ったことは、このテンソルの1行目が最初の5つの要素で、2行目が2番目の5つの要素であることがわかります。"
  },
  {
    "start": 466640,
    "end": 469879,
    "text": "例として5×2という見方もできるんです。"
  },
  {
    "start": 469879,
    "end": 475641,
    "text": "これらの数字の代わりに負の1も使えることを思い出してください。"
  },
  {
    "start": 475641,
    "end": 481786,
    "text": "PyTorchは、要素の数がうまくいくように、その数を計算します。"
  },
  {
    "start": 481786,
    "end": 484022,
    "text": "これでもか、これでもかというほどです。"
  },
  {
    "start": 484022,
    "end": 485140,
    "text": "どちらも有効です。"
  },
  {
    "start": 485140,
    "end": 487508,
    "text": "もちろん、これではダメでしょう。"
  },
  {
    "start": 487508,
    "end": 493437,
    "text": "これで連続した値の一部を行に分散させることができるわけですね。"
  },
  {
    "start": 493437,
    "end": 505827,
    "text": "今できることは、まず、floatのリストからTorch.Tensorを作成し、それを何であろうと表示することだからです、"
  },
  {
    "start": 505827,
    "end": 510267,
    "text": "それを1,000個の連続した要素の行に引き伸ばすのです。"
  },
  {
    "start": 510267,
    "end": 519061,
    "text": "200×1,000となり、各行が1,000個の連続した要素になります。"
  },
  {
    "start": 519061,
    "end": 526079,
    "text": "これで行に沿った平均を出すことができ、この形はちょうど200になるからです。"
  },
  {
    "start": 527040,
    "end": 529813,
    "text": "基本的にすべての行で平均値をとっています。"
  },
  {
    "start": 529813,
    "end": 532400,
    "text": "plt.plotは、もっと素敵なものになるはずです。"
  },
  {
    "start": 533360,
    "end": 534147,
    "text": "ずっといい。"
  },
  {
    "start": 534147,
    "end": 537292,
    "text": "基本的にはかなり進んでいることがわかります。"
  },
  {
    "start": 537292,
    "end": 539989,
    "text": "ここが、学習率の減衰です。"
  },
  {
    "start": 539989,
    "end": 548560,
    "text": "ここでは、学習率の低下によってシステムから大量のエネルギーが取り除かれ、この最適化におけるローカルミニマムのようなものに落ち着くことができることがわかります。"
  },
  {
    "start": 549280,
    "end": 551120,
    "text": "こちらはもっと素敵な区画です。"
  },
  {
    "start": 551120,
    "end": 556674,
    "text": "出てきてモンスターを削除して、今後はこれを使うことにしましょう。"
  },
  {
    "start": 556674,
    "end": 564066,
    "text": "次に気になるのは、フォワードパスが少々いびつで、コード行数が多すぎるということです。"
  },
  {
    "start": 564066,
    "end": 571011,
    "text": "特に、レイヤーリスト内の一部のレイヤーを整理したが、理由もなくすべてのレイヤーを整理していないことがわかる。"
  },
  {
    "start": 571011,
    "end": 576612,
    "text": "特に、レイヤーの外側にエンベッディングテーブルの特殊なケースが残っていることがわかります。"
  },
  {
    "start": 576612,
    "end": 581254,
    "text": "それに加えて、ここでの視聴操作もレイヤーの外側になります。"
  },
  {
    "start": 581254,
    "end": 585869,
    "text": "これらのレイヤーを作成し、そのレイヤーをリストだけに追加することができるようにしましょう。"
  },
  {
    "start": 585869,
    "end": 597934,
    "text": "特に、必要なのは、この埋め込みテーブルと、バッチxbやテンソルxbの中の整数をインデックス化することです。"
  },
  {
    "start": 597934,
    "end": 602055,
    "text": "エンベデッドテーブルのルックアップをインデックス化しただけのものです。"
  },
  {
    "start": 602055,
    "end": 612505,
    "text": "これは、前のビデオを思い出していただきたいのですが、単純に文字の埋め込みを並べ替えて、一列に引き伸ばすという操作です。"
  },
  {
    "start": 612505,
    "end": 622242,
    "text": "PyTorchでは閲覧が非常に安価で、メモリがコピーされないため、無料であることを除けば、この操作は基本的に連結操作と同じです。"
  },
  {
    "start": 622242,
    "end": 625360,
    "text": "そのテンソルをどう見るかを再表現しているだけなのです。"
  },
  {
    "start": 625920,
    "end": 633994,
    "text": "この「埋め込み操作」と「平坦化操作」の両方のモジュールを作りましょう。"
  },
  {
    "start": 633994,
    "end": 638175,
    "text": "実はこのコードは、時間短縮のために書いたものなんです。"
  },
  {
    "start": 638175,
    "end": 641570,
    "text": "モジュールembeddingとモジュールflattenがあります。"
  },
  {
    "start": 641570,
    "end": 649409,
    "text": "どちらも、フォワードパスでインデキシング処理を行い、こちらでフラット化処理を行うだけです。"
  },
  {
    "start": 649409,
    "end": 655904,
    "text": "このcは、現在では、エンベッディングモジュール内のself.weightになるだけです。"
  },
  {
    "start": 655904,
    "end": 663226,
    "text": "これらのレイヤーを特にembeddingとflattenと呼んでいるのは、この2つが実際にPyTorchに存在することがわかったからです。"
  },
  {
    "start": 663226,
    "end": 669170,
    "text": "PyTorchではnとdot embeddingがあり、embeddingの数とembeddingの次元を取ることもできます。"
  },
  {
    "start": 669170,
    "end": 670236,
    "text": "ここと同じように"
  },
  {
    "start": 670236,
    "end": 676401,
    "text": "PyTorchは、私たちの目的にはまだ使っていない他のキーワード引数をたくさん取り込みます。"
  },
  {
    "start": 676401,
    "end": 684281,
    "text": "PyTorchにも存在するflattenのためのもので、私たちが使っていない追加のキーワード引数も受け取ります。"
  },
  {
    "start": 684281,
    "end": 686218,
    "text": "非常にシンプルなflattenを持っています。"
  },
  {
    "start": 686218,
    "end": 690742,
    "text": "どちらもPyTorchに存在するもので、もう少しシンプルなものです。"
  },
  {
    "start": 690742,
    "end": 696934,
    "text": "これがあれば、特殊なケースに入ったものも簡単に取り出せますね。"
  },
  {
    "start": 696934,
    "end": 703681,
    "text": "Cの代わりにエンベッディングとボキャブラリーサイズとNエンベッディングを用意すればいいのです。"
  },
  {
    "start": 703681,
    "end": 707520,
    "text": "エンベッディングの後、フラット化するのです。"
  },
  {
    "start": 708880,
    "end": 712583,
    "text": "そのモジュールを組み立てて、このcを取り出せばいいのです。"
  },
  {
    "start": 712583,
    "end": 721103,
    "text": "cはエンベッディングの重さであり、レイヤーの中にあるものだからです。"
  },
  {
    "start": 721103,
    "end": 723120,
    "text": "そのまま使えるはずです。"
  },
  {
    "start": 723120,
    "end": 732884,
    "text": "このレイヤーの外側では、これらを行う必要がないため、フォワードパスが大幅に簡略化されているからです。"
  },
  {
    "start": 732884,
    "end": 736400,
    "text": "レイヤーの中に入っているので、それらを削除することができます。"
  },
  {
    "start": 737120,
    "end": 742673,
    "text": "まず始めに、この小さなxを用意します。"
  },
  {
    "start": 742673,
    "end": 747471,
    "text": "入力時のこれらの文字の同一性を指定する整数のテンソル。"
  },
  {
    "start": 747471,
    "end": 752847,
    "text": "これらの文字が直接1層目に入力されるようになり、これだけでもうまくいくはずです。"
  },
  {
    "start": 752847,
    "end": 759104,
    "text": "この最初の繰り返しが実行され、間違いがないことを確認したいからです。"
  },
  {
    "start": 759104,
    "end": 765040,
    "text": "ここではフォワードパスを大幅に簡略化しました。"
  },
  {
    "start": 765040,
    "end": 768560,
    "text": "マイクを変えたので、音声が少し良くなっているといいのですが、すみません。"
  },
  {
    "start": 769520,
    "end": 777600,
    "text": "もう一つ、私たちのコードをさらにPyTorch化するためにやりたいことがあるのですが、現在、私たちはすべてのモジュールを裸のレイヤーリストとして管理しています。"
  },
  {
    "start": 778160,
    "end": 783920,
    "text": "PyTorchのコンテナの概念を導入することで、これを簡略化することも可能です。"
  },
  {
    "start": 783920,
    "end": 795600,
    "text": "torch.nnにはコンテナという概念があり、コンテナは基本的にレイヤーをリストやディクテなどに整理する方法です。"
  },
  {
    "start": 795600,
    "end": 802046,
    "text": "特に、レイヤーのリストを保持するsequentialがあり、PyTorchのモジュールクラスになっています。"
  },
  {
    "start": 802046,
    "end": 808448,
    "text": "基本的に、与えられた入力を、ここで行っているように、すべてのレイヤーを順次通過させるだけです。"
  },
  {
    "start": 808448,
    "end": 810827,
    "text": "自分でシーケンシャルを書こう。"
  },
  {
    "start": 810827,
    "end": 816342,
    "text": "ここにコードを書きましたが、基本的にシーケンシャルのコードは非常に簡単です。"
  },
  {
    "start": 816342,
    "end": 818535,
    "text": "私たちは、ここで保管しているレイヤーのリストでパスします。"
  },
  {
    "start": 818535,
    "end": 823761,
    "text": "フォワードパスで任意の入力があれば、すべてのレイヤーを順次呼び出して結果を返すだけです。"
  },
  {
    "start": 823761,
    "end": 827648,
    "text": "パラメータに関しては、子モジュールのパラメータをすべて使用します。"
  },
  {
    "start": 827648,
    "end": 835461,
    "text": "これを実行すると、レイヤーの裸のリストを維持しないので、また大幅に簡素化することができます。"
  },
  {
    "start": 835461,
    "end": 843720,
    "text": "これで、モジュールであり、特にこれらすべてのレイヤーのシーケンシャルであるモデルの概念ができました。"
  },
  {
    "start": 843720,
    "end": 851840,
    "text": "パラメータは単にmodel.parametersとなり、そのリスト内包がここで生きるようになりました。"
  },
  {
    "start": 853840,
    "end": 856960,
    "text": "その後、私たちはここで、かつてやっていたことをすべてやっています。"
  },
  {
    "start": 857920,
    "end": 858467,
    "text": "今ここ。"
  },
  {
    "start": 858467,
    "end": 863680,
    "text": "ここで転送を行う必要がないため、コードが大幅に簡素化されます。"
  },
  {
    "start": 863680,
    "end": 866064,
    "text": "入力データに対してモデルを呼び出すだけです。"
  },
  {
    "start": 866064,
    "end": 868640,
    "text": "ここでいう入力データとは、xbの中の整数のことである。"
  },
  {
    "start": 868640,
    "end": 880725,
    "text": "モデルの出力であるlogitsは、xbで呼び出されたモデルであり、ここでクロスエントロピーはlogitsとターゲットを取るだけです。"
  },
  {
    "start": 880725,
    "end": 886831,
    "text": "大幅に簡略化され、そして、これは良い感じなので、これが実行されることを確認しましょう。"
  },
  {
    "start": 886831,
    "end": 887840,
    "text": "それは良さそうですね。"
  },
  {
    "start": 889360,
    "end": 893549,
    "text": "ここで実はまだやることがあるのですが、また後ほど。"
  },
  {
    "start": 893549,
    "end": 895358,
    "text": "今のところ、もうレイヤーはありません。"
  },
  {
    "start": 895358,
    "end": 896869,
    "text": "model.layersがありますね。"
  },
  {
    "start": 896869,
    "end": 902657,
    "text": "これらのクラスの属性に直接アクセスするのはいたずらなので、後で戻って修正することにしましょう。"
  },
  {
    "start": 902657,
    "end": 911920,
    "text": "ロジットはxで呼ばれるモデルで、そのロジットがここに来るからです。"
  },
  {
    "start": 914000,
    "end": 919715,
    "text": "これは、ニューラルネットを初期化した直後であるため、現在はひどい状態ですが、トレーニング検証の損失を評価することができます。"
  },
  {
    "start": 919715,
    "end": 921621,
    "text": "モデルからサンプリングすることも可能です。"
  },
  {
    "start": 921621,
    "end": 929360,
    "text": "これは、コンテキストと結果のロジットにモデルを呼び出すだけなので、劇的に単純化されます。"
  },
  {
    "start": 930160,
    "end": 937182,
    "text": "これらのロジットはソフトマックスに入り、確率などを得ることで、このモデルからサンプルを得ることができます。"
  },
  {
    "start": 937182,
    "end": 938880,
    "text": "私は何を失敗したのでしょうか？"
  },
  {
    "start": 942320,
    "end": 950441,
    "text": "これは、モデルをゼロから再初期化したため、モデルが学習されず、ちんぷんかんぷんな結果となっています。"
  },
  {
    "start": 950441,
    "end": 955251,
    "text": "問題は、このセルをlayersだけでなくmodel.layersに直したときでした。"
  },
  {
    "start": 955251,
    "end": 957080,
    "text": "実際にセルを走らせたわけではありません。"
  },
  {
    "start": 957080,
    "end": 959496,
    "text": "ニューラルネットはトレーニングモードになっていました。"
  },
  {
    "start": 959496,
    "end": 966807,
    "text": "この問題は、バッチノルム層がトレーニングモードになっていたため、バッチノルム層がしばしばやりたがるものです。"
  },
  {
    "start": 966807,
    "end": 973040,
    "text": "ここでは、コンテキストで構成された1つの例だけのバッチである入力を渡しています。"
  },
  {
    "start": 973040,
    "end": 977211,
    "text": "トレーニングモードにあるバッチノルムに1つの例を渡そうとすると"
  },
  {
    "start": 977211,
    "end": 980625,
    "text": "結局は入力を使って分散を推定することになります。"
  },
  {
    "start": 980625,
    "end": 985350,
    "text": "一つの数値の分散は、広がりを表す指標であるため、数値ではありません。"
  },
  {
    "start": 985350,
    "end": 990208,
    "text": "例えば、5という数字1つだけの分散は、数字ではないことがわかります。"
  },
  {
    "start": 990208,
    "end": 992027,
    "text": "そうなりました。"
  },
  {
    "start": 992027,
    "end": 997627,
    "text": "バッチノルムは基本的に問題を引き起こし、それがその後のすべての処理を汚染する。"
  },
  {
    "start": 997627,
    "end": 1013265,
    "text": "バッチノルムがトレーニングモードであったため、間違った結果が出たのです。"
  },
  {
    "start": 1013265,
    "end": 1023001,
    "text": "バッチの標本統計量を使用しているため、間違った結果になっている。"
  },
  {
    "start": 1023001,
    "end": 1035332,
    "text": "このように、トレーニング中か否かの状態を適切に維持しなかったために、インラインでバグが発生した例もあります、"
  },
  {
    "start": 1035332,
    "end": 1041656,
    "text": "トレーニングの損失は2.05、検証の損失は2.10で、これらの損失は互いに非常によく似ているからです。"
  },
  {
    "start": 1041656,
    "end": 1061180,
    "text": "このタスクではオーバーフィットしすぎていないという感覚があります。ニューラルネットワークのサイズを拡大し、すべてを大きく深くすることでパフォーマンスをさらに向上させることができます。"
  },
  {
    "start": 1061180,
    "end": 1074960,
    "text": "問題は、これを生産的に大きくする素朴な方法がないことです。もちろん、レイヤーを構成するブロックや材料を使って、ここに追加のレイヤーを導入し、ネットワークをより深くすることはできます。"
  },
  {
    "start": 1074960,
    "end": 1080636,
    "text": "やはり冒頭でずっとキャラクターを一枚に潰しているわけです。"
  },
  {
    "start": 1080636,
    "end": 1088904,
    "text": "これをもっと大きな層にしてニューロンを追加しても、一足飛びにすべての情報を潰してしまうのは、なんだかバカみたいですよね。"
  },
  {
    "start": 1088904,
    "end": 1094501,
    "text": "その代わり、WaveNetの場合、ネットワークはこのような形にしたいと思います。"
  },
  {
    "start": 1094501,
    "end": 1099492,
    "text": "WaveNetで次の文字を予測しようとするときに見ることができます。"
  },
  {
    "start": 1099492,
    "end": 1110071,
    "text": "それまでのキャラクターがフィードインする機能なのですが、その異なるキャラクターをすべて一重に潰して、サンドイッチにするわけではありません。"
  },
  {
    "start": 1110071,
    "end": 1111835,
    "text": "ゆっくり潰していきます。"
  },
  {
    "start": 1111835,
    "end": 1117407,
    "text": "特に、2つの文字を使って、ビグラム表現のように融合させます。"
  },
  {
    "start": 1117407,
    "end": 1120882,
    "text": "それを全キャラクターに連続して行う。"
  },
  {
    "start": 1120882,
    "end": 1131200,
    "text": "そのビグラムを4つの文字レベルのチャンクに融合させ、さらにそれを融合させるという、ツリーのような階層的な方法で行っています。"
  },
  {
    "start": 1131920,
    "end": 1141081,
    "text": "このように、前の文脈の情報を、深くなるにつれてゆっくりとネットワークに融合させていく、そんなアーキテクチャを実現したいのです。"
  },
  {
    "start": 1141081,
    "end": 1146128,
    "text": "WaveNetの場合、これは拡張された因果関係畳み込み層のスタックを視覚化したものです。"
  },
  {
    "start": 1146128,
    "end": 1149785,
    "text": "とても怖い感じがしますが、実は考え方はとてもシンプルです。"
  },
  {
    "start": 1149785,
    "end": 1155748,
    "text": "因果関係畳み込み層を拡張しているのは、高速化のための実装上の工夫にすぎません。"
  },
  {
    "start": 1155748,
    "end": 1157040,
    "text": "それは後で見ることにしよう。"
  },
  {
    "start": 1157040,
    "end": 1161417,
    "text": "今は、このプログレッシブ・フュージョンという基本的な考え方にとどめておいてください。"
  },
  {
    "start": 1161417,
    "end": 1163525,
    "text": "ネットワークをより深くしていきたい。"
  },
  {
    "start": 1163525,
    "end": 1172530,
    "text": "各レベルで、2つの文字、2つのビッグラム、2つのフォーグラムというように、連続する2つの要素だけを融合させたいので、これを実装してみましょう。"
  },
  {
    "start": 1172530,
    "end": 1178227,
    "text": "まず最初に、データセットを構築した場所までスクロールして、ブロックサイズを3から8に変更してみましょう。"
  },
  {
    "start": 1178227,
    "end": 1183001,
    "text": "8文字分の文脈から9文字目を予測することになります。"
  },
  {
    "start": 1183001,
    "end": 1193545,
    "text": "データセットはこのようになり、シーケンスの次の文字を予測するためのコンテキストが増え、8つの文字がこのツリー状の構造で処理されるようになりました。"
  },
  {
    "start": 1193545,
    "end": 1196848,
    "text": "ここにスクロールすれば、ここにあるすべてのものが使えるようになるはずです。"
  },
  {
    "start": 1196848,
    "end": 1202963,
    "text": "ネットワークの再定義ができるはずです。パラメータが1万個増えているのがわかります。"
  },
  {
    "start": 1202963,
    "end": 1208208,
    "text": "これは、ブロックサイズが大きくなったので、この最初のリニアレイヤーがより大きくなったからです。"
  },
  {
    "start": 1208208,
    "end": 1212239,
    "text": "リニアレイヤーは、このミドルレイヤーに8つのキャラクターを取り込みました。"
  },
  {
    "start": 1212239,
    "end": 1214484,
    "text": "そこには、もっとたくさんのパラメータがあるのです。"
  },
  {
    "start": 1214484,
    "end": 1215920,
    "text": "実行すればよいのです。"
  },
  {
    "start": 1216960,
    "end": 1219848,
    "text": "最初の1回を終えたところで、ちょっと休憩させてください。"
  },
  {
    "start": 1219848,
    "end": 1221680,
    "text": "これが問題なく動作することがわかります。"
  },
  {
    "start": 1221680,
    "end": 1223823,
    "text": "このネットワークはあまり意味がないんです。"
  },
  {
    "start": 1223823,
    "end": 1231040,
    "text": "このままでは情報が溢れすぎてしまうので、階層化スキームをどのように実装していくかを考えてみましょう。"
  },
  {
    "start": 1231840,
    "end": 1241863,
    "text": "ここで再実装の詳細を説明する前に、実際に実行してみて、コンテキスト長を遅延的にスケールアップした場合のベースライン・パフォーマンスがどの程度になるかを確認したいと思います。"
  },
  {
    "start": 1241863,
    "end": 1242953,
    "text": "走らせてみる。"
  },
  {
    "start": 1242953,
    "end": 1250674,
    "text": "きれいな損失曲線が得られ、その損失を評価すると、実はコンテキストラインの長さを増やすだけでかなり改善されることがわかります。"
  },
  {
    "start": 1250674,
    "end": 1253189,
    "text": "ちょっとだけパフォーマンスログを始めてみました。"
  },
  {
    "start": 1253189,
    "end": 1258099,
    "text": "以前は、検証の結果、2.10というパフォーマンスが得られていたのですが、今はどうでしょうか。"
  },
  {
    "start": 1258099,
    "end": 1264074,
    "text": "コンテキストの長さを3から8に拡大するだけで、2.02の性能を得ることができます。"
  },
  {
    "start": 1264074,
    "end": 1266356,
    "text": "ここでかなり改善されました。"
  },
  {
    "start": 1266356,
    "end": 1272160,
    "text": "モデルからサンプリングすると、名前も確実に質的に向上していることがわかります。"
  },
  {
    "start": 1273040,
    "end": 1283218,
    "text": "もちろん、ここで多くの時間をかけてチューニングし、さらに大きくして、このシンプルなセットアップでもネットワークをさらに拡大することができます。"
  },
  {
    "start": 1283218,
    "end": 1289644,
    "text": "続けて、階層モデルを実装し、これをあくまで大まかな基準性能として扱うことにしましょう。"
  },
  {
    "start": 1289644,
    "end": 1299874,
    "text": "ハイパーパラメータについては、多くの最適化がテーブルに残っています。"
  },
  {
    "start": 1299874,
    "end": 1318578,
    "text": "ここでやったことは、ニューラルネットの前進を見るためのちょっとしたスクラッチスペースを作り、ニューラルネットが前進する過程でテンソルの形状を検査することです。ここでは、デバッグのために一時的に4つの例、つまり4つのランダムな整数のバッチを作成します、"
  },
  {
    "start": 1318578,
    "end": 1325298,
    "text": "トレーニングセットからこれらの行を抜き出し、モデルに入力xbを渡します。"
  },
  {
    "start": 1325298,
    "end": 1334563,
    "text": "xbの形状は、4つの例しかないので、4×8であり、この8が現在のブロックサイズです。"
  },
  {
    "start": 1334563,
    "end": 1348980,
    "text": "xbを調べてみると、4つの例があり、それぞれがxbの行で、8つの文字があり、この整数テンソルにはそれらの文字のアイデンティティが含まれていることがわかります。"
  },
  {
    "start": 1348980,
    "end": 1352456,
    "text": "ニューラルネットの最初の層は、埋め込み層です。"
  },
  {
    "start": 1352456,
    "end": 1359218,
    "text": "この整数テンソルのxbを埋め込み層に通すと、4×8×10の出力が得られます。"
  },
  {
    "start": 1359218,
    "end": 1366140,
    "text": "埋め込みテーブルには、各文字に対して学習しようとする10次元のベクトルがあります。"
  },
  {
    "start": 1366140,
    "end": 1383953,
    "text": "埋め込み層が行うのは、これらの整数のそれぞれの埋め込みベクトルを取り出して、4×8×10のテンソルに整理することです。つまり、これらの整数はすべて、この3次元テンソル内で10次元のベクトルに変換されます。"
  },
  {
    "start": 1383953,
    "end": 1391850,
    "text": "このテンソルを4×80のテンソルとして見ることができます。"
  },
  {
    "start": 1391850,
    "end": 1407199,
    "text": "この場合、8つの文字に対する10次元の埋め込みはすべて長い列に引き伸ばされることになり、基本的には連結操作のように見えます、"
  },
  {
    "start": 1407199,
    "end": 1408479,
    "text": "4×80になりました。"
  },
  {
    "start": 1408479,
    "end": 1415441,
    "text": "この80の内部では、10次元のベクトルがすべて隣り合って連結されているのです。"
  },
  {
    "start": 1415441,
    "end": 1422892,
    "text": "リニアレイヤーはもちろん、マトリックスの乗算だけで80チャンネル、200チャンネルを作成します。"
  },
  {
    "start": 1422892,
    "end": 1427085,
    "text": "ここまではいいとして、次は意外なものをお見せしましょう。"
  },
  {
    "start": 1427085,
    "end": 1439035,
    "text": "リニアレイヤーの内部を見て、その働きを思い出してみましょう。フォワードパスのリニアレイヤーは、入力xを受け取り、重みと掛け合わせ、さらにオプションでバイアスを加えます。"
  },
  {
    "start": 1439035,
    "end": 1441774,
    "text": "ここでいう重さとは、ここで定義される2次元のことです。"
  },
  {
    "start": 1441774,
    "end": 1443832,
    "text": "ここでは、バイアスは一面的です。"
  },
  {
    "start": 1443832,
    "end": 1450606,
    "text": "このリニアレイヤーの内部で起こっていることは、効果的に形状に関わることで、今はこのように見えます。"
  },
  {
    "start": 1450606,
    "end": 1459278,
    "text": "ここでは乱数を使用していますが、形と何が起こるかを説明するために、基本的に4×80の入力がリニアレイヤーに入ります、"
  },
  {
    "start": 1459278,
    "end": 1470061,
    "text": "この80×200のウェイトマトリックスに、内部でプラス200のバイアスがかかり、リニアレイヤーから出る全体の形状は、このように4×200になります。"
  },
  {
    "start": 1470061,
    "end": 1475016,
    "text": "ここで注目したいのは、これが4×200のテンソルを作成することです。"
  },
  {
    "start": 1475016,
    "end": 1483260,
    "text": "4×200の放送があり、200で放送しているので、ここですべてが機能する。"
  },
  {
    "start": 1483260,
    "end": 1493579,
    "text": "意外と知られていないことですが、乗算されるこの入力は、実は2次元である必要はないんですよ、ということをお見せしたいと思います。"
  },
  {
    "start": 1493579,
    "end": 1510685,
    "text": "Pytorchのこの行列乗算演算子は非常に強力で、実際に高次元の配列やテンソルを渡してもすべてうまくいきます。例えば、これは4×5×80で、この場合の結果は4×5×200になります。"
  },
  {
    "start": 1510685,
    "end": 1523166,
    "text": "事実上、行列の掛け算は最後の次元にのみ作用し、入力テンソルのそれ以前の次元は変更されないままになっているのです。"
  },
  {
    "start": 1523166,
    "end": 1531234,
    "text": "基本的にこれらの左側のディメンジョンは、すべて単なるバッチディメンジョンとして扱われるからです。"
  },
  {
    "start": 1531234,
    "end": 1533863,
    "text": "複数のバッチディメンジョンを持つことができます。"
  },
  {
    "start": 1533863,
    "end": 1536043,
    "text": "そのすべての次元で並行して"
  },
  {
    "start": 1536043,
    "end": 1538640,
    "text": "最後の次元で行列の乗算を行っているのです。"
  },
  {
    "start": 1539440,
    "end": 1565145,
    "text": "8つの文字が入力されてくるので、それをすべて8次元の大きなベクトルに平らにするのではなく、80を行列掛けしてすぐに重み行列掛けにするのではなく、このようにグループ化したいからです。"
  },
  {
    "start": 1565145,
    "end": 1581672,
    "text": "連続する2つの要素1、2、3、4、5、6、7、8は、すべて基本的に平坦化され、重み行列が掛けられるはずですが、この4つのグループはすべて並行して処理したいのです。"
  },
  {
    "start": 1581672,
    "end": 1585756,
    "text": "導入することで、バッチ次元のようなものです。"
  },
  {
    "start": 1585756,
    "end": 1601295,
    "text": "これらのビグラムグループを、個々の例の4つのバッチ次元で、また、この例の4つの例の実際のバッチ次元で、基本的に並行して処理することができます。"
  },
  {
    "start": 1601295,
    "end": 1613677,
    "text": "80倍したものを200倍してリニアレイヤーにすると、このようになります。"
  },
  {
    "start": 1613677,
    "end": 1618625,
    "text": "その代わり、80文字や80個の数字が入ってくるのは困るんです。"
  },
  {
    "start": 1618625,
    "end": 1624987,
    "text": "最初のレイヤーで2つのキャラクターを登場させ、その2つのキャラクターを融合させることです。"
  },
  {
    "start": 1624987,
    "end": 1630631,
    "text": "20個入ってきてほしいだけなのですが、20個の数字が入ってきます。"
  },
  {
    "start": 1630631,
    "end": 1634159,
    "text": "ここでは、4×80がリニア層に食い込むことは避けたい。"
  },
  {
    "start": 1634159,
    "end": 1637649,
    "text": "このような2人1組のグループには、実際に投入してほしいですね。"
  },
  {
    "start": 1637649,
    "end": 1641760,
    "text": "4×80ではなく、4×4×20にしたい。"
  },
  {
    "start": 1643440,
    "end": 1649338,
    "text": "4つのグループからなり、それぞれが10次元のベクトルである。"
  },
  {
    "start": 1649338,
    "end": 1653029,
    "text": "今必要なのは、フラット化レイヤーを変更することです。"
  },
  {
    "start": 1653029,
    "end": 1668845,
    "text": "4×80は出力されませんが、4×4×20は出力されます。基本的に、この連続する2文字ごとが最後の次元に詰め込まれ、この4が最初のバッチの次元となります。"
  },
  {
    "start": 1668845,
    "end": 1675222,
    "text": "この4は、これらの例の一つ一つの中にある4つのグループを指す第2バッチ次元です。"
  },
  {
    "start": 1675222,
    "end": 1679143,
    "text": "このように増殖していくだけなので、このような状態にしたいのです。"
  },
  {
    "start": 1679143,
    "end": 1683640,
    "text": "リニアレイヤーが期待する入力数を変更する必要がありそうです。"
  },
  {
    "start": 1683640,
    "end": 1685016,
    "text": "80を期待してはいけない。"
  },
  {
    "start": 1685016,
    "end": 1690880,
    "text": "20個と予想されるので、フラット化レイヤーを変更して、完全にフラット化されないようにする必要があります。"
  },
  {
    "start": 1690880,
    "end": 1695506,
    "text": "この例全体では、4×80ではなく、4×4×20を作成する必要があります。"
  },
  {
    "start": 1695506,
    "end": 1708085,
    "text": "基本的には、4×8×10の入力があり、それを平坦化レイヤーに送り込み、平坦化レイヤーがそれを引き伸ばすだけです。"
  },
  {
    "start": 1708085,
    "end": 1714800,
    "text": "Flattenの実装を思い出すと、xを受け取って、それをバッチの次元が何であろうと見なすだけです。"
  },
  {
    "start": 1714800,
    "end": 1724561,
    "text": "ドットビューを表示し、さらにマイナス1も表示します。"
  },
  {
    "start": 1724561,
    "end": 1727116,
    "text": "現在のところです。"
  },
  {
    "start": 1727116,
    "end": 1734685,
    "text": "10次元の連続したベクトルが連結された4×4×20のような形にしたいのです。"
  },
  {
    "start": 1734685,
    "end": 1746156,
    "text": "Pythonでは、10の範囲のリストを取ることができます。"
  },
  {
    "start": 1746156,
    "end": 1752394,
    "text": "1から始めて2のステップで奇数部分をすべて取得するようなインデックスも可能です。"
  },
  {
    "start": 1752394,
    "end": 1756751,
    "text": "これを実現する一つの方法として、次のようになります。"
  },
  {
    "start": 1756751,
    "end": 1765448,
    "text": "使えば、eにすべてのバッチ要素、そしてこの次元の偶数要素をインデックスすることができます。"
  },
  {
    "start": 1765448,
    "end": 1767763,
    "text": "インデックス0 2 4 8で表示します。"
  },
  {
    "start": 1767763,
    "end": 1773409,
    "text": "この最後の次元からここにあるすべてのパーツ。"
  },
  {
    "start": 1773409,
    "end": 1776890,
    "text": "偶数文字が得られます。"
  },
  {
    "start": 1776890,
    "end": 1792520,
    "text": "ここにすべての奇数文字が表示されます。基本的に私たちがやりたいことは、これらがpytorchで連結されるようにし、2次元に沿ってこれら2つのテンソルを連結することなのです。"
  },
  {
    "start": 1792520,
    "end": 1796112,
    "text": "その形状は4×4×20となります。"
  },
  {
    "start": 1796112,
    "end": 1798480,
    "text": "これが、私たちが望む結果であることは間違いありません。"
  },
  {
    "start": 1798480,
    "end": 1809307,
    "text": "偶数部と奇数部を明示的に取得し、それらを4×4×10で隣り合わせに並べ、連結することで、このような動作を実現しています。"
  },
  {
    "start": 1809307,
    "end": 1815910,
    "text": "その結果、viewをもう一度使って、正しい形状を要求すればいいことがわかりました。"
  },
  {
    "start": 1815910,
    "end": 1823116,
    "text": "たまたまこの場合、これらのベクトルは、私たちが望むとおりに配置されることになります。"
  },
  {
    "start": 1823116,
    "end": 1828115,
    "text": "特に、Eを4×4×20と見立てた場合、私たちが望むような形になります。"
  },
  {
    "start": 1828115,
    "end": 1831180,
    "text": "と正確に等しいことが確認できる。"
  },
  {
    "start": 1831180,
    "end": 1835821,
    "text": "これを明示的連結と呼ぶことにする。"
  },
  {
    "start": 1835821,
    "end": 1836742,
    "text": "と思いますね。"
  },
  {
    "start": 1836742,
    "end": 1839707,
    "text": "明示的なドット形状は、4×4×20です。"
  },
  {
    "start": 1839707,
    "end": 1842280,
    "text": "4×4×20と見立てればいいのです。"
  },
  {
    "start": 1842280,
    "end": 1852440,
    "text": "explicitと比較したときに、bitになることを確認できます。これは要素単位の演算なので、すべての値がtrueであることを確認します。"
  },
  {
    "start": 1852440,
    "end": 1857572,
    "text": "「concatenate」を明示的に呼び出す必要はないのです。"
  },
  {
    "start": 1857572,
    "end": 1861899,
    "text": "この入力テンソルを単純に平坦化するために取ることができます。"
  },
  {
    "start": 1861899,
    "end": 1864901,
    "text": "私たちは、好きなように捉えればいいのです。"
  },
  {
    "start": 1864901,
    "end": 1868909,
    "text": "特に、ネガティブなものは引き伸ばしたくありません。"
  },
  {
    "start": 1868909,
    "end": 1885242,
    "text": "3次元の配列を作りたいのですが、連続したベクトルがいくつあるかによって、例えば2つ融合させたい場合は、この次元を20にして、ここでマイナス1を使ってくださいとお願いすればいいんです。"
  },
  {
    "start": 1885242,
    "end": 1890707,
    "text": "pytorchは、この追加されたバッチ次元にどれだけのグループを詰め込む必要があるのかを把握します。"
  },
  {
    "start": 1890707,
    "end": 1897727,
    "text": "「Flatten」に入り、これを実装してみましょう。ここまでスクロールして「Flatten」にたどり着きましたが、ここで変更したいことがあります。"
  },
  {
    "start": 1897727,
    "end": 1903761,
    "text": "コンストラクタを作成し、連結したい連続した要素の数を取得するようにします。"
  },
  {
    "start": 1903761,
    "end": 1909812,
    "text": "ここでは、solve dot n equals nを覚えておくことにします。"
  },
  {
    "start": 1909812,
    "end": 1917208,
    "text": "Pytorchは実際にはflattenをトーチしており、そのキーワード引数は異なっているからです。"
  },
  {
    "start": 1917208,
    "end": 1922560,
    "text": "私たちのFlattenはPytorchのFlattenから離れつつあります。"
  },
  {
    "start": 1922560,
    "end": 1925983,
    "text": "連続フラットにする、みたいな感じで呼ばせてください。"
  },
  {
    "start": 1925983,
    "end": 1929429,
    "text": "アピがほぼ同じであることを確認するためです。"
  },
  {
    "start": 1929429,
    "end": 1934960,
    "text": "この場合、基本的にn個の連続した要素のみを平坦化します。"
  },
  {
    "start": 1934960,
    "end": 1941245,
    "text": "それらを最後の次元に配置します。ここで、xの形状は、b×t×cです。"
  },
  {
    "start": 1941245,
    "end": 1957827,
    "text": "下の例でbが4、tが8、cが10だったことを思い出して、bをマイナス1でxドット表示する代わりに、それらを変数に飛び出します。"
  },
  {
    "start": 1957827,
    "end": 1971704,
    "text": "これは、以前はb×マイナス1×で、基本的にはc×nにするものでした。"
  },
  {
    "start": 1971704,
    "end": 1975375,
    "text": "連続した要素が何個欲しいか、ということです。"
  },
  {
    "start": 1975375,
    "end": 1985566,
    "text": "期待通りにいかなかったときにエラーメッセージが出るように、非常に明示的に表現したいからです。"
  },
  {
    "start": 1985566,
    "end": 1993768,
    "text": "ここで期待するのは、整数分割を使ったt divide nになることで、そうなることを期待する。"
  },
  {
    "start": 1993768,
    "end": 2000488,
    "text": "もう一つ、ここで思い出したいのは、以前、Nは3だったということです。"
  },
  {
    "start": 2000488,
    "end": 2006426,
    "text": "そこに存在した3つの文字をすべて連結しています。"
  },
  {
    "start": 2006426,
    "end": 2013130,
    "text": "基本的にすべてを連結しているため、ここで1という偽の次元を作り出すことがあります。"
  },
  {
    "start": 2013130,
    "end": 2019603,
    "text": "xドット形状が1であるとすれば、それはスプリアス次元のようなものです。"
  },
  {
    "start": 2019603,
    "end": 2023789,
    "text": "ここでは、3次元のテンソルを1で返したくはない。"
  },
  {
    "start": 2023789,
    "end": 2028475,
    "text": "先ほどと同じように2次元のテンソルを返したいだけなのです。"
  },
  {
    "start": 2028475,
    "end": 2044136,
    "text": "この場合、基本的には x は x に等しいと言うことになります。 dot squeeze は pytorch 関数で、squeeze は 1 のテンソルのすべての次元を絞り出すという次元をとります、"
  },
  {
    "start": 2044136,
    "end": 2052053,
    "text": "絞り込みたい寸法を正確に指定することも可能で、この場合もできるだけ明示するようにしています。"
  },
  {
    "start": 2052053,
    "end": 2063183,
    "text": "この3次元テンソルの1次元目だけを絞り出し、この次元が1であれば、bをc×nで返したいだけです。"
  },
  {
    "start": 2063183,
    "end": 2068000,
    "text": "self dot outはxとなり、self dot outを返す。"
  },
  {
    "start": 2069200,
    "end": 2071218,
    "text": "というのが実装候補です。"
  },
  {
    "start": 2071218,
    "end": 2079627,
    "text": "もちろん、これはnだけでなくself dot nであるべきである。"
  },
  {
    "start": 2079627,
    "end": 2090160,
    "text": "連続させて、最初のうちは8を使うことにして、これで以前の動作が回復するはずです。"
  },
  {
    "start": 2090159,
    "end": 2093759,
    "text": "現在のブロックサイズである 8 の連続を平らにする。"
  },
  {
    "start": 2095679,
    "end": 2099567,
    "text": "このようにすれば、以前の動作に戻るはずです。"
  },
  {
    "start": 2099567,
    "end": 2104340,
    "text": "モデルを実行し、ここで検査できるようにする必要があります。"
  },
  {
    "start": 2104340,
    "end": 2108960,
    "text": "このコードでは、すべてのレイヤーを繰り返し処理することができます。"
  },
  {
    "start": 2108960,
    "end": 2113219,
    "text": "このクラスの名前と形状を表示します。"
  },
  {
    "start": 2113219,
    "end": 2120040,
    "text": "1層ごとに期待通りの形が出力されるのです。"
  },
  {
    "start": 2120040,
    "end": 2126560,
    "text": "平坦化された連続体を用いて、階層的に再構築してみましょう。"
  },
  {
    "start": 2126560,
    "end": 2131961,
    "text": "特に、ブロックサイズだけでなく、2つだけ連続してフラット化したい。"
  },
  {
    "start": 2131961,
    "end": 2134762,
    "text": "これをリニアで処理したい。"
  },
  {
    "start": 2134762,
    "end": 2140825,
    "text": "この線形への入力数はブロックサイズのn倍にはならない。"
  },
  {
    "start": 2140825,
    "end": 2146421,
    "text": "n個の埋め込み回数2 20回で1層目を通過します。"
  },
  {
    "start": 2146421,
    "end": 2154550,
    "text": "次の線形レイヤーでは、nの2倍を隠蔽する必要があります。"
  },
  {
    "start": 2154550,
    "end": 2164107,
    "text": "最後の部分は、nの隠れ2倍を期待するもので、これはその素朴なバージョンのようなものです。"
  },
  {
    "start": 2164107,
    "end": 2168664,
    "text": "これを実行することで、より大きなモデルが出来上がりました。"
  },
  {
    "start": 2168664,
    "end": 2173237,
    "text": "基本的にはモデルを転送するだけでいいはずです。"
  },
  {
    "start": 2173237,
    "end": 2181920,
    "text": "4×8×20は4×4×20に連続的に平らになっていることになりますね。"
  },
  {
    "start": 2183120,
    "end": 2186098,
    "text": "4×4×200に投影しました。"
  },
  {
    "start": 2186098,
    "end": 2189673,
    "text": "と思ったら、バスティオンは箱から出してすぐ動いた。"
  },
  {
    "start": 2189673,
    "end": 2196305,
    "text": "bastionは、2次元のembedではなく3次元のembedを取るが、正しい動作をすることを確認する必要がある。"
  },
  {
    "start": 2196305,
    "end": 2198705,
    "text": "その後、エレメント単位で10hを設定しています。"
  },
  {
    "start": 2198705,
    "end": 2204436,
    "text": "潰し、また潰したので、連続して平らになり、4×2×400の大きさになりました。"
  },
  {
    "start": 2204436,
    "end": 2208244,
    "text": "その後、リニアは200バスティオン10hまで戻しました。"
  },
  {
    "start": 2208244,
    "end": 2216588,
    "text": "最後に、4×400の図ですが、最後の平坦化で連続した平坦化が行われ、1という寸法が絞り込まれたことがわかります。"
  },
  {
    "start": 2216588,
    "end": 2224310,
    "text": "結局400で4つ、リニアバスティオン10hと最後のリニアレイヤーでロジットを取得しただけでした。"
  },
  {
    "start": 2224310,
    "end": 2228002,
    "text": "ロジットは結局、以前と同じ形になるのです。"
  },
  {
    "start": 2228002,
    "end": 2240638,
    "text": "3層のニューラルネットができあがりました。"
  },
  {
    "start": 2240638,
    "end": 2265931,
    "text": "一方、この例では4つの層があり、受容野のサイズは8文字から16文字になります。"
  },
  {
    "start": 2265931,
    "end": 2272549,
    "text": "68を使うとパラメータの数が22000になるので、以前と全く同じになるのです。"
  },
  {
    "start": 2272549,
    "end": 2276979,
    "text": "このニューラルネットでは、パラメーターの数で同じだけの容量を持っています。"
  },
  {
    "start": 2276979,
    "end": 2281563,
    "text": "問題は、そのパラメータをより効率的なアーキテクチャで活用できているかどうかです。"
  },
  {
    "start": 2281563,
    "end": 2285037,
    "text": "そこで私は、デバッグ用のセルの多くを削除しました。"
  },
  {
    "start": 2285037,
    "end": 2292903,
    "text": "最適化を再実行し、結果をスクロールダウンすると、ほぼ同じパフォーマンスが得られていることがわかります。"
  },
  {
    "start": 2292903,
    "end": 2297040,
    "text": "検証損失は現在2.029で、以前は2.027でした。"
  },
  {
    "start": 2297040,
    "end": 2310098,
    "text": "フラットからヒエラルキーに変更した場合、パラメータの数をコントロールしても、まだ何も得られていません。しかし、2つのことを指摘しておきます。"
  },
  {
    "start": 2310098,
    "end": 2320199,
    "text": "これは私の最初の推測に過ぎませんが、パラメータの予算をどのように第2層に配分するかという点で、ハイパーパラメータの検索をたくさん行うことができます、"
  },
  {
    "start": 2320199,
    "end": 2323839,
    "text": "まだ、Bastion 1dレイヤーの中にバグがあるかもしれません。"
  },
  {
    "start": 2323840,
    "end": 2330708,
    "text": "走るけど、正しいことをやっているのか？"
  },
  {
    "start": 2330708,
    "end": 2336196,
    "text": "私は、ここにあるレイヤーインスペクターのようなものを引っ張り出して、途中の形状をプリントアウトしました。"
  },
  {
    "start": 2336196,
    "end": 2341782,
    "text": "現在、バッチノルムは32×4×68の入力を受けているようです。"
  },
  {
    "start": 2341782,
    "end": 2353396,
    "text": "右側には、バッチノルムの現在の実装があります。このバッチノルムは、私たちが書いた方法で、xが2次元であることを前提としています。"
  },
  {
    "start": 2353396,
    "end": 2356040,
    "text": "n×dで、nはバッチサイズである。"
  },
  {
    "start": 2356040,
    "end": 2360640,
    "text": "0次元の平均と分散を小さくしただけです。"
  },
  {
    "start": 2360640,
    "end": 2362731,
    "text": "基本的に3次元化されます。"
  },
  {
    "start": 2362731,
    "end": 2367963,
    "text": "今、バッチノームレイヤーの中で何が起こっているのか、そしてなぜエラーも出さず、まったく機能しているのか。"
  },
  {
    "start": 2367963,
    "end": 2371780,
    "text": "その理由は、基本的にすべてが正しく放送されるからです。"
  },
  {
    "start": 2371780,
    "end": 2374894,
    "text": "バッチノーマルは、私たちが必要とすることをやってくれないのです。"
  },
  {
    "start": 2374894,
    "end": 2383440,
    "text": "特に、バッチノルムの内部で何が起こっているのか、ここで何が起こっているのか、基本的に考えてみましょう。"
  },
  {
    "start": 2383440,
    "end": 2385016,
    "text": "ここにコードがあります。"
  },
  {
    "start": 2385016,
    "end": 2388763,
    "text": "32×4×68の入力を受けています。"
  },
  {
    "start": 2388763,
    "end": 2399061,
    "text": "xの代わりにeがありますが、0以上の平均を計算することで、1×4×68を計算することができるのです。"
  },
  {
    "start": 2399061,
    "end": 2402391,
    "text": "一番最初の次元に対してのみ平均をとっているのです。"
  },
  {
    "start": 2402391,
    "end": 2407014,
    "text": "平均と分散が得られるので、この次元を維持することができるのです。"
  },
  {
    "start": 2407014,
    "end": 2411520,
    "text": "これらの手段は、1次元目の32個の数字に対してのみ取られる。"
  },
  {
    "start": 2412160,
    "end": 2426785,
    "text": "実行すると、すべて正しく放送されますが、基本的には、走行平均を見ると、その形が変わってしまうのです。"
  },
  {
    "start": 2426785,
    "end": 2433635,
    "text": "私は、最初のバッチノルムの層である3を重ねたモデルを見ています。そして、走行平均がどのようになったか、その形を見ています、"
  },
  {
    "start": 2433635,
    "end": 2436841,
    "text": "今のこの走平均の形は、1×4×68です。"
  },
  {
    "start": 2436841,
    "end": 2448480,
    "text": "68のチャンネルがあり、68の平均値と分散を維持することが予想されるため、単に寸法の大きさではなく、右のようなものです。"
  },
  {
    "start": 2448480,
    "end": 2465096,
    "text": "このバッチノルムは、68チャンネルではなく、4×68チャンネルで並列に動作しているということです。"
  },
  {
    "start": 2465096,
    "end": 2473287,
    "text": "この4つのポジションは、基本的にそれぞれ個別に統計を取っています。"
  },
  {
    "start": 2473287,
    "end": 2480412,
    "text": "この4次元を0次元と同じようにバッチ次元として扱いたいのです。"
  },
  {
    "start": 2480412,
    "end": 2482827,
    "text": "バッチノーマルで見る限り。"
  },
  {
    "start": 2482827,
    "end": 2484258,
    "text": "平均化したくない。"
  },
  {
    "start": 2484258,
    "end": 2492994,
    "text": "この68のチャンネルを平均すると、32個以上の数字が必要なのではなく、32個の4倍以上の数字が必要なのです。"
  },
  {
    "start": 2492994,
    "end": 2495134,
    "text": "今度はこれを削除させてください。"
  },
  {
    "start": 2495134,
    "end": 2506343,
    "text": "ドキュメントを見ると、torch.meanのシグネチャの1つに、torch.mean uhがあることがわかりました、"
  },
  {
    "start": 2506343,
    "end": 2513796,
    "text": "指定すると、ここでいう次元は、intでもintのタプルでもないことがわかります。"
  },
  {
    "start": 2513796,
    "end": 2519490,
    "text": "複数の整数に対して、同時に複数の次元に対して削減することができます。"
  },
  {
    "start": 2519490,
    "end": 2524490,
    "text": "0以上減らすのではなく、タプル0 1とここ0 1も渡すことができます。"
  },
  {
    "start": 2524490,
    "end": 2530131,
    "text": "当然ながら出力は同じになります。"
  },
  {
    "start": 2530131,
    "end": 2542137,
    "text": "0次元と1次元を縮小して、平均点の形を見ると、0次元と1次元の両方の平均を取ったことがわかります。"
  },
  {
    "start": 2542137,
    "end": 2546080,
    "text": "68の数字が出るだけであり、このままでは偽の次元がたくさん出てしまう。"
  },
  {
    "start": 2547120,
    "end": 2558593,
    "text": "1×1×68となり、ランニング平均とランニング分散は、1×1×68となります。"
  },
  {
    "start": 2558593,
    "end": 2562012,
    "text": "たとえスプリアス・ディメンションがあったとしても、電流は電流です。"
  },
  {
    "start": 2562012,
    "end": 2573673,
    "text": "この場合、64チャンネル、つまり68チャンネルの平均と分散を維持するだけで、32×4次元の平均分散を計算していないという点で、正しいことが起こるでしょう。"
  },
  {
    "start": 2573673,
    "end": 2575440,
    "text": "それこそが、私たちが望んでいることなのです。"
  },
  {
    "start": 2576000,
    "end": 2584842,
    "text": "batchnorm1dの実装を変更し、2次元または3次元の入力を取り込み、それに応じた処理を行うことができるようにしましょう。"
  },
  {
    "start": 2584842,
    "end": 2588418,
    "text": "結局のところ、修正方法は比較的簡単です。"
  },
  {
    "start": 2588418,
    "end": 2595572,
    "text": "xの次元に応じて、0または0と1のタプルのいずれかになります。"
  },
  {
    "start": 2595572,
    "end": 2604146,
    "text": "x dot endimが2であれば、2次元のテンソルであるため、削減したい次元は整数である0になります。"
  },
  {
    "start": 2604146,
    "end": 2606363,
    "text": "3次元のテンソルである。"
  },
  {
    "start": 2606363,
    "end": 2615279,
    "text": "dimは0と1を想定しており、その上で削減を行い、ここではendimを渡すだけです、"
  },
  {
    "start": 2615279,
    "end": 2619595,
    "text": "というエラーが出るので、これで解決です。"
  },
  {
    "start": 2619595,
    "end": 2627878,
    "text": "もう一つ指摘しておくと、実はPytorchのAPIから少し離れているんです。Pytorchのbatchnorm1dに来るとき"
  },
  {
    "start": 2627878,
    "end": 2636727,
    "text": "この層への入力はn×cで、nはバッチサイズ、cは特徴量やチャンネルの数であることがわかります。"
  },
  {
    "start": 2636727,
    "end": 2640171,
    "text": "実際に3次元の入力を受け付けています。"
  },
  {
    "start": 2640171,
    "end": 2651000,
    "text": "n×c×lで、lは配列の長さとか、そんな感じです。"
  },
  {
    "start": 2651000,
    "end": 2658372,
    "text": "3次元の入力を受けると、このバッチノルム層は0と1ではなく、0と2にわたって縮小します。"
  },
  {
    "start": 2658372,
    "end": 2666435,
    "text": "基本的に pytorch batchnorm1d レイヤーは、c が常に 1 次元の um であることを仮定しています。"
  },
  {
    "start": 2666435,
    "end": 2669779,
    "text": "ただし、ここではcが最後の次元であると仮定する。"
  },
  {
    "start": 2669779,
    "end": 2678746,
    "text": "あらかじめいくつかのバッチ次元が存在するので、n×cまたはn×c×lを期待することになります。"
  },
  {
    "start": 2678746,
    "end": 2685685,
    "text": "私たちは、n×cまたはn×l×cを期待しているので、偏差値、大丈夫だと思います。"
  },
  {
    "start": 2685685,
    "end": 2688710,
    "text": "私は正直、この方が好きです。"
  },
  {
    "start": 2688710,
    "end": 2691720,
    "text": "私たちの目的のためにとっておく方法です。"
  },
  {
    "start": 2691720,
    "end": 2702423,
    "text": "レイヤーを再定義し、ニューラルネットを再初期化し、1ステップだけ休憩を挟んでシングルフォワードパスを行いました。途中の形状を見ると、もちろんすべての形状が同じです。"
  },
  {
    "start": 2702423,
    "end": 2705577,
    "text": "物事が実際に思い通りに動いていることを確認する方法です。"
  },
  {
    "start": 2705577,
    "end": 2710158,
    "text": "バッチノームレイヤーの走行平均の形状が1×1×68になっていることがわかります。"
  },
  {
    "start": 2710158,
    "end": 2719821,
    "text": "チャンネルごとに68の手段を保持するだけで、0次元と1次元の両方をバッチ次元として扱うことができ、まさに望むところです。"
  },
  {
    "start": 2719821,
    "end": 2721233,
    "text": "ニューラルネットを再トレーニングしてみます。"
  },
  {
    "start": 2721233,
    "end": 2724232,
    "text": "バグを修正したニューラルネットを再トレーニングしたところ、きれいなカーブを描くようになりました。"
  },
  {
    "start": 2724232,
    "end": 2730457,
    "text": "検証結果を見ると、2.029から2.022と、若干の改善が見られました。"
  },
  {
    "start": 2730457,
    "end": 2741972,
    "text": "基本的には、バッチノルム内のバグが原因で、私たちは少し遅れていたようです、"
  },
  {
    "start": 2741972,
    "end": 2754880,
    "text": "32個の数字を使って推定していた平均値や分散を、32×4個の数字を使って推定することで、より効率的に維持することができるからです。"
  },
  {
    "start": 2754880,
    "end": 2769455,
    "text": "平均と分散の1つの推定値に入る数値が増えるだけで、統計の推定値内がもう少し安定し、揺らぎが少なくなります、"
  },
  {
    "start": 2769455,
    "end": 2774007,
    "text": "ネットワークサイズを大きくすることで、さらに性能を高めることができるようになりました。"
  },
  {
    "start": 2774007,
    "end": 2780429,
    "text": "例えば、埋め込み数を10個から24個に増やし、隠しユニットの数も増やしました。"
  },
  {
    "start": 2780429,
    "end": 2782957,
    "text": "全く同じアーキテクチャを使用しています。"
  },
  {
    "start": 2782957,
    "end": 2787279,
    "text": "76 000のパラメータが必要となり、トレーニングに時間がかかる。"
  },
  {
    "start": 2787279,
    "end": 2788385,
    "text": "きれいなカーブを描くことができます。"
  },
  {
    "start": 2788385,
    "end": 2794044,
    "text": "その後、実際に性能を評価したところ、1.993という検証結果を得ることができました。"
  },
  {
    "start": 2794044,
    "end": 2810704,
    "text": "2.0の領域を超え、1.99くらいになりましたが、かなり長い時間待たなければならなくなってきました。"
  },
  {
    "start": 2810704,
    "end": 2820402,
    "text": "多くの実験を行い、このアーキテクチャをうまく調整するための実験用ハーネスのようなものが不足しているのですが、いくつか注意点を挙げて終わりにしたいと思います、"
  },
  {
    "start": 2820402,
    "end": 2824667,
    "text": "2.1のスタートから1.9まで、基本的にパフォーマンスを向上させることができました。"
  },
  {
    "start": 2824667,
    "end": 2828802,
    "text": "正直なところ、私たちは暗闇の中にいるようなものなので、そこに焦点を当てたくはないのです。"
  },
  {
    "start": 2828802,
    "end": 2830196,
    "text": "実験用ハーネスがない。"
  },
  {
    "start": 2830196,
    "end": 2833161,
    "text": "私たちは推測と確認をしているだけで、この全体はひどいものです。"
  },
  {
    "start": 2833161,
    "end": 2834937,
    "text": "トレーニングのロスに目を向けているだけなのです。"
  },
  {
    "start": 2834937,
    "end": 2848448,
    "text": "WaveNetの論文から、このアーキテクチャを実装したという数字を引き出そうとすると、訓練と検証の両方の損失を一緒に見ようとすると、全体が違って見えます。"
  },
  {
    "start": 2848448,
    "end": 2861246,
    "text": "私たちは、より複雑な線形層、つまりゲート線形層、残留接続、スキップ接続などがある、この特定のフォワードパスを実装していません。"
  },
  {
    "start": 2861246,
    "end": 2864691,
    "text": "実装したわけではなく、この構造を実装しただけです。"
  },
  {
    "start": 2864691,
    "end": 2872067,
    "text": "WaveNetの論文で使われている畳み込みニューラルネットワークとの関連性について、簡単に説明したいと思います。"
  },
  {
    "start": 2872067,
    "end": 2875213,
    "text": "基本的にコンボリューションの使用は、厳密に言えば効率化のためです。"
  },
  {
    "start": 2875213,
    "end": 2877759,
    "text": "実際には私たちが実装したモデルを変更することはありません。"
  },
  {
    "start": 2878400,
    "end": 2879867,
    "text": "例えばこちらです。"
  },
  {
    "start": 2879867,
    "end": 2883662,
    "text": "例に、具体的な名前を見てみましょう。"
  },
  {
    "start": 2883662,
    "end": 2886184,
    "text": "トレーニングセットの中に名前がある。"
  },
  {
    "start": 2886184,
    "end": 2892256,
    "text": "deandreで7文字なので、このモデルでは8つの独立した例となります。"
  },
  {
    "start": 2892256,
    "end": 2896509,
    "text": "この列はすべて、ディアンドラの独立した例です。"
  },
  {
    "start": 2896509,
    "end": 2900762,
    "text": "もちろん、これらの列のいずれかを単独で転送することも可能です。"
  },
  {
    "start": 2900762,
    "end": 2907891,
    "text": "私は、自分のモデルを持って、個々のインデックスに呼び出すことができます通知ところで、私は少しトリッキーなことをしています。"
  },
  {
    "start": 2907891,
    "end": 2914930,
    "text": "この理由は、extra at seven dot shapeは8つの1次元配列に過ぎないからです。"
  },
  {
    "start": 2914930,
    "end": 2922396,
    "text": "実際にモデルを呼び出すと、バッチディメンジョンがないため、エラーが発生します。"
  },
  {
    "start": 2922396,
    "end": 2928216,
    "text": "7つ並べると、この形は1×8になります。"
  },
  {
    "start": 2928216,
    "end": 2931319,
    "text": "私は、1つのバッチの次元を追加で取得します。"
  },
  {
    "start": 2931319,
    "end": 2936840,
    "text": "1つの例を前倒しするようにモデルを進めることができる。"
  },
  {
    "start": 2936840,
    "end": 2943272,
    "text": "この8つを同時に転送したいと思うかもしれません。"
  },
  {
    "start": 2943272,
    "end": 2952718,
    "text": "メモリを事前に確保し、forループを8回繰り返し、その8回をすべてここに転送することで、さまざまなケースのすべてのロジットを得ることができるのです。"
  },
  {
    "start": 2952718,
    "end": 2955964,
    "text": "今、私たちが実装しているモデルで、私たちのために。"
  },
  {
    "start": 2955964,
    "end": 2958223,
    "text": "これは、私たちのモデルに対する8つの独立した呼び出しです。"
  },
  {
    "start": 2958223,
    "end": 2972172,
    "text": "コンボリューションで何ができるかというと、基本的に入力シーケンスに対して効率的にモデルをスライドさせることができるので、このforループはpythonの外ではなく、cudaのカーネルの中で行うことができます。"
  },
  {
    "start": 2972172,
    "end": 2975212,
    "text": "このforループは畳み込みの中に隠されてしまうのです。"
  },
  {
    "start": 2975212,
    "end": 2983568,
    "text": "コンボリューションは、基本的には、入力シーケンスの空間上で小さな線形フィルタを適用するフォーループであると考えることができます。"
  },
  {
    "start": 2983568,
    "end": 2990301,
    "text": "この場合、私たちが関心を持つ空間は1次元であり、入力データに対してこれらのフィルタをスライドさせることに関心があります。"
  },
  {
    "start": 2990301,
    "end": 3002320,
    "text": "この図は実際にかなり良いものです。基本的に私たちが行ったことは、この計算のツリーのようなものを一つ一つ黒くハイライトしています。"
  },
  {
    "start": 3002320,
    "end": 3005919,
    "text": "ここでは、1つの出力例を計算しているだけです。"
  },
  {
    "start": 3005919,
    "end": 3012497,
    "text": "基本的にはこの黒い構造体を1つ実装しています。"
  },
  {
    "start": 3012497,
    "end": 3016784,
    "text": "実装し、一例のように一つのアウトプットを計算するようにしました。"
  },
  {
    "start": 3016784,
    "end": 3029323,
    "text": "コンボリューションでできることは、この黒い構造を入力配列の上にスライドさせながら、オレンジ色の出力を同時に計算することです。"
  },
  {
    "start": 3029323,
    "end": 3037755,
    "text": "ここでいうのは、ディアンドルのすべての位置のこれらの出力を同時に計算することに相当する。"
  },
  {
    "start": 3037755,
    "end": 3048362,
    "text": "この方法がより効率的である理由は、第一に、先ほど述べたように、forループがスライディング中のcudaカーネルの中にあるため、効率的だからです。"
  },
  {
    "start": 3048362,
    "end": 3051327,
    "text": "その2 変数の再利用に注目してください。"
  },
  {
    "start": 3051327,
    "end": 3053675,
    "text": "例えば、この円を見てみると"
  },
  {
    "start": 3053675,
    "end": 3060918,
    "text": "このノードは、このノードの右の子ですが、このノードの左の子でもあります。"
  },
  {
    "start": 3060918,
    "end": 3070000,
    "text": "基本的にこのノードとその値は2回使用されるので、この素朴な方法では再計算する必要があるのです。"
  },
  {
    "start": 3070000,
    "end": 3072320,
    "text": "ここでは、再利用を許可しています。"
  },
  {
    "start": 3072320,
    "end": 3074581,
    "text": "畳み込みニューラルネットワークで表現しています。"
  },
  {
    "start": 3074581,
    "end": 3081280,
    "text": "上の方にあるリニアレイヤーをフィルターとして考え、そのフィルターをリニアフィルターにするのです。"
  },
  {
    "start": 3081280,
    "end": 3083280,
    "text": "入力順でスライドさせると"
  },
  {
    "start": 3083280,
    "end": 3092000,
    "text": "第1層、第2層、第3層、そしてサンドイッチの出力層を計算するのですが、すべてこの畳み込みを使って非常に効率的に行われています。"
  },
  {
    "start": 3092640,
    "end": 3094431,
    "text": "そのあたりは、今後のビデオで紹介する予定です。"
  },
  {
    "start": 3094431,
    "end": 3095138,
    "text": "2つ目のこと"
  },
  {
    "start": 3095138,
    "end": 3096560,
    "text": "この映像から何かを感じ取っていただけたら幸いです。"
  },
  {
    "start": 3097120,
    "end": 3110938,
    "text": "私がレイヤーレゴブロックやモジュールビルディングブロックをすべて実装しているのはご覧の通りですが、ここではそれらを実装し、いくつかのレイヤーをまとめて実装しています。"
  },
  {
    "start": 3110938,
    "end": 3122259,
    "text": "全体的に Pytorch 化が進み、基本的には torch.tensor の上にニューラルネットワークライブラリである torch.nn を再実装しています。"
  },
  {
    "start": 3122259,
    "end": 3124493,
    "text": "という感じで、とてもよく似ています。"
  },
  {
    "start": 3124493,
    "end": 3130981,
    "text": "ただし、私のjupyterノートブックのjankilyの代わりにpytorchにあるため、はるかに優れています。"
  },
  {
    "start": 3130981,
    "end": 3136064,
    "text": "今後は、torch.nnのロックを解除した状態で検討することになると思います。"
  },
  {
    "start": 3136064,
    "end": 3143067,
    "text": "このモジュールがどのように動作し、どのようにネストされ、torch.tensorの上で何を行っているのか、おおよそ理解できたと思います。"
  },
  {
    "start": 3143067,
    "end": 3147987,
    "text": "できれば、このまま切り替えて、torch.nnを直接使うようにしたいですね。"
  },
  {
    "start": 3147987,
    "end": 3157785,
    "text": "次に、ディープニューラルネットワークを構築するための開発プロセスがどのようなものかを少し感じていただけたと思いますが、これはある程度代表的なものだったと思います。"
  },
  {
    "start": 3157785,
    "end": 3165904,
    "text": "第一に、私たちはpytorchのドキュメントページで多くの時間を費やしています、"
  },
  {
    "start": 3165904,
    "end": 3184666,
    "text": "入力の形状はどうなっているのか、その形状はどのようなものなのか、レイヤーは何をするのか、などなど、残念ながらPytorchのドキュメントはあまり良くありません。"
  },
  {
    "start": 3184666,
    "end": 3185801,
    "text": "あなたに嘘をつくでしょう。"
  },
  {
    "start": 3185801,
    "end": 3198763,
    "text": "残念ながら、それが現実であり、あなたはただ、彼らが与えてくれたものにベストを尽くすだけです。"
  },
  {
    "start": 3198763,
    "end": 3209591,
    "text": "もうひとつは、シェイプをうまく使うために、多次元配列と2次元配列の使い分けをする必要があることです、"
  },
  {
    "start": 3209591,
    "end": 3213962,
    "text": "三次元四次元 どんな層がどんな形になるのか？"
  },
  {
    "start": 3213962,
    "end": 3221894,
    "text": "NCLなのかNLCなのか、パーマをかけたり表示したりと、かなり面倒なことになるので、3番目に紹介します。"
  },
  {
    "start": 3221894,
    "end": 3232382,
    "text": "私は、このようなレイヤーや実装をjupyterノートブックでプロトタイプ化し、すべての形状がうまくいくことを確認することが非常に多く、基本的に形状を子守してすべてが正しいことを確認するのに多くの時間を費やしています。"
  },
  {
    "start": 3232382,
    "end": 3241544,
    "text": "Jupyterノートブックの機能に満足したら、そのコードをコピーして、実際にトレーニングしているコードのリポジトリに貼り付けます。"
  },
  {
    "start": 3241544,
    "end": 3243989,
    "text": "副業でVSコードに取り組んでいます。"
  },
  {
    "start": 3243989,
    "end": 3246264,
    "text": "私は普段、jupyter notebookとvs codeを持っています。"
  },
  {
    "start": 3246264,
    "end": 3247813,
    "text": "jupyter notebookを開発しています。"
  },
  {
    "start": 3247813,
    "end": 3253393,
    "text": "VSコードにペーストして、レポから、もちろんコードレポジトリから、実験をキックオフします。"
  },
  {
    "start": 3253393,
    "end": 3282023,
    "text": "最後に、この講義が今後の講義の可能性を大きく広げると思います。第1に、ニューラルネットワークを実際に拡張因果畳み込み層を使うように変換する必要があります。"
  },
  {
    "start": 3282023,
    "end": 3287023,
    "text": "今のところ、これは典型的なディープラーニングのワークフローを代表するものではないと思っています。"
  },
  {
    "start": 3287023,
    "end": 3290760,
    "text": "実験を開始するためには、評価用ハーネスを設置する必要があります。"
  },
  {
    "start": 3290760,
    "end": 3293134,
    "text": "スクリプトが受け取ることのできる引数をたくさん持っています。"
  },
  {
    "start": 3293134,
    "end": 3295440,
    "text": "多くの実験を開始することになるのです。"
  },
  {
    "start": 3295440,
    "end": 3300757,
    "text": "トレーニングや検証のロスのプロットをたくさん見て、何がうまくいっていて、何がうまくいっていないのかを見ているのですね。"
  },
  {
    "start": 3300757,
    "end": 3305784,
    "text": "集団レベルで作業をしていて、ハイパーパラメーターの検索をすべて行っています。"
  },
  {
    "start": 3305784,
    "end": 3308124,
    "text": "今のところ何一つしていません。"
  },
  {
    "start": 3308124,
    "end": 3310796,
    "text": "それをどのように設定し、どのように良いものにするのか。"
  },
  {
    "start": 3310796,
    "end": 3312928,
    "text": "全く別の話だと思うんです。"
  },
  {
    "start": 3312928,
    "end": 3313810,
    "text": "3番です。"
  },
  {
    "start": 3313810,
    "end": 3336092,
    "text": "そのほかにも、ニューラル・ネットワーク、RNS、LSDM、トランスフォーマーなど、いろいろなものをカバーする必要があります。"
  },
  {
    "start": 3336092,
    "end": 3344320,
    "text": "憧れの果実はまだまだ購入できる可能性があるので、このニューラルネットで他のチャンネルを割り当てる方法は試していません。"
  },
  {
    "start": 3344320,
    "end": 3347775,
    "text": "エンベッディングの次元数が全て間違っているのかもしれません。"
  },
  {
    "start": 3347775,
    "end": 3355688,
    "text": "もしかしたら、隠れ層が1つしかないオリジナルのネットワークを十分に大きくして、私の空想的な階層型ネットワークに実際に勝つことができるかもしれません。"
  },
  {
    "start": 3355688,
    "end": 3362202,
    "text": "当たり前といえば当たり前なのですが、少しいじったくらいで良くならなかったら、ちょっと恥ずかしいですよね、"
  },
  {
    "start": 3362202,
    "end": 3368153,
    "text": "ウェーブネットの論文を読んで、これらのレイヤーがどのように機能するかを理解し、私たちが持っているものを使って自分たちで実装してみてはいかがでしょうか、"
  },
  {
    "start": 3368153,
    "end": 3375312,
    "text": "もちろん、初期化や最適化の一部をチューニングして、その方法で改善できるかどうかを確認することはできます。"
  }
]