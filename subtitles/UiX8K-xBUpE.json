[
  {
    "start": 410,
    "end": 2030,
    "text": "やあ、みんな。"
  },
  {
    "start": 2100,
    "end": 3978,
    "text": "今日はミストラルについて話そう。"
  },
  {
    "start": 4074,
    "end": 14058,
    "text": "ご存知のように、ミストラルは数ヶ月前にミストラルAIから発表された新しい言語モデルで、ヨーロッパで今最もホットな言語モデルのスタートアップの一つです。"
  },
  {
    "start": 14154,
    "end": 19242,
    "text": "また、最近ユニコーンとなった。"
  },
  {
    "start": 19306,
    "end": 22094,
    "text": "ひとつは7,000,000,001で、700億分の8のモデルだ。"
  },
  {
    "start": 22212,
    "end": 24110,
    "text": "今日のトピックをおさらいしよう。"
  },
  {
    "start": 24260,
    "end": 30658,
    "text": "最初に紹介するのは、バニラ・トランスとミストラルのアーキテクチャーの違いだ。"
  },
  {
    "start": 30754,
    "end": 40002,
    "text": "後ほど、スライディングウィンドウ・アテンションとは何か、そしてそれが受容野の概念とどのように関係しているのかを見ていくことにしよう。受容野は通常、畳み込みニューラルネットワークで見られる概念である。"
  },
  {
    "start": 40146,
    "end": 48410,
    "text": "ローリング・バッファ・キャッシュのコンセプトを紹介し、プリフィリングとチャンキングがどのように行われるかを紹介したいので、KVキャッシュについて簡単にレビューする。"
  },
  {
    "start": 49150,
    "end": 59450,
    "text": "ここでは、パイプライン並列についてごく簡単に紹介しながら、エキスパートモデルのシャーディングのスパース混合とは何かを見ていく。"
  },
  {
    "start": 59610,
    "end": 69502,
    "text": "ミストラルのコードには、特にブロック・アテンションでxformersライブラリを使用する際に多くの工夫が見られるからだ。"
  },
  {
    "start": 69566,
    "end": 77506,
    "text": "初心者がコードを理解し、自分自身を見つけるのは本当に難しいからだ。"
  },
  {
    "start": 77688,
    "end": 84962,
    "text": "ミストラルに関連するトピックがいくつかあるが、それらはすでに以前のラマについてのビデオで取り上げているので、今回のビデオでは取り上げない。"
  },
  {
    "start": 85026,
    "end": 94486,
    "text": "特に、RMSの正規化、回転位置エンコーディング、グループ化されたクエリーアテンションについては、前回のラマについてのビデオですでに詳しく説明しているので、今回は割愛する。"
  },
  {
    "start": 94518,
    "end": 98330,
    "text": "ラマについて知りたければ、以前のラマについてのビデオを見てほしい。"
  },
  {
    "start": 98910,
    "end": 108238,
    "text": "このビデオで扱うトピックはかなり高度なものなので、このビデオを見る前に持っていてほしい唯一の前提条件は、トランス・モデルに精通していることである。"
  },
  {
    "start": 108324,
    "end": 120642,
    "text": "もしあなたが変圧器モデルや、特に注意のメカニズム、特に自己注意のメカニズムについてよく知らないのであれば、変圧器に関する私のビデオを見てほしい。"
  },
  {
    "start": 120776,
    "end": 126306,
    "text": "このビデオはかなり高度なトピックなので、これらを見ることが前提条件となる。"
  },
  {
    "start": 126498,
    "end": 128470,
    "text": "よし、もっと先に進もう。"
  },
  {
    "start": 128810,
    "end": 133880,
    "text": "バニラ・トランスとミストラルの違いをアーキテクチャレベルで見てみよう。"
  },
  {
    "start": 134570,
    "end": 149370,
    "text": "この画像からわかるように、ミストラルのアーキテクチャーは、論文ではアーキテクチャー画像が公開されていなかったので、私がコードを使って自作したものである。"
  },
  {
    "start": 149870,
    "end": 155354,
    "text": "このように多くのエンコーダーレイヤーとリニア、ソフトマックスで構成されるモデルがある。"
  },
  {
    "start": 155402,
    "end": 163786,
    "text": "このモデルはバニラ・トランスのデコーダーのように見えるので、デコーダのみのモデルについて話している。"
  },
  {
    "start": 163818,
    "end": 169438,
    "text": "クロスアテンションを除けば、ご覧のようにクロスアテンションはない。"
  },
  {
    "start": 169614,
    "end": 174978,
    "text": "リニアとソフトマックスを除いたモデルをエンコーダのみのモデルと呼ぶ。"
  },
  {
    "start": 175064,
    "end": 184038,
    "text": "例えば、Bertはエンコーダのみのモデルであり、Bertは最後にいくつかのヘッドを持っているからである。"
  },
  {
    "start": 184124,
    "end": 188770,
    "text": "それ自体、複数の下流タスクに使用できるため、バートにヘッドは必要ない。"
  },
  {
    "start": 188850,
    "end": 197350,
    "text": "エンコーダー・オンリー・モデルと呼ばれるのは、トランスのエンコーダー・サイドに似ているからで、エンコーダー・サイドにはリニア・マークもソフト・マークもない。"
  },
  {
    "start": 197510,
    "end": 203818,
    "text": "ミストラルはデコーダのみのモデルで、ラマと同等ではないにしろ、非常に似ている。"
  },
  {
    "start": 203914,
    "end": 208430,
    "text": "llamaとMistralの違いは、ここでは赤字で強調されている。"
  },
  {
    "start": 208580,
    "end": 222382,
    "text": "llamaとmistralの最初の違いは、自己注意ではスライディングウィンドウ注意を使い、推論にはグループ化されたクエリー注意とkvキャッシュを使うことだ。"
  },
  {
    "start": 222446,
    "end": 227762,
    "text": "これはローリング・バッファKVキャッシュで、実はスライディング・ウィンドウ注目を使っていることに関係している。"
  },
  {
    "start": 227826,
    "end": 229938,
    "text": "後ほど、これらすべてのコンセプトを紹介する。"
  },
  {
    "start": 230114,
    "end": 249082,
    "text": "もうひとつの違いは、ここでのフィードフォワード層で、バニラ・トランスで使っているrelu関数やllamaで使っているZwiglu関数の代わりに、ミストラルではsilu関数を使い、フィードフォワードはミストラル7Bの場合は1つである。"
  },
  {
    "start": 249136,
    "end": 259434,
    "text": "このモデルは、8つのフィード・フォワード・ネットワークが並列に接続されたもので、エキスパートが混在している。"
  },
  {
    "start": 259482,
    "end": 264814,
    "text": "ミストラルの8×7Bの場合、どのように機能するかは後述する。"
  },
  {
    "start": 264932,
    "end": 271182,
    "text": "とりあえず、ミストラルがエンベッディングに変換された入力からできていることだけは理解しておいてほしい。"
  },
  {
    "start": 271246,
    "end": 274258,
    "text": "そして、このブロックをn回繰り返す。"
  },
  {
    "start": 274344,
    "end": 292860,
    "text": "ミストラルの場合、各層の出力が入力として次の層に送られ、最後の層の出力がこのrmsノルム、リニア、ソフトマックスに送られ、モデルの出力が生成されるように、次々と32回繰り返されることがわかるだろう。"
  },
  {
    "start": 293470,
    "end": 297962,
    "text": "これは、他のトランス・モデルでやっていることとまったく同じだ。"
  },
  {
    "start": 298016,
    "end": 300990,
    "text": "通常、ここにはこのようなブロックがたくさんある。"
  },
  {
    "start": 301140,
    "end": 310330,
    "text": "ミストラルのコードでは、この部分はトランスフォーマー・ブロックとして知られているが、エンコーダー・ブロックやデコーダー・ブロックとしても知られている。"
  },
  {
    "start": 310410,
    "end": 320722,
    "text": "このブロックの内容にもよるが、エンコーダー側のブロックと同じように見えるので、ここではエンコーダー・ブロックと呼ぶことにする。"
  },
  {
    "start": 320776,
    "end": 325086,
    "text": "マルチヘッドアテンション、アド・アンド・ノルム、フィードフォワード、アド・アンド・ノルムを備えている。"
  },
  {
    "start": 325208,
    "end": 332390,
    "text": "唯一の違いは、ここでのノーマライゼーションがフィードフォワードとセルフ・アテンションのブロックの前に行われることだ。"
  },
  {
    "start": 333450,
    "end": 338178,
    "text": "では、2つのモデルを比較してみよう。"
  },
  {
    "start": 338194,
    "end": 341658,
    "text": "つはミストラル7B、1つはミストラル8B×7Bである。"
  },
  {
    "start": 341824,
    "end": 347654,
    "text": "パラメータdimは、埋め込みベクトルの次元を示す。"
  },
  {
    "start": 347702,
    "end": 349414,
    "text": "埋め込みベクトルの大きさは？"
  },
  {
    "start": 349462,
    "end": 354990,
    "text": "各トークンは4096次元の埋め込みベクトルで表現される。"
  },
  {
    "start": 355570,
    "end": 362480,
    "text": "エンコーダーのレイヤーは32あるので、このブロックは32回繰り返される。"
  },
  {
    "start": 363110,
    "end": 375762,
    "text": "ヘッド・ディメンションは、ご記憶の通り、我々が持っているマルチ・ヘッド・アテンションでは、各ヘッドは文全体を見ているが、各トークンのエンベッディングの一部しか見ていないことを示す。"
  },
  {
    "start": 375826,
    "end": 384690,
    "text": "これは、各ヘッドがマルチヘッドアテンションにおいていくつの次元に注意を向けるかを示している。"
  },
  {
    "start": 384850,
    "end": 389026,
    "text": "ここでの隠れ次元は、フィードフォワード層の隠れ次元を示す。"
  },
  {
    "start": 389058,
    "end": 402838,
    "text": "フィードフォワード層の場合、2つの線形層があり、1つは埋め込みベクトルの次元を隠れサイズに変換し、もう1つは隠れサイズを埋め込みベクトルの次元に戻す。"
  },
  {
    "start": 403014,
    "end": 408174,
    "text": "ミストラルの場合、彼らは14,336を隠しサイズとして使っている。"
  },
  {
    "start": 408292,
    "end": 413022,
    "text": "通常、これは寸法の倍数であり、3.5のように見える。"
  },
  {
    "start": 413076,
    "end": 421714,
    "text": "ここでの次元は、クエリーの注目ヘッド数が32であるのに対して、kとvのヘッド数は32である。"
  },
  {
    "start": 421752,
    "end": 426302,
    "text": "キーと値は8であり、それらはグループ化されたクエリの注意のために等しくない。"
  },
  {
    "start": 426366,
    "end": 446778,
    "text": "グループ化されたクエリー・アテンションについて話した前回のラマについてのビデオを覚えているのであれば、グループ化されたクエリー・アテンションの非常に単純なケースでは、マルチ・クエリー・アテンションがあり、これはクエリーだけがマルチ・ヘッドを持ち、kとvはマルチ・ヘッド・アテンションを持たないことを意味します。"
  },
  {
    "start": 446864,
    "end": 455102,
    "text": "グループ化されたクエリの場合、クエリの各グループはkとvに対して1つのアテンションヘッドを持つことを意味する。"
  },
  {
    "start": 455156,
    "end": 460826,
    "text": "この場合、4つのクエリーはキーと値に対して1つのアテンション・ヘッドを持つ。"
  },
  {
    "start": 460938,
    "end": 462478,
    "text": "このコンセプトが明確でない場合"
  },
  {
    "start": 462564,
    "end": 466130,
    "text": "以前のリャマについてのビデオで、私はそれを徹底的に説明した。"
  },
  {
    "start": 466470,
    "end": 475554,
    "text": "窓の大きさは、注目度の計算に使ったスライディングウィンドウの大きさである。"
  },
  {
    "start": 475672,
    "end": 481990,
    "text": "コンテキストの長さとは、モデルがトレーニングされたコンテキストのサイズである。"
  },
  {
    "start": 482330,
    "end": 484950,
    "text": "8×7のBの方がはるかに大きい。"
  },
  {
    "start": 485020,
    "end": 492006,
    "text": "語彙のサイズはどちらも同じで、最後の2つのパラメータは、エキスパートのスパース混合に関連している。"
  },
  {
    "start": 492118,
    "end": 494602,
    "text": "それがどのように機能するかは、後でわかるだろう。"
  },
  {
    "start": 494656,
    "end": 499814,
    "text": "エキスパートが8人いて、トークン1つにつき2人のエキスパートを起用する。"
  },
  {
    "start": 499862,
    "end": 502314,
    "text": "後ほど、その仕組みを明らかにする。"
  },
  {
    "start": 502512,
    "end": 503926,
    "text": "さらに話を進めよう。"
  },
  {
    "start": 504038,
    "end": 506314,
    "text": "スライディングウインドウの注意について話そう。"
  },
  {
    "start": 506362,
    "end": 511770,
    "text": "スライディング・ウインドウの注意について話す前に、自己の注意のメカニズムについて少し復習しておく必要がある。"
  },
  {
    "start": 511930,
    "end": 513950,
    "text": "自己注意とは何か？"
  },
  {
    "start": 514530,
    "end": 519410,
    "text": "自己注意は、モデルがトークン同士を関連付けることを可能にするメカニズムである。"
  },
  {
    "start": 519480,
    "end": 524270,
    "text": "同じセンテンスに含まれるトークンは、自己注意メカニズムによって互いに関連づけられる。"
  },
  {
    "start": 524350,
    "end": 531030,
    "text": "各トークンは同じ文の他のトークンを見ているので、これが自己注目と呼ばれる理由である。"
  },
  {
    "start": 532970,
    "end": 537590,
    "text": "これは基本的に、クエリのキーと値が同じメトリクスであることを意味する。"
  },
  {
    "start": 539150,
    "end": 541510,
    "text": "次のような文章があるとしよう。"
  },
  {
    "start": 541590,
    "end": 543530,
    "text": "猫は椅子の上にいる。"
  },
  {
    "start": 544030,
    "end": 554160,
    "text": "各トークンは4096次元で表現され、これは前に見たdimパラメータである。"
  },
  {
    "start": 554530,
    "end": 559966,
    "text": "これにキーの移調を掛けたものが4096×6となる。"
  },
  {
    "start": 560068,
    "end": 564634,
    "text": "クエリのキーと値は同じ行列なので、クエリ行列の転置に過ぎない。"
  },
  {
    "start": 564682,
    "end": 577874,
    "text": "自己注視の場合、内側の2つの次元がある種相殺されるため、これは6×6の行列を生成し、外側の次元はここでの出力行列の次元を示す。"
  },
  {
    "start": 578072,
    "end": 582242,
    "text": "さて、この行列の値は何を表しているのだろうか？"
  },
  {
    "start": 582386,
    "end": 592518,
    "text": "ここでの最初の値は、最初のトークンとクエリの最初の行とキーの最初の列との内積を示す。"
  },
  {
    "start": 592614,
    "end": 597180,
    "text": "基本的には、最初のトークンの埋め込みとそれ自身との内積である。"
  },
  {
    "start": 597630,
    "end": 612074,
    "text": "2つ目の値は、クエリ行列の1行目とキー行列の2列目、つまりキー行列の転置行列の内積を表し、基本的には最初のトークンのエンベッディングの内積を意味する。"
  },
  {
    "start": 612122,
    "end": 618766,
    "text": "vは2番目のトークンの埋め込みで、これはcatであり、他のすべての値についてはet cetera、et ceteraである。"
  },
  {
    "start": 618878,
    "end": 622494,
    "text": "ここに書いた値はすべてランダムなので、あまり値に集中しないでください。"
  },
  {
    "start": 622542,
    "end": 625266,
    "text": "また、これらの数字が1より小さいという事実もある。"
  },
  {
    "start": 625288,
    "end": 628214,
    "text": "ドット積は1より大きくなることがあるので、その必要はない。"
  },
  {
    "start": 628252,
    "end": 630840,
    "text": "ドットプロダクトの条件ではない。"
  },
  {
    "start": 632410,
    "end": 634690,
    "text": "通常、計算式では正規化も行う。"
  },
  {
    "start": 634770,
    "end": 638134,
    "text": "ここでは減衰の次元で割る。"
  },
  {
    "start": 638182,
    "end": 638646,
    "text": "衰える。"
  },
  {
    "start": 638678,
    "end": 646426,
    "text": "基本的に、それはサイズであり、この特定の注目ヘッドが注目するエンベッディングの部分である。"
  },
  {
    "start": 646528,
    "end": 650794,
    "text": "先が1本しかないことにしよう。"
  },
  {
    "start": 650912,
    "end": 652862,
    "text": "decayはdモデルに等しい。"
  },
  {
    "start": 652916,
    "end": 657230,
    "text": "基本的に、このヘッドは各トークンの完全な埋め込みを監視する。"
  },
  {
    "start": 658290,
    "end": 658846,
    "text": "いいかい？"
  },
  {
    "start": 658948,
    "end": 661274,
    "text": "通常は外側に攻撃的なモデルをトレーニングする。"
  },
  {
    "start": 661322,
    "end": 663182,
    "text": "言語モデルは外回帰モデルである。"
  },
  {
    "start": 663236,
    "end": 669694,
    "text": "つまり、出力は次のトークンに依存し、前のトークンだけに依存する。"
  },
  {
    "start": 669822,
    "end": 672222,
    "text": "これが、因果の仮面をかける理由である。"
  },
  {
    "start": 672286,
    "end": 683654,
    "text": "コーザル・マスクとは基本的に、注意のメカニズムにおいて、ある単語を未来の単語、つまりその単語の後に来る単語とは関連付けず、その単語の前に来る単語とだけ関連付けることを意味する。"
  },
  {
    "start": 683772,
    "end": 692722,
    "text": "例えば、catという単語はtheという単語の後に来るので、vという単語をcatという単語と関連させたくない。"
  },
  {
    "start": 692796,
    "end": 698778,
    "text": "一方、catという単語は、vという単語の前に来るので、vという単語と関連していてほしい。"
  },
  {
    "start": 698864,
    "end": 705470,
    "text": "そのため、アテンション・メカニズムがソフトマックス関数を使うので、この因果マスクを適用する。"
  },
  {
    "start": 705540,
    "end": 717586,
    "text": "ソフトマックスの公式は分子とeがxのべき乗であるため、ソフトマックス関数は基本的にこのマイナス無限大をすべてゼロに変換することがわかる。"
  },
  {
    "start": 717688,
    "end": 722722,
    "text": "xがマイナス無限大になるとき、eのマイナス無限大乗はゼロになる。"
  },
  {
    "start": 722856,
    "end": 740630,
    "text": "そのため、不要な値、トークン間の不要な相互作用をすべてマスクに入れ、それをマイナス無限大に置き換えてマスクする。"
  },
  {
    "start": 741290,
    "end": 742038,
    "text": "いいかい？"
  },
  {
    "start": 742204,
    "end": 752890,
    "text": "また、ソフトマックスは、このマイナス無限大をゼロに変換するだけでなく、各行の他の値も合計が1になるように修正する。"
  },
  {
    "start": 752960,
    "end": 757454,
    "text": "ご覧のように、これらの値は各行の合計が1になっていませんよね？"
  },
  {
    "start": 757492,
    "end": 760910,
    "text": "これは0.20.1とゼロ点なので、合計しても1にはならない。"
  },
  {
    "start": 760980,
    "end": 768180,
    "text": "ソフトマックスは、マイナス無限大をゼロに変換し、各行の残りの値を合計が1になるように変換する。"
  },
  {
    "start": 768550,
    "end": 771262,
    "text": "では、スライディングウインドウの注意について話そう。"
  },
  {
    "start": 771406,
    "end": 779014,
    "text": "そこで、ある単語と未来のすべての単語との間の相互作用を隠すために、因果マスクを適用した。"
  },
  {
    "start": 779132,
    "end": 787490,
    "text": "スライディング・ウィンドウ・アテンションでは、単語がそのローカル・コンテキストの外にある他の単語を見ないようにする。"
  },
  {
    "start": 787570,
    "end": 789046,
    "text": "これはどういう意味だろう？"
  },
  {
    "start": 789228,
    "end": 796362,
    "text": "先ほどの場合、因果関係のマスクだけを適用すると、たとえば椅子という単語は、ご覧のように前のトークンすべてと関連していた。"
  },
  {
    "start": 796416,
    "end": 802222,
    "text": "ここで椅子というトークンは、それ自身と関係しているが、a on is cat vとも関係している。"
  },
  {
    "start": 802356,
    "end": 805482,
    "text": "基本的にすべての文章を見ることができる。"
  },
  {
    "start": 805626,
    "end": 815690,
    "text": "スライディングウィンドウアテンションの場合、ワードチェアがスライディングウィンドウサイズより離れた単語を見ないようにする。"
  },
  {
    "start": 815780,
    "end": 819058,
    "text": "この場合のスライディングウィンドウのサイズは3である。"
  },
  {
    "start": 819144,
    "end": 823886,
    "text": "トークンは、検討中の単語から3つ以上離れている。"
  },
  {
    "start": 823918,
    "end": 832086,
    "text": "椅子という単語は、距離が4であるため、isという単語とは関係ないはずであり、aという単語は、距離が4であるため、catという単語とは関係ないはずである。"
  },
  {
    "start": 832188,
    "end": 842220,
    "text": "というのも、自己回帰モデルをトレーニングしているのだから、各トークンが将来の単語を見るようなモデルにはしたくないからだ。"
  },
  {
    "start": 843870,
    "end": 849878,
    "text": "スライディング・ウィンドウの注意は、基本的に実行するドット積の数を減らす。"
  },
  {
    "start": 849974,
    "end": 858942,
    "text": "というのも、ご覧のとおり、因果マスクだけを適用すると、ここにあるようなドット積をすべて実行してしまうからだ。"
  },
  {
    "start": 859076,
    "end": 864820,
    "text": "スライディング・ウインドウに注目すると、他のものがすべてマスクされるため、ドット積の実行が少なくなる。"
  },
  {
    "start": 865910,
    "end": 876994,
    "text": "しかし、スライディング・ウィンドウはモデルのパフォーマンスを低下させる可能性がある。"
  },
  {
    "start": 877032,
    "end": 881954,
    "text": "vとchairという単語からは情報が伝わらない。"
  },
  {
    "start": 882002,
    "end": 888446,
    "text": "椅子という単語は、この特定のトークンのローカルコンテキストに属する他のトークンのみに関連する。"
  },
  {
    "start": 888498,
    "end": 893770,
    "text": "このスライディング・ウィンドウの中にあるトークンのみ。"
  },
  {
    "start": 895230,
    "end": 915486,
    "text": "このウィンドウが小さすぎると、モデルのパフォーマンスが低下する可能性があるが、有益な場合もある。例えば、あなたが本を読んでいるとすると、第5章の単語と第1章の単語を関連付けることに関心がない。"
  },
  {
    "start": 915598,
    "end": 923570,
    "text": "第5章のトークンと他のトークンを関連付けたいのは確かだ。"
  },
  {
    "start": 923990,
    "end": 927302,
    "text": "ここで、「レセプティブ・フィールド」という概念を紹介したい。"
  },
  {
    "start": 927356,
    "end": 955754,
    "text": "なぜなら、スライディングウィンドウアテンションを用いると、たとえchairとvという単語が互いに関連していなくても、実際には、ミストラルやすべてのトランスフォーマーモデルでは多層のエンコーダーを使用しているため、shareという単語とdという単語は、直接ではなく、畳み込みニューラルネットワークの受容野に非常によく似た概念で間接的に、互いに関連していることがわかるからだ。"
  },
  {
    "start": 955802,
    "end": 958400,
    "text": "受容野について話そう。"
  },
  {
    "start": 958850,
    "end": 965814,
    "text": "覚えているだろうが、畳み込みニューラルネットワークでは、画像にマスクをかける。"
  },
  {
    "start": 965882,
    "end": 969522,
    "text": "これがオリジナルのイメージだと想像してほしい。"
  },
  {
    "start": 969656,
    "end": 977474,
    "text": "カーネルを実行すると、出力特徴が生成される。"
  },
  {
    "start": 977522,
    "end": 995814,
    "text": "例えば、この特徴は、最初の3×3のグリッドにカーネルを適用したときに出力されるもので、この値、黄色の2番目の値は、カーネルを次の3×3のピクセルのグループに移動したときに出力されます。"
  },
  {
    "start": 995862,
    "end": 999370,
    "text": "描かせてください、ペンを使いましょう。"
  },
  {
    "start": 999440,
    "end": 1006880,
    "text": "この値は、このグリッドでカーネルを動かしたときに生成される。"
  },
  {
    "start": 1009810,
    "end": 1022258,
    "text": "この値は、このレイヤー2に適用された3×3の畳み込みカーネルの出力特徴でもある。"
  },
  {
    "start": 1022424,
    "end": 1026166,
    "text": "これはレイヤー2に適用される3×3のカーネルである。"
  },
  {
    "start": 1026268,
    "end": 1032358,
    "text": "どうやら、このピクセルとこのピクセルとこのピクセルの間には何のつながりもないようだ。"
  },
  {
    "start": 1032524,
    "end": 1053438,
    "text": "なぜなら、この出力特徴はこのグリッドで適用されたカーネルに依存し、このグリッドにはこの特徴も含まれ、この特徴もこのピクセルに依存するからである。"
  },
  {
    "start": 1053524,
    "end": 1055694,
    "text": "これが受容野の概念である。"
  },
  {
    "start": 1055732,
    "end": 1073810,
    "text": "基本的に、畳み込みニューラルネットワークの1つの特徴は、畳み込みカーネルのカーネルを連続的に適用するため、レイヤーの上方により大きな受容野を見ることができる。"
  },
  {
    "start": 1074550,
    "end": 1077826,
    "text": "このコンセプトがスライディング・ウィンドウとどのように関係しているのか見てみよう。"
  },
  {
    "start": 1077858,
    "end": 1078920,
    "text": "ご注目ください。"
  },
  {
    "start": 1079450,
    "end": 1092234,
    "text": "さて、先ほどのマスクにソフトマックスを適用すると、先にお話ししたように、マイナス不等号はすべてゼロに変換され、その他の値はすべて合計が1になるように変更される。"
  },
  {
    "start": 1092272,
    "end": 1093418,
    "text": "戻ろう。"
  },
  {
    "start": 1093584,
    "end": 1098058,
    "text": "ここで覚えているように、私たちはこことこことここにマイナス無限大を持っている。"
  },
  {
    "start": 1098144,
    "end": 1103290,
    "text": "ここでソフトマックスを適用すると、ゼロになる。"
  },
  {
    "start": 1103360,
    "end": 1110990,
    "text": "また、ここにあるすべてのゼロ、ここにあるすべてのゼロ、その他のすべての値は、合計が1になるように変更される。"
  },
  {
    "start": 1111060,
    "end": 1113722,
    "text": "自己注視の中で次に行う作業は何か？"
  },
  {
    "start": 1113786,
    "end": 1117806,
    "text": "次に、ソフトマックスの出力を取り、v行列と掛け合わせる。"
  },
  {
    "start": 1117918,
    "end": 1126334,
    "text": "Vマトリックスは基本的に最初のシーケンスと同じだ。"
  },
  {
    "start": 1126382,
    "end": 1129650,
    "text": "クエリのキーと値は同じ行列です。"
  },
  {
    "start": 1129810,
    "end": 1135010,
    "text": "つまり、この掛け算をしたときに何が起こるかを手で分析してみようということだ。"
  },
  {
    "start": 1135170,
    "end": 1137078,
    "text": "ペンに持ち替えよう。"
  },
  {
    "start": 1137164,
    "end": 1137750,
    "text": "オーケー。"
  },
  {
    "start": 1137900,
    "end": 1147778,
    "text": "ここでのV行列はトークンの列であり、各トークンは4096次元で表現されるベクトルである。"
  },
  {
    "start": 1147954,
    "end": 1154266,
    "text": "この2つの行列の次元を見れば、自己注目の出力と言える。"
  },
  {
    "start": 1154298,
    "end": 1160462,
    "text": "が6×6で、6×4096の場合、出力は6×4096の別の行列になる。"
  },
  {
    "start": 1160516,
    "end": 1167566,
    "text": "V行列と同じ次元を持ち、q行列とクエリ行列も同じ次元を持つからである。"
  },
  {
    "start": 1167678,
    "end": 1171650,
    "text": "の場合、6トークンが出力される。"
  },
  {
    "start": 1172230,
    "end": 1177926,
    "text": "では、このアウトプットの最初の次元が何なのかを分析してみよう。"
  },
  {
    "start": 1178028,
    "end": 1188954,
    "text": "つまり、出力行列の1行1列の値は、この行列の1行目のドット積となる。"
  },
  {
    "start": 1188992,
    "end": 1195686,
    "text": "この行は、この行列の最初の列と一致する。"
  },
  {
    "start": 1195718,
    "end": 1213390,
    "text": "ということは、1行目から5行目まで、申し訳ないが2行目から6行目までは使用されず、1行目の値のみが使用されることになる。"
  },
  {
    "start": 1213460,
    "end": 1227610,
    "text": "というのも、ドット積とは、1番目の次元とこの列の1番目の次元、この行の2番目の次元とこの列の2番目の次元、この行の3番目の次元とこの列の3番目の次元のことだからだ。"
  },
  {
    "start": 1227630,
    "end": 1229522,
    "text": "そして、これらの値をすべて合計する。"
  },
  {
    "start": 1229666,
    "end": 1236562,
    "text": "この出力の最初の値は、v行列の最初のトークンのみに依存する。"
  },
  {
    "start": 1236626,
    "end": 1237714,
    "text": "こちらをご覧いただきたい。"
  },
  {
    "start": 1237852,
    "end": 1239386,
    "text": "つ目をチェックしよう。"
  },
  {
    "start": 1239488,
    "end": 1270530,
    "text": "出力行列の2行目の1番目の次元は、この行列の1行目とv行列の1列目との内積になるが、ほとんどの値はゼロである。つまり、ここの次元とこの行のすべての次元は、v行列の最初の2つのトークンのみに依存することになり、3番目についても同じことが言える。"
  },
  {
    "start": 1270600,
    "end": 1272466,
    "text": "ここで6つ目を分析してみよう。"
  },
  {
    "start": 1272568,
    "end": 1287352,
    "text": "出力行列の6行目の1次元目。この値は、この行とv行列の1列目とのドット積に由来する。"
  },
  {
    "start": 1287496,
    "end": 1307990,
    "text": "つまり、v行列の4、5、6番目のトークンのみに依存することになり、ここでのすべての次元がそうなる。各列で、v行列のどの列を使うにしても、最初の値は常に掛け算されるからだ。"
  },
  {
    "start": 1308760,
    "end": 1312132,
    "text": "この3行の値のみを使用する。"
  },
  {
    "start": 1312266,
    "end": 1325796,
    "text": "この自己注意メカニズムの出力行列の6番目のトークンは、V行列の最後の3つのトークンにのみ依存するベクトルになると言っていいだろう。"
  },
  {
    "start": 1325988,
    "end": 1330456,
    "text": "V行列はクエリ行列と等しい。"
  },
  {
    "start": 1330568,
    "end": 1346428,
    "text": "自己注目の出力は、入力シーケンスと同じ形状を持つマトリックスであるが、各トークンは他のトークンに関するより多くの情報を捕捉している、と言うことができる。"
  },
  {
    "start": 1346524,
    "end": 1350624,
    "text": "私たちのマスクは、最初のトークンは自分自身しか見ることができない。"
  },
  {
    "start": 1350742,
    "end": 1355556,
    "text": "最初の出力トークンは、それ自身にのみ依存するエンベッディングになる。"
  },
  {
    "start": 1355738,
    "end": 1359984,
    "text": "つ目のトークンは、最初の2つのトークンにのみ依存する。"
  },
  {
    "start": 1360112,
    "end": 1365104,
    "text": "3番目の出力トークンは、最初の3つのトークンにのみ依存する。"
  },
  {
    "start": 1365232,
    "end": 1381756,
    "text": "最初のトークンは使用されないので、4番目はトークン番号2に依存し、トークン番号2、トークン番号3、トークン番号4、エトセトラ、エトセトラ、最後のトークンまで、最後のトークンは最後の3つのトークンにのみ依存する。"
  },
  {
    "start": 1381858,
    "end": 1386232,
    "text": "これが、自己注意メカニズムに適用するマスクの重要性である。"
  },
  {
    "start": 1386376,
    "end": 1390252,
    "text": "これからお見せするこのコンセプトは、ビデオの続きを理解するためにとても重要です。"
  },
  {
    "start": 1390306,
    "end": 1400240,
    "text": "もし理解できなかったのなら、少し間を置いて、自分でやってみるといい。この自己注意メカニズムがマスクとどのように作用するのかを理解することが本当に重要だからだ。"
  },
  {
    "start": 1401140,
    "end": 1405204,
    "text": "さて、このコンセプトを見たところで、次のコンセプトを紹介したい。"
  },
  {
    "start": 1405242,
    "end": 1417656,
    "text": "先に見たように、自己注意メカニズムの出力は、クエリー行列と同じ形をした別の行列であり、その中で各トークンはサイズ4096の埋め込みによって表現される。"
  },
  {
    "start": 1417758,
    "end": 1423748,
    "text": "各埋め込みは、マスクに従って他のトークンに関する情報も取り込むようになった。"
  },
  {
    "start": 1423844,
    "end": 1436808,
    "text": "このマスクをチェックすれば、つまりこの出力をチェックすれば、スライディング・ウィンドウ注目の入力は初期シーケンスであったと言える。"
  },
  {
    "start": 1436904,
    "end": 1443968,
    "text": "ディカットは椅子の上にいるが、セルフ・アテンションをかけると、最初のトークンは自分自身に関係するようになる。"
  },
  {
    "start": 1444134,
    "end": 1448128,
    "text": "つ目のトークンは、それ自身とその前のトークンに関連している。"
  },
  {
    "start": 1448214,
    "end": 1452336,
    "text": "3つ目は、その前のトークン、さらにその前のトークンに関連している。"
  },
  {
    "start": 1452438,
    "end": 1456064,
    "text": "最後のトークンは、前の2つのトークンにのみ依存する。"
  },
  {
    "start": 1456112,
    "end": 1460244,
    "text": "マスクによると、今、これを食べさせたらどうなるんだ？"
  },
  {
    "start": 1460282,
    "end": 1474632,
    "text": "ご存知のように、トランスフォーマーの世界では、またミストラルやラマでも、エンコーダーが何層にも重なっていて、コードではトランスフォーマー・ブロックとも呼ばれている。"
  },
  {
    "start": 1474686,
    "end": 1477796,
    "text": "これがトランスの第1層である。"
  },
  {
    "start": 1477828,
    "end": 1485896,
    "text": "各トークンは他のトークンに関する情報を含んでいる。"
  },
  {
    "start": 1486008,
    "end": 1491948,
    "text": "これが次の層の入力となり、出力を生成する。"
  },
  {
    "start": 1492044,
    "end": 1496892,
    "text": "この出力は、さらに多くのトークンに関する情報をキャプチャすることを証明する。"
  },
  {
    "start": 1496956,
    "end": 1505476,
    "text": "スライディング・ウインドウのサイズから、前の2つのトークンしか見ることができないはずだというスライディング・ウインドウの注意があったとしても、私たちは3つを選んだ。"
  },
  {
    "start": 1505578,
    "end": 1509364,
    "text": "スライディングウインドウのサイズとして、それを証明したい。"
  },
  {
    "start": 1509402,
    "end": 1513712,
    "text": "これが第1層の出力であると想像してほしい。"
  },
  {
    "start": 1513776,
    "end": 1518096,
    "text": "これは、他のトークンに関する情報をキャプチャするトークンのリストである。"
  },
  {
    "start": 1518128,
    "end": 1521700,
    "text": "前のスライドで構築した測定基準です。"
  },
  {
    "start": 1521860,
    "end": 1525476,
    "text": "これをエンコーダーの別のレイヤーの入力として使ってみよう。"
  },
  {
    "start": 1525508,
    "end": 1538764,
    "text": "クエリーとキーの転置行列を掛け合わせると、このような行列ができる。各トークンは1つのトークンであるだけでなく、複数のトークンに関する情報をすでに捕捉していることになる。"
  },
  {
    "start": 1538802,
    "end": 1539960,
    "text": "マスクによると"
  },
  {
    "start": 1540040,
    "end": 1544828,
    "text": "このクエリーとこのクエリーがクエリーのキーとバリューになる。"
  },
  {
    "start": 1544924,
    "end": 1548768,
    "text": "このクエリーにキーを掛けると、次のような行列が返される。"
  },
  {
    "start": 1548854,
    "end": 1554196,
    "text": "最初のトークンは自分自身にのみ依存し、2番目のトークンは自分自身と前のトークンに依存する。"
  },
  {
    "start": 1554218,
    "end": 1561940,
    "text": "このトークンの埋め込みは2つのトークンに関する情報をキャプチャし、このトークンの埋め込みは3つのトークンに関する情報をキャプチャする。"
  },
  {
    "start": 1562520,
    "end": 1564680,
    "text": "もう一度掛け算をしてみよう。"
  },
  {
    "start": 1564750,
    "end": 1581784,
    "text": "出力もトークンのリストになるが、各トークンは他のトークンに関する情報を持っている。"
  },
  {
    "start": 1581912,
    "end": 1585132,
    "text": "さて、ここでドット積を分析してみよう。"
  },
  {
    "start": 1585266,
    "end": 1587752,
    "text": "最初の行の最初の値。"
  },
  {
    "start": 1587816,
    "end": 1595312,
    "text": "出力行列の最初の行の最初の次元は、ここではこの行列の最初の行のドット積となる。"
  },
  {
    "start": 1595366,
    "end": 1600032,
    "text": "この行は、この行列の最初の列とここにある。"
  },
  {
    "start": 1600086,
    "end": 1601810,
    "text": "このコラムはこちら"
  },
  {
    "start": 1602580,
    "end": 1612512,
    "text": "スライディングウィンドウ・アテンションマスクによるこの因果マスクのおかげで、出力はvマトリックスの最初の行にのみ依存することになる。"
  },
  {
    "start": 1612656,
    "end": 1619784,
    "text": "というのも、v行列はここではこれらのトークンからなる行列であり、vという単語にのみ依存するからである。"
  },
  {
    "start": 1619902,
    "end": 1627852,
    "text": "ここでわかるように、第2層の出力は単語vにのみ依存するので、第2層の出力もそうなる。"
  },
  {
    "start": 1627906,
    "end": 1630632,
    "text": "つ目のトークンをチェックしよう。"
  },
  {
    "start": 1630696,
    "end": 1635932,
    "text": "例えば、この4番目のトークンを見てみよう。"
  },
  {
    "start": 1635986,
    "end": 1647276,
    "text": "この値は、この行列の4行目とv行列の1列目との積となる。"
  },
  {
    "start": 1647308,
    "end": 1653476,
    "text": "しかし、最初のトークンはゼロを掛けるので使われない。"
  },
  {
    "start": 1653578,
    "end": 1656196,
    "text": "ここにどんな価値があろうと、それを使うことはない。"
  },
  {
    "start": 1656298,
    "end": 1661168,
    "text": "私たちは2つ目のトークン、3つ目のトークン、そして4つ目のトークンを使っている。"
  },
  {
    "start": 1661344,
    "end": 1667864,
    "text": "各トークンは、実際にはこれらの値を集約している。"
  },
  {
    "start": 1667982,
    "end": 1673384,
    "text": "このトークンは、すでにdとcatという2つのトークンの値を集約している。"
  },
  {
    "start": 1673502,
    "end": 1706470,
    "text": "このエンベッディングはすでにdとcatについて話しており、このトークンはd、cat、isについての情報を集約しているので、d cat and is、そして4番目のトークンはcat is onの情報を集約しているので、cat is and onとなる。"
  },
  {
    "start": 1707320,
    "end": 1715328,
    "text": "この出力値は、他のトークンに関する情報をすでに含んでいる3つのトークンに依存する。"
  },
  {
    "start": 1715424,
    "end": 1721236,
    "text": "この値は、これらすべてのトークンの結合に関する情報を集約する。"
  },
  {
    "start": 1721268,
    "end": 1727448,
    "text": "dという単語は2番目のトークンに含まれているため、この単語を掛けるかどうかは確実にdに依存する。"
  },
  {
    "start": 1727614,
    "end": 1733548,
    "text": "このトークンにはcatという単語も含まれているので、catという単語に関する情報も含まれることは間違いない。"
  },
  {
    "start": 1733634,
    "end": 1750050,
    "text": "isについては、掛け合わせる2番目の値に含まれているので確実に含まれるし、onについては、掛け合わせるv行列の4番目のトークンに含まれているので確実に含まれる。"
  },
  {
    "start": 1750820,
    "end": 1759716,
    "text": "ご覧のように、エンコーダーをもう1層適用した後、4番目のトークンはその情報に別のトークンを含むようになった。"
  },
  {
    "start": 1759818,
    "end": 1770692,
    "text": "以前はこの3つのトークンのみを含んでいたが、現在はvという新しいトークンにも依存しており、5番目のトークンと6番目のトークンについても同じことが証明できる。"
  },
  {
    "start": 1770756,
    "end": 1781196,
    "text": "エンコーダー・レイヤーを適用するたびに、これらのドット積に蓄積されるトークンの数を次々に増やしていく。"
  },
  {
    "start": 1781378,
    "end": 1785230,
    "text": "私はこれを視覚化するためにPythonでノートブックを作った。"
  },
  {
    "start": 1785600,
    "end": 1795904,
    "text": "私のGitHubレポジトリを見れば、スライディング・ウィンドウ・アテンションと呼ばれるノートブックを見ることができる。"
  },
  {
    "start": 1796022,
    "end": 1799052,
    "text": "また、この自己注意をどのように行うかのコードも共有している。"
  },
  {
    "start": 1799116,
    "end": 1812320,
    "text": "基本的に、私は各トークンをセットとして表現する。つまり、各トークンは、そのトークンが依存するすべての単語のセットとして修正として表現されるのではなく、そのトークンが依存するすべての単語のセットとして表現される。"
  },
  {
    "start": 1812480,
    "end": 1821172,
    "text": "これは基本的に、シーケンスから2つのトークンを取り出し、累積することを意味する。"
  },
  {
    "start": 1821236,
    "end": 1829556,
    "text": "なぜなら、すでに複数のトークンに関する情報を含む2つのベクトルを掛け合わせるからだ。"
  },
  {
    "start": 1829588,
    "end": 1833112,
    "text": "出力は2つの集合の和である。"
  },
  {
    "start": 1833256,
    "end": 1836764,
    "text": "Vを掛けるときも同じように、視覚化できるんだ。"
  },
  {
    "start": 1836802,
    "end": 1842204,
    "text": "最初のレイヤーを適用した後、最初のレイヤーの入力が通常のシーケンスであることがわかる。"
  },
  {
    "start": 1842252,
    "end": 1843612,
    "text": "猫は椅子の上にいる。"
  },
  {
    "start": 1843676,
    "end": 1852764,
    "text": "第1層の出力は、適用したマスクに応じて、各位置が複数のトークンに関する情報を含む別のシーケンスになる。"
  },
  {
    "start": 1852812,
    "end": 1855076,
    "text": "マスクも見せてもらった。"
  },
  {
    "start": 1855258,
    "end": 1859648,
    "text": "2層目を適用すると、情報量が増えているのがわかる。"
  },
  {
    "start": 1859744,
    "end": 1865860,
    "text": "この最後のトークンは、前の3つのトークンだけを見ているのではなく、前の4つのトークンを見ているのだ。"
  },
  {
    "start": 1866680,
    "end": 1869956,
    "text": "すみません、前の2つのトークンだけでなく、前の4つのトークンです。"
  },
  {
    "start": 1869988,
    "end": 1876996,
    "text": "スライディング・ウィンドウのサイズを3にして、各レイヤーに2つのトークンを含める。"
  },
  {
    "start": 1877188,
    "end": 1885976,
    "text": "ここでは5レイヤーで表示しているが、しばらくするとシーケンスが最大長に達するので、その必要はない。"
  },
  {
    "start": 1886088,
    "end": 1891200,
    "text": "必要であれば、より多くのトークンを含めることで、ここでシーケンス長を長くすることができる。"
  },
  {
    "start": 1892580,
    "end": 1898444,
    "text": "これは受容野の概念をセルフ・ウィンドウ・アテンションに応用したものである。"
  },
  {
    "start": 1898492,
    "end": 1922170,
    "text": "基本的に、スライディング・ウィンドウ注目では、2つのトークンを直接結びつけることはしないが、複数のレイヤーを次々に適用していけば、この情報はレイヤーの連続適用における埋め込みによって捕捉され、最後のレイヤーでは基本的に、たとえ非常に長い文章であっても、すべての文章を見ることができるようになる。"
  },
  {
    "start": 1923500,
    "end": 1929404,
    "text": "このミストラル紙をご覧いただきたい。"
  },
  {
    "start": 1929522,
    "end": 1931944,
    "text": "基本的にこれが入力シーケンスだ。"
  },
  {
    "start": 1931992,
    "end": 1933310,
    "text": "書かせてください。"
  },
  {
    "start": 1934880,
    "end": 1940092,
    "text": "これが入力であり、元の文である。"
  },
  {
    "start": 1940156,
    "end": 1944604,
    "text": "猫は椅子の上にいる。"
  },
  {
    "start": 1944652,
    "end": 1946556,
    "text": "これが第1層の出力である。"
  },
  {
    "start": 1946588,
    "end": 1955364,
    "text": "レイヤー1では、4番目のトークンはスライディング・ウィンドウのサイズが4であることがわかる。"
  },
  {
    "start": 1955482,
    "end": 1962228,
    "text": "このトークンは、それ自身、前のトークン、前のトークン、そしてこのトークンに依存する。"
  },
  {
    "start": 1962394,
    "end": 1970072,
    "text": "この埋め込みは、前のトークンに関する情報も含んでいる。"
  },
  {
    "start": 1970206,
    "end": 1974570,
    "text": "すると、これが次のレイヤー（レイヤー番号2）の入力となる。"
  },
  {
    "start": 1977370,
    "end": 1980982,
    "text": "これは、例えばこの位置にエンベッディングを生成する。"
  },
  {
    "start": 1981116,
    "end": 1994842,
    "text": "スライディング・ウィンドウのサイズは4つなので、前の4つのトークンに依存することは確かだが、例えば、このトークンはすでに前の4つのトークンの集合体なので、実際にはスライディング・ウィンドウの可視性を倍増させることになる。"
  },
  {
    "start": 1994906,
    "end": 2006020,
    "text": "このトークンは、ここに見える最初のトークンとは直接関係なく、ここに見える中間のトークンを介して間接的に関係している。"
  },
  {
    "start": 2007910,
    "end": 2009522,
    "text": "このコンセプトが明確であることを願っている。"
  },
  {
    "start": 2009576,
    "end": 2023270,
    "text": "もし分かりにくかったら、私のノートブックを使うことをお勧めします。そうすれば、複数のシークエンスを使って実験することができますし、情報の流れがすべてのレイヤーをどのように通過していくかを見ることができます。"
  },
  {
    "start": 2024970,
    "end": 2028626,
    "text": "さて、次の話題はKVキャッシュについてだ。"
  },
  {
    "start": 2028658,
    "end": 2037558,
    "text": "というのも、KVキャッシュについては、前回のlamaのビデオですでに説明したが、後でローリング・バッファ・キャッシュを紹介したいので、もう一度紹介し、復習しておきたいからだ。"
  },
  {
    "start": 2037654,
    "end": 2044818,
    "text": "KVキャッシュを理解するために必要なことなので、まずは言語モデルをどのようにトレーニングするかについて話そう。"
  },
  {
    "start": 2044854,
    "end": 2049914,
    "text": "言語モデルは、次のトークン予測タスクとして知られるものを使って学習される。"
  },
  {
    "start": 2049962,
    "end": 2056882,
    "text": "プロンプトが与えられると、言語モデルのゴールは、与えられたプロンプトで意味をなす次のトークンを予測することである。"
  },
  {
    "start": 2057016,
    "end": 2063022,
    "text": "ダンテ・アッレゲーリの詩「神曲」の言語モデルを学習したいとする。"
  },
  {
    "start": 2063086,
    "end": 2072710,
    "text": "特に私たちは、この英語版で見ることができるライン上でそれを訓練するので、すぐに優しい心を見ることができる愛。"
  },
  {
    "start": 2072860,
    "end": 2074022,
    "text": "どのように機能するのか？"
  },
  {
    "start": 2074156,
    "end": 2082886,
    "text": "言語モデルの入力として、文頭というトークンが付加された行を用意する。"
  },
  {
    "start": 2082998,
    "end": 2088342,
    "text": "次に、同じ行であるが、文末と呼ばれるトークンを最後に持つターゲットを構築する。"
  },
  {
    "start": 2088486,
    "end": 2091254,
    "text": "この変圧器モデルに入力を通す。"
  },
  {
    "start": 2091312,
    "end": 2093690,
    "text": "これは出力シーケンスを生成する。"
  },
  {
    "start": 2093770,
    "end": 2101290,
    "text": "前に見たように、自己注意の出力は入力配列と同じ長さの別の配列である。"
  },
  {
    "start": 2101370,
    "end": 2107822,
    "text": "埋め込みは、各トークンが他のトークンに関する情報を取り込むように変更されます。"
  },
  {
    "start": 2107966,
    "end": 2111202,
    "text": "これは実際にモデルを訓練するために使うものだ。"
  },
  {
    "start": 2111256,
    "end": 2117538,
    "text": "モデルに9つのトークンを与えると、モデルは出力として9つのトークンを生成する。"
  },
  {
    "start": 2117634,
    "end": 2119398,
    "text": "どのように機能するのか？"
  },
  {
    "start": 2119484,
    "end": 2134090,
    "text": "基本的に、このモデルは入力と出力の間のマッピングを学習し、入力として文頭のトークンだけを与えると、出力として最初のトークン、つまりloveという単語を生成する。"
  },
  {
    "start": 2134240,
    "end": 2146106,
    "text": "最初の2つのトークンを入力としてモデルに与える場合、つまり文loveの開始、モデルは出力として2つのトークンを生成するので、loveは3つのトークンを入力としてモデルに与えることになる。"
  },
  {
    "start": 2146138,
    "end": 2150750,
    "text": "文頭の \"愛 \"は、そのモデルが生み出す \"愛 \"である。"
  },
  {
    "start": 2150820,
    "end": 2160062,
    "text": "モデルを訓練するときは、このように訓練する。このように入力を用意し、このようにターゲットを用意し、出力を計算し、クロスエントロピー損失を使って損失を計算し、そして逆伝播を実行する。"
  },
  {
    "start": 2160206,
    "end": 2162146,
    "text": "これはすべてワンステップで行われる。"
  },
  {
    "start": 2162248,
    "end": 2164814,
    "text": "推論を行う際には、複数のステップを踏む。"
  },
  {
    "start": 2164862,
    "end": 2172518,
    "text": "時間ステップ1で推論を行う場合、最初のトークンのみをモデルに与えるので、文頭とモデルは愛を出力する。"
  },
  {
    "start": 2172684,
    "end": 2182378,
    "text": "そして、出力の最後のトークンを取り出し、それを入力の前に追加する。"
  },
  {
    "start": 2182464,
    "end": 2184726,
    "text": "このモデルは愛の負債を生む。"
  },
  {
    "start": 2184838,
    "end": 2194106,
    "text": "出力の最後のトークンを取り出し、それをステップ3の入力に追加する。"
  },
  {
    "start": 2194208,
    "end": 2203582,
    "text": "そして、出力の最後のトークンを取り出し、それをタイムステップ4の入力に追加する。"
  },
  {
    "start": 2203716,
    "end": 2214654,
    "text": "そして、この単語を素早く取り出し、次の時間ステップの入力に加え、次のトークンを出力として生成する。"
  },
  {
    "start": 2214702,
    "end": 2230698,
    "text": "そうすれば、モデルが新しいトークンを生成するのを止めたことがわかるので、推論を止めることができる。"
  },
  {
    "start": 2230784,
    "end": 2243582,
    "text": "もちろん、プロンプトに属するモデルには以前のトークンをすべて送り込む必要がある。なぜなら、モデルは次にどのトークンを生成するかを理解するためにプロンプトにアクセスする必要があるからだ。"
  },
  {
    "start": 2243636,
    "end": 2248718,
    "text": "例えば、zという単語を与えるだけで、gentleという単語を作り出すことはできない。"
  },
  {
    "start": 2248804,
    "end": 2253280,
    "text": "この優しいアウトプットを出すためには、この文章をすべて与える必要がある。"
  },
  {
    "start": 2253650,
    "end": 2258878,
    "text": "と同時に、私たちは最後の優しい言葉にしか興味がない。"
  },
  {
    "start": 2258974,
    "end": 2275926,
    "text": "KVCacheを導入したのはこのためだ。KVCacheを使えば、一度に必要な出力を1つだけ生成することで、やっている計算を減らすことができる。"
  },
  {
    "start": 2276028,
    "end": 2285510,
    "text": "基本的に、ハートという言葉が欲しいとき、私たちは愛という言葉のアウトプットを出したくない。"
  },
  {
    "start": 2285590,
    "end": 2289690,
    "text": "これらのトークンをすべて生成する必要はなく、トークン・ハートの出力を生成したいだけなのだ。"
  },
  {
    "start": 2289760,
    "end": 2292106,
    "text": "計算量を減らしたい。"
  },
  {
    "start": 2292208,
    "end": 2317854,
    "text": "自己注目のメカニズムがどのように機能するか見てみよう。トークンのリストとして考えることができるクエリを掛け合わせ、各トークンはサイズ4096のエンベッディングである。"
  },
  {
    "start": 2317902,
    "end": 2318738,
    "text": "こちらをご覧いただきたい。"
  },
  {
    "start": 2318824,
    "end": 2321062,
    "text": "トークン1つずつやっていこう。"
  },
  {
    "start": 2321116,
    "end": 2326322,
    "text": "言語モデルを推論するときは、文頭である最初のトークンから始める。"
  },
  {
    "start": 2326466,
    "end": 2330998,
    "text": "これはサイズ4096の埋め込みで表現される1トークンである。"
  },
  {
    "start": 2331084,
    "end": 2336902,
    "text": "キーの移調を掛け合わせるが、これも自己注意なので1トークンである。"
  },
  {
    "start": 2336966,
    "end": 2340054,
    "text": "クエリでは、キーと値は同じ行列です。"
  },
  {
    "start": 2340102,
    "end": 2348266,
    "text": "これは基本的にクエリの転置に過ぎないので、列ベクトルであり、1×1の行列を生成する。"
  },
  {
    "start": 2348298,
    "end": 2351018,
    "text": "これにvを掛けると、出力トークンが生成される。"
  },
  {
    "start": 2351114,
    "end": 2360142,
    "text": "この出力トークンを線形層に送り、次にソフトマックスに送って、これが語彙のどのトークンに対応するかを理解する。"
  },
  {
    "start": 2360206,
    "end": 2368846,
    "text": "このトークンをボキャブラリーから取り出し、次の推論ステップでキーと値に追加する。"
  },
  {
    "start": 2368958,
    "end": 2373250,
    "text": "次に、クエリとキーの積を計算する。"
  },
  {
    "start": 2373330,
    "end": 2377586,
    "text": "その結果にvを掛けると、2つのトークンからなる出力が得られる。"
  },
  {
    "start": 2377618,
    "end": 2381074,
    "text": "入力として2つのトークンがあるので、出力として2つのトークンを生成する。"
  },
  {
    "start": 2381202,
    "end": 2383094,
    "text": "私たちは皆、最後のトークンに興味がある。"
  },
  {
    "start": 2383142,
    "end": 2387542,
    "text": "この出力トークン2を線形層に送り、次にソフトマークに送る。"
  },
  {
    "start": 2387606,
    "end": 2391462,
    "text": "この結果、トークンが語彙の中で何に対応しているかがわかる。"
  },
  {
    "start": 2391526,
    "end": 2397338,
    "text": "このトークンを語彙から取り出し、次のステップでクエリーのキーと値に追加する。"
  },
  {
    "start": 2397434,
    "end": 2403466,
    "text": "この処理をもう一度行い、最後のトークンを出力とする。"
  },
  {
    "start": 2403498,
    "end": 2414318,
    "text": "リニア・レイヤー、ソフト・マーク、どのトークンに対応するかを送信し、クエリー・キーと値に追加して、再び自己アテンションを計算する。"
  },
  {
    "start": 2414414,
    "end": 2424930,
    "text": "というのも、まずこの行列は、クエリーの結果にキーの転置率を掛けたものだからだ。"
  },
  {
    "start": 2425010,
    "end": 2429958,
    "text": "各ステップでは、前のステップですでに計算されたドット積がたくさんある。"
  },
  {
    "start": 2430044,
    "end": 2434378,
    "text": "ステップ4の時点で、これらのドット積を計算していることをお見せしよう。"
  },
  {
    "start": 2434464,
    "end": 2443118,
    "text": "ステップ3の時点ですでにドット積を計算しており、ステップ4で再びドット積を計算している。"
  },
  {
    "start": 2443204,
    "end": 2445360,
    "text": "ご覧の通り、このドット製品はここにある。"
  },
  {
    "start": 2446690,
    "end": 2453210,
    "text": "もうひとつは、通常、言語モデルを扱う場合、因果関係のあるマスクを持っているということだ。"
  },
  {
    "start": 2453290,
    "end": 2469650,
    "text": "というのも、最初のトークンが2番目のトークン、3番目のトークン、4番目のトークンを見ることは避けたいからだ。"
  },
  {
    "start": 2469720,
    "end": 2472646,
    "text": "トークン4番には、前のトークンを見てもらいたい。"
  },
  {
    "start": 2472668,
    "end": 2478790,
    "text": "トークン番号4は、それ自身、前のトークン番号1、トークン番号2、トークン番号1に関連していなければならない。"
  },
  {
    "start": 2479530,
    "end": 2484682,
    "text": "また、私たちは最後のトークンにしか興味がないので、これらすべての出力トークンを生成したくない。"
  },
  {
    "start": 2484736,
    "end": 2500094,
    "text": "我々は、アテンションによって生成された最後のトークンが何であるかを知ることだけに興味がある。そうすれば、それを線形層に送り、次にソフトマックスに送って、我々の語彙の中で対応する単語が何であるかを理解し、それを次のトークンを推論するプロンプトに使うことができる。"
  },
  {
    "start": 2500212,
    "end": 2504720,
    "text": "では、Kvcacheと、Kvcacheがこの問題をどのように解決したかを紹介しよう。"
  },
  {
    "start": 2505490,
    "end": 2509982,
    "text": "Kvcacheで何をするかというと、最初のステップの推論から始める。"
  },
  {
    "start": 2510046,
    "end": 2515950,
    "text": "文頭トークンから開始し、それを掛け合わせるので、クエリは文頭トークンのみになる。"
  },
  {
    "start": 2516030,
    "end": 2517966,
    "text": "それにキーの移調を掛ける。"
  },
  {
    "start": 2517998,
    "end": 2520118,
    "text": "これで1対1のマトリックスが出来上がる。"
  },
  {
    "start": 2520204,
    "end": 2524482,
    "text": "そして、これにDiviを掛けると、最初のトークンが出力される。"
  },
  {
    "start": 2524546,
    "end": 2526946,
    "text": "それをリニアレイヤーに送り、次にソフトマークに送る。"
  },
  {
    "start": 2526978,
    "end": 2529190,
    "text": "そうすれば、どのトークンに対応するかがわかる。"
  },
  {
    "start": 2529340,
    "end": 2543110,
    "text": "KVキャッシュでは、出力として生成したこの新しいトークンをクエリのキーと値に追加するのではなく、キーと値に追加するだけで、前のクエリをこの新しいトークンで完全に置き換える。"
  },
  {
    "start": 2543270,
    "end": 2553034,
    "text": "kvcacheを使用しない以前は、すべての出力トークンをクエリのキーと値に追加していました。"
  },
  {
    "start": 2553082,
    "end": 2564322,
    "text": "kvキャッシュでは、クエリーのキーと値に追加するのではなく、キーと値のみに追加し、最後の出力トークンのみを次のステップのクエリーとして使用します。"
  },
  {
    "start": 2564456,
    "end": 2577874,
    "text": "これが最初のステップの出力であれば、文頭のトークンに対応する出力となり、これを次のステップのクエリとして使用するが、キーと値に追加する。"
  },
  {
    "start": 2577922,
    "end": 2590330,
    "text": "これはKvキャッシュと呼ばれる所以で、各ステップで前のkとvのキャッシュを保持しているが、クエリについては保持していないからである。"
  },
  {
    "start": 2591230,
    "end": 2593658,
    "text": "とにかく、これで製品ができる。"
  },
  {
    "start": 2593744,
    "end": 2597950,
    "text": "この行列とこの行列を掛け合わせると、1×2の行列ができる。"
  },
  {
    "start": 2598020,
    "end": 2602794,
    "text": "これにvを掛けると、出力としてトークンが1つだけ生成されることがわかる。"
  },
  {
    "start": 2602922,
    "end": 2606422,
    "text": "そしてこのトークンをリニアレイヤー、ソフトマッチに送る。"
  },
  {
    "start": 2606426,
    "end": 2608338,
    "text": "そうすれば、どのトークンに対応するかがわかる。"
  },
  {
    "start": 2608424,
    "end": 2615438,
    "text": "そして、次の反復のためのクエリとして使用するが、k行列とv行列のみに追加する。"
  },
  {
    "start": 2615614,
    "end": 2624242,
    "text": "これにより1×3の行列が生成され、これにvが乗算されることで、この出力トークンが生成される。"
  },
  {
    "start": 2624306,
    "end": 2626280,
    "text": "基本的に興味があるのはこれだ。"
  },
  {
    "start": 2626650,
    "end": 2633526,
    "text": "そして、それを次の反復のクエリーとして使用するが、kとvに追加する。"
  },
  {
    "start": 2633558,
    "end": 2642518,
    "text": "おわかりのように、推論の4番目のステップとして、kvキャッシュがなかったときに興味を持っていた最後の行だけを生成している。"
  },
  {
    "start": 2642534,
    "end": 2643390,
    "text": "お見せしましょう。"
  },
  {
    "start": 2643460,
    "end": 2647322,
    "text": "これはkvキャッシュを使った4回目のタイムステップである。"
  },
  {
    "start": 2647386,
    "end": 2650590,
    "text": "Kvcacheなしで4番目のタイムステップを見てみよう。"
  },
  {
    "start": 2651810,
    "end": 2655102,
    "text": "ご覧の通り、ここではこの行だけを生産している。"
  },
  {
    "start": 2655156,
    "end": 2658990,
    "text": "この最後のトークンを作るために、私たちが興味を持っているのはこれだけだ。"
  },
  {
    "start": 2659070,
    "end": 2673670,
    "text": "Kvcacheを使用することで、基本的に各ステップで行っている計算の回数を減らすことができます。なぜなら、ドット積のいくつかは前のステップですでに行っており、出力として生成されるトークンは1つだけだからです。"
  },
  {
    "start": 2675450,
    "end": 2678466,
    "text": "さて、次はローリング・バッファ・キャッシュについて話そう。"
  },
  {
    "start": 2678498,
    "end": 2703146,
    "text": "スライディング・ウインドウのアテンションをwのサイズで使っていて、前にお見せした例では3つのサイズのスライディング・ウインドウを使っていたので、可能性のあるkとvをすべてキャッシュに入れておく必要はないが、いずれにせよこのwのウインドウの外ではアテンションを計算しないので、kとvをwのトークンのみに制限することができる。"
  },
  {
    "start": 2703178,
    "end": 2706250,
    "text": "ウィンドウが10トークンであることを想像する必要はない。"
  },
  {
    "start": 2706330,
    "end": 2712558,
    "text": "いずれにせよ、私たちの注意は前の10トークンのみで計算されるため、前の1000トークンは保持しない。"
  },
  {
    "start": 2712654,
    "end": 2714878,
    "text": "これがローリング・バッファ・キャッシュの考え方だ。"
  },
  {
    "start": 2714894,
    "end": 2716100,
    "text": "どう動くか見てみよう。"
  },
  {
    "start": 2717350,
    "end": 2721990,
    "text": "kvキャッシュを使って推論のトークン8にたどり着いたとしよう。"
  },
  {
    "start": 2722410,
    "end": 2738490,
    "text": "例えば、kvキャッシュがあり、スライディング・ウィンドウのサイズを4とすると、クエリとして前のステップの出力を使用し、キーと値として8つのトークンで構成されるキャッシュ全体を使用することがわかる。"
  },
  {
    "start": 2739070,
    "end": 2754478,
    "text": "というのも、スライディングウィンドウアテンションで使っているマスクは、このトークンとこのトークンの間の距離がスライディングウィンドウアテンションの外側にあるため、いずれにせよマスクされてしまうからである。"
  },
  {
    "start": 2754574,
    "end": 2761634,
    "text": "というのも、これらのドット積はマスクされてしまうからだ。"
  },
  {
    "start": 2761752,
    "end": 2785238,
    "text": "というのも、これらの値はスライディング・ウィンドウのマスクによってマスクされ、基本的にはゼロになるからだ。"
  },
  {
    "start": 2785334,
    "end": 2802846,
    "text": "マスクがこの10、この10、この10、この10、この井戸を点積、この井戸を点積、この井戸を点積、この井戸を点積、この井戸を点積、この井戸を点積、この井戸を点積、この井戸を点積、この井戸を点積、この井戸を点積、この井戸を点積、この井戸を点積、この井戸を点積、この井戸を点積、この井戸を点積、この井戸を点積とするようにマスクが引き受けてくれることを想像してほしい。"
  },
  {
    "start": 2802948,
    "end": 2812578,
    "text": "ここにどんな値があろうと、ここにどんな値があろうと、ここにどんな値があろうと、ここにどんな値があろうと、このトークンの出力に寄与することはない。"
  },
  {
    "start": 2812664,
    "end": 2821906,
    "text": "というのも、いずれにせよ、これらの値はスライディングウィンドウアテンションでは使用されないからである。"
  },
  {
    "start": 2822018,
    "end": 2831820,
    "text": "だから、kとvのキャッシュのサイズをwトークンのみに制限することができるのだ。"
  },
  {
    "start": 2832750,
    "end": 2835734,
    "text": "では、このローリング・バッファ・キャッシュがどのように実装されたかを見てみよう。"
  },
  {
    "start": 2835782,
    "end": 2844062,
    "text": "基本的に、ローリング・バッファ・キャッシュとは、キャッシュのサイズを限られたサイズ（この場合はw）に制限する方法である。"
  },
  {
    "start": 2844116,
    "end": 2846126,
    "text": "私たちのWがまだ4歳であることを想像してみてほしい。"
  },
  {
    "start": 2846228,
    "end": 2853082,
    "text": "猫が椅子の上にいて、それをKVキャッシュに使いたいという文章があるとする。"
  },
  {
    "start": 2853146,
    "end": 2860670,
    "text": "最初の推論では、KVキャッシュを使って、最初のトークンをKVキャッシュに追加する。"
  },
  {
    "start": 2860750,
    "end": 2863842,
    "text": "そして2つ目、3つ目、4つ目を加える。"
  },
  {
    "start": 2863896,
    "end": 2867038,
    "text": "kvのキャッシュがいっぱいになってしまった。"
  },
  {
    "start": 2867134,
    "end": 2877334,
    "text": "基本的には、最後に追加したアイテムがどこにあるのか、いつ次のトークン（トークンa）に到達するのかを、ポインターを用いて追跡している。"
  },
  {
    "start": 2877452,
    "end": 2884054,
    "text": "基本的には、ここで一番古い値を最初から置き換え、右ポインタの値を更新する。"
  },
  {
    "start": 2884182,
    "end": 2886826,
    "text": "さて、どうやって戻ろうか？"
  },
  {
    "start": 2886928,
    "end": 2896218,
    "text": "というのも、トークンの順番が文と一致していないからだ。キャッシュにはcat is onが含まれているが、これは元の文の順番とは違う。"
  },
  {
    "start": 2896314,
    "end": 2900366,
    "text": "原文では、cat is on a.とすべきである。"
  },
  {
    "start": 2900468,
    "end": 2903994,
    "text": "私たちがやっているのは、巻き戻しや回転の解除だ。"
  },
  {
    "start": 2904122,
    "end": 2906126,
    "text": "どうやるのか？"
  },
  {
    "start": 2906148,
    "end": 2916450,
    "text": "基本的には、この右ポインターを追跡しているので、右ポインター以降のすべての値を取り、ゼロから右ポインターそのものまでの値を入れるだけでいい。"
  },
  {
    "start": 2916600,
    "end": 2920834,
    "text": "右ポインターの後のすべての値、そして右ポインターの前のすべての値。"
  },
  {
    "start": 2920882,
    "end": 2922482,
    "text": "これが回転を解除する方法だ。"
  },
  {
    "start": 2922546,
    "end": 2930802,
    "text": "この操作は、コード内のunrotateと呼ばれる関数で行われる。"
  },
  {
    "start": 2930866,
    "end": 2934534,
    "text": "キャッシュが満杯でなければ、未充填の項目を無視すればよい。"
  },
  {
    "start": 2934582,
    "end": 2940658,
    "text": "キャッシュがこの状態であれば、ゼロから右ポインタまでのすべての値を取る。"
  },
  {
    "start": 2940774,
    "end": 2948714,
    "text": "キャッシュが一杯なら、ゼロから右ポインタの値までの値を取る。"
  },
  {
    "start": 2948842,
    "end": 2955354,
    "text": "右ポインタの値がすでにある値を上書きしている場合は、回転を解除する必要がある。"
  },
  {
    "start": 2955402,
    "end": 2958194,
    "text": "これは、ここでの3つ目の条件で行われている。"
  },
  {
    "start": 2958232,
    "end": 2962766,
    "text": "ポインタ以降のすべての値を取り、ポインタまでの値を取る。"
  },
  {
    "start": 2962798,
    "end": 2966530,
    "text": "これが、このバッファキャッシュをアンローテートする方法だ。"
  },
  {
    "start": 2967110,
    "end": 2972274,
    "text": "チャンキングとプリフィーディングだ。"
  },
  {
    "start": 2972402,
    "end": 2979314,
    "text": "基本的に、言語モデルを使用してテキストを生成するときは、プロンプトを使用し、このプロンプトを使用して将来のトークンを生成します。"
  },
  {
    "start": 2979442,
    "end": 2982798,
    "text": "KVキャッシュを扱う場合、このKVキャッシュを構築する必要がある。"
  },
  {
    "start": 2982834,
    "end": 2990470,
    "text": "私たちは、プロンプトのトークンをkvキャッシュに追加する必要があり、そうすることで、このkvキャッシュを利用して新しいトークンを構築することができる。"
  },
  {
    "start": 2990550,
    "end": 2991770,
    "text": "将来のトークン"
  },
  {
    "start": 2992430,
    "end": 2995226,
    "text": "今、プロンプトは事前に分かっていますよね？"
  },
  {
    "start": 2995328,
    "end": 2997146,
    "text": "なぜなら、それはユーザーの入力だからだ。"
  },
  {
    "start": 2997178,
    "end": 2998506,
    "text": "チャットに何を求めるかだ。"
  },
  {
    "start": 2998538,
    "end": 2999662,
    "text": "例えばGBDとかね。"
  },
  {
    "start": 2999716,
    "end": 3002474,
    "text": "詩を教えてくれ、教えてくれ、詩を書いてくれ、冗談を言ってくれ。"
  },
  {
    "start": 3002522,
    "end": 3007554,
    "text": "これは僕らのプロンプトだから、あらかじめわかっていることなんだ。"
  },
  {
    "start": 3007672,
    "end": 3012558,
    "text": "私たちにできることは、プロンプトのトークンを使ってkvキャッシュをプリフィルすることです。"
  },
  {
    "start": 3012654,
    "end": 3017838,
    "text": "以前KVcacheについてお教えしたときのように、いろいろな方法があります。"
  },
  {
    "start": 3017854,
    "end": 3019462,
    "text": "私たちは一度に1つのトークンを扱う。"
  },
  {
    "start": 3019516,
    "end": 3025446,
    "text": "KVキャッシュにトークンを追加する一つの方法は、一度に一つのトークンを追加することである。"
  },
  {
    "start": 3025548,
    "end": 3036054,
    "text": "というのも、リトリーバル・オーグメンテッド・ジェネレーションでは、5000 6000トークンとか、もっと大きなプロンプトがある。"
  },
  {
    "start": 3036182,
    "end": 3049054,
    "text": "一度にトークンを1つずつ追加すると、ネットワーク内で5000、6000の前進ステップを踏まなければならないことになり、非常に時間がかかるだけでなく、GPUをあまり活用できない。"
  },
  {
    "start": 3049252,
    "end": 3053966,
    "text": "もう1つの方法は、これらのトークンをすべて受け取り、一度にすべてモデルに与えることだ。"
  },
  {
    "start": 3054068,
    "end": 3056926,
    "text": "それはGPUのサイズによって制限されるかもしれない。"
  },
  {
    "start": 3056958,
    "end": 3060046,
    "text": "というのも、10,000のトークンがあるとしよう。"
  },
  {
    "start": 3060158,
    "end": 3063006,
    "text": "そうなると、GPUは1万トークンを保持することさえできないかもしれない。"
  },
  {
    "start": 3063038,
    "end": 3071570,
    "text": "もしかしたら、4000トークンか2000トークンしか保持できないかもしれない。それは、私たちが選んだアテンション・スライディング・ウィンドウのアテンションのWサイズにもよる。"
  },
  {
    "start": 3071730,
    "end": 3074290,
    "text": "この場合の解決策は、チャンキングを使うことだ。"
  },
  {
    "start": 3074370,
    "end": 3083580,
    "text": "基本的に、私たちはプロンプトを固定サイズのチャンクに分割し、このサイズはスライディングウィンドウのアテンションサイズであるwに等しい。"
  },
  {
    "start": 3084190,
    "end": 3090118,
    "text": "非常に大きなプロンプトがあり、注目度の計算に4つのスライディングウィンドウサイズを選んだとする。"
  },
  {
    "start": 3090214,
    "end": 3091914,
    "text": "このプロンプトを想像してみてほしい。"
  },
  {
    "start": 3091952,
    "end": 3097566,
    "text": "歴史上最も裕福な人物を教えてください。"
  },
  {
    "start": 3097668,
    "end": 3099534,
    "text": "私たちの仕事のやり方はこうだ。"
  },
  {
    "start": 3099732,
    "end": 3103418,
    "text": "基本的には、プロンプトの最初のチャンクを取る。"
  },
  {
    "start": 3103514,
    "end": 3108578,
    "text": "スライディングウィンドウサイズを4としたので、チャンクサイズも4とする。"
  },
  {
    "start": 3108664,
    "end": 3111342,
    "text": "プロンプトの最初のトークンを取る。"
  },
  {
    "start": 3111406,
    "end": 3113650,
    "text": "教えてくれる？"
  },
  {
    "start": 3113800,
    "end": 3120150,
    "text": "の自己注意を計算する。"
  },
  {
    "start": 3120300,
    "end": 3122950,
    "text": "どうやってアテンションマスクを作るのか？"
  },
  {
    "start": 3123290,
    "end": 3134598,
    "text": "基本的にクエリとして、このチャンクに入力されるトークンをすべて取り込むので、このカラムをクエリ、このカラムをキーと考えることができる。"
  },
  {
    "start": 3134694,
    "end": 3139610,
    "text": "これは、クエリーの結果にキーの移調とマスクをかけたものである。"
  },
  {
    "start": 3140270,
    "end": 3144394,
    "text": "クエリでは、最初のチャンクとチャンクをキーとする。"
  },
  {
    "start": 3144442,
    "end": 3145406,
    "text": "後でお見せします。"
  },
  {
    "start": 3145508,
    "end": 3153710,
    "text": "KVキャッシュの現在の内容を取得するが、最初は空であり、さらに現在のチャンクの受信トークンを取得する。"
  },
  {
    "start": 3154050,
    "end": 3160386,
    "text": "これは、次のステップでお見せする非常に特別な理由から作られている。"
  },
  {
    "start": 3160488,
    "end": 3174610,
    "text": "次のステップでは、基本的に、トークンが最も豊富な現在のチャンクを取り出し、前のチャンクのトークンを使ってKVキャッシュのコンテンツと集約する。"
  },
  {
    "start": 3174690,
    "end": 3179154,
    "text": "このプレフィリングの最初のステップに戻ろう。"
  },
  {
    "start": 3179202,
    "end": 3182026,
    "text": "プロンプトの最初のチャンクを取るので、教えてもらえますか？"
  },
  {
    "start": 3182048,
    "end": 3194234,
    "text": "アテンション・マスクは、sクエリ、最初の4つのトークン、およびキーとして、空であるkvキャッシュの内容と最初のチャンクのトークンを加えた値を使って計算する。"
  },
  {
    "start": 3194282,
    "end": 3202026,
    "text": "注目度を計算した後、このチャンクのトークンを使ってkvキャッシュの内容を更新する。"
  },
  {
    "start": 3202138,
    "end": 3207934,
    "text": "次のステップで、kvcacheは前のチャンクのトークンを含む。"
  },
  {
    "start": 3207982,
    "end": 3209378,
    "text": "教えてくれる？"
  },
  {
    "start": 3209464,
    "end": 3212734,
    "text": "今のチャンクは、誰が一番リッチか、ということになっている。"
  },
  {
    "start": 3212862,
    "end": 3225474,
    "text": "クエリとしては、やはり現在のチャンクのトークンを取るが、キーと値としてはKVキャッシュの内容と現在のチャンクのトークンを取る。"
  },
  {
    "start": 3225602,
    "end": 3226280,
    "text": "なぜですか？"
  },
  {
    "start": 3226890,
    "end": 3240838,
    "text": "というのも、トークン生成をしているとき、KVキャッシュをお教えしたときに、最後に出力されたトークンをまず追加し、それをKとVに追加して、次の反復のクエリとして使用するのです。"
  },
  {
    "start": 3240934,
    "end": 3242398,
    "text": "これはここでやることではない。"
  },
  {
    "start": 3242484,
    "end": 3247258,
    "text": "ここではまず注目度を計算し、次にKVキャッシュを更新する。"
  },
  {
    "start": 3247354,
    "end": 3262878,
    "text": "クエリーを構築する際には、現在のチャンクのトークンのみを使用し、キーと値としてKVキャッシュの内容を使用する。"
  },
  {
    "start": 3262974,
    "end": 3263620,
    "text": "なぜですか？"
  },
  {
    "start": 3264070,
    "end": 3277234,
    "text": "というのも、もし前のチャンクの内容を使わなかったらどうなるかと想像してみると、現在のチャンクのトークンのみで構成されたアテンション・マスクができることになる。"
  },
  {
    "start": 3277282,
    "end": 3280298,
    "text": "それはこのマトリックスに限られるだろう。"
  },
  {
    "start": 3280464,
    "end": 3281594,
    "text": "描かせてください。"
  },
  {
    "start": 3281632,
    "end": 3285260,
    "text": "このマトリックスだけがここにある。"
  },
  {
    "start": 3287390,
    "end": 3301120,
    "text": "このマトリックスだけを使えば、whoという単語は、たとえスライディング・ウインドウの大きさを使っても、me tellやyouという単語とは関連しない。"
  },
  {
    "start": 3301890,
    "end": 3316594,
    "text": "現在のチャンクと前のチャンクを関連付けたいので、基本的にはKVキャッシュの内容と現在のチャンクのトークンをキーと値として取り、チャンク間の注意を構築できるようにする。"
  },
  {
    "start": 3316722,
    "end": 3319158,
    "text": "そうでなければ、この注目は生まれない。"
  },
  {
    "start": 3319324,
    "end": 3323602,
    "text": "クエリとして、常に現在のチャンクのトークンを使用する。"
  },
  {
    "start": 3323746,
    "end": 3327720,
    "text": "このメカニズムがどのようにコードに組み込まれているのか、おさらいしてみよう。"
  },
  {
    "start": 3328730,
    "end": 3332742,
    "text": "基本的にプレフィリングはチャンク単位で行われる。"
  },
  {
    "start": 3332806,
    "end": 3347550,
    "text": "最初のチャンクがあり、それに続くチャンクがあり、そして最後に、最初のプリフィル中にkvcashにプロンプトをプリフィルした後にトークン生成がある。"
  },
  {
    "start": 3348130,
    "end": 3357182,
    "text": "アテンション・マスクとして、私たちは現在のチャンク内の入力トークンのサイズのみを考慮するが、それ以降のチャンクについては考慮しない。"
  },
  {
    "start": 3357246,
    "end": 3366254,
    "text": "最初のチャンクの後、クエリーのアテンションマスクを構築するために、入力されたチャンクのサイズを使用します。"
  },
  {
    "start": 3366382,
    "end": 3370694,
    "text": "kとVには、このkVキャッシュのサイズを使用する。"
  },
  {
    "start": 3370732,
    "end": 3377590,
    "text": "キャッシュされたsと、現在のチャンクのサイズであるs変数。"
  },
  {
    "start": 3377740,
    "end": 3383550,
    "text": "トークン生成については、以前KVキャッシュを使って教えていたときと同じシステムを使っている。"
  },
  {
    "start": 3383570,
    "end": 3394320,
    "text": "トークンを1つずつ取り出し、キーに追加し、値に追加し、クエリーを前のステップの出力トークンで置き換える。"
  },
  {
    "start": 3394850,
    "end": 3399166,
    "text": "この場合、最後のチャンクは歴史上のトークン・メンとなる。"
  },
  {
    "start": 3399348,
    "end": 3411118,
    "text": "キーは基本的に、前のチャンクと現在のチャンクのトークンを足したものになる。"
  },
  {
    "start": 3411294,
    "end": 3416098,
    "text": "最も裕福なのは誰なのか？"
  },
  {
    "start": 3416184,
    "end": 3423698,
    "text": "そうしないと、現在のチャンクの単語が、前のチャンクの単語と関連づけられないからだ。"
  },
  {
    "start": 3423794,
    "end": 3428818,
    "text": "では、専門家がまばらに混じっていることについて話そう。"
  },
  {
    "start": 3428994,
    "end": 3442074,
    "text": "つまり、専門家の混合は、複数の専門家モデルがあり、各モデルがデータの部分集合に特化するように、各モデルがデータの部分集合で訓練される組み立て技法である。"
  },
  {
    "start": 3442192,
    "end": 3449130,
    "text": "そして、この専門家の混合物の出力を作るとき、それぞれの専門家の出力を取る。"
  },
  {
    "start": 3449210,
    "end": 3455626,
    "text": "私たちは通常、加重合計を使用するか、平均化することによってそれを組み合わせ、1つの出力を生成する。"
  },
  {
    "start": 3455818,
    "end": 3465634,
    "text": "ミストラルの場合、専門家だけの混合という言い方はしないが、専門家のスパース混合という言い方をする。"
  },
  {
    "start": 3465752,
    "end": 3466706,
    "text": "お見せしましょう。"
  },
  {
    "start": 3466808,
    "end": 3472226,
    "text": "ミストラルの場合、フィードフォワード層として8人のエキスパートが存在する。"
  },
  {
    "start": 3472258,
    "end": 3476914,
    "text": "自己注意を計算した後、覚えているように、このフィード・フォワード・ネットワークがある。"
  },
  {
    "start": 3477042,
    "end": 3482022,
    "text": "ミストラルの場合、8×7bで8つのフィードフォワード層がある。"
  },
  {
    "start": 3482086,
    "end": 3484090,
    "text": "並行して考えなければならない。"
  },
  {
    "start": 3484990,
    "end": 3490614,
    "text": "ゲートは基本的に、各トークンに対してどのエキスパートを選ぶかを決定する関数である。"
  },
  {
    "start": 3490662,
    "end": 3495242,
    "text": "どのフィード・フォワード・ネットワークがそのトークンで働くべきか。"
  },
  {
    "start": 3495306,
    "end": 3499146,
    "text": "各トークンに対して2つのフィード・フォワード・ネットワークを選択する。"
  },
  {
    "start": 3499258,
    "end": 3500734,
    "text": "トークンを通過させる。"
  },
  {
    "start": 3500772,
    "end": 3515010,
    "text": "これらのフィード・フォワード・ネットワークは出力を受け取り、ゲートが生成するロジックに従って重み付けを行い、その特定のトークンに対する自己注目の出力となる重み付き和を生成する。"
  },
  {
    "start": 3515910,
    "end": 3517378,
    "text": "例を挙げよう。"
  },
  {
    "start": 3517464,
    "end": 3520002,
    "text": "これがミストラルの建築だ。"
  },
  {
    "start": 3520066,
    "end": 3524178,
    "text": "ご覧のように、このエンコーダー・レイヤーの入力がある。"
  },
  {
    "start": 3524274,
    "end": 3529302,
    "text": "まず、スライディングウィンドウ・アテンションとKVキャッシュを使った自己アテンションを実行する。"
  },
  {
    "start": 3529446,
    "end": 3531430,
    "text": "そして正規化を実行する。"
  },
  {
    "start": 3531510,
    "end": 3548250,
    "text": "このゲート関数は、基本的に単なる線形レイヤーで、エキスパート、つまり最も優れた2人のエキスパートの値、スコア値と呼ぶことにする。"
  },
  {
    "start": 3548330,
    "end": 3553502,
    "text": "最も高い2つのスコアは、トークンがどの専門家と仕事をすべきかを示す。"
  },
  {
    "start": 3553636,
    "end": 3558846,
    "text": "そして、それぞれのトークンを2つの最高のパフォーマンスを持つエキスパートで実行する。"
  },
  {
    "start": 3558958,
    "end": 3561134,
    "text": "そして、この2人のエキスパートの出力を取る。"
  },
  {
    "start": 3561182,
    "end": 3564798,
    "text": "重量は何キロですか？"
  },
  {
    "start": 3564894,
    "end": 3570262,
    "text": "基本的にゲートが生成するロジックは、ここに8つの値があるとする。"
  },
  {
    "start": 3570316,
    "end": 3572694,
    "text": "ああ、スペースがないから4つしか描かないんだ。"
  },
  {
    "start": 3572732,
    "end": 3577158,
    "text": "8つの値があると仮定して、上から2つを選ぶ。"
  },
  {
    "start": 3577244,
    "end": 3579254,
    "text": "1.5と3.4。"
  },
  {
    "start": 3579292,
    "end": 3582630,
    "text": "この場合、この2人のエキスパートにトークンを渡すことになる。"
  },
  {
    "start": 3582790,
    "end": 3586550,
    "text": "2つの最も良い値のソフトマックスを取る。"
  },
  {
    "start": 3586630,
    "end": 3590490,
    "text": "これが、加重合計に使用するウェイトとなる。"
  },
  {
    "start": 3591650,
    "end": 3595040,
    "text": "基本的に、なぜそれをするのか？"
  },
  {
    "start": 3597730,
    "end": 3599246,
    "text": "なぜそうするのか？"
  },
  {
    "start": 3599428,
    "end": 3609342,
    "text": "なぜなら、エキスパートのスパースミスタを使用することで、多くのエキスパートモデルを持つことができるが、推論時には8人のうち2人しかアクティブにならないからである。"
  },
  {
    "start": 3609486,
    "end": 3615006,
    "text": "覚えているように、ウォールネットワークのフィードは基本的に2つのリニアレイヤーである。"
  },
  {
    "start": 3615118,
    "end": 3620466,
    "text": "線形層は、重み行列と入力の行列乗算と考えることができる。"
  },
  {
    "start": 3620578,
    "end": 3629590,
    "text": "もしエキスパートのスパース混合を使わなければ、トークンを8人のエキスパートすべてに通すことになり、8回の行列乗算が必要になる。"
  },
  {
    "start": 3630410,
    "end": 3648138,
    "text": "各トークンに対してエキスパートのスパース混合を使用することで、行列の乗算を2回行うだけとなり、推論が高速化すると同時に、トークンのサブセットに対してのみいくつかのパラメータを使用するため、モデルのパワーとモデルのパラメータを増加させることができます。"
  },
  {
    "start": 3648154,
    "end": 3653278,
    "text": "あるトークンは1番のエキスパートを使い、あるトークンは2番と3番のエキスパートを使う。"
  },
  {
    "start": 3653364,
    "end": 3660830,
    "text": "いくつかのトークンは、専門家の番号8と3、または他の、例えば6と4、エトセトラ、エトセトラを使用します。"
  },
  {
    "start": 3660910,
    "end": 3665186,
    "text": "各トークンのエキスパートをすべて使うのではなく、そのうちの2つだけを使う。"
  },
  {
    "start": 3665288,
    "end": 3672306,
    "text": "これにより、各エキスパートがトークンのサブセットに特化することができる。"
  },
  {
    "start": 3672338,
    "end": 3675394,
    "text": "例えば、モデルが複数の言語で学習されたとしよう。"
  },
  {
    "start": 3675442,
    "end": 3695550,
    "text": "基本的には、日本語のトークンに特化したフィード・フォワード・ネットワークもあれば、英語のトークンに特化したフィード・フォワード・ネットワークもあるし、動詞に特化したものもあれば、名詞に特化したものもあれば、形容詞に特化したものもある。"
  },
  {
    "start": 3696050,
    "end": 3701966,
    "text": "専門家の混合を使うのは、モデルのパラメーターのサイズを大きくしたいからである。"
  },
  {
    "start": 3702068,
    "end": 3705730,
    "text": "このモデルは、より強力に情報を捉えることができるようになる。"
  },
  {
    "start": 3705880,
    "end": 3713730,
    "text": "同時に、各トークンに対してエキスパートのサブセットのみを使用するため、パフォーマンスを犠牲にすることはない。"
  },
  {
    "start": 3714150,
    "end": 3717250,
    "text": "これはコードで実行されている実装である。"
  },
  {
    "start": 3717320,
    "end": 3725430,
    "text": "ご覧のように、ミストラル7 Bの場合、フィードフォワードのネブラルネットワークがあり、これは2つのリニアレイヤーである。"
  },
  {
    "start": 3726090,
    "end": 3727814,
    "text": "ミストラル8の場合は7。"
  },
  {
    "start": 3727852,
    "end": 3732118,
    "text": "Bは、1つのフィード・フォワード・ネットワークだけでなく、8つのフィード・フォワード・ネットワークがある。"
  },
  {
    "start": 3732134,
    "end": 3745310,
    "text": "ご覧のように、これは8つのフィード・フォワード・ネットワークとゲート関数の配列で、エンベッディングのサイズをエキスパートの数である8つに変換する線形レイヤーに過ぎない。"
  },
  {
    "start": 3745730,
    "end": 3747754,
    "text": "各埋め込みに対して生成される。"
  },
  {
    "start": 3747802,
    "end": 3756626,
    "text": "各トークンに対して、このトークンがどのエキスパートを通過すべきかを示すロジットを生成する。"
  },
  {
    "start": 3756728,
    "end": 3760206,
    "text": "この2人を経由して、トップ2人の専門家に送られる。"
  },
  {
    "start": 3760318,
    "end": 3763970,
    "text": "ロジックスコアがトップだった2人のエキスパート。"
  },
  {
    "start": 3765350,
    "end": 3770242,
    "text": "では、なぜトップkの専門家を選んだ後にソフトマックスを適用するのか。"
  },
  {
    "start": 3770306,
    "end": 3776642,
    "text": "ここでお見せするように、ロジットを生成するget関数がある。"
  },
  {
    "start": 3776706,
    "end": 3795898,
    "text": "トークンにどの専門家を通すべきかを理解するために、上位2つの論理を選択します。次に、最も優れた成績を収めた2人の専門家のスコアを取り、それらのソフトマークを取って、重み付き合計を作成するために使用する重みを作成します。"
  },
  {
    "start": 3795994,
    "end": 3801598,
    "text": "なぜ、全員のソフト・マークを取るのではなく、成績優秀者2人のソフト・マークを取るのか？"
  },
  {
    "start": 3801764,
    "end": 3817314,
    "text": "さて、最初の問題は、すべてのロジットのソフト・マークを取った場合、2つのベスト・パフォーマンスの合計が1にならない可能性があるということです。これは、複数のモデルを訓練して比較する場合に必要な条件です。"
  },
  {
    "start": 3817352,
    "end": 3820502,
    "text": "というのも、ミストラルの連中が1つのモデルだけをトレーニングしたわけではないことは確かだからだ。"
  },
  {
    "start": 3820556,
    "end": 3823922,
    "text": "たぶん、複数のハイパーパラメーターで複数のモデルをトレーニングしたのだろう。"
  },
  {
    "start": 3823986,
    "end": 3830554,
    "text": "もしかしたら、4人の専門家だけでなく、3人の専門家や2人の専門家も混ぜて試してみて、その中からベストなものを選ぶのかもしれない。"
  },
  {
    "start": 3830672,
    "end": 3838442,
    "text": "つまり、モデルを比較したいのであれば、加重和が常に1だけとなるようにしたいのである。"
  },
  {
    "start": 3838496,
    "end": 3841818,
    "text": "そうでなければ、モデルによって出力範囲が変わる可能性がある。"
  },
  {
    "start": 3841904,
    "end": 3847774,
    "text": "通常、出力の範囲がモデルごとに変わるのは良い考えではない。"
  },
  {
    "start": 3847892,
    "end": 3860290,
    "text": "そこで、出力の幅を安定させるために、何人のエキスパートを使うかを選択し、最も成績の良い2人のエキスパートのロジックを選択した後に、ソフトマークを適用する。"
  },
  {
    "start": 3862790,
    "end": 3870758,
    "text": "さて、次にお話しするのは、ミストラル・モデルのコードにも実装されているモデル・シャーディングについてです。"
  },
  {
    "start": 3870844,
    "end": 3872680,
    "text": "では、それについて話そう。"
  },
  {
    "start": 3873050,
    "end": 3883206,
    "text": "単一のGPUに収まらないほど大きなモデルがある場合、モデルをレイヤーのグループに分割し、レイヤーの各グループを単一のGPUに配置することができる。"
  },
  {
    "start": 3883318,
    "end": 3887542,
    "text": "例えば、ミンストレルの場合、32層のエンコーダーがある。"
  },
  {
    "start": 3887606,
    "end": 3889306,
    "text": "次から次へと出てくる。"
  },
  {
    "start": 3889408,
    "end": 3891530,
    "text": "32人全員を描いたわけではない。"
  },
  {
    "start": 3891600,
    "end": 3894110,
    "text": "これは1層から8層まであると思うんだ。"
  },
  {
    "start": 3894180,
    "end": 3898798,
    "text": "これは9歳から16歳まで、17歳から24歳まで、25歳から32歳までである。"
  },
  {
    "start": 3898884,
    "end": 3902026,
    "text": "レイヤーの各グループを別のGPUに入れた。"
  },
  {
    "start": 3902058,
    "end": 3903630,
    "text": "我々は4つのGPUを持っている。"
  },
  {
    "start": 3905330,
    "end": 3908590,
    "text": "このようなモデルを推論する方法は以下の通りである。"
  },
  {
    "start": 3908670,
    "end": 3913454,
    "text": "入力があったら、それをエンベッディングに変換し、最初の8層に通す。"
  },
  {
    "start": 3913502,
    "end": 3919522,
    "text": "最初のGPUでは、第8層の出力となる出力が生成される。"
  },
  {
    "start": 3919666,
    "end": 3925138,
    "text": "この出力を2番目のGPUに転送し、第9層の入力として使用する。"
  },
  {
    "start": 3925234,
    "end": 3933206,
    "text": "次に、この入力をすべてのレイヤーに次々と通し、レイヤー番号16に到達させ、出力を生成する。"
  },
  {
    "start": 3933238,
    "end": 3939862,
    "text": "この出力を次のGPUに移し、レイヤー番号17の入力とする。"
  },
  {
    "start": 3940006,
    "end": 3945354,
    "text": "そして、レイヤー番号24の出力が得られるまで、すべてのレイヤーを繰り返し実行する。"
  },
  {
    "start": 3945402,
    "end": 3946826,
    "text": "それを次のGPUに移す。"
  },
  {
    "start": 3946858,
    "end": 3950030,
    "text": "これをレイヤー番号32まで繰り返し実行する。"
  },
  {
    "start": 3950100,
    "end": 3955380,
    "text": "そして、最後の線形層とソフトマックスを取り、モデルの出力を生成する。"
  },
  {
    "start": 3955750,
    "end": 3963266,
    "text": "しかし、この方法は非常に効率が悪いことにお気づきだろう。"
  },
  {
    "start": 3963448,
    "end": 3968790,
    "text": "ミストラルのコードには実装されていないが、論文では言及されている。"
  },
  {
    "start": 3968860,
    "end": 3972514,
    "text": "これからお話しするのは、パイプラインの並列性についてです。"
  },
  {
    "start": 3972642,
    "end": 3974200,
    "text": "どう動くか見てみよう。"
  },
  {
    "start": 3974970,
    "end": 3976514,
    "text": "このパイプラインの並列性。"
  },
  {
    "start": 3976562,
    "end": 3980466,
    "text": "本稿で紹介したアルゴリズムについて話す。"
  },
  {
    "start": 3980508,
    "end": 3984122,
    "text": "GPipeは基本的に次のように動作する。"
  },
  {
    "start": 3984176,
    "end": 3985882,
    "text": "まず問題を紹介しよう。"
  },
  {
    "start": 3986016,
    "end": 3992754,
    "text": "これは通常、推論するときではなくモデルをトレーニングするときに使われるが、推論にも適用できる。"
  },
  {
    "start": 3992902,
    "end": 3996622,
    "text": "シャード化されたモデルでモデルをトレーニングしたいとしよう。"
  },
  {
    "start": 3996676,
    "end": 4000794,
    "text": "つまり、複数のレイヤーに分割されたモデルだ。"
  },
  {
    "start": 4000842,
    "end": 4003790,
    "text": "レイヤーの各グループは異なるGPU上に存在する。"
  },
  {
    "start": 4004130,
    "end": 4008318,
    "text": "4つのGpusがあり、それぞれがレイヤーのグループを持っているとしよう。"
  },
  {
    "start": 4008414,
    "end": 4010386,
    "text": "このモデルをトレーニングするとしよう。"
  },
  {
    "start": 4010488,
    "end": 4013054,
    "text": "最初のGPUに入力を実行する。"
  },
  {
    "start": 4013102,
    "end": 4015326,
    "text": "最初のGPUまでフォワード・ステップを実行する。"
  },
  {
    "start": 4015358,
    "end": 4018190,
    "text": "この出力を次のGPUに送る。"
  },
  {
    "start": 4018350,
    "end": 4019730,
    "text": "そして、私たちは前進する。"
  },
  {
    "start": 4019800,
    "end": 4023794,
    "text": "そこから出力を取り出し、次のGPU、3番GPUに走らせる。"
  },
  {
    "start": 4023832,
    "end": 4027106,
    "text": "その出力を次のGPU、4番GPUに送る。"
  },
  {
    "start": 4027208,
    "end": 4032006,
    "text": "これでモデルの出力が得られたので、損失を計算し、逆伝播を実行することができる。"
  },
  {
    "start": 4032038,
    "end": 4034934,
    "text": "ラン・プロパゲーションは基本的にその逆だ。"
  },
  {
    "start": 4034982,
    "end": 4037174,
    "text": "最後のGPUから最初のGPUへ。"
  },
  {
    "start": 4037222,
    "end": 4040262,
    "text": "4番目のGPUで逆伝播を実行する。"
  },
  {
    "start": 4040406,
    "end": 4047450,
    "text": "次に、4番目のGPUで勾配を計算し、それを使って3番目のGPUで前の勾配を計算します。"
  },
  {
    "start": 4047530,
    "end": 4051226,
    "text": "そして、これらの勾配を取り、それを使って前の勾配を計算する。"
  },
  {
    "start": 4051338,
    "end": 4055758,
    "text": "そして、これらの勾配をとって、前の勾配を計算する。"
  },
  {
    "start": 4055854,
    "end": 4067670,
    "text": "前方ステップは入力から損失へ、後方ステップは損失から入力へ、そしてすべてのパラメータは計算グラフの離脱ノードとしても知られている。"
  },
  {
    "start": 4068250,
    "end": 4076482,
    "text": "しかし、このケースのように、各ステップで1つのGPUしか使用していないことがわかる。"
  },
  {
    "start": 4076546,
    "end": 4081450,
    "text": "他のすべてのGpusはまったく動いておらず、アイドル状態だ。"
  },
  {
    "start": 4081870,
    "end": 4085126,
    "text": "より良い方法は、パイプライン並列を使うことだ。"
  },
  {
    "start": 4085238,
    "end": 4089974,
    "text": "つまり、前の段階のトレーニングが非常に大きなバッチを使って行われたとしよう。"
  },
  {
    "start": 4090022,
    "end": 4093190,
    "text": "このバッチが8品目で構成されているとする。"
  },
  {
    "start": 4093350,
    "end": 4098906,
    "text": "パイプラインの並列処理では、このバッチをマイクロバッチに分割する。"
  },
  {
    "start": 4099018,
    "end": 4102234,
    "text": "の代わりにマイクロバッチを作る。"
  },
  {
    "start": 4102282,
    "end": 4105220,
    "text": "2アイテムずつの4マイクロバッチ。"
  },
  {
    "start": 4105670,
    "end": 4111102,
    "text": "私たちがすることは、最初のマイクロバッチを最初のGPUで実行することです。"
  },
  {
    "start": 4111166,
    "end": 4116126,
    "text": "これで最初のマイクロバッチの出力が得られ、それを次のGPUに送ることができる。"
  },
  {
    "start": 4116238,
    "end": 4120166,
    "text": "今、ステップ1の時点で、GPUの1つがフリーであることに気づく。"
  },
  {
    "start": 4120268,
    "end": 4123670,
    "text": "彼女はすでに2回目のマイクロバッチに取り掛かっている。"
  },
  {
    "start": 4124330,
    "end": 4127346,
    "text": "一方、2番目のGPUは1番目のマイクロバッチに取り組んでいる。"
  },
  {
    "start": 4127378,
    "end": 4130546,
    "text": "それが終われば、次のGpuに送ることができる。"
  },
  {
    "start": 4130658,
    "end": 4134682,
    "text": "その一方で、2つ目のGPUが空いていることに気づいた。"
  },
  {
    "start": 4134816,
    "end": 4140858,
    "text": "GPU1が終了したら、GPU1の出力をGPU2に転送する。"
  },
  {
    "start": 4140944,
    "end": 4142986,
    "text": "GPUは無料になる。"
  },
  {
    "start": 4143088,
    "end": 4145306,
    "text": "3つ目のマイクロバッチでも機能する。"
  },
  {
    "start": 4145338,
    "end": 4146398,
    "text": "こちらをご覧いただきたい。"
  },
  {
    "start": 4146564,
    "end": 4153370,
    "text": "そして3番目のGPUが終了した後、3番目のGPUの出力を4番目のGPUに送ります。"
  },
  {
    "start": 4153450,
    "end": 4155790,
    "text": "3つ目のGPUがフリーになった。"
  },
  {
    "start": 4155860,
    "end": 4168242,
    "text": "前のGPUが終了していれば、2番目のマイクロバッチを3番目のGPUに、3番目のマイクロバッチを2番目のGPUに転送し、空いた1番目のGPUは新しいマイクロバッチ（4番目のマイクロバッチ）に取り掛かることができる。"
  },
  {
    "start": 4168306,
    "end": 4173430,
    "text": "基本的に、私たちはマイクロバッチをタイムシフトする仕事をしている。"
  },
  {
    "start": 4174170,
    "end": 4177106,
    "text": "その結果、GPUの使用率が向上する。"
  },
  {
    "start": 4177138,
    "end": 4182394,
    "text": "というのも、すべての時間ステップで、たとえばこの時間ステップでは、4つのGpusがすべて動作しているからだ。"
  },
  {
    "start": 4182432,
    "end": 4191382,
    "text": "各マイクロバッチについて勾配を計算するが、パラメータは更新しない。"
  },
  {
    "start": 4191446,
    "end": 4201838,
    "text": "これは基本的に、各マイクロバッチの勾配を計算し、それを既存の勾配と合計し続けることを意味するが、モデルのパラメータは更新しない。"
  },
  {
    "start": 4201924,
    "end": 4207826,
    "text": "すべてのマイクロバッチが前進と後退の処理を終えた後、モデルのパラメータを更新する。"
  },
  {
    "start": 4208008,
    "end": 4213650,
    "text": "勾配累積は、以前の分散トレーニングのビデオで紹介したテクニックだ。"
  },
  {
    "start": 4213720,
    "end": 4222600,
    "text": "もし、この仕組みを理解したいのであれば、私が以前に作成した分散トレーニングに関するビデオを参照してほしい。そこでは、グリッド・データの蓄積とその仕組みの背後にある数学についても説明している。"
  },
  {
    "start": 4223210,
    "end": 4225842,
    "text": "基本的には、これがパイプライン並列による解決策である。"
  },
  {
    "start": 4225906,
    "end": 4229634,
    "text": "バッチをマイクロバッチに分けることができる。"
  },
  {
    "start": 4229682,
    "end": 4236234,
    "text": "これは推論でも同じことが言える。推論をする場合、後戻りはできないからね。"
  },
  {
    "start": 4236272,
    "end": 4249470,
    "text": "このテーブルの後半を削除するだけで、最初の大きなバッチをマイクロバッチに分割し、GPUの稼働状況に応じてタイムシフトさせることができる。"
  },
  {
    "start": 4250370,
    "end": 4258642,
    "text": "このパイプライン並列性は、基本的に、すべてのGPUが動作していない時間ステップを導入する。"
  },
  {
    "start": 4258696,
    "end": 4260366,
    "text": "これをバブルと呼ぶ。"
  },
  {
    "start": 4260478,
    "end": 4270470,
    "text": "この大きな泡を避けるためにできることは、最初のバッチサイズを大きくして、複数のマイクロバッチを作ることだ。"
  },
  {
    "start": 4271610,
    "end": 4275254,
    "text": "さて、みんな、このビデオの最後の部分に行こう。"
  },
  {
    "start": 4275452,
    "end": 4282442,
    "text": "ミストラル・コードはラマ・コードに比べ、理解するのがはるかに複雑であることは承知している。"
  },
  {
    "start": 4282496,
    "end": 4292250,
    "text": "また、コードの中で最も複雑なトピックであるXformersライブラリの理解にも役立つだろう。Xformersライブラリは、推論のパフォーマンスを向上させるために使われているトリックだ。"
  },
  {
    "start": 4292330,
    "end": 4294442,
    "text": "実はとても高度なテクニックなんだ。"
  },
  {
    "start": 4294586,
    "end": 4297840,
    "text": "どのように機能するのか、その一端をお見せしたい。"
  },
  {
    "start": 4298770,
    "end": 4305598,
    "text": "基本的には、あなたがAI会社を経営していて、LLM推論サービスを提供しているとします。"
  },
  {
    "start": 4305684,
    "end": 4316610,
    "text": "例えば、APIを提供している顧客がいて、その顧客がAPIにプロンプトを送信し、大規模な言語モデルを使って推論を実行したいとします。"
  },
  {
    "start": 4316770,
    "end": 4324290,
    "text": "もちろん、それぞれのプロンプトは異なる長さを持つかもしれない。なぜなら、それぞれの顧客が異なる目的で大規模な言語モデルを使用するかもしれないからだ。"
  },
  {
    "start": 4324450,
    "end": 4328162,
    "text": "簡単のため、各単語をトークンと仮定する。"
  },
  {
    "start": 4328226,
    "end": 4329746,
    "text": "3人の顧客がいるとする。"
  },
  {
    "start": 4329868,
    "end": 4332198,
    "text": "最初の客は詩を書けと言う。"
  },
  {
    "start": 4332294,
    "end": 4334950,
    "text": "二人目の客は歴史小説を書けと言う。"
  },
  {
    "start": 4335030,
    "end": 4338090,
    "text": "三人目の客は面白いジョークを言ってくれと言う。"
  },
  {
    "start": 4338830,
    "end": 4348494,
    "text": "もちろん、これらのプロンプトを1つずつ処理することもできるが、それでは他の2人の顧客が最初の顧客の処理を待っていることになり、効率が悪い。"
  },
  {
    "start": 4348532,
    "end": 4350910,
    "text": "多くの顧客を抱えている場合、それは良くない。"
  },
  {
    "start": 4350980,
    "end": 4354954,
    "text": "第二に、GPUのメモリを十分に活用していない可能性がある。"
  },
  {
    "start": 4355002,
    "end": 4357854,
    "text": "一番いいのはバッチ処理だ。"
  },
  {
    "start": 4357902,
    "end": 4364910,
    "text": "あなたはこれらのプロンプトをすべて作成し、1つの大きなバッチを作成しますが、問題はプロンプトの長さが異なることです。"
  },
  {
    "start": 4364990,
    "end": 4372050,
    "text": "最初のプロンプトは3つのトークン、2番目のプロンプトは4つのトークン、3番目のプロンプトは5つのトークンで構成される。"
  },
  {
    "start": 4372210,
    "end": 4375746,
    "text": "ひとつの解決策は、これらのトークンにパディングを追加することだ。"
  },
  {
    "start": 4375858,
    "end": 4385258,
    "text": "基本的には、入力シーケンスにパディングトークンを追加するバッチを作成し、それらがすべて同じサイズになるまで追加する。"
  },
  {
    "start": 4385424,
    "end": 4393950,
    "text": "そして、これらのシーケンスを、例えばllamaやMistralのような大規模な言語モデルにバッチすることができる。"
  },
  {
    "start": 4394770,
    "end": 4413486,
    "text": "前に見たように、n個のトークンの入力シーケンスがあるとき、アテンション・メカニズムはn個のトークンの出力シーケンスを生成し、我々は通常、最後のトークンの埋め込みを取り、それを線形層に送り、次に語彙から次のトークンが何であるかを理解するためにソフトマックスを送る。"
  },
  {
    "start": 4413598,
    "end": 4424018,
    "text": "最初のプロンプトでは、2つのパディングトークンを追加していることがわかるので、最後のトークンに対応するエンベッディングを使用することはできない。"
  },
  {
    "start": 4424194,
    "end": 4435190,
    "text": "私たちがすべきことは、最後の非パディングトークンに対応するエンベッディングを取り、それを線形層に送り、次のトークンが何かを理解するためにソフトマックスに送ることである。"
  },
  {
    "start": 4435270,
    "end": 4440154,
    "text": "2番目のプロンプトの場合、最後のトークンではなく、4番目のトークンを使うべきである。"
  },
  {
    "start": 4440272,
    "end": 4446030,
    "text": "最後のプロンプトでのみ、最後のトークンを使うことができる。"
  },
  {
    "start": 4447650,
    "end": 4453598,
    "text": "さて、ここまでやってきて、実際にどのようにアテンション・マスクを作って走らせるか。"
  },
  {
    "start": 4453684,
    "end": 4461774,
    "text": "私たちは基本的に、各トークンが前のトークンのみを視覚化するような、カウザルであるアテンション・マスクを作成するだけである。"
  },
  {
    "start": 4461822,
    "end": 4466690,
    "text": "各トークンは、以前のトークンとは関連づけられるが、将来のトークンとは関連づけられない。"
  },
  {
    "start": 4467110,
    "end": 4472134,
    "text": "このマスクは、ここにある3つのシナリオすべてでうまく機能する。"
  },
  {
    "start": 4472172,
    "end": 4480354,
    "text": "すべてのプロンプトが同じ長さであるため、プロンプトごとに異なるマスクを使用することができないことは後で説明する。"
  },
  {
    "start": 4480402,
    "end": 4493470,
    "text": "なぜなら、このプロンプトには3×3のマスクを、このプロンプトには4×4のマトリックスマスクを、そして入力シーケンスが5なので、このプロンプトには5×5のマスクを使うことができないからである。"
  },
  {
    "start": 4493540,
    "end": 4503694,
    "text": "5×5のマスクを使わなければならず、因果関係のある5×5のマスクを使わなければならない。"
  },
  {
    "start": 4503732,
    "end": 4516100,
    "text": "例えば、スライディング・ウィンドウのサイズが4だとすると、このトークンを4以上の距離のトークンに見せたくないので、この値もマスクすることができる。"
  },
  {
    "start": 4516550,
    "end": 4525606,
    "text": "ここで問題なのは、特に最初のプロンプトと2番目のプロンプトで、使われることのないドット積をたくさん計算していることである。"
  },
  {
    "start": 4525708,
    "end": 4546746,
    "text": "このマスク、つまり5×5のマスクをこの入力シーケンスに適用すると、次のようなアテンション・マスクが生成される。"
  },
  {
    "start": 4546778,
    "end": 4549230,
    "text": "それはマスクの因果関係のせいだ。"
  },
  {
    "start": 4550050,
    "end": 4556334,
    "text": "最後のプロンプトなどに必要なので、ここでこの値をマスクすることはできない。"
  },
  {
    "start": 4556452,
    "end": 4560798,
    "text": "また、最後のプロンプトに必要なこの値をここでマスクすることもできない。"
  },
  {
    "start": 4560894,
    "end": 4568694,
    "text": "最初のプロンプトと2番目のプロンプトでは、パディングトークンと他のトークンとの間のドット積など、多くのドット積を行っている。"
  },
  {
    "start": 4568812,
    "end": 4586454,
    "text": "というのも、最初のプロンプトではモデルの出力として、3番目のトークンでの出力を2番目のプロンプトに使用し、4番目のトークンでの出力を使用し、最後のトークンのみ、自己注目の出力の最後の出力をチェックすることを思い出してほしいからだ。"
  },
  {
    "start": 4586502,
    "end": 4593578,
    "text": "最初の2つのプロンプトでは、自己アテンションから出力される最後のトークンはパディングトークンに対応するため、チェックさえしない。"
  },
  {
    "start": 4593674,
    "end": 4606210,
    "text": "パディング・トークンが計算に混入し、ドット積を計算することで、使用することもないトークンが出力されるのを避ける方法はありますか？"
  },
  {
    "start": 4606360,
    "end": 4609954,
    "text": "もっといい解決策がある。"
  },
  {
    "start": 4610152,
    "end": 4617898,
    "text": "解決策は、すべてのプロンプトのすべてのトークンを1つの大きなシーケンスに連続して結合することである。"
  },
  {
    "start": 4618014,
    "end": 4623154,
    "text": "各プロンプトの実際のサイズも記録しています。"
  },
  {
    "start": 4623282,
    "end": 4629458,
    "text": "私たちはAI企業を経営しており、このAPIを持っているので、プロンプトが私たちのAPIから来ていることは知っています。"
  },
  {
    "start": 4629554,
    "end": 4638518,
    "text": "最初の顧客は3トークン、2番目の顧客は4トークン、3番目の顧客は5トークンのサイズのプロンプトを持っていることがわかる。"
  },
  {
    "start": 4638614,
    "end": 4642426,
    "text": "例えば、これらのサイズを配列で管理することができる。"
  },
  {
    "start": 4642528,
    "end": 4647770,
    "text": "そして、受け取ったすべてのプロンプトを連結したシーケンスを構築する。"
  },
  {
    "start": 4647930,
    "end": 4655806,
    "text": "このメガシーケンスをLLMモデルにかけると、ミストラルかもしれないしラマかもしれない。"
  },
  {
    "start": 4655998,
    "end": 4664450,
    "text": "前にもお話ししたように、n個の変換器にn個の入力シーケンスを入れると、出力にはn個の出力トークンが生じる。"
  },
  {
    "start": 4665290,
    "end": 4668502,
    "text": "ここに3プラス4がある。"
  },
  {
    "start": 4668556,
    "end": 4672306,
    "text": "7、7＋512トークンを入力とする。"
  },
  {
    "start": 4672338,
    "end": 4674850,
    "text": "12個のトークンが出力される。"
  },
  {
    "start": 4675010,
    "end": 4692630,
    "text": "各プロンプトの次のトークンが何であるかを理解するためには、最初のプロンプトのトークン番号3、2番目のプロンプトのトークン番号7、3番目のプロンプトの最後のトークンに対応するエンベッディングをチェックする必要がある。"
  },
  {
    "start": 4692710,
    "end": 4703050,
    "text": "これらの埋め込みをすべて受け取り、線形層に通し、ソフトマックスを適用して、語彙から次のトークンが何かを理解する。"
  },
  {
    "start": 4703130,
    "end": 4723254,
    "text": "あるシーケンスのトークンは別のプロンプトのトークンにアテンションすべきでないが、同じプロンプトのトークンのみにはアテンションすべきでないような、1つのシーケンスに組み合わされた複数のプロンプトで機能するアテンションマスクをどうやって作ればいいのか、不思議に思うかもしれないね？"
  },
  {
    "start": 4723452,
    "end": 4734038,
    "text": "xformersライブラリーを使えば、ブロック対角因果マスクと呼ばれる方法でそれを行うことができる。"
  },
  {
    "start": 4734134,
    "end": 4736154,
    "text": "その仕組みをお見せしたい。"
  },
  {
    "start": 4736352,
    "end": 4738326,
    "text": "基本的に農家は×。"
  },
  {
    "start": 4738518,
    "end": 4743610,
    "text": "ブロックダイアゴナル因果マスクと呼ばれるこの方法では、このようなマスクができる。"
  },
  {
    "start": 4743760,
    "end": 4754494,
    "text": "基本的にすべてのプロンプトをグループ化し、各トークンが同じグループ内のトークンにしか対応できないようにする。"
  },
  {
    "start": 4754612,
    "end": 4762254,
    "text": "ここでは3つのプロンプトがあるので、たとえばトークン・ポイントは同じプロンプトのトークンにしか出席できない。"
  },
  {
    "start": 4762382,
    "end": 4768998,
    "text": "例えば、トークンの小説はトークンの詩とは関係ないので、ここにマイナス無限大を置く。"
  },
  {
    "start": 4769164,
    "end": 4783526,
    "text": "最後のプロンプトのトークンは、同じプロンプト内の他のトークンにしかアテンションできない。"
  },
  {
    "start": 4783638,
    "end": 4787610,
    "text": "これはXformersライブラリを使って作られた特別なマスクである。"
  },
  {
    "start": 4788510,
    "end": 4791200,
    "text": "コードでその仕組みをお見せしよう。"
  },
  {
    "start": 4791650,
    "end": 4794398,
    "text": "では、実際にどのように機能するかをお見せしましょう。"
  },
  {
    "start": 4794484,
    "end": 4801630,
    "text": "ミストラルのソースコードでは、xformersというライブラリを使っている。"
  },
  {
    "start": 4802210,
    "end": 4814882,
    "text": "Xformersライブラリを使えば、非常に複雑なアテンション・マスクを計算することができるし、メモリ効率の良いアテンション計算を使って非常に効率的にアテンションを計算することもできる。"
  },
  {
    "start": 4814936,
    "end": 4816866,
    "text": "将来、それについてのビデオを作るかもしれない。"
  },
  {
    "start": 4816968,
    "end": 4827650,
    "text": "基本的に、ミストラルのソースコードで行われていることは、複数のプロンプトがある場合、1つの大きなシーケンスを作成し、各プロンプトのトークン数を記録することだ。"
  },
  {
    "start": 4827810,
    "end": 4839110,
    "text": "そして、Xファーマーズ・ライブラリで利用可能なこれらのメソッドを使って、kvキャッシュの異なるサイズを追跡する複雑なアテンション・マップを構築する。"
  },
  {
    "start": 4839190,
    "end": 4843542,
    "text": "各プロンプトは、他のプロンプトとは異なるkvキャッシュを持っている可能性があるからである。"
  },
  {
    "start": 4843686,
    "end": 4853518,
    "text": "5000トークンのプロンプトと10トークンしかないプロンプトがあったとする。"
  },
  {
    "start": 4853604,
    "end": 4858020,
    "text": "私たちが作るマスク・アテンション・マスクは、この点に注意しなければならない。"
  },
  {
    "start": 4858390,
    "end": 4867186,
    "text": "もうひとつは、トークンの各グループは、同じグループのトークンのみに関連付けられ、他のグループには関連付けられないことだ。"
  },
  {
    "start": 4867218,
    "end": 4870306,
    "text": "他のプロンプトからのトークンではない"
  },
  {
    "start": 4870498,
    "end": 4875090,
    "text": "これはブロック対角因果マスクで行われる。"
  },
  {
    "start": 4875170,
    "end": 4884630,
    "text": "最初のプロンプトは7つのトークンで構成され、2番目のプロンプトは5つのトークンで構成され、3番目のプロンプトは6つのトークンで構成されている。"
  },
  {
    "start": 4884710,
    "end": 4888858,
    "text": "また、スライディング・ウィンドウ・アテンションも使用しており、スライディング・ウィンドウのサイズは3である。"
  },
  {
    "start": 4888944,
    "end": 4893022,
    "text": "基本的にこれで、ここに見られるような複雑なMAXができる。"
  },
  {
    "start": 4893156,
    "end": 4895434,
    "text": "これがトークンの最初のグループだ。"
  },
  {
    "start": 4895482,
    "end": 4905758,
    "text": "0から6までが最初のプロンプト、7から11までが2番目のプロンプト、12から17までが3番目のプロンプトだ。"
  },
  {
    "start": 4905854,
    "end": 4909890,
    "text": "見ての通り、スライディング・ウィンドウのサイズも考慮されている。"
  },
  {
    "start": 4909960,
    "end": 4914206,
    "text": "各トークンは最大2つ前のトークンを見ることができる。"
  },
  {
    "start": 4914398,
    "end": 4919320,
    "text": "サイズ3のスライディングウィンドウサイズに含まれるトークン。"
  },
  {
    "start": 4920250,
    "end": 4922962,
    "text": "もうひとつは、ブロック・ダイアゴナル・マスクだ。"
  },
  {
    "start": 4923026,
    "end": 4928386,
    "text": "オーケー、これはプリフィル中の最初のチャンクに使われる。"
  },
  {
    "start": 4928578,
    "end": 4932550,
    "text": "これはプリフィリングで後続のチャンクに使用される。"
  },
  {
    "start": 4932710,
    "end": 4941606,
    "text": "というのも、最初のプリフィリングではKVキャッシュが最初は空なのだが、その後のステップでは空ではなくなるからだ。"
  },
  {
    "start": 4941638,
    "end": 4945546,
    "text": "kvキャッシュのサイズの違いも考慮する必要がある。"
  },
  {
    "start": 4945738,
    "end": 4953294,
    "text": "たとえば、最初のトークンは、プロンプトが非常に短いので、kvキャッシュのサイズが10かもしれないが、2番目のプロンプトは非常に大きいかもしれない。"
  },
  {
    "start": 4953332,
    "end": 4954458,
    "text": "仮に5000トークンだとする。"
  },
  {
    "start": 4954474,
    "end": 4957220,
    "text": "5000のKVキャッシュがあるかもしれない。"
  },
  {
    "start": 4957670,
    "end": 4966930,
    "text": "KVキャッシュのサイズも考慮したマスクを生成する。"
  },
  {
    "start": 4967090,
    "end": 4981814,
    "text": "なぜなら、各プロンプトはKVキャッシュのサイズが異なるが、このKVCでは一部のトークンのみが異なる可能性があるからである。"
  },
  {
    "start": 4981862,
    "end": 4991206,
    "text": "kVキャッシュのサイズは固定で、固定辺wのテンソルだが、このkVキャッシュで実際に満たされるのは一部のトークンだけかもしれない。"
  },
  {
    "start": 4991238,
    "end": 4994174,
    "text": "kvのキャッシュ・サイズが10である場合だけかもしれない。"
  },
  {
    "start": 4994292,
    "end": 5000350,
    "text": "というのも、最初のプロンプトは非常に短いため、実際にkvキャッシュにあるのは3つのトークンのみだからだ。"
  },
  {
    "start": 5001170,
    "end": 5009358,
    "text": "kvキャッシュを注目度の計算に渡すとき、テンソル（10個の項目）をすべて渡す。"
  },
  {
    "start": 5009454,
    "end": 5018910,
    "text": "マスクに、ヒートはKVキャッシュから最初の3つのアイテムだけを使うべきで、KVキャッシュのすべてやテンソルのすべてを使うべきでないことを伝える方法が必要だ。"
  },
  {
    "start": 5018990,
    "end": 5022818,
    "text": "これはブロック対角で行われ、オフセットパディングのキーがマスクされる。"
  },
  {
    "start": 5022834,
    "end": 5029046,
    "text": "この方法はとても長い名前で、とても複雑なのだが、これが彼らがこの方法を使う理由であり、このようなマスクができるのだ。"
  },
  {
    "start": 5029068,
    "end": 5043710,
    "text": "これはKVキャッシュの実際のサイズを考慮し、たとえすべてのKVキャッシュが同じサイズであったとしても、固定サイズのテンソルであるため、各キャッシュから実際に使用すべきアイテムの数を教えてくれる。"
  },
  {
    "start": 5047810,
    "end": 5052320,
    "text": "みんな、とても厳しいビデオだったと言わざるを得ないよ。"
  },
  {
    "start": 5052770,
    "end": 5055698,
    "text": "何度も録画しなければならなかった。"
  },
  {
    "start": 5055784,
    "end": 5060500,
    "text": "実際、私でさえ混乱することがあったので、いくつかの部分をカットしなければならなかった。"
  },
  {
    "start": 5061430,
    "end": 5083340,
    "text": "とても複雑なトピックで、把握しなければならないことがたくさんあるのですが、ミストラルのコードを理解したいときに、皆さんの生活が楽になることを願って、私のノートもオンラインで公開しています。"
  },
  {
    "start": 5084110,
    "end": 5092254,
    "text": "ミストラルのソースコードですが、実は私のコンピューターはあまり高性能ではないので、実際のモデルを私のコンピューターで実行することはありません。"
  },
  {
    "start": 5092452,
    "end": 5105066,
    "text": "モデルを研究するためにやったことは、いくつかのランダムなテンソルをモデルを通して実行することだった。基本的には、ランダムに初期化された重みを持つモデルを作成したが、私のgpuに収まるように層数を少なくした。"
  },
  {
    "start": 5105098,
    "end": 5111518,
    "text": "そして、テンソルのすべての形と情報の受け渡しを研究するために、いくつかのテンソルをランダムに実行するだけだ。"
  },
  {
    "start": 5111694,
    "end": 5114514,
    "text": "このコードがうまくいくかどうかはわからないが、うまくいくことを願っている。"
  },
  {
    "start": 5114552,
    "end": 5117590,
    "text": "つまり、ロジックには触れず、コメントを加えただけだ。"
  },
  {
    "start": 5117930,
    "end": 5130762,
    "text": "いずれにせよ、私がコメントしたコードをミストラルの公式コードと補完するための学習ツールとして使うことで、この偉大なモデルの内部構造をより深く理解することができる。"
  },
  {
    "start": 5130816,
    "end": 5137260,
    "text": "実際、コードを勉強するのは本当に楽しかったし、多くのことを学んだ。"
  },
  {
    "start": 5137710,
    "end": 5147086,
    "text": "とても複雑なことをやっているときは、多くのことを学べるからとてもいいと思う。"
  },
  {
    "start": 5147188,
    "end": 5149566,
    "text": "とにかく、みんな、僕のビデオを見てくれてありがとう。"
  },
  {
    "start": 5149668,
    "end": 5153562,
    "text": "非常に複雑ではあったが、私とのこの旅を楽しんでいただけたなら幸いである。"
  },
  {
    "start": 5153706,
    "end": 5157582,
    "text": "このビデオを気に入ってくださり、私のチャンネルを登録してくださることを願っています。"
  },
  {
    "start": 5157636,
    "end": 5159026,
    "text": "もしやっていないなら、やってください。"
  },
  {
    "start": 5159128,
    "end": 5163314,
    "text": "僕を応援してくれる最高の方法は、このビデオを知り合いにシェアしてくれることだ。"
  },
  {
    "start": 5163352,
    "end": 5167502,
    "text": "ソーシャルメディアで共有したり、LinkedInやTwitterで共有したり。"
  },
  {
    "start": 5167646,
    "end": 5171890,
    "text": "私のチャンネルを成長させることが、あなたが私にできる最善の方法だからです。"
  },
  {
    "start": 5172040,
    "end": 5175810,
    "text": "何かわからないことがあったら教えてください。"
  },
  {
    "start": 5175880,
    "end": 5179750,
    "text": "いつでもお手伝いしますので、LinkedInで私とつながってください。"
  },
  {
    "start": 5179830,
    "end": 5180038,
    "text": "さようなら。"
  }
]