[
  {
    "start": 250,
    "end": 4778,
    "text": "こんにちは、私の名前はアンドレで、10年以上ディープ・ニューラル・ネットワークをトレーニングしてきました。"
  },
  {
    "start": 4874,
    "end": 9546,
    "text": "この講義では、ニューラルネットワークのトレーニングがボンネットの中でどのように見えるかをお見せしたい。"
  },
  {
    "start": 9658,
    "end": 22526,
    "text": "特に、空白のJupyterノートブックから始めて、この講義の終わりまでにニューラルネットを定義し、訓練します。"
  },
  {
    "start": 22708,
    "end": 28434,
    "text": "さて、具体的に何をしたいかというと、マイクログラッドのビルドについて説明したい。"
  },
  {
    "start": 28562,
    "end": 32038,
    "text": "さて、Microgradは2年ほど前にGitHubで公開したライブラリだ。"
  },
  {
    "start": 32124,
    "end": 39046,
    "text": "当時はソースコードしかアップロードしていなかったので、自分で中に入って実際にどう動くのかを理解する必要がある。"
  },
  {
    "start": 39228,
    "end": 44010,
    "text": "このレクチャーでは、それをステップ・バイ・ステップで説明し、そのすべての部分についてコメントする。"
  },
  {
    "start": 44080,
    "end": 46540,
    "text": "マイクログラッドとは何か？"
  },
  {
    "start": 49790,
    "end": 52030,
    "text": "マイクログラッドは基本的にオートグラッドエンジンだ。"
  },
  {
    "start": 52100,
    "end": 54506,
    "text": "オートグラッドとは、オートマチック・グラディエントの略である。"
  },
  {
    "start": 54618,
    "end": 57434,
    "text": "バックプロパゲーションを実装しているのだ。"
  },
  {
    "start": 57562,
    "end": 67918,
    "text": "さて、バックプロパゲーションとは、ニューラルネットワークの重みに対するある種の損失関数の勾配を効率的に評価できるアルゴリズムである。"
  },
  {
    "start": 68014,
    "end": 76254,
    "text": "その結果、ニューラルネットワークの重みを繰り返し調整することで、損失関数を最小化し、ネットワークの精度を向上させることができる。"
  },
  {
    "start": 76382,
    "end": 83590,
    "text": "バックプロパゲーションは、例えばpytorchやJAXのような最新のディープ・ニューラル・ネットワーク・ライブラリーの数学的中核をなすものだ。"
  },
  {
    "start": 83930,
    "end": 87366,
    "text": "マイクログラッドの機能性は、例によって説明するのが一番わかりやすいと思う。"
  },
  {
    "start": 87468,
    "end": 93930,
    "text": "ここを下にスクロールしていくと、microgradは基本的に数式を組み立てることができる。"
  },
  {
    "start": 94270,
    "end": 104334,
    "text": "ここでやっているのは、2つの入力、aとbがある式を作り上げることだ。"
  },
  {
    "start": 104452,
    "end": 110954,
    "text": "私たちはこれらの値を、microgradの一部として構築しようとしているこのバリュー・オブジェクトにラップしている。"
  },
  {
    "start": 111082,
    "end": 114734,
    "text": "この値オブジェクトは、数値そのものをラップする。"
  },
  {
    "start": 114932,
    "end": 123778,
    "text": "そこで、a、bをc、d、そして最終的にはe、f、gに変換する数式を構築する。"
  },
  {
    "start": 123944,
    "end": 128770,
    "text": "microgradの機能の一部と、それがサポートするオペレーションを紹介する。"
  },
  {
    "start": 128850,
    "end": 142550,
    "text": "足し算、掛け算、定数乗、1オフセット、2乗否定、定数割り、割り算などなど。"
  },
  {
    "start": 142710,
    "end": 155018,
    "text": "つまり、a、bという2つの入力で式グラフを構築し、gという出力値を作成しているわけだが、microgradはバックグラウンドでこの数式全体を構築する。"
  },
  {
    "start": 155114,
    "end": 158446,
    "text": "であれば、たとえばcも値であることがわかる。"
  },
  {
    "start": 158628,
    "end": 161258,
    "text": "Cは加算操作の結果だった。"
  },
  {
    "start": 161434,
    "end": 169902,
    "text": "なぜなら、Cの子ノードはaとbであり、aとbの値オブジェクトへのポインタを保持するからである。"
  },
  {
    "start": 169966,
    "end": 173154,
    "text": "私たちは基本的に、このすべてがどのように配置されているかを正確に知ることになる。"
  },
  {
    "start": 173352,
    "end": 179842,
    "text": "そうすれば、フォワードパスと呼ばれる、gの値を実際に見ることができるだけでなく、もちろんそれは非常に簡単なことだ。"
  },
  {
    "start": 179986,
    "end": 183074,
    "text": "ドット・データ属性を使ってアクセスする。"
  },
  {
    "start": 183202,
    "end": 188886,
    "text": "つまり、フォワードパスの出力、gの値は24.7であることが判明した。"
  },
  {
    "start": 188988,
    "end": 199100,
    "text": "大きなポイントは、このg値オブジェクトを受け取り、dot backwardを呼び出すことである。"
  },
  {
    "start": 199790,
    "end": 209978,
    "text": "バックプロパゲーションが行うのは、gから始まり、その式グラフを後方へ進み、微積分の連鎖規則を再帰的に適用することだ。"
  },
  {
    "start": 210154,
    "end": 223090,
    "text": "これでできることは、基本的にGの導関数をedやcのようなすべての内部ノードに関して評価し、さらに入力aやbに関しても評価することだ。"
  },
  {
    "start": 223240,
    "end": 227890,
    "text": "とすれば、aに関するGの導関数を問い合わせることができる。"
  },
  {
    "start": 227960,
    "end": 229710,
    "text": "例えば、これはドット・グラードだ。"
  },
  {
    "start": 229790,
    "end": 231146,
    "text": "この場合はたまたま138だった。"
  },
  {
    "start": 231148,
    "end": 235814,
    "text": "はbに関するGの導関数であり、ここでもたまたまbであった。"
  },
  {
    "start": 235852,
    "end": 236246,
    "text": "645."
  },
  {
    "start": 236268,
    "end": 246646,
    "text": "というのも、Aとbがこの数式を通してgにどのような影響を与えるかを教えてくれるからだ。"
  },
  {
    "start": 246758,
    "end": 250266,
    "text": "特にドットグラッドは138である。"
  },
  {
    "start": 250368,
    "end": 257742,
    "text": "aを少し動かして少し大きくすると、138はgが大きくなると言っているのだ。"
  },
  {
    "start": 257876,
    "end": 264366,
    "text": "の成長の傾きは138となり、bの成長の傾きは645となる。"
  },
  {
    "start": 264468,
    "end": 270770,
    "text": "これは、aとbをプラス方向にほんの少しいじった場合、gがどのように反応するかを教えてくれる。"
  },
  {
    "start": 271110,
    "end": 277282,
    "text": "さて、ここで作り上げたこの表現が何なのか、混乱しているかもしれない。"
  },
  {
    "start": 277336,
    "end": 279698,
    "text": "ところで、この表現はまったく意味がない。"
  },
  {
    "start": 279794,
    "end": 280854,
    "text": "作り話だよ。"
  },
  {
    "start": 280892,
    "end": 284834,
    "text": "マイクログラッドがサポートするオペレーションについて、私はただフレキシブルに考えているだけだ。"
  },
  {
    "start": 284962,
    "end": 287378,
    "text": "私たちが実際に気にかけているのは、ニューラルネットワークだ。"
  },
  {
    "start": 287474,
    "end": 293562,
    "text": "ニューラルネットワークはこのような数式に過ぎないが、実際にはもう少しクレイジーではないことがわかった。"
  },
  {
    "start": 293616,
    "end": 297078,
    "text": "ニューラルネットワークだって、単なる数式だ。"
  },
  {
    "start": 297174,
    "end": 307550,
    "text": "入力データを入力とし、ニューラルネットワークの重みを入力とし、数式で表し、出力がニューラルネットの予測値や損失関数となる。"
  },
  {
    "start": 307620,
    "end": 308862,
    "text": "もう少ししたら分かるだろう。"
  },
  {
    "start": 308996,
    "end": 313642,
    "text": "基本的にニューラルネットワークは、ある種の数学的表現である。"
  },
  {
    "start": 313786,
    "end": 316754,
    "text": "バックプロパゲーションは、実際にはもっと一般的だ。"
  },
  {
    "start": 316872,
    "end": 319202,
    "text": "ニューラルネットワークにはまったく関心がない。"
  },
  {
    "start": 319256,
    "end": 321918,
    "text": "任意の数式にしか関心がない。"
  },
  {
    "start": 322014,
    "end": 326350,
    "text": "そして、その機械をニューラルネットワークのトレーニングに使う。"
  },
  {
    "start": 326430,
    "end": 332214,
    "text": "さて、この段階でもうひとつ言っておきたいのは、ここにあるように、MicrograDはスカラー値オートグラッドエンジンだということだ。"
  },
  {
    "start": 332332,
    "end": 342810,
    "text": "私たちはニューラルネットを使って、個々のスカラーを原子レベルまで分解し、小さなプラスや時間まで分解している。"
  },
  {
    "start": 342880,
    "end": 344614,
    "text": "やりすぎだ。"
  },
  {
    "start": 344742,
    "end": 347830,
    "text": "だから、本番でこんなことをするはずがない。"
  },
  {
    "start": 347910,
    "end": 356746,
    "text": "これは教育的な理由によるもので、最新のディープ・ニューラル・ネットワーク・ライブラリで使うようなn次元のテンソルを扱わなくて済むようにするためだ。"
  },
  {
    "start": 356858,
    "end": 364766,
    "text": "これは、バック・レプリケーションとチェーン・ルール、そしてニューラル・トレーニングを理解し、リファクタリングするためのものだ。"
  },
  {
    "start": 364948,
    "end": 369054,
    "text": "ということは、より大きなネットワークを実際に訓練したいのであれば、これらのテンソルを使わなければならない。"
  },
  {
    "start": 369102,
    "end": 370322,
    "text": "どの計算も変わらない。"
  },
  {
    "start": 370376,
    "end": 372222,
    "text": "これは純粋に効率のためだ。"
  },
  {
    "start": 372366,
    "end": 379346,
    "text": "私たちは基本的に、スケール値、つまりすべてのスカラー値を、スカラーの単なる配列であるテンソルにパッケージングしている。"
  },
  {
    "start": 379458,
    "end": 384146,
    "text": "ということは、大きな配列があるわけだから、その大きな配列に対してオペレーションを行うことになる。"
  },
  {
    "start": 384258,
    "end": 392198,
    "text": "そうすることで、コンピュータの並列性を利用することができ、すべての演算を並列処理することができる。"
  },
  {
    "start": 392294,
    "end": 395366,
    "text": "計算方法は何も変わらないし、純粋に効率のためにやっている。"
  },
  {
    "start": 395478,
    "end": 399610,
    "text": "テンソルをゼロから扱うことは、教育上有益だとは思わない。"
  },
  {
    "start": 400110,
    "end": 407806,
    "text": "だから僕は根本的にマイクログラッドを書いたんだ。物事の仕組みを基礎的なレベルで理解し、後からそれをスピードアップさせることができるからね"
  },
  {
    "start": 407988,
    "end": 409438,
    "text": "さて、ここからが楽しいところだ。"
  },
  {
    "start": 409524,
    "end": 414666,
    "text": "私の主張は、ニューラルネットワークのトレーニングに必要なのはマイクログラッドであり、それ以外は効率に過ぎないというものだ。"
  },
  {
    "start": 414778,
    "end": 420818,
    "text": "マイクログラッドは非常に複雑なコードだと思うだろう。"
  },
  {
    "start": 420984,
    "end": 426922,
    "text": "microgradに行けば、microgradには2つのファイルしかないことがわかるだろう。"
  },
  {
    "start": 427006,
    "end": 428246,
    "text": "これが実際のエンジンだ。"
  },
  {
    "start": 428348,
    "end": 430162,
    "text": "ニューラルネットについては何も知らない。"
  },
  {
    "start": 430226,
    "end": 433970,
    "text": "これはmicrogradの上にあるニューラルネット・ライブラリ全体である。"
  },
  {
    "start": 434050,
    "end": 437074,
    "text": "エンジンとnnパイ。"
  },
  {
    "start": 437202,
    "end": 451994,
    "text": "ニューラルネットワークのパワーを与えてくれる実際のバックプロパゲーション・オートグラッド・エンジンは、文字通り非常に単純なパイソンのような100行のコードだ。"
  },
  {
    "start": 452122,
    "end": 459530,
    "text": "となると、オートグラッド・エンジンの上に構築されたこのニューラルネットワーク・ライブラリは、まるで冗談のようだ。"
  },
  {
    "start": 459610,
    "end": 469726,
    "text": "ニューロンとは何かを定義し、ニューロンの層とは何かを定義し、ニューロンの層のシーケンスである多層パーセプトロンとは何かを定義しなければならない。"
  },
  {
    "start": 469838,
    "end": 471934,
    "text": "というわけで、まったくのジョークだ。"
  },
  {
    "start": 472062,
    "end": 477490,
    "text": "基本的に、たった150行のコードから生まれるパワーは大きい。"
  },
  {
    "start": 477640,
    "end": 479078,
    "text": "それだけを理解すればいい"
  },
  {
    "start": 479164,
    "end": 482546,
    "text": "ニューラルネットワークのトレーニングを理解すること、そしてそれ以外のことは、単なる効率でしかない。"
  },
  {
    "start": 482658,
    "end": 487766,
    "text": "もちろん効率化することはたくさんあるが、根本的にはそれがすべてだ。"
  },
  {
    "start": 487868,
    "end": 491366,
    "text": "では、さっそくマイクログラッドを導入してみよう。"
  },
  {
    "start": 491468,
    "end": 499594,
    "text": "まず最初に、デリバティブとは何か、そしてデリバティブがどのような情報を与えてくれるのかを直感的によく理解していただきたい。"
  },
  {
    "start": 499712,
    "end": 504720,
    "text": "Jupyterノートブックにいつもコピーペーストしている基本的なインポートから始めよう。"
  },
  {
    "start": 505250,
    "end": 511242,
    "text": "xのスカラー値関数fを次のように定義しよう。"
  },
  {
    "start": 511386,
    "end": 513146,
    "text": "これは適当に作ったんだ。"
  },
  {
    "start": 513178,
    "end": 518338,
    "text": "私はただ、スカラーxを受け取り、スカラーyを返すスカラー有効関数が欲しかっただけだ。"
  },
  {
    "start": 518504,
    "end": 520194,
    "text": "もちろん、この関数を呼び出すこともできる。"
  },
  {
    "start": 520232,
    "end": 523540,
    "text": "例えば3.0でパスし、20ドルを取り戻す。"
  },
  {
    "start": 523910,
    "end": 526930,
    "text": "この関数をプロットして、その形状を把握することもできる。"
  },
  {
    "start": 527010,
    "end": 531762,
    "text": "数式を見れば、これはおそらく放物線であり、二次曲線であることがわかるだろう。"
  },
  {
    "start": 531906,
    "end": 554254,
    "text": "例えば、マイナス5から5までの範囲を00:25刻みで入力できるスカラー値のセットを作れば、軸はマイナス5から5までで、00:25刻みで5を含まない。"
  },
  {
    "start": 554292,
    "end": 557466,
    "text": "軸上でfを呼び出すと、yのセットが得られる。"
  },
  {
    "start": 557658,
    "end": 565178,
    "text": "これらのYは、基本的に、これらの要素のひとつひとつに独立して関数を適用している。"
  },
  {
    "start": 565354,
    "end": 567894,
    "text": "matplotlibを使ってプロットできる。"
  },
  {
    "start": 567962,
    "end": 572238,
    "text": "pltでxとyをプロットすると、きれいな放物線が得られる。"
  },
  {
    "start": 572334,
    "end": 578930,
    "text": "以前、ここで3.0を入力したところ、20が返ってきた。"
  },
  {
    "start": 579010,
    "end": 585846,
    "text": "では、この関数の導関数が、任意の1つの入力点xでどのようになるかを考えてみたいと思います。"
  },
  {
    "start": 585868,
    "end": 589590,
    "text": "この関数の異なる点xにおける導関数は？"
  },
  {
    "start": 589740,
    "end": 593190,
    "text": "さて、微積分の授業を思い出していただければ、微分を導き出したことがあるだろう。"
  },
  {
    "start": 593270,
    "end": 605918,
    "text": "この3×2乗マイナス4×プラス5という数式を紙に書き出し、積の法則と他のすべての法則を適用して、元の関数の大微分の数式を導き出すのである。"
  },
  {
    "start": 606004,
    "end": 609360,
    "text": "そうすれば、さまざまな税金を差し込むことができ、その派生値を見ることができる。"
  },
  {
    "start": 609730,
    "end": 616014,
    "text": "なぜなら、ニューラルネットの分野で実際にニューラルネットの式を書き出す人はいないからだ。"
  },
  {
    "start": 616052,
    "end": 617650,
    "text": "大げさな表現になってしまう。"
  },
  {
    "start": 618070,
    "end": 620434,
    "text": "何千、何万という用語になるだろう。"
  },
  {
    "start": 620472,
    "end": 623314,
    "text": "もちろん、実際にデリバティブを導き出す人はいない。"
  },
  {
    "start": 623432,
    "end": 626222,
    "text": "だから、このような象徴的なアプローチを取るつもりはない。"
  },
  {
    "start": 626286,
    "end": 634534,
    "text": "その代わりに、微分の定義を見て、微分が何を測定しているのか、関数について何を語っているのかを本当に理解しているかどうかを確認したい。"
  },
  {
    "start": 634732,
    "end": 645750,
    "text": "デリバティブを調べてみると、これはデリバティブの定義としてはあまり良くないことがわかる。"
  },
  {
    "start": 645830,
    "end": 648342,
    "text": "これは、微分可能であることの意味についての定義である。"
  },
  {
    "start": 648486,
    "end": 655694,
    "text": "微積分を覚えているなら、hがfのゼロになるにつれて、xプラスhマイナスfのxがhを超える極限である。"
  },
  {
    "start": 655812,
    "end": 667540,
    "text": "基本的に言っていることは、少しアップさせれば、あなたが興味を持っているある点x、つまりaになり、少しアップさせれば、小さな数hだけそれをわずかに増やすということです。"
  },
  {
    "start": 668070,
    "end": 669502,
    "text": "機能はどう反応するのか？"
  },
  {
    "start": 669566,
    "end": 671214,
    "text": "どのような感度で反応するのか？"
  },
  {
    "start": 671262,
    "end": 672706,
    "text": "その地点での傾斜は？"
  },
  {
    "start": 672808,
    "end": 675010,
    "text": "機能は上がるのか、それとも下がるのか？"
  },
  {
    "start": 675080,
    "end": 676194,
    "text": "いくらで？"
  },
  {
    "start": 676312,
    "end": 681622,
    "text": "これが関数の傾き、つまりその時点での反応の傾きだ。"
  },
  {
    "start": 681756,
    "end": 687670,
    "text": "したがって、微分を数値的に評価するには、hを非常に小さくすればよい。"
  },
  {
    "start": 687740,
    "end": 690726,
    "text": "もちろん、この定義ではhをゼロにすることが求められる。"
  },
  {
    "start": 690828,
    "end": 693866,
    "text": "ここではhを0.1と非常に小さくする。"
  },
  {
    "start": 693968,
    "end": 696026,
    "text": "ゼロ3.0に興味があるとしよう。"
  },
  {
    "start": 696128,
    "end": 700810,
    "text": "もちろん、xのfを20と見ることもできるし、xのfにhを足したものと見ることもできる。"
  },
  {
    "start": 700960,
    "end": 705450,
    "text": "xを正の方向に少し動かすと、関数はどのように反応するのだろうか？"
  },
  {
    "start": 705610,
    "end": 711214,
    "text": "これを見ただけで、xにhを足したfが20より少し大きくなると思いますか？"
  },
  {
    "start": 711332,
    "end": 714398,
    "text": "それとも20より少し低くなると予想しているのか。"
  },
  {
    "start": 714564,
    "end": 721054,
    "text": "この3がここで、これが20だから、少しでもプラスに行けば、その機能はプラスに反応する。"
  },
  {
    "start": 721182,
    "end": 729154,
    "text": "20より少し大きいと予想されるが、それがどれくらいかによって、その傾斜の強さのようなものがわかる。"
  },
  {
    "start": 729202,
    "end": 731170,
    "text": "そう、あの斜面の大きさだ。"
  },
  {
    "start": 731250,
    "end": 733622,
    "text": "xのfにhを足したものからxのfを引いたもの。"
  },
  {
    "start": 733676,
    "end": 737426,
    "text": "これは関数がどれだけプラス方向に反応したかを示している。"
  },
  {
    "start": 737538,
    "end": 740358,
    "text": "ランでノーマライズしなければならない。"
  },
  {
    "start": 740444,
    "end": 743370,
    "text": "スロープを得るために、ライズ・オーバー・ランがある。"
  },
  {
    "start": 743790,
    "end": 752060,
    "text": "もちろん、これは勾配の数値近似に過ぎない。正確な勾配に収束させるためには、年齢を非常に小さくしなければならないからだ。"
  },
  {
    "start": 752750,
    "end": 764478,
    "text": "なぜなら、浮動小数点演算を使用しており、コンピュータのメモリにあるすべての数値の表現は有限だからだ。"
  },
  {
    "start": 764574,
    "end": 766334,
    "text": "いつかはトラブルに巻き込まれる。"
  },
  {
    "start": 766462,
    "end": 769410,
    "text": "このアプローチで正しい答えに収束させることができる。"
  },
  {
    "start": 770390,
    "end": 774398,
    "text": "基本的に3点で、スロープは14。"
  },
  {
    "start": 774574,
    "end": 780310,
    "text": "を頭の中で微分すればわかる。"
  },
  {
    "start": 780460,
    "end": 786854,
    "text": "xの2乗は6となり、xから4を引いて、xは3となる。"
  },
  {
    "start": 786972,
    "end": 788662,
    "text": "18から4を引いて14。"
  },
  {
    "start": 788726,
    "end": 789820,
    "text": "これは正しい。"
  },
  {
    "start": 790830,
    "end": 792874,
    "text": "それは3時だ。"
  },
  {
    "start": 792992,
    "end": 796940,
    "text": "では、例えばマイナス3での傾斜はどうだろう？"
  },
  {
    "start": 798510,
    "end": 800374,
    "text": "傾斜はどうなると思いますか？"
  },
  {
    "start": 800502,
    "end": 804590,
    "text": "正確な値を言うのは難しいが、その傾きの正弦は？"
  },
  {
    "start": 804930,
    "end": 811806,
    "text": "のとき、xをプラス方向に少し進めば、関数は実際に下がることになる。"
  },
  {
    "start": 811908,
    "end": 813790,
    "text": "つまり、傾きはマイナスになる。"
  },
  {
    "start": 813870,
    "end": 817762,
    "text": "20をわずかに下回るだろう。"
  },
  {
    "start": 817896,
    "end": 821938,
    "text": "だから、スロープをとれば、マイナス22となる。"
  },
  {
    "start": 822024,
    "end": 822660,
    "text": "オーケー。"
  },
  {
    "start": 823830,
    "end": 827030,
    "text": "勾配はゼロになる。"
  },
  {
    "start": 827180,
    "end": 832038,
    "text": "さて、この具体的な関数については、以前調べたことがある。"
  },
  {
    "start": 832204,
    "end": 839050,
    "text": "この微分はゼロになる。"
  },
  {
    "start": 839200,
    "end": 848294,
    "text": "基本的に、その正確なポイントでは、そう、その正確なポイントでは、プラス方向にナッジしても、関数は反応しない。"
  },
  {
    "start": 848342,
    "end": 849706,
    "text": "これはほとんど変わらない。"
  },
  {
    "start": 849808,
    "end": 851374,
    "text": "だから傾きはゼロなのだ。"
  },
  {
    "start": 851492,
    "end": 853840,
    "text": "では、もう少し複雑なケースを見てみよう。"
  },
  {
    "start": 854290,
    "end": 857166,
    "text": "これから少し複雑にしていくつもりだ。"
  },
  {
    "start": 857268,
    "end": 865986,
    "text": "ここで、出力変数bを持つ関数があるが、これは3つのスカラー入力a、b、cの関数である。"
  },
  {
    "start": 866168,
    "end": 872500,
    "text": "a、b、cは、式グラフへの3つの入力と1つの出力dである。"
  },
  {
    "start": 872870,
    "end": 875860,
    "text": "ということは、dと表示すれば、4となる。"
  },
  {
    "start": 876230,
    "end": 887318,
    "text": "ここでもう一度、a、b、cに対するdの導関数を見て、この導関数が何を物語っているのかを直感的に考えてみたい。"
  },
  {
    "start": 887484,
    "end": 892138,
    "text": "この導関数を評価するために、ここでは少し手こずることになる。"
  },
  {
    "start": 892224,
    "end": 900250,
    "text": "ここでもhの値を非常に小さくして、入力を我々が興味のある値に固定することにする。"
  },
  {
    "start": 900400,
    "end": 909360,
    "text": "これは点Abcで、その点におけるすべてのa、b、cに関してdの導関数を評価することになる。"
  },
  {
    "start": 909730,
    "end": 913578,
    "text": "インプットがあり、そして今、dがその表現である。"
  },
  {
    "start": 913754,
    "end": 917106,
    "text": "では、例えば、aに関するdの導関数を見てみよう。"
  },
  {
    "start": 917208,
    "end": 922980,
    "text": "を取り、それをhでぶつけ、そして2つのdをまったく同じ関数にする。"
  },
  {
    "start": 923750,
    "end": 929062,
    "text": "これからdをプリントする。"
  },
  {
    "start": 929116,
    "end": 934550,
    "text": "1つはd、2つはdで、傾きを印刷する。"
  },
  {
    "start": 935130,
    "end": 943980,
    "text": "ここでの微分または傾きは、もちろんd2からd1を引いたものをhで割ったものになる。"
  },
  {
    "start": 944350,
    "end": 955694,
    "text": "d 2からd 1を引いた値は、私たちが興味を持っている特定の入力をほんの少し増やしたときに、関数がどれだけ増えたかを示している。"
  },
  {
    "start": 955892,
    "end": 960590,
    "text": "これをhで正規化して傾きを求める。"
  },
  {
    "start": 962770,
    "end": 967038,
    "text": "そう、そう、これだ。"
  },
  {
    "start": 967124,
    "end": 974660,
    "text": "このまま実行すると、d1がプリントされる。"
  },
  {
    "start": 975370,
    "end": 979640,
    "text": "今、dが2つぶつかり、aがhにぶつかる。"
  },
  {
    "start": 980330,
    "end": 988520,
    "text": "ここでは特にD2がプリントアウトされる内容を少し考えてみよう。"
  },
  {
    "start": 989210,
    "end": 990906,
    "text": "D1は4となる。"
  },
  {
    "start": 991088,
    "end": 995690,
    "text": "d 2は4より少し大きい数字になるのか、それとも4より少し小さい数字になるのか？"
  },
  {
    "start": 995760,
    "end": 999530,
    "text": "これで微分の正弦がわかる。"
  },
  {
    "start": 1000210,
    "end": 1008000,
    "text": "aをh、bをマイナス3、cを10とする。"
  },
  {
    "start": 1008530,
    "end": 1011678,
    "text": "このデリバティブが何をしているのか、直感的に考えることができる。"
  },
  {
    "start": 1011764,
    "end": 1017060,
    "text": "Aはわずかにプラスになるが、Bはマイナスになる。"
  },
  {
    "start": 1017510,
    "end": 1027000,
    "text": "bがマイナス3なので、aがわずかにプラスであれば、実際にはdの足し算が少なくなる。"
  },
  {
    "start": 1028170,
    "end": 1033160,
    "text": "ということは、関数の値が下がることを期待していることになる。"
  },
  {
    "start": 1033690,
    "end": 1035320,
    "text": "これを見てみよう。"
  },
  {
    "start": 1036810,
    "end": 1040380,
    "text": "というわけで、4から3.996になった。"
  },
  {
    "start": 1040750,
    "end": 1053280,
    "text": "ということは、勾配は負になり、下降したので負の数になり、勾配の正確な数は負の3であることを意味する。"
  },
  {
    "start": 1053730,
    "end": 1068306,
    "text": "というのも、aのb乗にcを足したものを微積分すれば、aのb乗にcを足したものをaに関して微分すれば、bが得られるからである。"
  },
  {
    "start": 1068488,
    "end": 1072578,
    "text": "実際、bの値は負の3であり、これは我々が持っている微分である。"
  },
  {
    "start": 1072664,
    "end": 1074340,
    "text": "その通りだとわかるだろう。"
  },
  {
    "start": 1074870,
    "end": 1082738,
    "text": "bをプラス方向に少し動かすと、異なるスロープになる。"
  },
  {
    "start": 1082754,
    "end": 1085314,
    "text": "bが出力に与える影響は？"
  },
  {
    "start": 1085362,
    "end": 1085960,
    "text": "D."
  },
  {
    "start": 1086330,
    "end": 1093706,
    "text": "bをプラス方向にほんの少し動かすと、aがプラスなので、Dが増えることになる。"
  },
  {
    "start": 1093888,
    "end": 1094620,
    "text": "そうだね。"
  },
  {
    "start": 1095710,
    "end": 1097654,
    "text": "さて、感度はどうだろう？"
  },
  {
    "start": 1097702,
    "end": 1099718,
    "text": "その足し算の傾きは？"
  },
  {
    "start": 1099894,
    "end": 1103680,
    "text": "これが2つになるのは驚くことではないかもしれない。"
  },
  {
    "start": 1104210,
    "end": 1105422,
    "text": "なぜ2つなのか？"
  },
  {
    "start": 1105476,
    "end": 1113438,
    "text": "dのdbをbに関して微分するとaになり、aの値は2だからだ。"
  },
  {
    "start": 1113524,
    "end": 1115246,
    "text": "それもうまくいっている。"
  },
  {
    "start": 1115428,
    "end": 1121902,
    "text": "ということは、もしcがhの中でhにほんの少しぶつかったとしても、もちろんa倍b倍は影響を受けない。"
  },
  {
    "start": 1121966,
    "end": 1124242,
    "text": "これでcが少し高くなった。"
  },
  {
    "start": 1124376,
    "end": 1125746,
    "text": "それが機能に何をもたらすのか？"
  },
  {
    "start": 1125848,
    "end": 1133126,
    "text": "単純にcを足すだけなので、cに足したのとまったく同じ分だけ少し高くなる。"
  },
  {
    "start": 1133228,
    "end": 1135880,
    "text": "ということは、傾きは1ということだ。"
  },
  {
    "start": 1136570,
    "end": 1144906,
    "text": "これが、cのスケールに伴ってdが増加する割合となる。"
  },
  {
    "start": 1145088,
    "end": 1149226,
    "text": "さて、これで微分が関数について何を語っているのか、ある程度直感的に理解できただろう。"
  },
  {
    "start": 1149328,
    "end": 1151062,
    "text": "ニューラルネットワークに移りたい。"
  },
  {
    "start": 1151126,
    "end": 1155018,
    "text": "今申し上げたように、ニューラルネットワークはかなり巨大な表現、数学的表現になる。"
  },
  {
    "start": 1155114,
    "end": 1157754,
    "text": "これらの式を保持するデータ構造が必要だ。"
  },
  {
    "start": 1157802,
    "end": 1160080,
    "text": "それを今から作り上げるんだ。"
  },
  {
    "start": 1160450,
    "end": 1167422,
    "text": "microgradのreadmeページでお見せしたバリュー・オブジェクトを作り出そう。"
  },
  {
    "start": 1167566,
    "end": 1172820,
    "text": "最初の非常にシンプルなバリュー・オブジェクトのスケルトンをコピー・ペーストしてみよう。"
  },
  {
    "start": 1173510,
    "end": 1180630,
    "text": "つまり、クラス・バリューは、ラップして追跡するスカラー値を1つ取る。"
  },
  {
    "start": 1180700,
    "end": 1187682,
    "text": "例えば、2.0という値を設定し、その内容を見ることができる。"
  },
  {
    "start": 1187836,
    "end": 1197820,
    "text": "Pythonは内部的にラッパー関数を使い、このような文字列を返します。"
  },
  {
    "start": 1198590,
    "end": 1203034,
    "text": "これはデータ・イコール2の値オブジェクトで、ここで作成している。"
  },
  {
    "start": 1203232,
    "end": 1212014,
    "text": "2つの値だけでなく、aにbを足したような値も指定したい。"
  },
  {
    "start": 1212052,
    "end": 1213474,
    "text": "彼らを加えたい。"
  },
  {
    "start": 1213672,
    "end": 1220660,
    "text": "なぜならPythonは2つの値オブジェクトを足す方法を知らないからだ。"
  },
  {
    "start": 1221430,
    "end": 1223330,
    "text": "ここに追加する。"
  },
  {
    "start": 1226330,
    "end": 1232914,
    "text": "これらのオブジェクトに対して演算子を定義するには、基本的にPythonの特別なダブル・アンダースコアのメソッドを使わなければならない。"
  },
  {
    "start": 1233042,
    "end": 1243722,
    "text": "このプラス演算子を使うと、Pythonは内部的にbのドット加算を呼び出す。"
  },
  {
    "start": 1243856,
    "end": 1245622,
    "text": "それが内部で起こることだ。"
  },
  {
    "start": 1245766,
    "end": 1250858,
    "text": "ということは、bは他者であり、自己はaになる。"
  },
  {
    "start": 1251024,
    "end": 1259614,
    "text": "というわけで、これから返すのは新しい値オブジェクトで、データのプラスをラップしていることがわかる。"
  },
  {
    "start": 1259812,
    "end": 1264126,
    "text": "というのも、データは実際のPythonの番号だからだ。"
  },
  {
    "start": 1264228,
    "end": 1268814,
    "text": "この演算子は、典型的な浮動小数点＋加算である。"
  },
  {
    "start": 1268862,
    "end": 1273522,
    "text": "これで、バリュー・オブジェクトの追加ではなく、新しいバリューを返すことになる。"
  },
  {
    "start": 1273656,
    "end": 1280182,
    "text": "これで、a＋bが機能し、マイナス1の値が表示されるはずだ。"
  },
  {
    "start": 1280316,
    "end": 1281414,
    "text": "これでよし。"
  },
  {
    "start": 1281612,
    "end": 1286662,
    "text": "では、この式を再現できるように乗算を実装してみよう。"
  },
  {
    "start": 1286796,
    "end": 1290600,
    "text": "倍率は、驚くことではないと思うが、かなり似ているだろう。"
  },
  {
    "start": 1291610,
    "end": 1294238,
    "text": "addの代わりにmoleを使う。"
  },
  {
    "start": 1294354,
    "end": 1296778,
    "text": "それなら、ここで、もちろん回数をこなしたい。"
  },
  {
    "start": 1296944,
    "end": 1300586,
    "text": "これで、10.0となるc値オブジェクトを作成することができる。"
  },
  {
    "start": 1300688,
    "end": 1303280,
    "text": "これでa回b回ができるはずだ。"
  },
  {
    "start": 1303970,
    "end": 1306080,
    "text": "では、まずタイムズBをやってみよう。"
  },
  {
    "start": 1308450,
    "end": 1309854,
    "text": "これはマイナス6点だ。"
  },
  {
    "start": 1309892,
    "end": 1312718,
    "text": "さて、ところで、私はこれを少し読み飛ばした。"
  },
  {
    "start": 1312804,
    "end": 1318798,
    "text": "仮にここでリッパー関数を使わなかったとしたら、何か醜い表現になるだけだ。"
  },
  {
    "start": 1318894,
    "end": 1331640,
    "text": "REPPERがやっているのは、パイソンでより見栄えのする式をプリントアウトする方法を提供してくれている。"
  },
  {
    "start": 1332090,
    "end": 1335382,
    "text": "これで時間がわかり、次にこれだ。"
  },
  {
    "start": 1335436,
    "end": 1340554,
    "text": "パイソンにmullとaddの方法を定義して教えたので、cを追加することができるはずです。"
  },
  {
    "start": 1340672,
    "end": 1347770,
    "text": "つまり、これは基本的にBのミュルに相当する。"
  },
  {
    "start": 1347920,
    "end": 1352378,
    "text": "とすると、この新しい値オブジェクトはcのドット加算になる。"
  },
  {
    "start": 1352544,
    "end": 1354220,
    "text": "それでうまくいったかどうか見てみよう。"
  },
  {
    "start": 1354690,
    "end": 1358720,
    "text": "そう、それでうまくいって、以前から期待していた4人になった。"
  },
  {
    "start": 1359170,
    "end": 1362078,
    "text": "手動で呼び出すこともできると思う。"
  },
  {
    "start": 1362164,
    "end": 1363120,
    "text": "これでよし。"
  },
  {
    "start": 1365090,
    "end": 1368910,
    "text": "さて、私たちに欠けているのは、この表現の連結組織である。"
  },
  {
    "start": 1368990,
    "end": 1376450,
    "text": "前述したように、私たちはこれらの式グラフを保持したいので、どの値がどのような他の値を生み出すかを知り、ポインターを保持する必要がある。"
  },
  {
    "start": 1376790,
    "end": 1383362,
    "text": "例えば、ここではchildrenと呼ぶ新しい変数を導入し、デフォルトでは空のタプルにする。"
  },
  {
    "start": 1383506,
    "end": 1390680,
    "text": "アンダースコアPrevと呼ぶことにする。"
  },
  {
    "start": 1391530,
    "end": 1394422,
    "text": "これは私がやった方法で、オリジナルのマイクログラッドでやった。"
  },
  {
    "start": 1394486,
    "end": 1397242,
    "text": "ここにある私のコードを見ても、その理由ははっきり思い出せない。"
  },
  {
    "start": 1397296,
    "end": 1398646,
    "text": "効率性だと思う。"
  },
  {
    "start": 1398758,
    "end": 1401670,
    "text": "このアンダースコアの子供たちは、便宜上タプルになる。"
  },
  {
    "start": 1401750,
    "end": 1407150,
    "text": "そして、実際にクラスで維持するときは、効率を考えて、このセットだけになると思う。"
  },
  {
    "start": 1408850,
    "end": 1415874,
    "text": "このようにコンストラクタで値を作成する場合、childrenは空になり、prepは空集合になる。"
  },
  {
    "start": 1415992,
    "end": 1425140,
    "text": "足し算や掛け算で値を作る場合、この値の子（この場合は自己と他者）を投入することになる。"
  },
  {
    "start": 1426410,
    "end": 1428840,
    "text": "ここにいる子どもたちだ。"
  },
  {
    "start": 1430650,
    "end": 1440086,
    "text": "D prevを実行すると、Dの子がマイナス6と10の値であることがわかる。"
  },
  {
    "start": 1440188,
    "end": 1445900,
    "text": "これはもちろん、aにbを掛けた値と、cの値である10を掛けた値である。"
  },
  {
    "start": 1446590,
    "end": 1449386,
    "text": "さて、最後の情報だが、我々は知らない。"
  },
  {
    "start": 1449488,
    "end": 1454590,
    "text": "我々は今、すべての値の子を知っているが、どの操作がこの値を作ったのかは知らない。"
  },
  {
    "start": 1454740,
    "end": 1456494,
    "text": "ここでもうひとつ必要な要素がある。"
  },
  {
    "start": 1456532,
    "end": 1458240,
    "text": "アンダースコア・ポップと呼ぼう。"
  },
  {
    "start": 1459090,
    "end": 1462170,
    "text": "デフォルトでは、これは葉のための空のセットである。"
  },
  {
    "start": 1462330,
    "end": 1464260,
    "text": "それなら、ここで維持することにしよう。"
  },
  {
    "start": 1465590,
    "end": 1468366,
    "text": "これで、操作は単なる文字列になる。"
  },
  {
    "start": 1468478,
    "end": 1470706,
    "text": "足し算の場合はプラスになる。"
  },
  {
    "start": 1470808,
    "end": 1473060,
    "text": "掛け算の場合は回数だ。"
  },
  {
    "start": 1473830,
    "end": 1478574,
    "text": "今はDプレップだけでなく、Dオペもある。"
  },
  {
    "start": 1478712,
    "end": 1482306,
    "text": "Dはこの2つの値の足し算で作られたことがわかる。"
  },
  {
    "start": 1482418,
    "end": 1492570,
    "text": "これで完全な数式を手に入れ、このデータ構造を構築しているわけだが、各値がどのような式で、どのような他の値からどのように得られたのかが正確に分かっている。"
  },
  {
    "start": 1494670,
    "end": 1502058,
    "text": "さて、これらの式はかなり大きくなりそうなので、作り上げる式をうまく視覚化する方法が欲しい。"
  },
  {
    "start": 1502144,
    "end": 1509502,
    "text": "そのために、式グラフを視覚化するちょっと怖いコードをコピーペーストしておこう。"
  },
  {
    "start": 1509636,
    "end": 1511934,
    "text": "これがそのコードで、少し説明しよう。"
  },
  {
    "start": 1512052,
    "end": 1514798,
    "text": "まず、このコードが何をするのかをお見せしよう。"
  },
  {
    "start": 1514964,
    "end": 1522466,
    "text": "基本的に、これは新しい関数draw dotを作成し、ルート・ノード上で呼び出すことで、それを視覚化する。"
  },
  {
    "start": 1522568,
    "end": 1531110,
    "text": "最終的な値であるDにドロー・ドットを呼び出すと、このようになる。"
  },
  {
    "start": 1531180,
    "end": 1539560,
    "text": "これはdであり、これはaにbを掛けたものであることがわかるだろう。解釈値にcを加えたものが出力ノードDとなる。"
  },
  {
    "start": 1540330,
    "end": 1542282,
    "text": "Dから引き出される。"
  },
  {
    "start": 1542416,
    "end": 1545322,
    "text": "私はこれを完全に詳しく説明するつもりはない。"
  },
  {
    "start": 1545456,
    "end": 1547910,
    "text": "グラフィスとそのAPIを見てみるといい。"
  },
  {
    "start": 1548070,
    "end": 1551222,
    "text": "Graphisはオープンソースのグラフ視覚化ソフトウェアです。"
  },
  {
    "start": 1551366,
    "end": 1555626,
    "text": "ここでやっているのは、Graphis APIでグラフを構築することだ。"
  },
  {
    "start": 1555818,
    "end": 1562682,
    "text": "基本的に、traceはグラフのノードとエッジをすべて列挙するヘルパー関数であることがわかる。"
  },
  {
    "start": 1562826,
    "end": 1565278,
    "text": "これは、すべてのノードとエッジの集合を構築するだけである。"
  },
  {
    "start": 1565374,
    "end": 1573038,
    "text": "その後、すべてのノードを繰り返し、ドット・ノードを使って特別なノード・オブジェクトを作成する。"
  },
  {
    "start": 1573214,
    "end": 1576734,
    "text": "ドット・ドット・エッジを使ってエッジを作る。"
  },
  {
    "start": 1576862,
    "end": 1583890,
    "text": "ただ、ここで少し厄介なのは、基本的に偽のノードを追加していることだ。"
  },
  {
    "start": 1583970,
    "end": 1587534,
    "text": "例えば、このノードはプラス・ノードのようなものだ。"
  },
  {
    "start": 1587682,
    "end": 1596742,
    "text": "私はここに特別なオペノードを作り、それに従って接続する。"
  },
  {
    "start": 1596886,
    "end": 1601590,
    "text": "もちろん、これらのノードは元のグラフの実際のノードではない。"
  },
  {
    "start": 1601750,
    "end": 1603630,
    "text": "実際には価値のあるオブジェクトではない。"
  },
  {
    "start": 1603780,
    "end": 1607130,
    "text": "ここで価値のあるものは、四角の中にあるものだけだ。"
  },
  {
    "start": 1607210,
    "end": 1610378,
    "text": "それらは実際の価値のあるもの、あるいはその表現である。"
  },
  {
    "start": 1610474,
    "end": 1613802,
    "text": "これらの音符は、このドロー・ルーチンで作成される。"
  },
  {
    "start": 1613866,
    "end": 1615314,
    "text": "いい感じだよ。"
  },
  {
    "start": 1615512,
    "end": 1619810,
    "text": "どの変数がどこにあるかがわかるように、これらのグラフにもラベルを付けてみよう。"
  },
  {
    "start": 1619960,
    "end": 1622690,
    "text": "特別なアンダースコア・ラベルを作ってみよう。"
  },
  {
    "start": 1623910,
    "end": 1629510,
    "text": "あるいは、デフォルトでlabel equals emptyにして、各ノードに保存しておこう。"
  },
  {
    "start": 1631210,
    "end": 1653200,
    "text": "そして、ここではラベルをa、ラベルをb、ラベルをcとし、特別にeイコールa×bを作り、eラベルをeとする。"
  },
  {
    "start": 1654050,
    "end": 1655418,
    "text": "ちょっとエッチなんだ。"
  },
  {
    "start": 1655594,
    "end": 1661520,
    "text": "eはeにcを足したものとなり、Dラベルはdとなる。"
  },
  {
    "start": 1662530,
    "end": 1663962,
    "text": "そう、何も変わらない。"
  },
  {
    "start": 1664036,
    "end": 1668050,
    "text": "新しいe関数とe変数を追加したところだ。"
  },
  {
    "start": 1668390,
    "end": 1674002,
    "text": "そして、これを印刷するときに、ここにラベルを印刷する。"
  },
  {
    "start": 1674136,
    "end": 1678870,
    "text": "これがパーセントSバーで、これがNラベルになる。"
  },
  {
    "start": 1681290,
    "end": 1685094,
    "text": "これで左のラベルができた。"
  },
  {
    "start": 1685132,
    "end": 1690682,
    "text": "abがeを作り、eにcを足すとdになる。"
  },
  {
    "start": 1690816,
    "end": 1694118,
    "text": "最後に、この表現をもう一段階深くしてみよう。"
  },
  {
    "start": 1694294,
    "end": 1697078,
    "text": "Dは最終的な出力ノードにはならない。"
  },
  {
    "start": 1697254,
    "end": 1702766,
    "text": "代わりに、dの後にfという新しい値オブジェクトを作る。"
  },
  {
    "start": 1702868,
    "end": 1705134,
    "text": "もうすぐ変数が足りなくなる。"
  },
  {
    "start": 1705252,
    "end": 1709920,
    "text": "Fはマイナス2.0となり、ラベルはもちろんfとなる。"
  },
  {
    "start": 1710530,
    "end": 1715262,
    "text": "とすると、l capital lがグラフの出力となる。"
  },
  {
    "start": 1715406,
    "end": 1717460,
    "text": "lはfのd倍となる。"
  },
  {
    "start": 1718710,
    "end": 1721250,
    "text": "lは負の8を出力する。"
  },
  {
    "start": 1722470,
    "end": 1728120,
    "text": "今はdだけでなくlも描く。"
  },
  {
    "start": 1730010,
    "end": 1730760,
    "text": "オーケー。"
  },
  {
    "start": 1732010,
    "end": 1735074,
    "text": "lのラベルがなぜか未定義だった。"
  },
  {
    "start": 1735122,
    "end": 1739160,
    "text": "おっと、lドットのラベルは明示的に付けなければならない。"
  },
  {
    "start": 1739690,
    "end": 1740246,
    "text": "これでよし。"
  },
  {
    "start": 1740268,
    "end": 1741622,
    "text": "lは出力である。"
  },
  {
    "start": 1741766,
    "end": 1744042,
    "text": "これまでのことを簡単に振り返ってみよう。"
  },
  {
    "start": 1744176,
    "end": 1747850,
    "text": "プラスとタイムだけで数式を組み立てることができる。"
  },
  {
    "start": 1747920,
    "end": 1751434,
    "text": "スカラー値で評価される。"
  },
  {
    "start": 1751552,
    "end": 1756314,
    "text": "このフォワード・パスで数式を構築することができる。"
  },
  {
    "start": 1756442,
    "end": 1763838,
    "text": "ここでは、abcとfという複数の入力が、lという単一の出力を生成する数式に入力されている。"
  },
  {
    "start": 1764004,
    "end": 1767090,
    "text": "これはフォワードパスを視覚化したものだ。"
  },
  {
    "start": 1767240,
    "end": 1769938,
    "text": "フォワード・パスの出力はマイナス8である。"
  },
  {
    "start": 1770024,
    "end": 1771060,
    "text": "それが価値だ。"
  },
  {
    "start": 1771590,
    "end": 1775198,
    "text": "さて、次にやりたいのは逆伝播だ。"
  },
  {
    "start": 1775374,
    "end": 1785346,
    "text": "バックプロパゲーションでは、最後から始めて、すべての中間値に沿った勾配を逆算する。"
  },
  {
    "start": 1785458,
    "end": 1794780,
    "text": "ここで計算しているのは、ひとつひとつの値について、そのノードのlに対する導関数を計算することだ。"
  },
  {
    "start": 1795390,
    "end": 1800234,
    "text": "Lのlに関する導関数は1だけである。"
  },
  {
    "start": 1800432,
    "end": 1810158,
    "text": "次に、lのfに対する微分、dに対する微分、cに対する微分、eに対する微分、bに対する微分、aに対する微分を導く。"
  },
  {
    "start": 1810324,
    "end": 1819262,
    "text": "ニューラルネットワークの設定では、基本的にこの損失関数lの、ニューラルネットワークの重みに対する導関数に非常に興味があるだろう。"
  },
  {
    "start": 1819406,
    "end": 1825746,
    "text": "もちろん、ここにはAbcとfという変数しかないが、このうちのいくつかは最終的にニューラルネットの重みを表すことになる。"
  },
  {
    "start": 1825848,
    "end": 1830174,
    "text": "そのため、これらの重みが損失関数にどのような影響を与えているかを知る必要がある。"
  },
  {
    "start": 1830312,
    "end": 1835282,
    "text": "我々は基本的に、リーフノードのいくつかに関する出力の導関数に興味がある。"
  },
  {
    "start": 1835346,
    "end": 1840754,
    "text": "これらのリーフノードはニューラルネットの重みになり、その他のリーフノードはもちろんデータそのものになる。"
  },
  {
    "start": 1840892,
    "end": 1852086,
    "text": "通常、データは固定されているため、データに対する損失関数の導関数を使用することはない。"
  },
  {
    "start": 1852208,
    "end": 1860734,
    "text": "次に、その値に対するlの導関数を保持する変数を値クラス内に作成する。"
  },
  {
    "start": 1860932,
    "end": 1863626,
    "text": "この変数をgradと呼ぶことにする。"
  },
  {
    "start": 1863818,
    "end": 1867134,
    "text": "ドット・データがあり、セルフ・ドット・グラードがある。"
  },
  {
    "start": 1867262,
    "end": 1869362,
    "text": "最初はゼロである。"
  },
  {
    "start": 1869496,
    "end": 1872834,
    "text": "ゼロは基本的に効果がないことを意味する。"
  },
  {
    "start": 1872952,
    "end": 1879442,
    "text": "初期化時には、どの値も出力に影響を与えない、影響を与えないと仮定している。"
  },
  {
    "start": 1879586,
    "end": 1879942,
    "text": "そうだろう？"
  },
  {
    "start": 1879996,
    "end": 1885240,
    "text": "勾配がゼロなら、この変数を変えても損失関数は変わらないということだからだ。"
  },
  {
    "start": 1885610,
    "end": 1888666,
    "text": "デフォルトでは、勾配はゼロであると仮定する。"
  },
  {
    "start": 1888848,
    "end": 1899594,
    "text": "そして0.0になったので、データの後にここで可視化できるようにする。"
  },
  {
    "start": 1899712,
    "end": 1904670,
    "text": "ここでgradは0、fは4、これがgradとなる。"
  },
  {
    "start": 1905650,
    "end": 1912400,
    "text": "ここでは、ゼロで初期化されたデータとグラッドの両方を表示する。"
  },
  {
    "start": 1913570,
    "end": 1917262,
    "text": "逆伝播を計算する準備が整ったところだ。"
  },
  {
    "start": 1917406,
    "end": 1924900,
    "text": "もちろん、この勾配は、先ほども述べたように、この値に対する出力（この場合はl）の導関数を表している。"
  },
  {
    "start": 1925990,
    "end": 1930334,
    "text": "これはfに対するlのdに対する導関数である。"
  },
  {
    "start": 1930472,
    "end": 1934194,
    "text": "それでは、グラデーションを塗りつぶし、実際に手動で逆伝播を行ってみよう。"
  },
  {
    "start": 1934322,
    "end": 1937206,
    "text": "グラデーションを塗りつぶし、最後から始めてみよう。"
  },
  {
    "start": 1937228,
    "end": 1938390,
    "text": "ここでも述べたように。"
  },
  {
    "start": 1938540,
    "end": 1941674,
    "text": "まず、このグラデーションを埋めることに興味がある。"
  },
  {
    "start": 1941792,
    "end": 1945098,
    "text": "lに関するlの導関数とは？"
  },
  {
    "start": 1945264,
    "end": 1951740,
    "text": "言い換えれば、もし私がlをわずかhだけ変えたら、lはどれくらい変わるのか？"
  },
  {
    "start": 1952350,
    "end": 1957086,
    "text": "hだけ変化するので比例し、したがって微分は1になる。"
  },
  {
    "start": 1957268,
    "end": 1963166,
    "text": "もちろん、これまで見てきたように、これらの数値勾配を測定したり、数値的に推定することはできる。"
  },
  {
    "start": 1963268,
    "end": 1970514,
    "text": "この式を使ってdeflol関数を作り、これをここに置く。"
  },
  {
    "start": 1970632,
    "end": 1976786,
    "text": "さて、なぜここでゲート関数を作っているかというと（笑）、ここでグローバル・スコープを汚したり混乱させたりしたくないからだ。"
  },
  {
    "start": 1976888,
    "end": 1978738,
    "text": "これはちょっとしたステージング・エリアのようなものだ。"
  },
  {
    "start": 1978824,
    "end": 1982470,
    "text": "ご存知のように、Pythonでは、これらはすべてこの関数のローカル変数になる。"
  },
  {
    "start": 1982620,
    "end": 1985526,
    "text": "グローバル・スコープは変えていない。"
  },
  {
    "start": 1985708,
    "end": 1988140,
    "text": "ここでは1人がLになる。"
  },
  {
    "start": 1989950,
    "end": 1999340,
    "text": "この式をコピー・ペーストして、たとえばaに少量のhを加える。"
  },
  {
    "start": 2000590,
    "end": 2001146,
    "text": "そうだろう？"
  },
  {
    "start": 2001248,
    "end": 2005214,
    "text": "これは、aに関するlの導関数を測定することになる。"
  },
  {
    "start": 2005412,
    "end": 2007520,
    "text": "ここがL2になる。"
  },
  {
    "start": 2008050,
    "end": 2010202,
    "text": "その派生物を印刷したい。"
  },
  {
    "start": 2010266,
    "end": 2017234,
    "text": "l 2からl 1を引いた値（lがどれだけ変化したか）を表示し、それをhで正規化する。"
  },
  {
    "start": 2017352,
    "end": 2019378,
    "text": "これが上昇気流だ。"
  },
  {
    "start": 2019544,
    "end": 2028886,
    "text": "というのも、lはバリュー・ノードだからである。"
  },
  {
    "start": 2028988,
    "end": 2035478,
    "text": "これは、aに関するlの導関数を表示するはずである。"
  },
  {
    "start": 2035644,
    "end": 2039194,
    "text": "aに対するlの導関数は？"
  },
  {
    "start": 2039312,
    "end": 2040380,
    "text": "6歳だ。"
  },
  {
    "start": 2040990,
    "end": 2041642,
    "text": "オーケー。"
  },
  {
    "start": 2041776,
    "end": 2050302,
    "text": "lをhに変えれば、ここになるのは明らかだ。"
  },
  {
    "start": 2050356,
    "end": 2058240,
    "text": "事実上、これはとても厄介に見えるが、lをhに変えると、ほら、ここでの導関数は1だ。"
  },
  {
    "start": 2060850,
    "end": 2064622,
    "text": "それが、私たちがここでやっていることの基本的なケースのようなものだ。"
  },
  {
    "start": 2064756,
    "end": 2069762,
    "text": "基本的に、ここではコンプはできない。"
  },
  {
    "start": 2069816,
    "end": 2071570,
    "text": "これが手動でのバックプロパゲーションだ。"
  },
  {
    "start": 2071910,
    "end": 2073250,
    "text": "Lドットの卒業生がそうだ。"
  },
  {
    "start": 2073320,
    "end": 2077394,
    "text": "再描画してみよう。"
  },
  {
    "start": 2077432,
    "end": 2078950,
    "text": "グラッドはLのためのものだ。"
  },
  {
    "start": 2079100,
    "end": 2081186,
    "text": "これからバックプロパゲーションを続ける。"
  },
  {
    "start": 2081298,
    "end": 2085334,
    "text": "ここで、dとfに関するlの導関数を見てみよう。"
  },
  {
    "start": 2085532,
    "end": 2087526,
    "text": "まずはdをやろう。"
  },
  {
    "start": 2087708,
    "end": 2094186,
    "text": "ここでマークダウンを作成するとして、我々が知りたいのは、基本的に、lはd×fであるということだ。"
  },
  {
    "start": 2094288,
    "end": 2099370,
    "text": "ddのDLとは何ですか？"
  },
  {
    "start": 2100190,
    "end": 2101386,
    "text": "それは何ですか？"
  },
  {
    "start": 2101568,
    "end": 2104286,
    "text": "微積分を知っていれば、lはd×fである。"
  },
  {
    "start": 2104308,
    "end": 2106010,
    "text": "DDとは何ですか？"
  },
  {
    "start": 2106090,
    "end": 2107600,
    "text": "そうだろう。"
  },
  {
    "start": 2108050,
    "end": 2111166,
    "text": "私の言うことが信じられないなら、それを導き出すこともできる。"
  },
  {
    "start": 2111188,
    "end": 2113230,
    "text": "なぜなら、その証明は極めて簡単だからだ。"
  },
  {
    "start": 2113570,
    "end": 2125774,
    "text": "微分の定義は、xのfにhを加えたものからxのfを引いたもので、hをhの極限として割ると、この種の式はゼロになる。"
  },
  {
    "start": 2125902,
    "end": 2135080,
    "text": "lをfのd倍とすると、dをh倍すれば、dにfのh倍を加えた出力が得られる。"
  },
  {
    "start": 2135770,
    "end": 2138060,
    "text": "基本的にはxのfにhを足したものだよね？"
  },
  {
    "start": 2138990,
    "end": 2143690,
    "text": "dをfで割って、hを引く。"
  },
  {
    "start": 2143840,
    "end": 2152122,
    "text": "ここで記号的に展開すると、基本的にはd×f＋h×fマイナスd×fでHを割ることになる。"
  },
  {
    "start": 2152256,
    "end": 2154986,
    "text": "そうすれば、dfマイナスdfがどのようにキャンセルされるかがわかるだろう。"
  },
  {
    "start": 2155018,
    "end": 2159758,
    "text": "とすると、h×f÷hが残り、これがfとなる。"
  },
  {
    "start": 2159924,
    "end": 2171140,
    "text": "微分定義のhがゼロになる極限では、d倍のfが得られるだけである。"
  },
  {
    "start": 2172310,
    "end": 2177800,
    "text": "対称的に、DL by d fは単にdとなる。"
  },
  {
    "start": 2178490,
    "end": 2185640,
    "text": "つまり、今見えているfのドットグラッドはdの値であり、4である。"
  },
  {
    "start": 2188670,
    "end": 2194220,
    "text": "dグラッドは単にfの値であることがわかる。"
  },
  {
    "start": 2196910,
    "end": 2200640,
    "text": "従って、fの値はマイナス2である。"
  },
  {
    "start": 2201170,
    "end": 2203390,
    "text": "手動で設定する。"
  },
  {
    "start": 2205170,
    "end": 2208960,
    "text": "このマークダウン・ノードを消して、今あるものを描き直してみよう。"
  },
  {
    "start": 2210850,
    "end": 2211646,
    "text": "オーケー。"
  },
  {
    "start": 2211828,
    "end": 2213902,
    "text": "これが正しいかどうか確認しよう。"
  },
  {
    "start": 2214036,
    "end": 2217522,
    "text": "我々は、DDによるDLはマイナス2だと考えているようだ。"
  },
  {
    "start": 2217576,
    "end": 2218820,
    "text": "ダブルチェックしよう。"
  },
  {
    "start": 2220150,
    "end": 2222210,
    "text": "さっきのプラスhは消しておこう。"
  },
  {
    "start": 2222360,
    "end": 2224834,
    "text": "今度はfに関する導関数が欲しい。"
  },
  {
    "start": 2225032,
    "end": 2228614,
    "text": "fを作ったらここに来て、ここでプラスhをしよう。"
  },
  {
    "start": 2228732,
    "end": 2231622,
    "text": "これはfに関するlの導関数を表示するはずである。"
  },
  {
    "start": 2231676,
    "end": 2233320,
    "text": "我々は4人を期待している。"
  },
  {
    "start": 2234090,
    "end": 2238070,
    "text": "ああ、これは浮動小数点のファンクネスまで4つある。"
  },
  {
    "start": 2238910,
    "end": 2244460,
    "text": "となると、DL by ddはfとなり、負の2となる。"
  },
  {
    "start": 2244990,
    "end": 2246538,
    "text": "グラッドはマイナス2。"
  },
  {
    "start": 2246704,
    "end": 2254880,
    "text": "ここでもう一度dを変更すると、dのデータプラスはhに等しくなる。"
  },
  {
    "start": 2256290,
    "end": 2263700,
    "text": "hを少し足し、Lがどのように変化したかを見て、マイナス2がプリントされることを期待する。"
  },
  {
    "start": 2264630,
    "end": 2265700,
    "text": "これでよし。"
  },
  {
    "start": 2267350,
    "end": 2272434,
    "text": "ここでやっていることは、インライン・グラデーション・チェックのようなものだと数値で確認した。"
  },
  {
    "start": 2272552,
    "end": 2279906,
    "text": "勾配チェックとは、この逆伝播を導き出し、すべての中間結果に関して導関数を得ることである。"
  },
  {
    "start": 2280018,
    "end": 2285910,
    "text": "その場合、数値勾配は小さなステップサイズを使ってそれを推定するだけである。"
  },
  {
    "start": 2286060,
    "end": 2289058,
    "text": "さて、バックプロパゲーションの核心に迫っていこう。"
  },
  {
    "start": 2289154,
    "end": 2299180,
    "text": "このノードの勾配を理解すれば、バックプロパゲーションのすべて、そしてニューラルネットのトレーニングのすべてを理解することになるからだ。"
  },
  {
    "start": 2299550,
    "end": 2309246,
    "text": "DLをDcで導出する必要がある。言い換えれば、Cに関するLの導関数である。"
  },
  {
    "start": 2309428,
    "end": 2313250,
    "text": "ここに来て、手動でバックプロパゲーションを続けている。"
  },
  {
    "start": 2313590,
    "end": 2317860,
    "text": "DcでDLを求め、さらにdeでDLを導く。"
  },
  {
    "start": 2318390,
    "end": 2319620,
    "text": "問題はここからだ。"
  },
  {
    "start": 2319990,
    "end": 2323170,
    "text": "どうやってDLをDcで導き出すのか？"
  },
  {
    "start": 2324070,
    "end": 2327206,
    "text": "私たちはDに関する微分lを知っている。"
  },
  {
    "start": 2327308,
    "end": 2332710,
    "text": "LがどのようにDに敏感であるかは知っているが、LがどのようにCに敏感なのか？"
  },
  {
    "start": 2332780,
    "end": 2336760,
    "text": "仮にcを微調整した場合、LからDにどのような影響が出るのか？"
  },
  {
    "start": 2338010,
    "end": 2340310,
    "text": "我々はDLをDcで知っている。"
  },
  {
    "start": 2341870,
    "end": 2344666,
    "text": "また、ここではcがDにどのような影響を与えるかもわかっている。"
  },
  {
    "start": 2344768,
    "end": 2356254,
    "text": "だから、ごく直感的に、CがDに与えている影響と、DがLに与えている影響が分かれば、その情報を組み合わせて、CがLにどのような影響を与えるかを考えることができるはずだ。"
  },
  {
    "start": 2356452,
    "end": 2359102,
    "text": "実際、これが私たちにできることなのだ。"
  },
  {
    "start": 2359236,
    "end": 2361982,
    "text": "特にDに集中している。"
  },
  {
    "start": 2362036,
    "end": 2366466,
    "text": "まず、Cに対するDの基本的な導関数が何であるかを見てみよう。"
  },
  {
    "start": 2366568,
    "end": 2368850,
    "text": "言い換えれば、dd by dcとは何か？"
  },
  {
    "start": 2371590,
    "end": 2375490,
    "text": "ここで、DはCのc倍＋eであることがわかる。"
  },
  {
    "start": 2375640,
    "end": 2376610,
    "text": "それが私たちが知っていることだ。"
  },
  {
    "start": 2376680,
    "end": 2379458,
    "text": "今はDcのddに興味がある。"
  },
  {
    "start": 2379634,
    "end": 2387718,
    "text": "微積分を覚えていて、Cとeをcに関して微分すると1.0になることを覚えている。"
  },
  {
    "start": 2387884,
    "end": 2399786,
    "text": "hがゼロになるにつれて、微分の定義として、XのfにHを足したものからXのfをHで割ったものを引くことができるからだ。"
  },
  {
    "start": 2399968,
    "end": 2406494,
    "text": "だから、ここではCとそのBへの影響に注目し、基本的にはX＋Hのfを行うことができる。"
  },
  {
    "start": 2406532,
    "end": 2410734,
    "text": "CはH＋Eでインクリメントされる。"
  },
  {
    "start": 2410932,
    "end": 2418162,
    "text": "これが関数の最初の評価で、CにEを加え、Hを除算する。"
  },
  {
    "start": 2418296,
    "end": 2419954,
    "text": "これは何なんだ？"
  },
  {
    "start": 2420152,
    "end": 2421442,
    "text": "ただ、これを拡大解釈しただけだ。"
  },
  {
    "start": 2421496,
    "end": 2426338,
    "text": "これは、CプラスHプラスeマイナスCマイナスeで、Hを割ることになる。"
  },
  {
    "start": 2426424,
    "end": 2430306,
    "text": "CマイナスC、eマイナスeがどのようにキャンセルされるかがわかるだろう。"
  },
  {
    "start": 2430338,
    "end": 2432840,
    "text": "H以上のhが残り、これは1.0である。"
  },
  {
    "start": 2433530,
    "end": 2442220,
    "text": "ということは、対称性によって、deのdも同様に1.0となる。"
  },
  {
    "start": 2442910,
    "end": 2446266,
    "text": "基本的に和の微分式は非常に単純である。"
  },
  {
    "start": 2446368,
    "end": 2448102,
    "text": "これが局所微分である。"
  },
  {
    "start": 2448246,
    "end": 2453522,
    "text": "私はこれを局所微分と呼んでいる。なぜなら、このグラフの最後に最終的な出力値があるからだ。"
  },
  {
    "start": 2453606,
    "end": 2457914,
    "text": "これは小さなプラス・ノードだ。"
  },
  {
    "start": 2458042,
    "end": 2463486,
    "text": "小さなプラス・ノードは、それが埋め込まれているグラフの残りの部分については何も知らない。"
  },
  {
    "start": 2463588,
    "end": 2469010,
    "text": "それが知っているのは、cとeを足してdを作ったということだけだ。"
  },
  {
    "start": 2469160,
    "end": 2476018,
    "text": "このプラスノードは、Dに対するcの局所的な影響、つまりcに対するdの微分も知っている。"
  },
  {
    "start": 2476104,
    "end": 2479718,
    "text": "また、eに関するDの導関数も知っている。"
  },
  {
    "start": 2479884,
    "end": 2481254,
    "text": "それは私たちが望んでいることではない。"
  },
  {
    "start": 2481292,
    "end": 2482706,
    "text": "それは単なる局所的な派生物だ。"
  },
  {
    "start": 2482818,
    "end": 2489142,
    "text": "私たちが実際に望んでいるのは、DcによるDLであり、Lはその一歩手前にある。"
  },
  {
    "start": 2489276,
    "end": 2494406,
    "text": "一般的なケースでは、この小さなプラス記号は、巨大なグラフのように埋め込むことができる。"
  },
  {
    "start": 2494598,
    "end": 2500746,
    "text": "LがDにどのような影響を与えるか、そしてcとeがDにどのような影響を与えるか。"
  },
  {
    "start": 2500848,
    "end": 2504186,
    "text": "DCによるDLに到達するために、どのように情報をまとめるのか？"
  },
  {
    "start": 2504298,
    "end": 2507546,
    "text": "答えはもちろん、微積分の連鎖法則である。"
  },
  {
    "start": 2507738,
    "end": 2515550,
    "text": "というわけで、ウィキペディアからチェーンルールを引っ張り出してきた。"
  },
  {
    "start": 2515630,
    "end": 2521486,
    "text": "連鎖法則 ウィキペディアは時に非常に混乱することがあり、微積分も非常に混乱することがある。"
  },
  {
    "start": 2521518,
    "end": 2527910,
    "text": "私がチェーンルールを学んだのはこの方法だった。"
  },
  {
    "start": 2528060,
    "end": 2529426,
    "text": "ただ複雑なんだ。"
  },
  {
    "start": 2529538,
    "end": 2531720,
    "text": "私はこの表現の方が好きだ。"
  },
  {
    "start": 2532970,
    "end": 2542250,
    "text": "変数zが変数yに依存し、それ自体が変数xに依存する場合、zは明らかに中間変数yを通してxにも依存する。"
  },
  {
    "start": 2542400,
    "end": 2553310,
    "text": "この場合、連鎖法則は、DzをDxで求めたいなら、Dzをdyで求め、dyをdxで掛けると表現される。"
  },
  {
    "start": 2553650,
    "end": 2562438,
    "text": "チェーン・ルールは、基本的に、これらのデリバティブをどのように正しく連鎖させるかを示している。"
  },
  {
    "start": 2562554,
    "end": 2571010,
    "text": "を関数合成で微分するには、これらの微分の掛け算を適用しなければならない。"
  },
  {
    "start": 2571670,
    "end": 2574530,
    "text": "というのが、チェーンルールが私たちに伝えていることなのだ。"
  },
  {
    "start": 2574680,
    "end": 2579054,
    "text": "直感的な説明もあって、ちょっとかわいい。"
  },
  {
    "start": 2579182,
    "end": 2592378,
    "text": "連鎖法則によれば、yに対するzの瞬間変化率とxに対するyの瞬間変化率を知ることで、xに対するzの瞬間変化率をこれら2つの変化率の積、つまり単純に積として計算することができる。"
  },
  {
    "start": 2592544,
    "end": 2594234,
    "text": "これがいい。"
  },
  {
    "start": 2594352,
    "end": 2604894,
    "text": "自動車が自転車の2倍の速さで走り、自転車が歩く人間の4倍の速さだとすると、自動車は人間の2×4、8倍の速さで走ることになる。"
  },
  {
    "start": 2605092,
    "end": 2610670,
    "text": "ということは、ある意味、正しいのは掛け算だということがはっきりした。"
  },
  {
    "start": 2611010,
    "end": 2616402,
    "text": "自動車は自転車の2倍速く、自転車は人間の4倍速い。"
  },
  {
    "start": 2616536,
    "end": 2619940,
    "text": "車は男の8倍の速さになる。"
  },
  {
    "start": 2620310,
    "end": 2626178,
    "text": "というわけで、これらの中間的な変化率を取って、掛け合わせることができる。"
  },
  {
    "start": 2626344,
    "end": 2630098,
    "text": "これは直感的に連鎖法則を正当化する。"
  },
  {
    "start": 2630194,
    "end": 2631442,
    "text": "チェーンルールを見てみよう。"
  },
  {
    "start": 2631506,
    "end": 2638570,
    "text": "つまり、DCのDLという非常にシンプルなレシピがあるということだ。"
  },
  {
    "start": 2639550,
    "end": 2650522,
    "text": "今のところわかっているのは、私たちが望んでいること、そしてDがLに与える影響についてだ。"
  },
  {
    "start": 2650576,
    "end": 2657310,
    "text": "DDに対するLの微分であるddによって、DLが負の2であることがわかる。"
  },
  {
    "start": 2657460,
    "end": 2663630,
    "text": "さて、ここで行った局所的な推論のおかげで、我々はDCによってddを知っている。"
  },
  {
    "start": 2664470,
    "end": 2666306,
    "text": "CがDに与える影響は？"
  },
  {
    "start": 2666408,
    "end": 2669182,
    "text": "特に、これはプラスノードだ。"
  },
  {
    "start": 2669246,
    "end": 2671714,
    "text": "局所導関数は単純に1.0である。"
  },
  {
    "start": 2671752,
    "end": 2672820,
    "text": "とてもシンプルだ。"
  },
  {
    "start": 2673270,
    "end": 2691446,
    "text": "したがって、連鎖法則によれば、この中間変数を経由するDL by DCは、単にDL by dd x dd by dCとなる。"
  },
  {
    "start": 2691638,
    "end": 2693078,
    "text": "それがチェーンルールだ。"
  },
  {
    "start": 2693254,
    "end": 2702990,
    "text": "これは、zがRL、yがRD、xがRCであることを除けば、ここで起こっていることと同じである。"
  },
  {
    "start": 2703730,
    "end": 2706000,
    "text": "文字通り、これを掛け合わせるだけだ。"
  },
  {
    "start": 2706690,
    "end": 2719140,
    "text": "というのも、dd by dcのような局所導関数は1回だけなので、基本的にはDL by ddをコピーするだけだからだ。"
  },
  {
    "start": 2720390,
    "end": 2725170,
    "text": "DDのDLはマイナス2だから、DCのDLは？"
  },
  {
    "start": 2725910,
    "end": 2731206,
    "text": "まあ、これは局所的な勾配で、DLの1.0倍のddで、マイナス2だ。"
  },
  {
    "start": 2731388,
    "end": 2740666,
    "text": "文字通り、プラス・ノードがすることは、そのように見ることができる。プラス・ノードの局所導関数は1つだけなので、文字通り、勾配をルーティングするだけだ。"
  },
  {
    "start": 2740768,
    "end": 2749798,
    "text": "つまり、チェーンルールでは、DDの1回分のDLはddの1回分のDLということになる。"
  },
  {
    "start": 2749894,
    "end": 2755040,
    "text": "この場合、微分はcとeの両方にルーティングされる。"
  },
  {
    "start": 2755410,
    "end": 2767940,
    "text": "基本的には、eのドットグラッド、あるいはcから始めよう。"
  },
  {
    "start": 2768470,
    "end": 2772674,
    "text": "同様に、対称性により、e dot gradは負の2となる。"
  },
  {
    "start": 2772792,
    "end": 2774046,
    "text": "それが主張だ。"
  },
  {
    "start": 2774238,
    "end": 2778370,
    "text": "それを設定し、再描画することができる。"
  },
  {
    "start": 2779350,
    "end": 2781942,
    "text": "負2、負2をどのように割り当てているかがわかるだろう。"
  },
  {
    "start": 2782076,
    "end": 2791910,
    "text": "このバックプロパゲート信号は、すべての中間ノードに対するlの導関数が何であるかというような情報を運んでいる。"
  },
  {
    "start": 2791990,
    "end": 2798540,
    "text": "プラス・ノードは単純にすべてのリーフ・ノードに派生物を分配する。"
  },
  {
    "start": 2799070,
    "end": 2800134,
    "text": "これが主張だ。"
  },
  {
    "start": 2800182,
    "end": 2801706,
    "text": "では、検証してみよう。"
  },
  {
    "start": 2801888,
    "end": 2804620,
    "text": "さっきのhを外しておこう。"
  },
  {
    "start": 2805070,
    "end": 2807758,
    "text": "その代わりに、cをインクリメントする。"
  },
  {
    "start": 2807844,
    "end": 2810510,
    "text": "Cドットのデータはhだけインクリメントされる。"
  },
  {
    "start": 2810660,
    "end": 2813460,
    "text": "これを実行すると、マイナス2が表示される。"
  },
  {
    "start": 2814310,
    "end": 2815460,
    "text": "マイナス2だ。"
  },
  {
    "start": 2815990,
    "end": 2817780,
    "text": "もちろん、e."
  },
  {
    "start": 2818870,
    "end": 2822740,
    "text": "e ドット データに h を加え、マイナス２が見えることを期待しています。"
  },
  {
    "start": 2823110,
    "end": 2823860,
    "text": "シンプルだ。"
  },
  {
    "start": 2827210,
    "end": 2830790,
    "text": "これらはこれらの内部ノードの導関数である。"
  },
  {
    "start": 2831370,
    "end": 2835000,
    "text": "さて、もう一度逆から再帰してみよう。"
  },
  {
    "start": 2835450,
    "end": 2837858,
    "text": "再びチェーンルールを適用する。"
  },
  {
    "start": 2837954,
    "end": 2838774,
    "text": "さあ、行くぞ"
  },
  {
    "start": 2838812,
    "end": 2840482,
    "text": "チェーン・ルールの2つ目の応用である。"
  },
  {
    "start": 2840546,
    "end": 2842694,
    "text": "これをグラフ全体に適用する。"
  },
  {
    "start": 2842742,
    "end": 2845238,
    "text": "たまたま、あと1ノードしか残っていない。"
  },
  {
    "start": 2845414,
    "end": 2850798,
    "text": "今計算したように、DeのDLはマイナス2である。"
  },
  {
    "start": 2850884,
    "end": 2852080,
    "text": "我々はそれを知っている。"
  },
  {
    "start": 2852530,
    "end": 2855120,
    "text": "Eに関するlの導関数がわかる。"
  },
  {
    "start": 2856690,
    "end": 2862480,
    "text": "今、我々はダのDLを望んでいるよね？"
  },
  {
    "start": 2862790,
    "end": 2872110,
    "text": "連鎖法則に従えば、ローカル勾配のマイナス2倍がDeによってDLされることになる。"
  },
  {
    "start": 2872270,
    "end": 2873854,
    "text": "ローカル・グラデーションとは何か？"
  },
  {
    "start": 2873982,
    "end": 2876786,
    "text": "基本的にはデ・バイ・ダ。"
  },
  {
    "start": 2876978,
    "end": 2878840,
    "text": "それを見なければならない。"
  },
  {
    "start": 2879930,
    "end": 2888360,
    "text": "私は巨大なグラフの中の小さなノードであり、aをbに変換してeを生成したことしか知らない。"
  },
  {
    "start": 2889050,
    "end": 2892678,
    "text": "では、\"de by da \"と \"de by db \"とは何か？"
  },
  {
    "start": 2892774,
    "end": 2894586,
    "text": "私がなんとなく知っているのはそれだけだ。"
  },
  {
    "start": 2894608,
    "end": 2896010,
    "text": "これが私の地元のグラデーションだ。"
  },
  {
    "start": 2897070,
    "end": 2902990,
    "text": "aはbの倍だから、daのdeは？"
  },
  {
    "start": 2904050,
    "end": 2906590,
    "text": "もちろん、ここでそれをやったばかりだ。"
  },
  {
    "start": 2906660,
    "end": 2914578,
    "text": "回があったので、再ドライブはしませんが、Aに関して差別化したいのであれば、Bが出るだけです。"
  },
  {
    "start": 2914664,
    "end": 2919460,
    "text": "右はbの値で、この場合はマイナス3.0。"
  },
  {
    "start": 2921030,
    "end": 2924770,
    "text": "基本的にはDAがDLしている。"
  },
  {
    "start": 2925110,
    "end": 2926566,
    "text": "じゃあ、ちゃんとやらせてもらうよ。"
  },
  {
    "start": 2926588,
    "end": 2937080,
    "text": "ここではドットのグラッドがあり、連鎖法則を適用している。"
  },
  {
    "start": 2937550,
    "end": 2939370,
    "text": "daのdeとは？"
  },
  {
    "start": 2939710,
    "end": 2943180,
    "text": "これはbの値で、マイナス3である。"
  },
  {
    "start": 2944910,
    "end": 2945900,
    "text": "それだけだ。"
  },
  {
    "start": 2947790,
    "end": 2963378,
    "text": "ということは、bのドット・グラッドは再びdeによってDLされ、これはマイナス2である。同じように、dbによってdeされるのはaの値であり、これは2.0である。"
  },
  {
    "start": 2963544,
    "end": 2965060,
    "text": "それがaの価値だ。"
  },
  {
    "start": 2965670,
    "end": 2968638,
    "text": "これが私たちの主張するデリバティブである。"
  },
  {
    "start": 2968814,
    "end": 2971918,
    "text": "描き直そう。"
  },
  {
    "start": 2972094,
    "end": 2984840,
    "text": "というのも、これはマイナス2のマイナス3倍であり、bはマイナス4のマイナス2倍、つまりマイナス4だからである。"
  },
  {
    "start": 2985370,
    "end": 2986818,
    "text": "それが私たちの主張だ。"
  },
  {
    "start": 2986914,
    "end": 2989020,
    "text": "これを削除して、検証してみよう。"
  },
  {
    "start": 2990510,
    "end": 2995820,
    "text": "ここにドットデータ・プラス・イコールhがある。"
  },
  {
    "start": 2997470,
    "end": 3004220,
    "text": "ドットのグラードが6であるという主張である。"
  },
  {
    "start": 3004830,
    "end": 3008638,
    "text": "この場合、bドット・データ・プラスはhに等しい。"
  },
  {
    "start": 3008804,
    "end": 3017982,
    "text": "hでbをナディングし、何が起こるかを見て、我々はそれがマイナス4であると主張する。"
  },
  {
    "start": 3018046,
    "end": 3019970,
    "text": "またしてもフロートの異様さ。"
  },
  {
    "start": 3021830,
    "end": 3024354,
    "text": "それだけだ。"
  },
  {
    "start": 3024552,
    "end": 3031250,
    "text": "これが、ここからすべてのリーフノードへの手動によるバックプロパゲーションだ。"
  },
  {
    "start": 3031330,
    "end": 3033206,
    "text": "私たちはそれを少しずつやってきた。"
  },
  {
    "start": 3033308,
    "end": 3039826,
    "text": "本当にやったことは、ご覧のように、すべてのノードを1つずつ繰り返し、局所的にチェーンルールを適用したことだけだ。"
  },
  {
    "start": 3039938,
    "end": 3044118,
    "text": "我々は常に、この小さな出力に対するlの導関数が何であるかを知っている。"
  },
  {
    "start": 3044214,
    "end": 3046406,
    "text": "次に、このアウトプットがどのように作られたかを見てみよう。"
  },
  {
    "start": 3046518,
    "end": 3048886,
    "text": "この出力は、何らかの操作によって生み出されたものだ。"
  },
  {
    "start": 3048998,
    "end": 3052482,
    "text": "この操作の子ノードへのポインタがある。"
  },
  {
    "start": 3052646,
    "end": 3059406,
    "text": "だから、この小さな操作では、局所導関数が何であるかを知り、それを導関数に乗算するだけである。"
  },
  {
    "start": 3059588,
    "end": 3064090,
    "text": "局所導関数を再帰的に掛け合わせる。"
  },
  {
    "start": 3064170,
    "end": 3065594,
    "text": "それがバックプロパゲーションだ。"
  },
  {
    "start": 3065652,
    "end": 3070366,
    "text": "これは、連鎖法則を再帰的に適用しているだけである。"
  },
  {
    "start": 3070558,
    "end": 3072334,
    "text": "このパワーを実際に見てみよう。"
  },
  {
    "start": 3072382,
    "end": 3079506,
    "text": "ごく簡単に説明すると、私たちがやろうとしているのは、インプットをナッジしてLOWを上げようとすることだ。"
  },
  {
    "start": 3079688,
    "end": 3084022,
    "text": "特に、私たちがしていることは、ドットデータが欲しいので、それを変更することです。"
  },
  {
    "start": 3084156,
    "end": 3088722,
    "text": "lを上げたいなら、グラデーションの方向に進めばいいということだ。"
  },
  {
    "start": 3088866,
    "end": 3094826,
    "text": "aは、勾配の方向に、ある小さなステップ量のように増加するはずである。"
  },
  {
    "start": 3094928,
    "end": 3096298,
    "text": "これがステップサイズである。"
  },
  {
    "start": 3096464,
    "end": 3105998,
    "text": "baだけでなく、bも、cも、fも欲しい。"
  },
  {
    "start": 3106164,
    "end": 3110478,
    "text": "これらのノードはリーフノードで、通常私たちがコントロールできる。"
  },
  {
    "start": 3110644,
    "end": 3115602,
    "text": "勾配の方向にナッジすれば、lにポジティブな影響を与えると予想される。"
  },
  {
    "start": 3115736,
    "end": 3119646,
    "text": "私たちはLがポジティブに上昇すると予想している。"
  },
  {
    "start": 3119838,
    "end": 3121742,
    "text": "ネガティブになることはないはずだ。"
  },
  {
    "start": 3121806,
    "end": 3125300,
    "text": "マイナス6とか、そのくらいまで上がるはずだ。"
  },
  {
    "start": 3126230,
    "end": 3129498,
    "text": "正確に言うのは難しいし、フォワードパスを再実行しなければならない。"
  },
  {
    "start": 3129534,
    "end": 3133560,
    "text": "ここでやらせてくれ。"
  },
  {
    "start": 3136570,
    "end": 3139970,
    "text": "これがフォワードパスとなり、Fは変化しない。"
  },
  {
    "start": 3140050,
    "end": 3141754,
    "text": "これが事実上のフォワードパスである。"
  },
  {
    "start": 3141872,
    "end": 3151054,
    "text": "今、l個のドット・データを印刷すると、すべての値、すべての入力を方向勾配でナッジしたため、負のlが少なくなると予想される。"
  },
  {
    "start": 3151092,
    "end": 3152686,
    "text": "もっと上がると予想している。"
  },
  {
    "start": 3152868,
    "end": 3154894,
    "text": "もしかしたらマイナス6くらいかもしれない。"
  },
  {
    "start": 3154932,
    "end": 3156000,
    "text": "どうなるか見てみよう。"
  },
  {
    "start": 3156610,
    "end": 3158094,
    "text": "よし、マイナス7点だ。"
  },
  {
    "start": 3158292,
    "end": 3163774,
    "text": "これは基本的に、最終的に実行される最適化の1つのステップである。"
  },
  {
    "start": 3163892,
    "end": 3169166,
    "text": "この勾配は、最終的な結果を左右する方法を知っている私たちに力を与えてくれる。"
  },
  {
    "start": 3169278,
    "end": 3171714,
    "text": "これはネオレッツのトレーニングにも大いに役立つだろう。"
  },
  {
    "start": 3171752,
    "end": 3172674,
    "text": "そうだろう。"
  },
  {
    "start": 3172872,
    "end": 3177746,
    "text": "では、手動バックプロパゲーションの例をもうひとつ。"
  },
  {
    "start": 3177858,
    "end": 3185030,
    "text": "もう少し複雑で有用な例を使って、ニューロンをバックプロパゲートしてみよう。"
  },
  {
    "start": 3185370,
    "end": 3189698,
    "text": "最終的にはニューラルネットワークを構築したい。"
  },
  {
    "start": 3189794,
    "end": 3193002,
    "text": "最も単純なケースでは、多層パーセプトロンと呼ばれるものだ。"
  },
  {
    "start": 3193056,
    "end": 3199882,
    "text": "これは2層のニューラルネットで、ニューロンで構成された隠れ層があり、これらのニューロンは互いに完全に接続されている。"
  },
  {
    "start": 3200016,
    "end": 3205950,
    "text": "さて、生物学的には、ニューロンは非常に複雑な装置だが、われわれには非常に単純な数学的モデルがある。"
  },
  {
    "start": 3206100,
    "end": 3209306,
    "text": "これはニューロンの非常に単純な数学的モデルである。"
  },
  {
    "start": 3209418,
    "end": 3215362,
    "text": "いくつかの入力軸があり、そこに重みを持つシナプスがある。"
  },
  {
    "start": 3215496,
    "end": 3224510,
    "text": "シナプスはこのニューロンへの入力と乗法的に相互作用する。"
  },
  {
    "start": 3224590,
    "end": 3234774,
    "text": "このニューロンの細胞体に流れ込むのはxのw倍だが、複数の入力があるので、細胞体にはxのw倍がたくさん流れ込む。"
  },
  {
    "start": 3234972,
    "end": 3237762,
    "text": "細胞体にも偏りがある。"
  },
  {
    "start": 3237906,
    "end": 3243526,
    "text": "これは、この神経細胞が生まれながらにして持っている幸福の引き金のようなものだ。"
  },
  {
    "start": 3243638,
    "end": 3248294,
    "text": "このバイアスによって、入力に関係なく、トリガーハッピーになったり、そうでなくなったりする。"
  },
  {
    "start": 3248422,
    "end": 3256400,
    "text": "基本的には、すべての入力のw×xを取り、バイアスを加え、それを活性化関数に通す。"
  },
  {
    "start": 3256930,
    "end": 3263138,
    "text": "この活性化関数は通常、シグモイドやten hのような、ある種のつぶし関数である。"
  },
  {
    "start": 3263224,
    "end": 3266162,
    "text": "例として、10個のhを使う。"
  },
  {
    "start": 3266216,
    "end": 3276418,
    "text": "この例では、numpyにはnp ten hがあるので、これを範囲指定して呼び出すことで、プロットすることができる。"
  },
  {
    "start": 3276584,
    "end": 3277974,
    "text": "これは10時間機能である。"
  },
  {
    "start": 3278092,
    "end": 3283846,
    "text": "入ってきた入力がy座標で潰れているのがわかるだろう。"
  },
  {
    "start": 3283948,
    "end": 3288022,
    "text": "をゼロにすると、ちょうどゼロになる。"
  },
  {
    "start": 3288156,
    "end": 3295340,
    "text": "入力がプラスになればなるほど、関数は1までしか上がらず、その後はプラトーになることがわかるだろう。"
  },
  {
    "start": 3295710,
    "end": 3300842,
    "text": "だから、非常にポジティブな入力があった場合は、スムーズに1回を上限とする。"
  },
  {
    "start": 3300976,
    "end": 3304254,
    "text": "マイナス側では、スムーズにマイナス1までキャップするつもりだ。"
  },
  {
    "start": 3304452,
    "end": 3309694,
    "text": "これは10時間で、これがスカッシュ関数、つまり活性化関数だ。"
  },
  {
    "start": 3309812,
    "end": 3317730,
    "text": "このニューロンから出てくるのは、重みと入力のドット積に活性化関数を適用したものだ。"
  },
  {
    "start": 3318070,
    "end": 3320740,
    "text": "ひとつ書き出してみよう。"
  },
  {
    "start": 3322470,
    "end": 3329266,
    "text": "あまりタイプしたくないので、コピペしますが。"
  },
  {
    "start": 3329288,
    "end": 3332502,
    "text": "さて、ここでx1とx2を入力する。"
  },
  {
    "start": 3332556,
    "end": 3333826,
    "text": "これは2次元のニューロンである。"
  },
  {
    "start": 3333858,
    "end": 3335560,
    "text": "つのインプットが入ってくる。"
  },
  {
    "start": 3335930,
    "end": 3340586,
    "text": "これらは、このニューロンの重みw 1、w 2と考えられる。"
  },
  {
    "start": 3340688,
    "end": 3344886,
    "text": "これらの重みはまた、各入力に対するシナプスの強さである。"
  },
  {
    "start": 3345078,
    "end": 3348220,
    "text": "これはニューロンbのバイアスである。"
  },
  {
    "start": 3349150,
    "end": 3357534,
    "text": "このモデルによれば、x1とw1を掛け合わせ、x2とw2を掛け合わせる必要がある。"
  },
  {
    "start": 3357732,
    "end": 3361246,
    "text": "その上でバイアスを加える必要がある。"
  },
  {
    "start": 3361428,
    "end": 3363086,
    "text": "ここでちょっと面倒なことになる。"
  },
  {
    "start": 3363108,
    "end": 3367794,
    "text": "私たちがやろうとしているのは、x1、w1＋x2、w2＋bだ。"
  },
  {
    "start": 3367912,
    "end": 3369714,
    "text": "それがここで倍増した。"
  },
  {
    "start": 3369912,
    "end": 3375114,
    "text": "ただし、すべての中間ノードへのポインターを持つように、小さなステップを踏んでいる。"
  },
  {
    "start": 3375182,
    "end": 3379362,
    "text": "x1、w1の変数、x2、w2の変数がある。"
  },
  {
    "start": 3379426,
    "end": 3381000,
    "text": "ラベルも貼っている。"
  },
  {
    "start": 3381690,
    "end": 3390314,
    "text": "nは今のところ、活性化関数を除いたセル本体の生の活性化である。"
  },
  {
    "start": 3390512,
    "end": 3393514,
    "text": "基本的にはこれで十分だろう。"
  },
  {
    "start": 3393552,
    "end": 3403050,
    "text": "nのドロー・ドットでx1倍w1倍、x2倍w2倍が加算される。"
  },
  {
    "start": 3403210,
    "end": 3409146,
    "text": "その上にバイアスが加算され、このnがこの合計となる。"
  },
  {
    "start": 3409338,
    "end": 3412254,
    "text": "これから活性化関数を使う。"
  },
  {
    "start": 3412452,
    "end": 3416382,
    "text": "例えば、tan hを使って出力を出すとしよう。"
  },
  {
    "start": 3416526,
    "end": 3424994,
    "text": "ここでやりたいことは、出力することだ。"
  },
  {
    "start": 3425192,
    "end": 3428118,
    "text": "オーケー、でもまだ10Hは書いていないよ。"
  },
  {
    "start": 3428284,
    "end": 3435846,
    "text": "ここでもうひとつten h関数を実装する必要があるのは、ten hが双曲線関数だからである。"
  },
  {
    "start": 3435948,
    "end": 3438390,
    "text": "今のところ、プラスとマイナスしか実装していない。"
  },
  {
    "start": 3438460,
    "end": 3441786,
    "text": "プラスとタイムだけでは10Hは作れない。"
  },
  {
    "start": 3441968,
    "end": 3454350,
    "text": "指数関数も必要なので、10 hはこのような式で、このどちらかを使うことができ、指数関数が関係していることがわかる。"
  },
  {
    "start": 3454420,
    "end": 3458846,
    "text": "まだ10時間の生産はできそうにない。"
  },
  {
    "start": 3459028,
    "end": 3467154,
    "text": "ここで1つのオプションとして、実際に指数を実装することができるよね？"
  },
  {
    "start": 3467272,
    "end": 3472258,
    "text": "値の10hの代わりに値のxを返すことができる。"
  },
  {
    "start": 3472424,
    "end": 3483330,
    "text": "というのも、足し算も掛け算も知っているからだ。"
  },
  {
    "start": 3483410,
    "end": 3486758,
    "text": "もしXの作り方を知っていれば、10個のHを作ることができるだろう。"
  },
  {
    "start": 3486924,
    "end": 3498122,
    "text": "この例では、このバリュー・オブジェクトに必ずしも最もアトミックなピースを入れる必要はないことを示したかった。"
  },
  {
    "start": 3498256,
    "end": 3504342,
    "text": "実際、私たちは任意の抽象度で関数を作ることができる。"
  },
  {
    "start": 3504406,
    "end": 3508526,
    "text": "それらは複雑な機能であることもあるが、プラスのような非常に単純な機能であることもある。"
  },
  {
    "start": 3508628,
    "end": 3509998,
    "text": "それは完全に私たち次第だ。"
  },
  {
    "start": 3510084,
    "end": 3514062,
    "text": "重要なのは、どの関数を通して微分する方法を知っているかということだけだ。"
  },
  {
    "start": 3514196,
    "end": 3516682,
    "text": "いくつかのインプットを受けて、アウトプットを出す。"
  },
  {
    "start": 3516746,
    "end": 3522878,
    "text": "重要なのは、局所導関数の作り方さえ知っていれば、どんな複雑な関数でも構わないということだ。"
  },
  {
    "start": 3522974,
    "end": 3527186,
    "text": "インプットがアウトプットにどのような影響を与えるかの局所微分を知っていれば、それだけで十分だ。"
  },
  {
    "start": 3527288,
    "end": 3533250,
    "text": "私たちはこの式をすべてクラスタ化し、原子的な断片に分解するつもりはない。"
  },
  {
    "start": 3533330,
    "end": 3535414,
    "text": "tan hを直接実装するだけだ。"
  },
  {
    "start": 3535532,
    "end": 3536760,
    "text": "そうしよう"
  },
  {
    "start": 3537210,
    "end": 3545242,
    "text": "深さtan hとなり、outの値となり、ここでこの式が必要となる。"
  },
  {
    "start": 3545296,
    "end": 3551630,
    "text": "実際にコピーペーストしてみよう。"
  },
  {
    "start": 3554130,
    "end": 3557018,
    "text": "塩のシータであるnを掴んでみよう。"
  },
  {
    "start": 3557194,
    "end": 3570386,
    "text": "ということは、これは2、いや、nマイナスnマイナス1が2、nプラス1を上回った場合の10H計算のxだと思う。"
  },
  {
    "start": 3570568,
    "end": 3574740,
    "text": "これをxと呼んで、正確に一致させることができるかもしれない。"
  },
  {
    "start": 3575670,
    "end": 3582402,
    "text": "さて、次はこのノードのtと子になる。"
  },
  {
    "start": 3582466,
    "end": 3585906,
    "text": "子供は一人で、それをタプルに包んでいる。"
  },
  {
    "start": 3585938,
    "end": 3588140,
    "text": "これは1つのオブジェクト、selfだけのタプルである。"
  },
  {
    "start": 3588510,
    "end": 3594060,
    "text": "ここでは、この操作の名前をten hとし、それを返すことにする。"
  },
  {
    "start": 3596270,
    "end": 3610800,
    "text": "さて、これでten hの値が実装されたはずだ。ここで下にスクロールして、実際にn ten hを実行すると、nのten h出力が返される。"
  },
  {
    "start": 3611250,
    "end": 3614610,
    "text": "これで、nではなくoの点を描くことができるはずだ。"
  },
  {
    "start": 3614760,
    "end": 3616340,
    "text": "どうだったか見てみよう。"
  },
  {
    "start": 3618630,
    "end": 3623810,
    "text": "このアウトプットを出すのに10時間かかった。"
  },
  {
    "start": 3624150,
    "end": 3631590,
    "text": "今、10時間というのは、この小さなマイクログラッドでサポートされているノードのようなものだ。"
  },
  {
    "start": 3633050,
    "end": 3638246,
    "text": "hの導関数さえわかっていれば、それを逆伝播することができる。"
  },
  {
    "start": 3638348,
    "end": 3640166,
    "text": "では、この10時間を実際に見てみよう。"
  },
  {
    "start": 3640278,
    "end": 3644522,
    "text": "今のところ、入力がかなり少ないので、あまりつぶれていない。"
  },
  {
    "start": 3644656,
    "end": 3656814,
    "text": "バイアスを例えば8まで増やした場合、現在10Hに流れているのは2であり、10Hはそれをゼロに押し込んでいることがわかる。"
  },
  {
    "start": 3656932,
    "end": 3663262,
    "text": "この10時間はすでに尻尾を巻いており、順調に1時間まで上昇し、そこでプラトーとなる。"
  },
  {
    "start": 3663396,
    "end": 3665838,
    "text": "さて、それではちょっと変わったことをしようと思う。"
  },
  {
    "start": 3665934,
    "end": 3671646,
    "text": "このバイアスを8からこの数字、6.88などに変えてみる。"
  },
  {
    "start": 3671838,
    "end": 3676830,
    "text": "これからバックプロパゲーションを始めるからだ。"
  },
  {
    "start": 3676990,
    "end": 3680134,
    "text": "いい数字が出るようにしたい。"
  },
  {
    "start": 3680252,
    "end": 3681734,
    "text": "とてもクレイジーな数字というわけではない。"
  },
  {
    "start": 3681772,
    "end": 3684646,
    "text": "頭の中でなんとなく理解できる素敵な数字だ。"
  },
  {
    "start": 3684748,
    "end": 3686738,
    "text": "oのラベルも付け加えさせてください。"
  },
  {
    "start": 3686914,
    "end": 3689080,
    "text": "Oとは、ここではアウトプットの略である。"
  },
  {
    "start": 3690010,
    "end": 3691366,
    "text": "それがRだ。"
  },
  {
    "start": 3691548,
    "end": 3695962,
    "text": "つまり、ゼロ8が10に流れ込み、hが出て、ゼロ7となる。"
  },
  {
    "start": 3696096,
    "end": 3699850,
    "text": "これから逆伝播を行い、すべてのグラデーションを埋めていく。"
  },
  {
    "start": 3700190,
    "end": 3706094,
    "text": "ここでのすべての入力に対するOの微分は何ですか？"
  },
  {
    "start": 3706212,
    "end": 3717914,
    "text": "もちろん、典型的なニューラルネットワークの設定において、私たちが最も気にするのは、重み、特にw 2とw 1に関するこれらのニューロンの微分である。"
  },
  {
    "start": 3717962,
    "end": 3725970,
    "text": "最適化の一部であり、もうひとつ覚えておかなければならないのは、ここではニューロンがひとつしかないが、ニューラルネットでは通常、多くのニューロンがあり、それらがつながっているということだ。"
  },
  {
    "start": 3727110,
    "end": 3730594,
    "text": "これは小さなニューロンのひとつに過ぎず、もっと大きなプズルの一部でしかない。"
  },
  {
    "start": 3730642,
    "end": 3734502,
    "text": "最終的には、ニューラルネットの精度を測る損失関数がある。"
  },
  {
    "start": 3734556,
    "end": 3738360,
    "text": "私たちはその精度を高め、さらに向上させようとしている。"
  },
  {
    "start": 3739210,
    "end": 3741298,
    "text": "まずはバックプロパゲーションから始めよう。"
  },
  {
    "start": 3741324,
    "end": 3745306,
    "text": "結局、oに関するOの導関数は何ですか？"
  },
  {
    "start": 3745408,
    "end": 3750198,
    "text": "基本的なケースとして、私たちは常にグラデーションが1.0であることを知っている。"
  },
  {
    "start": 3750384,
    "end": 3764722,
    "text": "それを埋めてから、描画機能をこことここに分けてみよう。"
  },
  {
    "start": 3764856,
    "end": 3768754,
    "text": "セルはこの出力をここで消去する。"
  },
  {
    "start": 3768872,
    "end": 3773810,
    "text": "さて、ではoを描くと、その勾配が1であることがわかるだろう。"
  },
  {
    "start": 3773960,
    "end": 3776502,
    "text": "これから10時間かけてバックプロパゲートする。"
  },
  {
    "start": 3776636,
    "end": 3781110,
    "text": "を逆伝播するためには、ten hの局所微分を知る必要がある。"
  },
  {
    "start": 3781260,
    "end": 3791530,
    "text": "oがnの10hだとすると、dnはどうなるのか？"
  },
  {
    "start": 3791950,
    "end": 3800710,
    "text": "ここに来て、この式を微分して、微分積分をすることができる。"
  },
  {
    "start": 3800800,
    "end": 3812558,
    "text": "また、ウィキペディアをスクロールしていくと、xの10乗hのdxによる微分dがこれらのどれかになることを教えてくれるセクションがある。"
  },
  {
    "start": 3812644,
    "end": 3813338,
    "text": "私はこれが好きだ。"
  },
  {
    "start": 3813364,
    "end": 3815058,
    "text": "xの1マイナス10乗。"
  },
  {
    "start": 3815224,
    "end": 3819262,
    "text": "これはxの2乗から10のhを引いたものである。"
  },
  {
    "start": 3819406,
    "end": 3829830,
    "text": "要するに、dnによるdoはnの10乗から1を引いたものになるということだ。"
  },
  {
    "start": 3830970,
    "end": 3832742,
    "text": "我々はすでにnのhを10個持っている。"
  },
  {
    "start": 3832796,
    "end": 3834120,
    "text": "ただのOだ。"
  },
  {
    "start": 3834490,
    "end": 3836322,
    "text": "1マイナスOの2乗だ。"
  },
  {
    "start": 3836466,
    "end": 3838074,
    "text": "oはここでの出力である。"
  },
  {
    "start": 3838192,
    "end": 3840860,
    "text": "出力はこの数字である。"
  },
  {
    "start": 3841710,
    "end": 3845900,
    "text": "Oドットのデータはこの数字だ。"
  },
  {
    "start": 3846510,
    "end": 3852582,
    "text": "ということは、dnによるdoはこの2乗から1を引いたものになるということだ。"
  },
  {
    "start": 3852646,
    "end": 3857438,
    "text": "1マイナスoのドットデータの2乗はゼロ5である。"
  },
  {
    "start": 3857524,
    "end": 3863620,
    "text": "好都合なことに、ここでのtan h演算の局所微分はゼロ5である。"
  },
  {
    "start": 3865190,
    "end": 3867342,
    "text": "それはdnがやることだ。"
  },
  {
    "start": 3867486,
    "end": 3874594,
    "text": "を埋めることができる。"
  },
  {
    "start": 3874632,
    "end": 3875860,
    "text": "それを埋めるだけだ。"
  },
  {
    "start": 3882410,
    "end": 3885046,
    "text": "これはちょうど0.5と1.5だ"
  },
  {
    "start": 3885228,
    "end": 3888150,
    "text": "これからバックプロパゲーションを続ける。"
  },
  {
    "start": 3889290,
    "end": 3891750,
    "text": "これはゼロ5で、これはプラスノードだ。"
  },
  {
    "start": 3892090,
    "end": 3896458,
    "text": "バックプロップはどうするのか、バックプロップはここで何をするのか？"
  },
  {
    "start": 3896624,
    "end": 3901542,
    "text": "前回の例を思い出せば、プラスはグラデーションのディストリビューターに過ぎない。"
  },
  {
    "start": 3901686,
    "end": 3905082,
    "text": "この勾配は、単純にこれらの両方に等しく流れる。"
  },
  {
    "start": 3905226,
    "end": 3910330,
    "text": "なぜなら、この演算の局所微分は、そのノードの1つ1つに対して1だからだ。"
  },
  {
    "start": 3910410,
    "end": 3912698,
    "text": "1×0.5は0.5である。"
  },
  {
    "start": 3912884,
    "end": 3920500,
    "text": "従って、このノード（これをグラッドと呼ぶ）はちょうど0.5であることがわかる。"
  },
  {
    "start": 3920950,
    "end": 3924180,
    "text": "bのドットグラッドもゼロ5であることがわかる。"
  },
  {
    "start": 3924790,
    "end": 3926920,
    "text": "それをセットしてドローにしよう。"
  },
  {
    "start": 3928810,
    "end": 3930326,
    "text": "それがゼロ5だ。"
  },
  {
    "start": 3930508,
    "end": 3934646,
    "text": "続けて、もう1つプラスゼロ5がある。"
  },
  {
    "start": 3934748,
    "end": 3937218,
    "text": "ゼロ5はこの両方に流れる。"
  },
  {
    "start": 3937404,
    "end": 3945866,
    "text": "xを2つ、wを2つにすることもできる。"
  },
  {
    "start": 3945968,
    "end": 3947674,
    "text": "グラッドもゼロ5だ。"
  },
  {
    "start": 3947872,
    "end": 3949110,
    "text": "描き直そう。"
  },
  {
    "start": 3949190,
    "end": 3954320,
    "text": "プラスはバックプロパゲートするのに一番好きなオペレーションだ。"
  },
  {
    "start": 3955010,
    "end": 3957726,
    "text": "今、これらの表現に流れ込んでいるのはゼロ5だ。"
  },
  {
    "start": 3957828,
    "end": 3962346,
    "text": "だから、ここでもデリバティブが各時点で私たちに何を伝えているのかに留意してほしい。"
  },
  {
    "start": 3962468,
    "end": 3967010,
    "text": "これは、もしこのニューロンの出力を増やしたいのなら、と言っているのだ。"
  },
  {
    "start": 3968070,
    "end": 3972414,
    "text": "すると、これらの表現が出力に与える影響はプラスになる。"
  },
  {
    "start": 3972462,
    "end": 3978710,
    "text": "どちらも出力にプラスに貢献している。"
  },
  {
    "start": 3980490,
    "end": 3983414,
    "text": "現在はx2とw2に逆伝播している。"
  },
  {
    "start": 3983452,
    "end": 3988822,
    "text": "まず、このノードはタイムズ・ノードなので、局所導関数がもう一方の項であることがわかる。"
  },
  {
    "start": 3988966,
    "end": 3995820,
    "text": "もし2つのドットグラッドを計算したいのであれば、それが何になるか考えてみてくれる？"
  },
  {
    "start": 4000820,
    "end": 4009040,
    "text": "x two gradは、このx two, w two gradにw twoのデータを掛けたものになる。"
  },
  {
    "start": 4009120,
    "end": 4009750,
    "text": "そうだね。"
  },
  {
    "start": 4011080,
    "end": 4018104,
    "text": "w two gradはx twoのデータをx two倍したものになる。"
  },
  {
    "start": 4018142,
    "end": 4019400,
    "text": "W2回生。"
  },
  {
    "start": 4021340,
    "end": 4021848,
    "text": "そうだね。"
  },
  {
    "start": 4021934,
    "end": 4024840,
    "text": "それがチェーン・ルールの小さなローカル・ピースだ。"
  },
  {
    "start": 4027020,
    "end": 4029020,
    "text": "それをセットして再描画しよう。"
  },
  {
    "start": 4029760,
    "end": 4036284,
    "text": "ここで、ウェイト2の勾配がゼロであることがわかる。"
  },
  {
    "start": 4036402,
    "end": 4040428,
    "text": "x 2の勾配は0.5となる。"
  },
  {
    "start": 4040594,
    "end": 4050096,
    "text": "ここで興味深いのは、入力×2がゼロであったため、当然、この勾配はゼロになるということだ。"
  },
  {
    "start": 4050198,
    "end": 4052470,
    "text": "その理由を直感的に考えてみよう。"
  },
  {
    "start": 4053160,
    "end": 4057856,
    "text": "微分は常に、これが最終出力に与える影響を教えてくれる。"
  },
  {
    "start": 4057968,
    "end": 4061424,
    "text": "この2つを小刻みに動かすと、出力はどう変化する？"
  },
  {
    "start": 4061552,
    "end": 4064148,
    "text": "ゼロを掛けているから変わらない。"
  },
  {
    "start": 4064314,
    "end": 4066724,
    "text": "なぜなら、それは変わらないからだ。"
  },
  {
    "start": 4066772,
    "end": 4071290,
    "text": "ゼロで潰しているのだから、ゼロが正解だ。"
  },
  {
    "start": 4072140,
    "end": 4073400,
    "text": "ここでやろう"
  },
  {
    "start": 4073470,
    "end": 4077310,
    "text": "ゼロファイブはここに来て、この時代を流れるべきだ。"
  },
  {
    "start": 4077680,
    "end": 4081070,
    "text": "というわけで、その×1を採点してもらうことにしよう。"
  },
  {
    "start": 4081920,
    "end": 4085710,
    "text": "どうあるべきか、少し考えてみてくれる？"
  },
  {
    "start": 4087360,
    "end": 4092396,
    "text": "x1に関する時間の局所微分はw1になる。"
  },
  {
    "start": 4092578,
    "end": 4104096,
    "text": "w oneのデータにx oneを掛けたものがw one grad、w one gradはx oneのデータにx oneを掛けたものになる。"
  },
  {
    "start": 4104118,
    "end": 4104836,
    "text": "W2だ。"
  },
  {
    "start": 4104938,
    "end": 4106340,
    "text": "W一期生。"
  },
  {
    "start": 4107240,
    "end": 4109108,
    "text": "その結果を見てみよう。"
  },
  {
    "start": 4109274,
    "end": 4110420,
    "text": "これはゼロ5だ。"
  },
  {
    "start": 4110490,
    "end": 4114150,
    "text": "これはマイナス1.5で、これは1となる。"
  },
  {
    "start": 4114600,
    "end": 4117044,
    "text": "この式でバックプロパゲートした。"
  },
  {
    "start": 4117092,
    "end": 4119060,
    "text": "これが実際の最終的なデリバティブである。"
  },
  {
    "start": 4119140,
    "end": 4128680,
    "text": "このニューロンの出力を増加させたいのであれば、必要なのはw 2で、勾配はない。"
  },
  {
    "start": 4128760,
    "end": 4134972,
    "text": "W 2は今このニューロンには関係ないが、このニューロン、このウェイトは上がるはずだ。"
  },
  {
    "start": 4135106,
    "end": 4139324,
    "text": "もしこのウェイトが上がれば、このニューロンの出力は上がったことになる。"
  },
  {
    "start": 4139442,
    "end": 4141836,
    "text": "勾配は1なので、比例する。"
  },
  {
    "start": 4141938,
    "end": 4145164,
    "text": "バックプロパゲーションを手動で行うのは明らかに馬鹿げている。"
  },
  {
    "start": 4145212,
    "end": 4147708,
    "text": "私たちは今、この苦しみに終止符を打とうとしている。"
  },
  {
    "start": 4147804,
    "end": 4152044,
    "text": "バックワードパスをもう少し自動的に実装する方法を見ていこう。"
  },
  {
    "start": 4152092,
    "end": 4154796,
    "text": "ここですべてを手作業でやるつもりはない。"
  },
  {
    "start": 4154918,
    "end": 4159824,
    "text": "これらのプラスとタイムがどのように逆伝播グラデーションになっているかは、例を見れば一目瞭然だ。"
  },
  {
    "start": 4159952,
    "end": 4168570,
    "text": "バリュー・オブジェクトに移動して、以下の例で見たことをコード化してみよう。"
  },
  {
    "start": 4169500,
    "end": 4176532,
    "text": "これは、特別なセルフ・ドット・バックワードとアンダースコア・バックワードを保存することで行う。"
  },
  {
    "start": 4176596,
    "end": 4185016,
    "text": "これは、入力を受けて出力を生成する各ノードで、連鎖ルールの小さな部分を実行する関数になる。"
  },
  {
    "start": 4185208,
    "end": 4191996,
    "text": "どのように出力グラデーションを入力グラデーションに連鎖させるかを保存する。"
  },
  {
    "start": 4192188,
    "end": 4197650,
    "text": "デフォルトでは、これは何もしない関数になる。"
  },
  {
    "start": 4199860,
    "end": 4202800,
    "text": "マイクログラッドの値にもそれが表れている。"
  },
  {
    "start": 4203220,
    "end": 4207750,
    "text": "この後方関数はデフォルトでは何もしない。"
  },
  {
    "start": 4208440,
    "end": 4209876,
    "text": "これは空の関数である。"
  },
  {
    "start": 4210058,
    "end": 4212624,
    "text": "例えばリーフノードのような場合だ。"
  },
  {
    "start": 4212672,
    "end": 4214570,
    "text": "リーフノードには何もすることがない。"
  },
  {
    "start": 4215740,
    "end": 4223610,
    "text": "もし、このアウト・バリューを作るときに、このアウト・バリューがセルフとアナザーの足し算だとしたら。"
  },
  {
    "start": 4224140,
    "end": 4232220,
    "text": "そこで、勾配を伝搬する関数として後方に設定したい。"
  },
  {
    "start": 4234160,
    "end": 4242156,
    "text": "何が起こるべきかを定義し、それをクロージャーに格納しよう。"
  },
  {
    "start": 4242268,
    "end": 4248960,
    "text": "足し算のためにoutsgradを呼び出すときに何が起こるべきかを定義してみよう。"
  },
  {
    "start": 4250020,
    "end": 4256928,
    "text": "私たちの仕事は、アウトスグラッドをセルフスグラッドや他のグラッドに伝播させることだ。"
  },
  {
    "start": 4257024,
    "end": 4263750,
    "text": "基本的に、私たちは自分が何かに卒業したいし、他人を何かに卒業させたい。"
  },
  {
    "start": 4265800,
    "end": 4282510,
    "text": "鎖の法則がどのように機能するかは後述するが、局所導関数の時間、つまりグローバル導関数と呼ぶべきものを取りたい。"
  },
  {
    "start": 4282960,
    "end": 4289150,
    "text": "加算におけるselfの局所導関数は1.0である。"
  },
  {
    "start": 4289540,
    "end": 4293520,
    "text": "それはちょうど1.0倍の卒業生だ。"
  },
  {
    "start": 4294340,
    "end": 4295740,
    "text": "それがチェーンルールだ。"
  },
  {
    "start": 4295900,
    "end": 4299052,
    "text": "その他の新卒者は新卒者の1.0倍となる。"
  },
  {
    "start": 4299196,
    "end": 4309460,
    "text": "基本的に、ここであなたが見ているのは、加算操作で起こったように、アウトスグラッドが単に自己のグラッドと他者のグラッドにコピーされるということだ。"
  },
  {
    "start": 4309880,
    "end": 4313516,
    "text": "後でこの関数を呼び出してグラデーションを伝播させる。"
  },
  {
    "start": 4313648,
    "end": 4317284,
    "text": "足し算の次は掛け算だ。"
  },
  {
    "start": 4317412,
    "end": 4325800,
    "text": "バックワードも定義し、そのバックワードを後方に設定する。"
  },
  {
    "start": 4327820,
    "end": 4336140,
    "text": "私たちは、新卒を自己新卒と他者新卒に連鎖させたい。"
  },
  {
    "start": 4337120,
    "end": 4340080,
    "text": "これは掛け算の連鎖法則の小さな断片になるだろう。"
  },
  {
    "start": 4341540,
    "end": 4343088,
    "text": "これはどうあるべきか？"
  },
  {
    "start": 4343254,
    "end": 4344530,
    "text": "考え抜くことができるか？"
  },
  {
    "start": 4348580,
    "end": 4351232,
    "text": "現地でのデリバティブは？"
  },
  {
    "start": 4351286,
    "end": 4358084,
    "text": "局所的な誘導体は他人のデータ、そして他人のデータだった。"
  },
  {
    "start": 4358202,
    "end": 4361350,
    "text": "その後、タイムアウトになる。"
  },
  {
    "start": 4362520,
    "end": 4365536,
    "text": "ここに塩のデータがある。"
  },
  {
    "start": 4365648,
    "end": 4367030,
    "text": "それが私たちのやってきたことだ。"
  },
  {
    "start": 4369480,
    "end": 4372840,
    "text": "ここでようやく10時間後退。"
  },
  {
    "start": 4374780,
    "end": 4378680,
    "text": "そうであるならば、私たちはただ後方へ後方へと出発したい。"
  },
  {
    "start": 4380460,
    "end": 4383572,
    "text": "ここではバックプロパゲートが必要だ。"
  },
  {
    "start": 4383636,
    "end": 4388060,
    "text": "私たちはグラードを持っていて、それをソルトグラードにつなげたい。"
  },
  {
    "start": 4389600,
    "end": 4395936,
    "text": "salt gradは、ここで行った操作の局所微分となる。"
  },
  {
    "start": 4396118,
    "end": 4402690,
    "text": "ということは、局所勾配はxの10乗から1を引いたものであり、ここではtであることがわかる。"
  },
  {
    "start": 4403620,
    "end": 4407408,
    "text": "これが局所微分であり、tがこの10のhの出力だからだ。"
  },
  {
    "start": 4407494,
    "end": 4409824,
    "text": "1マイナスtの2乗が局所微分である。"
  },
  {
    "start": 4409952,
    "end": 4414736,
    "text": "この場合、連鎖法則のため、勾配は乗算されなければならない。"
  },
  {
    "start": 4414848,
    "end": 4418580,
    "text": "outgradは局所的な勾配を通してソルトoutgradに連鎖する。"
  },
  {
    "start": 4419400,
    "end": 4421268,
    "text": "それが基本的なことだ。"
  },
  {
    "start": 4421354,
    "end": 4424520,
    "text": "バリュー・ノードを再定義する。"
  },
  {
    "start": 4424860,
    "end": 4434750,
    "text": "ここで式を再定義し、すべてのグラッドがゼロになるようにする。"
  },
  {
    "start": 4435280,
    "end": 4436076,
    "text": "オーケー。"
  },
  {
    "start": 4436258,
    "end": 4439020,
    "text": "これでもう手動でやる必要はない。"
  },
  {
    "start": 4439760,
    "end": 4443490,
    "text": "我々は基本的に、ドットを正しい順序で逆方向へ呼び出すつもりだ。"
  },
  {
    "start": 4444020,
    "end": 4449600,
    "text": "まず、Oを後ろ向きに呼びたい。"
  },
  {
    "start": 4453940,
    "end": 4458416,
    "text": "Oが10時間の結果でしょう？"
  },
  {
    "start": 4458518,
    "end": 4464004,
    "text": "を後方に呼び出すと、この関数になる。"
  },
  {
    "start": 4464122,
    "end": 4465510,
    "text": "これがその結果だ。"
  },
  {
    "start": 4465980,
    "end": 4475050,
    "text": "ここで注意しなければならないのは、out gradの回数とout gradがゼロに初期化されていることだ。"
  },
  {
    "start": 4478800,
    "end": 4480508,
    "text": "ここでは、「ゼロ」である。"
  },
  {
    "start": 4480594,
    "end": 4488590,
    "text": "基本的なケースとして、oath gradを1.0に設定し、これを1で初期化する必要がある。"
  },
  {
    "start": 4493460,
    "end": 4497020,
    "text": "これが1つになれば、Oドットを後方に呼び出すことができる。"
  },
  {
    "start": 4497180,
    "end": 4502032,
    "text": "そうすることで、この成績が10時間の間に伝搬するはずだ。"
  },
  {
    "start": 4502166,
    "end": 4507076,
    "text": "局所導関数に大域導関数を掛けたもので、1 で初期化される。"
  },
  {
    "start": 4507178,
    "end": 4509270,
    "text": "そのはずだ。"
  },
  {
    "start": 4517010,
    "end": 4522046,
    "text": "やり直そうかとも思ったんだけど、かなり面白いから、ここにエラーを残しておこうと思ったんだ。"
  },
  {
    "start": 4522158,
    "end": 4524450,
    "text": "なぜNantaオブジェクトはコールできないのか？"
  },
  {
    "start": 4524870,
    "end": 4528050,
    "text": "私がしくじったからだ。"
  },
  {
    "start": 4528120,
    "end": 4530158,
    "text": "私たちはこれらの機能を保存しようとしている。"
  },
  {
    "start": 4530254,
    "end": 4531746,
    "text": "これは正しい。"
  },
  {
    "start": 4531928,
    "end": 4536002,
    "text": "ここでは、何も返さないので関数を呼び出したくない。"
  },
  {
    "start": 4536066,
    "end": 4537538,
    "text": "これらの関数は何も返さない。"
  },
  {
    "start": 4537634,
    "end": 4539414,
    "text": "関数を保存したいだけなのだ。"
  },
  {
    "start": 4539612,
    "end": 4541720,
    "text": "値オブジェクトを再定義してみよう。"
  },
  {
    "start": 4542170,
    "end": 4547786,
    "text": "それからまた戻って、draw、dotという式を再定義する。"
  },
  {
    "start": 4547888,
    "end": 4549500,
    "text": "オー・ドット・グラードがそうだ。"
  },
  {
    "start": 4550030,
    "end": 4551594,
    "text": "オー・ドット・グラードがそうだ。"
  },
  {
    "start": 4551712,
    "end": 4554314,
    "text": "これでうまくいくはずだ。"
  },
  {
    "start": 4554352,
    "end": 4555020,
    "text": "もちろんだ。"
  },
  {
    "start": 4555710,
    "end": 4563918,
    "text": "もし、すべてが正しく行われたなら、0.5となるはずだ。"
  },
  {
    "start": 4564004,
    "end": 4564750,
    "text": "イェーイ。"
  },
  {
    "start": 4565250,
    "end": 4571214,
    "text": "さて、では両端を逆向きに呼ぶ必要がある。"
  },
  {
    "start": 4571262,
    "end": 4574286,
    "text": "申し訳ない。"
  },
  {
    "start": 4574478,
    "end": 4576100,
    "text": "それが功を奏したようだ。"
  },
  {
    "start": 4577750,
    "end": 4579434,
    "text": "は後方で終わる。"
  },
  {
    "start": 4579582,
    "end": 4582118,
    "text": "この両方にグラディエントをルーティングした。"
  },
  {
    "start": 4582204,
    "end": 4583640,
    "text": "これは素晴らしい。"
  },
  {
    "start": 4584490,
    "end": 4588354,
    "text": "もちろん、bグラッドをbバックワードと呼ぶこともできる。"
  },
  {
    "start": 4588402,
    "end": 4589000,
    "text": "申し訳ない。"
  },
  {
    "start": 4590170,
    "end": 4591400,
    "text": "どうなるんだ？"
  },
  {
    "start": 4592030,
    "end": 4594022,
    "text": "まあ、bは逆ではないよ。"
  },
  {
    "start": 4594166,
    "end": 4597382,
    "text": "bはリーフノードなので、Bの後方。"
  },
  {
    "start": 4597526,
    "end": 4601180,
    "text": "Bの後進は初期化によって空関数となる。"
  },
  {
    "start": 4601550,
    "end": 4603242,
    "text": "何も起こらないだろう。"
  },
  {
    "start": 4603296,
    "end": 4605658,
    "text": "我々はそれを呼び出すことができる。"
  },
  {
    "start": 4605824,
    "end": 4617318,
    "text": "これを呼び出すと、後方から、このゼロ5がさらにルーティングされることになる。"
  },
  {
    "start": 4617514,
    "end": 4618066,
    "text": "そうだね。"
  },
  {
    "start": 4618168,
    "end": 4620340,
    "text": "さあ、ゼロ5.5だ。"
  },
  {
    "start": 4620870,
    "end": 4632440,
    "text": "そして最後に、x2にはw2、x1にはw1と呼びたい。"
  },
  {
    "start": 4635690,
    "end": 4637240,
    "text": "その両方をやろう。"
  },
  {
    "start": 4637690,
    "end": 4639160,
    "text": "さあ、行こう。"
  },
  {
    "start": 4639690,
    "end": 4645322,
    "text": "の場合、0.5、マイナス1.5、そして1となる。"
  },
  {
    "start": 4645456,
    "end": 4652278,
    "text": "今は、手動で後方に呼び出すことでそれを実現している。"
  },
  {
    "start": 4652454,
    "end": 4658026,
    "text": "つまり、手動でアンダースコアを後方に呼び出すのだ。"
  },
  {
    "start": 4658138,
    "end": 4660640,
    "text": "私たちは実際に何をしているのか、よく考えてみよう。"
  },
  {
    "start": 4661410,
    "end": 4666350,
    "text": "私たちは数式を並べ、今度はその数式を逆算しようとしている。"
  },
  {
    "start": 4667190,
    "end": 4679506,
    "text": "式を逆行させるということは、どのノードに対しても、そのノードの後をすべて処理する前にドットを逆行させないということだ。"
  },
  {
    "start": 4679688,
    "end": 4684066,
    "text": "どのノードでもドット・バックワードを呼び出す前に、それ以降のすべてを行わなければならない。"
  },
  {
    "start": 4684098,
    "end": 4685730,
    "text": "すべての依存関係を取得しなければならない。"
  },
  {
    "start": 4685810,
    "end": 4691750,
    "text": "逆伝播を続ける前に、それに依存するすべてのものが伝搬しなければならない。"
  },
  {
    "start": 4692270,
    "end": 4697626,
    "text": "このグラフの順序付けは、トポロジカル・ソートと呼ばれるものを使って実現できる。"
  },
  {
    "start": 4697808,
    "end": 4706380,
    "text": "トポロジカル・ソートとは、基本的にすべての辺が左から右にしか進まないようにグラフを並べることである。"
  },
  {
    "start": 4706690,
    "end": 4721540,
    "text": "これはディレクトリ非循環グラフ（directory acyclic graph）、ダグ（dag）と呼ばれるグラフで、2つの異なるトポロジーの順序があると思いますが、基本的には、すべての辺が左から右に一方向にしか進まないようにノードを並べたものです。"
  },
  {
    "start": 4721910,
    "end": 4726146,
    "text": "トポロジカル・ソートの実装については、ウィキペディアなどで調べることができる。"
  },
  {
    "start": 4726168,
    "end": 4733030,
    "text": "詳しくは説明しないが、基本的にはこれがトポロジカル・グラフを構築するものだ。"
  },
  {
    "start": 4734090,
    "end": 4743222,
    "text": "私たちは訪問したノードの集合を管理し、あるルート・ノード（ここではo）から出発して、通過していく。"
  },
  {
    "start": 4743276,
    "end": 4745498,
    "text": "トポロジカル・ソート（位相幾何学的ソート）だ。"
  },
  {
    "start": 4745664,
    "end": 4751740,
    "text": "oを起点に、そのすべての子を左から右に並べていく。"
  },
  {
    "start": 4752590,
    "end": 4754842,
    "text": "基本的にはOから始まる。"
  },
  {
    "start": 4754976,
    "end": 4763760,
    "text": "もしそれが訪問されていなければ、訪問されたものとしてマークし、それからすべての子プロセスを繰り返し、それらの子プロセスに対してビルド・トポロジカルを呼び出す。"
  },
  {
    "start": 4764210,
    "end": 4768000,
    "text": "そして、すべての子どもたちに目を通した後、自分自身を追加する。"
  },
  {
    "start": 4768370,
    "end": 4779102,
    "text": "基本的に、このノードを呼び出すのは、例えばoのように、すべての子ノードが処理された後に、トポ・リストに追加される。"
  },
  {
    "start": 4779166,
    "end": 4785702,
    "text": "つまり、この関数は、自分の子どもたちが全員リストに入って初めて、自分がリストに入ることを保証しているのだ。"
  },
  {
    "start": 4785756,
    "end": 4787746,
    "text": "それが維持されている不変性だ。"
  },
  {
    "start": 4787858,
    "end": 4801526,
    "text": "topone oを構築し、このリストを検査すると、バリュー・オブジェクトが順番に並んでいることがわかる。"
  },
  {
    "start": 4801718,
    "end": 4808640,
    "text": "これがoで、これがnで、その前に他のすべてのノードが並べられる。"
  },
  {
    "start": 4809650,
    "end": 4811818,
    "text": "トポロジカル・グラフを構築した"
  },
  {
    "start": 4811914,
    "end": 4819460,
    "text": "今やっているのは、トポロジカルな順序ですべてのノードに対してドット・アンダースコアを逆向きに呼び出しているだけなのだ。"
  },
  {
    "start": 4819830,
    "end": 4823266,
    "text": "グラデーションをリセットすれば、すべてゼロになる。"
  },
  {
    "start": 4823448,
    "end": 4824642,
    "text": "私たちは何をしたのか？"
  },
  {
    "start": 4824776,
    "end": 4830920,
    "text": "私たちはまず、卒業生を1人にすることから始めた。"
  },
  {
    "start": 4831450,
    "end": 4833000,
    "text": "それがベースケースだ。"
  },
  {
    "start": 4833610,
    "end": 4845190,
    "text": "それからトポロジカル・オーダーを作り、逆オクトポでノードを探した。"
  },
  {
    "start": 4846350,
    "end": 4853674,
    "text": "さて、このリストは逆順になっているので、逆の順番で見ていく必要がある。"
  },
  {
    "start": 4853872,
    "end": 4862000,
    "text": "oノードから後方へ。"
  },
  {
    "start": 4863330,
    "end": 4864480,
    "text": "これでよし。"
  },
  {
    "start": 4865570,
    "end": 4867098,
    "text": "それが正しい派生語だ。"
  },
  {
    "start": 4867194,
    "end": 4870010,
    "text": "最後に、この機能を非表示にする。"
  },
  {
    "start": 4870170,
    "end": 4877954,
    "text": "このコードをコピーして、バリュー・クラスの中に隠してしまおう。"
  },
  {
    "start": 4878152,
    "end": 4884870,
    "text": "アンダースコアの後ろ向きではなく、実際の後ろ向きを定義する。"
  },
  {
    "start": 4886170,
    "end": 4888850,
    "text": "私たちが今到着したのは、このようなことをすべて実行するためのものだ。"
  },
  {
    "start": 4889010,
    "end": 4890966,
    "text": "ちょっと整理させてください。"
  },
  {
    "start": 4891068,
    "end": 4900620,
    "text": "そこでまず、selfから始まるトポロジカル・グラフを作ることにする。"
  },
  {
    "start": 4901230,
    "end": 4909066,
    "text": "build topo of selfは、ローカル変数であるtopoリストにトポロジー順を入力する。"
  },
  {
    "start": 4909258,
    "end": 4912000,
    "text": "そして、self dot gratを1つにする。"
  },
  {
    "start": 4912930,
    "end": 4915886,
    "text": "次に、反転リストの各ノードに対して"
  },
  {
    "start": 4915988,
    "end": 4921330,
    "text": "私たちから始まり、すべての子どもたちへ、アンダースコアは後ろへ。"
  },
  {
    "start": 4922390,
    "end": 4924658,
    "text": "それでいいはずだ。"
  },
  {
    "start": 4924744,
    "end": 4929170,
    "text": "セーブして、こっちに来て。"
  },
  {
    "start": 4929320,
    "end": 4930370,
    "text": "再定義する。"
  },
  {
    "start": 4931050,
    "end": 4932760,
    "text": "よし、新卒は全員ゼロだ。"
  },
  {
    "start": 4933530,
    "end": 4937074,
    "text": "今できることは、アンダースコアなしで後方にオドットすることだ。"
  },
  {
    "start": 4937202,
    "end": 4942520,
    "text": "さあ、行こう。"
  },
  {
    "start": 4942890,
    "end": 4948246,
    "text": "これは1つのニューロンの逆伝播場所である。"
  },
  {
    "start": 4948438,
    "end": 4959280,
    "text": "というのも、悪いバグがあるにもかかわらず、そのバグを表面化させることができていない。"
  },
  {
    "start": 4959810,
    "end": 4962910,
    "text": "デバッグを表示する最も単純なケースを紹介しよう。"
  },
  {
    "start": 4963970,
    "end": 4973010,
    "text": "例えば、ノードaを1つ作り、aにaを足したノードbを作り、バックワードと呼ぶ。"
  },
  {
    "start": 4974790,
    "end": 4979362,
    "text": "aが3で、bはaにaを足したものになる。"
  },
  {
    "start": 4979416,
    "end": 4982100,
    "text": "ここに2つの矢印が重なっている。"
  },
  {
    "start": 4983690,
    "end": 4986710,
    "text": "すると、bはもちろんフォワードパスが機能することがわかる。"
  },
  {
    "start": 4986860,
    "end": 4989766,
    "text": "Bはaにaを足しただけの6。"
  },
  {
    "start": 4989948,
    "end": 4994730,
    "text": "ここでの勾配は自動的に計算されるもので、実際には正しくない。"
  },
  {
    "start": 4995950,
    "end": 5008702,
    "text": "というのも、頭の中で微積分をするだけで、aに対するbの導関数は2、つまり1プラス1になるはずだからだ。"
  },
  {
    "start": 5008836,
    "end": 5010080,
    "text": "ひとつじゃない。"
  },
  {
    "start": 5010770,
    "end": 5012414,
    "text": "直感的に、ここで何が起こっているんだろう？"
  },
  {
    "start": 5012452,
    "end": 5016446,
    "text": "bはaにAを足した結果で、それを逆算した。"
  },
  {
    "start": 5016628,
    "end": 5019680,
    "text": "上がって見てみよう。"
  },
  {
    "start": 5023490,
    "end": 5027060,
    "text": "Bは足し算の結果なので、bと出る。"
  },
  {
    "start": 5027750,
    "end": 5036840,
    "text": "その後、バックワードで呼び出すと、自己採点が1にセットされ、次に他者採点が1にセットされた。"
  },
  {
    "start": 5037210,
    "end": 5043094,
    "text": "というのも、プラスaをしているのだから、selfとotherは実際にはまったく同じオブジェクトなのだ。"
  },
  {
    "start": 5043292,
    "end": 5049306,
    "text": "グラデーションをオーバーライドし、それを1に設定し、さらにそれを1に設定している。"
  },
  {
    "start": 5049408,
    "end": 5052700,
    "text": "だから1のままなんだ"
  },
  {
    "start": 5053070,
    "end": 5054300,
    "text": "それは問題だ。"
  },
  {
    "start": 5054690,
    "end": 5058510,
    "text": "これをもう少し複雑な表現で見る方法もある。"
  },
  {
    "start": 5061490,
    "end": 5071700,
    "text": "ここではaとbがあり、dはその2つの掛け算、eはその2つの足し算となる。"
  },
  {
    "start": 5072070,
    "end": 5077090,
    "text": "そして、eにdを掛けてfとし、これをfドットバックと呼ぶ。"
  },
  {
    "start": 5077590,
    "end": 5080590,
    "text": "これらのグラデーションをチェックすると、間違っていることになる。"
  },
  {
    "start": 5080750,
    "end": 5088998,
    "text": "基本的に、ここで起こっていることは、基本的に、変数を複数回使用するたびに問題が起こるということだ。"
  },
  {
    "start": 5089164,
    "end": 5093110,
    "text": "これまで、上記の式では、すべての変数が正確に一度だけ使われていた。"
  },
  {
    "start": 5093180,
    "end": 5094600,
    "text": "私たちはその問題に気づかなかった。"
  },
  {
    "start": 5094970,
    "end": 5098970,
    "text": "ここで、変数が複数回使用された場合、バックワードパスの間に何が起こるのか？"
  },
  {
    "start": 5099120,
    "end": 5101802,
    "text": "fからe、そしてdへと伝搬していく。"
  },
  {
    "start": 5101856,
    "end": 5102826,
    "text": "ここまでは順調だ。"
  },
  {
    "start": 5102928,
    "end": 5107406,
    "text": "今、eはそれを後方に呼び出し、その勾配をaとbに預ける。"
  },
  {
    "start": 5107508,
    "end": 5113520,
    "text": "次にdに戻って後方にコールすると、aとbの勾配が上書きされる。"
  },
  {
    "start": 5114450,
    "end": 5116480,
    "text": "それは明らかに問題だ。"
  },
  {
    "start": 5117090,
    "end": 5127982,
    "text": "ここで、鎖の法則の多変量ケースとその一般化を見れば、その解決策は基本的に、これらの勾配を累積することである。"
  },
  {
    "start": 5128046,
    "end": 5129380,
    "text": "これらのグラデーションは加える。"
  },
  {
    "start": 5130010,
    "end": 5136674,
    "text": "だから、グラデーションを設定する代わりに、単純にプラス・イコールにすればいい。"
  },
  {
    "start": 5136802,
    "end": 5138866,
    "text": "その勾配を積み重ねる必要がある。"
  },
  {
    "start": 5139058,
    "end": 5145850,
    "text": "プラス・イコール、プラス・イコール、プラス・イコール、プラス・イコール。"
  },
  {
    "start": 5146430,
    "end": 5147722,
    "text": "これなら大丈夫だろう。"
  },
  {
    "start": 5147776,
    "end": 5151290,
    "text": "ゼロで初期化しているので、ゼロから始まることを覚えておいてほしい。"
  },
  {
    "start": 5151440,
    "end": 5158606,
    "text": "その場合、後方に流れる貢献は単純に加算される。"
  },
  {
    "start": 5158788,
    "end": 5167918,
    "text": "ここで、プラス・イコールだからといってこれを再定義すると、ドット・グラデーションはゼロから始まるので、これはこれで機能する。"
  },
  {
    "start": 5168004,
    "end": 5172610,
    "text": "ベータをバックワードと呼ぶと、1回預けて、また1回預ける。"
  },
  {
    "start": 5172680,
    "end": 5174706,
    "text": "これで2つ。"
  },
  {
    "start": 5174888,
    "end": 5176578,
    "text": "ここでも使える。"
  },
  {
    "start": 5176664,
    "end": 5182318,
    "text": "e dot backwardを呼び出すと、このブランチからグラデーションが預けられるからだ。"
  },
  {
    "start": 5182414,
    "end": 5186834,
    "text": "その後、ダビングを再開すれば、グラデーションがつく。"
  },
  {
    "start": 5186962,
    "end": 5189974,
    "text": "その場合、これらのグラデーションは単純にお互いの上に追加される。"
  },
  {
    "start": 5190092,
    "end": 5191986,
    "text": "だから、そのグラデーションを蓄積していくだけだ。"
  },
  {
    "start": 5192018,
    "end": 5193222,
    "text": "これで問題は解決した。"
  },
  {
    "start": 5193356,
    "end": 5200250,
    "text": "さて、次に進む前に、ここで少し後片付けをして、この中間の作業の一部を削除しておこう。"
  },
  {
    "start": 5200320,
    "end": 5202682,
    "text": "こんなものは必要ない。"
  },
  {
    "start": 5202736,
    "end": 5209360,
    "text": "すべて導き出したので、また戻ってきたいので、このままにしておく。"
  },
  {
    "start": 5209730,
    "end": 5211198,
    "text": "10個のhを削除する。"
  },
  {
    "start": 5211364,
    "end": 5213200,
    "text": "モードをもう一度削除する。"
  },
  {
    "start": 5213890,
    "end": 5224580,
    "text": "ステップを削除し、これを削除し、描画するコードを残し、この例を削除して値の定義だけを残す。"
  },
  {
    "start": 5225190,
    "end": 5229234,
    "text": "ここで、tan hを実装したこの非線形性に戻ってみよう。"
  },
  {
    "start": 5229352,
    "end": 5237542,
    "text": "さて、xp関数があれば、10個のhを他の式で明示された原子に分解できたとお話しした。"
  },
  {
    "start": 5237676,
    "end": 5242886,
    "text": "覚えているだろうか、ten hはこのように定義されている。"
  },
  {
    "start": 5242988,
    "end": 5251082,
    "text": "しかし、tan hを分解してxの関数として表すこともできる。"
  },
  {
    "start": 5251216,
    "end": 5259898,
    "text": "なぜなら、同じ結果、同じグラデーションが得られることを証明したいからである。"
  },
  {
    "start": 5259994,
    "end": 5265134,
    "text": "指数計算、足し算、引き算、割り算、そういったことをやらなければならない。"
  },
  {
    "start": 5265172,
    "end": 5267918,
    "text": "あと数回、これを経験するのはいい練習になると思う。"
  },
  {
    "start": 5268084,
    "end": 5271620,
    "text": "では、価値の定義までスクロールしてみよう。"
  },
  {
    "start": 5272070,
    "end": 5284520,
    "text": "ここで、現在できないことのひとつは、たとえば2.0のような値を設定することはできますが、たとえば定数1を追加するようなことはできません。"
  },
  {
    "start": 5285050,
    "end": 5288486,
    "text": "intオブジェクトには属性データがありません。"
  },
  {
    "start": 5288588,
    "end": 5294694,
    "text": "プラス1がここに来て足し算になり、その他が整数になるからだ。"
  },
  {
    "start": 5294812,
    "end": 5298602,
    "text": "となると、ここでPythonはあるデータにアクセスしようとしていることになる。"
  },
  {
    "start": 5298736,
    "end": 5303766,
    "text": "というのも、基本的に1はバリュー・オブジェクトではなく、バリュー・オブジェクトに対してのみ加算があるからだ。"
  },
  {
    "start": 5303878,
    "end": 5311200,
    "text": "便宜上、このような式を作成して意味を持たせるためには、次のようにすればよい。"
  },
  {
    "start": 5312370,
    "end": 5327982,
    "text": "基本的に、otherがvalueのインスタンスである場合はotherを放っておくが、valueのインスタンスでない場合は、valueを整数や浮動小数点数のような数値と仮定し、単純にvalueでラップする。"
  },
  {
    "start": 5328046,
    "end": 5329106,
    "text": "これでうまくいくはずだ。"
  },
  {
    "start": 5329208,
    "end": 5333090,
    "text": "この値を再定義すれば、うまくいくはずだ。"
  },
  {
    "start": 5333240,
    "end": 5334134,
    "text": "これでよし。"
  },
  {
    "start": 5334252,
    "end": 5341174,
    "text": "オーケー、では次に、まったく同じことを掛け算でやってみよう。なぜなら、まったく同じ理由でこのようなことは二度とできないからだ。"
  },
  {
    "start": 5341292,
    "end": 5342818,
    "text": "ムルに行くしかない。"
  },
  {
    "start": 5342914,
    "end": 5348810,
    "text": "otherがvalueでないなら、valueに包んでvalueを再定義しよう。"
  },
  {
    "start": 5348880,
    "end": 5350314,
    "text": "これでうまくいく。"
  },
  {
    "start": 5350512,
    "end": 5353786,
    "text": "さて、ここでちょっと残念な、目立たない部分がある。"
  },
  {
    "start": 5353888,
    "end": 5354970,
    "text": "回2作品。"
  },
  {
    "start": 5355040,
    "end": 5355866,
    "text": "私たちはそれを見た。"
  },
  {
    "start": 5355968,
    "end": 5358880,
    "text": "1回に2回、それでうまくいくのか？"
  },
  {
    "start": 5359650,
    "end": 5361182,
    "text": "そうだろう？"
  },
  {
    "start": 5361316,
    "end": 5362862,
    "text": "実際はそうではない。"
  },
  {
    "start": 5362996,
    "end": 5369200,
    "text": "そうならないのは、Pythonが知らないからだ。"
  },
  {
    "start": 5369570,
    "end": 5375730,
    "text": "を2倍にすると、Pythonは基本的に2のドットモルみたいなことをする。"
  },
  {
    "start": 5375800,
    "end": 5377138,
    "text": "それが基本的な呼び方だ。"
  },
  {
    "start": 5377224,
    "end": 5385478,
    "text": "には、aの2倍はaの2ドットモルと同じであり、2は値を掛け算することはできない。"
  },
  {
    "start": 5385564,
    "end": 5387302,
    "text": "だから、本当に混乱しているんだ。"
  },
  {
    "start": 5387436,
    "end": 5393830,
    "text": "その代わり、Pythonでは、armleと呼ばれるものを自由に定義することができる。"
  },
  {
    "start": 5394330,
    "end": 5397254,
    "text": "armleは予備みたいなものだ。"
  },
  {
    "start": 5397302,
    "end": 5401100,
    "text": "Pythonが2回aを実行できない場合、チェックする。"
  },
  {
    "start": 5402350,
    "end": 5408110,
    "text": "もし万が一、ある人が2の掛け算を知っていたら、それはアルモルに呼ばれるだろう。"
  },
  {
    "start": 5408770,
    "end": 5413454,
    "text": "なぜなら、Pythonはaを2回実行できないからだ。"
  },
  {
    "start": 5413572,
    "end": 5416494,
    "text": "なぜなら、それがあるからだ。"
  },
  {
    "start": 5416692,
    "end": 5420430,
    "text": "ここで行うのは、オペランドの順番を入れ替えることだ。"
  },
  {
    "start": 5420510,
    "end": 5427780,
    "text": "基本的に2回のaはアーマルにリダイレクトされ、アーマルは基本的に2回のaを呼び出す。"
  },
  {
    "start": 5428310,
    "end": 5432434,
    "text": "アルモルとの再定義で、2回が4回になる。"
  },
  {
    "start": 5432552,
    "end": 5437058,
    "text": "さて、まだ必要な他の要素を見てみよう。指数と除算の方法を知る必要がある。"
  },
  {
    "start": 5437154,
    "end": 5440198,
    "text": "まず指数の部分をやってみよう。"
  },
  {
    "start": 5440284,
    "end": 5452970,
    "text": "ここでは一つの関数xを導入し、xpは一つのスカラー値を変換して一つのスカラー値を出力する一つの関数という意味で、10のhをミラーリングすることになる。"
  },
  {
    "start": 5453120,
    "end": 5460654,
    "text": "pythonの数値を取り出し、math xを使ってそれを指数化し、新しい値オブジェクトを作成する。"
  },
  {
    "start": 5460772,
    "end": 5466558,
    "text": "もちろん厄介なのは、eからxへの逆伝播をどうするかということだ。"
  },
  {
    "start": 5466644,
    "end": 5470500,
    "text": "ビデオを一時停止して、ここに何を入れるべきか考えることができる。"
  },
  {
    "start": 5473270,
    "end": 5478418,
    "text": "さて、基本的には、xに対するeの局所微分を知る必要がある。"
  },
  {
    "start": 5478504,
    "end": 5482198,
    "text": "xに対するeのdxによるdは、単にxに対するeであることは有名である。"
  },
  {
    "start": 5482284,
    "end": 5484646,
    "text": "すでにeをxまで計算したところだ。"
  },
  {
    "start": 5484748,
    "end": 5490594,
    "text": "これはインサイド・アウトのデータだから、アウトのデータ・タイムとアウトのグラッドができるんだ。"
  },
  {
    "start": 5490642,
    "end": 5491858,
    "text": "それがチェーンルールだ。"
  },
  {
    "start": 5492034,
    "end": 5495106,
    "text": "今走っているグラードにチェーンしているだけだ。"
  },
  {
    "start": 5495298,
    "end": 5497210,
    "text": "このような表現になる。"
  },
  {
    "start": 5497280,
    "end": 5501290,
    "text": "少しわかりにくいが、これが指数である。"
  },
  {
    "start": 5501790,
    "end": 5505334,
    "text": "つまり、再定義すれば、xpを呼び出せるようになるはずだ。"
  },
  {
    "start": 5505462,
    "end": 5508282,
    "text": "バックパスもうまくいくことを願っている。"
  },
  {
    "start": 5508336,
    "end": 5512026,
    "text": "オーケー、そして最後にやりたいことは、もちろん、分割できるようにすることだ。"
  },
  {
    "start": 5512218,
    "end": 5519802,
    "text": "というのも、除算はもう少し強力なものの特殊なケースに過ぎないからだ。"
  },
  {
    "start": 5519946,
    "end": 5531220,
    "text": "特に、並べ替えるだけで、a bが4.0に等しい場合、基本的にはa divide bができるようにしたい。"
  },
  {
    "start": 5531590,
    "end": 5535186,
    "text": "さて、実際にディビジョンは次のように入れ替えられる。"
  },
  {
    "start": 5535298,
    "end": 5543800,
    "text": "もしbを割るなら、それはbに1を掛けるのと同じであり、bの負の1乗と同じである。"
  },
  {
    "start": 5544250,
    "end": 5551466,
    "text": "だから、その代わりにやりたいことは、基本的には、ある定数kに対してxをkに変換する演算を実装することだ。"
  },
  {
    "start": 5551568,
    "end": 5553530,
    "text": "これは整数か浮動小数点数である。"
  },
  {
    "start": 5554030,
    "end": 5556154,
    "text": "これを区別できるようにしたい。"
  },
  {
    "start": 5556192,
    "end": 5560586,
    "text": "であれば、特殊なケースとして、マイナス1が除算される。"
  },
  {
    "start": 5560778,
    "end": 5563518,
    "text": "だから、その方が一般的だからそうしているんだ。"
  },
  {
    "start": 5563604,
    "end": 5566318,
    "text": "ああ、その方がいい。"
  },
  {
    "start": 5566404,
    "end": 5573810,
    "text": "基本的に私が言っているのは、ディビジョンを再定義できるということだ。"
  },
  {
    "start": 5574470,
    "end": 5576126,
    "text": "そうだ、どこかに置こう。"
  },
  {
    "start": 5576238,
    "end": 5585320,
    "text": "私が言っているのは、分裂を再定義することで、自己を分裂させ、他者を自己×他者と書き換えて、マイナス1の力を発揮させることができるということだ。"
  },
  {
    "start": 5585690,
    "end": 5589622,
    "text": "をマイナス1乗した値。"
  },
  {
    "start": 5589676,
    "end": 5591160,
    "text": "それを定義しなければならない。"
  },
  {
    "start": 5593530,
    "end": 5595754,
    "text": "pow関数を実装する必要がある。"
  },
  {
    "start": 5595952,
    "end": 5597770,
    "text": "パウ関数をどこに置けばいいのか？"
  },
  {
    "start": 5597840,
    "end": 5599050,
    "text": "たぶん、ここのどこかだろう。"
  },
  {
    "start": 5599950,
    "end": 5601820,
    "text": "これがその骨格だ。"
  },
  {
    "start": 5602430,
    "end": 5608474,
    "text": "この関数は、値をあるべき乗に上げようとするときに呼び出され、他の値はそのべき乗になる。"
  },
  {
    "start": 5608672,
    "end": 5612138,
    "text": "ここで、otherがintかfloatだけであることを確認したい。"
  },
  {
    "start": 5612234,
    "end": 5618550,
    "text": "通常、otherは何らかの別の値オブジェクトだが、ここではotherは強制的にintかfloatになる。"
  },
  {
    "start": 5618650,
    "end": 5623326,
    "text": "そうでなければ、私たちが達成しようとしていることに対して計算が成り立たない。"
  },
  {
    "start": 5623358,
    "end": 5628980,
    "text": "この具体的なケースでは、他を値にしたいのであれば、別の微分式になる。"
  },
  {
    "start": 5629590,
    "end": 5634678,
    "text": "ここでは、このデータを他のデータよりべき乗した値である \"up the value \"を作成する。"
  },
  {
    "start": 5634764,
    "end": 5636694,
    "text": "ここでいう \"other \"とは、例えば \"negative \"のことである。"
  },
  {
    "start": 5636732,
    "end": 5638790,
    "text": "それが私たちが達成したいと思っていることだ。"
  },
  {
    "start": 5639370,
    "end": 5641858,
    "text": "であれば、これは後方のスタブである。"
  },
  {
    "start": 5641954,
    "end": 5655498,
    "text": "これは楽しい部分で、べき乗関数を逆伝播するための連鎖法則式は何かということだ。"
  },
  {
    "start": 5655674,
    "end": 5656794,
    "text": "これがその練習だ。"
  },
  {
    "start": 5656842,
    "end": 5661280,
    "text": "ここでビデオを一時停止して、ここに何を入れるべきか自分で考えてみてください。"
  },
  {
    "start": 5666930,
    "end": 5672434,
    "text": "では、実際にデリバティブのルールを例として見てみよう。"
  },
  {
    "start": 5672552,
    "end": 5676450,
    "text": "微分積分学で習うような微分のルールがたくさん出てくる。"
  },
  {
    "start": 5676520,
    "end": 5688214,
    "text": "というのも、dをxのdx倍してnにする、つまりここでやっていることは、xをn倍してnから1を引くだけだからだ。"
  },
  {
    "start": 5688412,
    "end": 5689160,
    "text": "そうだろう？"
  },
  {
    "start": 5689530,
    "end": 5695530,
    "text": "さて、これでこの電力操作の局所微分についてわかった。"
  },
  {
    "start": 5695870,
    "end": 5703258,
    "text": "ここで欲しいのは、基本的にnがotherになり、自己のドットデータがxになることだ。"
  },
  {
    "start": 5703424,
    "end": 5712858,
    "text": "つまり、これはotherとなり、selfのn倍のdot dataとなり、これはpythonのintまたはfloatとなる。"
  },
  {
    "start": 5713034,
    "end": 5714190,
    "text": "バリュー・オブジェクトではない。"
  },
  {
    "start": 5714260,
    "end": 5720914,
    "text": "他のマイナス1乗、あるいはnマイナス1乗のデータ属性にアクセスしているのだ。"
  },
  {
    "start": 5721112,
    "end": 5727966,
    "text": "括弧で囲むこともできるが、ピホンでは倍率よりもパワーが優先されるので、これは問題ではない。"
  },
  {
    "start": 5727998,
    "end": 5731282,
    "text": "というのも問題なかっただろう。"
  },
  {
    "start": 5731336,
    "end": 5735442,
    "text": "そして、アルパスのグラッドを掛けるだけで連鎖させることができる。"
  },
  {
    "start": 5735506,
    "end": 5736658,
    "text": "それがチェーンルールだ。"
  },
  {
    "start": 5736834,
    "end": 5742246,
    "text": "技術的にはうまくいくはずだし、すぐに分かるだろう。"
  },
  {
    "start": 5742348,
    "end": 5747850,
    "text": "こうすれば、うまくいくはずだ。"
  },
  {
    "start": 5747920,
    "end": 5751034,
    "text": "フォワードパスは機能するが、バックワードパスは機能するのか？"
  },
  {
    "start": 5751152,
    "end": 5754006,
    "text": "実は、引き算の仕方も知らなければならないことに気づいたんだ。"
  },
  {
    "start": 5754118,
    "end": 5757262,
    "text": "今はマイナスBでは通用しない。"
  },
  {
    "start": 5757396,
    "end": 5761566,
    "text": "そのためには、もうひとつコードが必要だ。"
  },
  {
    "start": 5761748,
    "end": 5766202,
    "text": "基本的にはこれが引き算だ。"
  },
  {
    "start": 5766266,
    "end": 5770698,
    "text": "引き算を実装する方法は、否定の足し算で実装することになる。"
  },
  {
    "start": 5770794,
    "end": 5773714,
    "text": "否定を実装するために、マイナス1を掛ける。"
  },
  {
    "start": 5773832,
    "end": 5778786,
    "text": "ただ、もう一度言うけれど、僕らがすでに作り上げたものを使って、僕らが持っているものだけで表現しているんだ。"
  },
  {
    "start": 5778888,
    "end": 5780950,
    "text": "aマイナスBが機能するようになった。"
  },
  {
    "start": 5781100,
    "end": 5784790,
    "text": "では、このニューロンの式をもう一度スクロールしてみよう。"
  },
  {
    "start": 5785210,
    "end": 5791640,
    "text": "oを定義したら、ここでバックワードパスを計算し、それを描画してみよう。"
  },
  {
    "start": 5792010,
    "end": 5798106,
    "text": "この2次元ニューロンの葉節点の勾配は、前に見たように10hである。"
  },
  {
    "start": 5798288,
    "end": 5804202,
    "text": "では、この10個のhをこの式に分割したいと思います。"
  },
  {
    "start": 5804336,
    "end": 5806960,
    "text": "ここにコピーペーストさせてください。"
  },
  {
    "start": 5807330,
    "end": 5813406,
    "text": "ラベルを保存する代わりに、oの定義方法を変更する。"
  },
  {
    "start": 5813588,
    "end": 5816622,
    "text": "特に、ここではこの公式を実行する。"
  },
  {
    "start": 5816756,
    "end": 5820734,
    "text": "の場合、2つのxから1を引いた値にeが必要で、xから1を足した値にeが必要である。"
  },
  {
    "start": 5820852,
    "end": 5822642,
    "text": "eを2つのxに変換する。"
  },
  {
    "start": 5822776,
    "end": 5826194,
    "text": "nを2倍して指数化する必要がある。"
  },
  {
    "start": 5826312,
    "end": 5827938,
    "text": "これは2つのxに対するeだ。"
  },
  {
    "start": 5828024,
    "end": 5842360,
    "text": "を2回使うので、中間変数eを作り、oをeプラス1オーバーeマイナス1オーバーeプラス1、eマイナス1オーバーeプラス1と定義しよう。"
  },
  {
    "start": 5842730,
    "end": 5844230,
    "text": "それでいいはずだ。"
  },
  {
    "start": 5844300,
    "end": 5846850,
    "text": "であれば、oのドットを描くことができるはずだ。"
  },
  {
    "start": 5847020,
    "end": 5850746,
    "text": "さて、これを実行する前に、我々は何を期待しているのだろうか？"
  },
  {
    "start": 5850928,
    "end": 5857466,
    "text": "その1、10時間という長丁場を他のオペレーションに分割したため、グラフはもっと長くなることが予想される。"
  },
  {
    "start": 5857658,
    "end": 5860170,
    "text": "これらの操作は数学的に同等である。"
  },
  {
    "start": 5860250,
    "end": 5864622,
    "text": "だから、我々が期待しているのは、第一に、ここでも同じ結果だ。"
  },
  {
    "start": 5864676,
    "end": 5872830,
    "text": "そして2つ目は、その数学的等価性から、同じ後方パスと同じ勾配がこれらのリーフノードに見られると予想されることだ。"
  },
  {
    "start": 5872910,
    "end": 5875006,
    "text": "これらのグラデーションは同一でなければならない。"
  },
  {
    "start": 5875198,
    "end": 5876980,
    "text": "これを実行しよう。"
  },
  {
    "start": 5877990,
    "end": 5886950,
    "text": "番、10hのノードが1つではなく、xとプラスマイナス1倍になっていることを確認しよう。"
  },
  {
    "start": 5887100,
    "end": 5891622,
    "text": "ここが分かれ目で、結局はここでも同じフォワードパスになる。"
  },
  {
    "start": 5891756,
    "end": 5894902,
    "text": "次にグラデーションだが、順番が少し違うので注意しなければならない。"
  },
  {
    "start": 5894956,
    "end": 5907094,
    "text": "w 2とx 2の勾配はゼロとゼロ5、w 2とx 2の勾配はゼロとゼロ5、w 1とx 1の勾配は1とマイナス1.51とマイナス1.5である。"
  },
  {
    "start": 5907232,
    "end": 5915280,
    "text": "ということは、前方へのパスも後方へのパスも正しかったということだ。"
  },
  {
    "start": 5915730,
    "end": 5923070,
    "text": "だから、この練習をやりたかった理由は、第一に、もう少し演算を練習し、もっとバックワードパスを書くことができたからだ。"
  },
  {
    "start": 5923150,
    "end": 5931330,
    "text": "その2は、どのレベルで作戦を実行するかは、完全にあなた次第だという点を説明したかった。"
  },
  {
    "start": 5931400,
    "end": 5946130,
    "text": "バックワードパスは、プラス1回やマイナス1回といった小さな表現に対して実装することもできるし、例えば10回のhに対して実装することもできる。"
  },
  {
    "start": 5946290,
    "end": 5948530,
    "text": "本当にすべてが偽りのコンセプトのようなものだ。"
  },
  {
    "start": 5948610,
    "end": 5953830,
    "text": "重要なのは、ある種のインプットとある種のアウトプットがあり、このアウトプットが何らかの形でインプットの関数であるということだ。"
  },
  {
    "start": 5953900,
    "end": 5962942,
    "text": "フォワード・パスとバックワード・パスができれば、その操作が何であろうと、どれだけ複合的であろうと構わない。"
  },
  {
    "start": 5963076,
    "end": 5967338,
    "text": "局所勾配を書くことができれば、勾配を連鎖させることができ、逆伝播を続けることができる。"
  },
  {
    "start": 5967434,
    "end": 5971440,
    "text": "その機能をどうデザインするかは、完全にあなた次第だ。"
  },
  {
    "start": 5971910,
    "end": 5982594,
    "text": "しかし、最新のディープ・ニューラル・ネットワーク・ライブラリ、たとえばPytorchを使えば、まったく同じことができる。"
  },
  {
    "start": 5982792,
    "end": 5989474,
    "text": "だからPytorchは本番で使うもので、まったく同じことをPytorch APIでできる方法を紹介しよう。"
  },
  {
    "start": 5989602,
    "end": 5992662,
    "text": "コピーペーストして、少し説明するよ。"
  },
  {
    "start": 5992716,
    "end": 5994060,
    "text": "こんな感じだ。"
  },
  {
    "start": 5994670,
    "end": 6001642,
    "text": "Pytorchをインポートして、ここにあるような値オブジェクトを定義する必要がある。"
  },
  {
    "start": 6001776,
    "end": 6008702,
    "text": "マイクログラッドはスカラー値エンジンなので、2.0のようなスカラー値しかない。"
  },
  {
    "start": 6008836,
    "end": 6011322,
    "text": "Pytorchでは、すべてがテンソルに基づいている。"
  },
  {
    "start": 6011386,
    "end": 6015546,
    "text": "さっきも言ったように、テンソルはスカラーのn次元配列にすぎない。"
  },
  {
    "start": 6015738,
    "end": 6018770,
    "text": "だから、ここで少しややこしくなる。"
  },
  {
    "start": 6018840,
    "end": 6023198,
    "text": "必要なのはスカラー値のテンソル、つまり単一の要素だけを持つテンソルだ。"
  },
  {
    "start": 6023374,
    "end": 6030278,
    "text": "デフォルトでは、Pytorchで作業する場合、このような複雑なテンソルを使うことになる。"
  },
  {
    "start": 6030364,
    "end": 6035974,
    "text": "pytorchをインポートすると、このようにテンソルを作ることができる。"
  },
  {
    "start": 6036092,
    "end": 6044786,
    "text": "例えば、このテンソルは2×3のスカラーの配列で、1つのコンパクトな表現である。"
  },
  {
    "start": 6044898,
    "end": 6049290,
    "text": "形状をチェックすると、2×3の配列であることがわかる。"
  },
  {
    "start": 6049440,
    "end": 6053414,
    "text": "これは通常、実際のライブラリーで扱うものだ。"
  },
  {
    "start": 6053542,
    "end": 6067838,
    "text": "Pythonはデフォルトで浮動小数点数に倍精度を使うからだ。"
  },
  {
    "start": 6067924,
    "end": 6069854,
    "text": "すべてを同じにしたいんだ。"
  },
  {
    "start": 6069982,
    "end": 6074418,
    "text": "デフォルトでは、これらのテンソルのデータ型は float 32 である。"
  },
  {
    "start": 6074504,
    "end": 6076590,
    "text": "これは単精度の浮動小数点数しか使っていない。"
  },
  {
    "start": 6076670,
    "end": 6082130,
    "text": "Pythonのようにdoubleにキャストして、float64にする。"
  },
  {
    "start": 6082550,
    "end": 6087830,
    "text": "ダブルにキャストすると、2の値に近いものが得られるんだ。"
  },
  {
    "start": 6087980,
    "end": 6090562,
    "text": "次にしなければならないのは、これらはリーフ・ノードだからだ。"
  },
  {
    "start": 6090626,
    "end": 6093714,
    "text": "デフォルトでは、Pytorchは勾配を必要としないと仮定している。"
  },
  {
    "start": 6093842,
    "end": 6097798,
    "text": "これらのノードのすべてにグラデーションが必要であることを明示する必要がある。"
  },
  {
    "start": 6097974,
    "end": 6103174,
    "text": "さて、これでスカラー値の1要素テンソルを構成することになる。"
  },
  {
    "start": 6103302,
    "end": 6105878,
    "text": "Pytorchがグラデーションを要求していることを確認する。"
  },
  {
    "start": 6106054,
    "end": 6115530,
    "text": "ちなみにデフォルトでは、これらはfalseに設定されている。効率上の理由からで、通常はネットワークの入力のようなリーフ・ノードには勾配をつけたくないからだ。"
  },
  {
    "start": 6115610,
    "end": 6118830,
    "text": "これは最も一般的なケースで効率的であろうとしているだけだ。"
  },
  {
    "start": 6119170,
    "end": 6125806,
    "text": "パイトークランドですべての値を定義したら、ここマイクログランドランドでできるのと同じように算術演算を行うことができる。"
  },
  {
    "start": 6125918,
    "end": 6127058,
    "text": "これなら大丈夫だ。"
  },
  {
    "start": 6127144,
    "end": 6129394,
    "text": "それから、トーチ10本もある。"
  },
  {
    "start": 6129592,
    "end": 6132274,
    "text": "というテンソルが返ってくる。"
  },
  {
    "start": 6132472,
    "end": 6138018,
    "text": "microgradのように、data属性とgrad属性を持つことができる。"
  },
  {
    "start": 6138194,
    "end": 6142654,
    "text": "これらのテンソル・オブジェクトは、マイクログラッドと同様、ドット・データとドット・グラッドを持っている。"
  },
  {
    "start": 6142802,
    "end": 6157370,
    "text": "ここでの唯一の違いは、ドット・アイテムを呼び出す必要があることだ。そうでなければ、pytorchのドット・アイテムは基本的に、1つの要素からなる1つのテンソルを受け取り、テンソルを取り除いてその要素を返すだけだからだ。"
  },
  {
    "start": 6157790,
    "end": 6170994,
    "text": "うまくいけば、フォワード・パスが印刷され、ゼロ、7、そしてグラデーションが印刷される。"
  },
  {
    "start": 6171112,
    "end": 6175554,
    "text": "これを実行すれば、ゼロ7だ。"
  },
  {
    "start": 6175592,
    "end": 6180354,
    "text": "フォワードパスは同意し、00:50マイナス1.5と1。"
  },
  {
    "start": 6180552,
    "end": 6182582,
    "text": "ピトーチも同意見だ。"
  },
  {
    "start": 6182716,
    "end": 6194086,
    "text": "基本的に、oは1つの要素を持つテンソルであり、それはdoubleであり、itemを呼び出すことで1つの数値を取り出すことができる。"
  },
  {
    "start": 6194268,
    "end": 6195866,
    "text": "それがアイテムの役割だ。"
  },
  {
    "start": 6195968,
    "end": 6201530,
    "text": "oは先ほど言ったようなテンソル・オブジェクトで、我々が実装したような後方関数を持っている。"
  },
  {
    "start": 6202030,
    "end": 6204294,
    "text": "であれば、これらもすべてドット・グラードである。"
  },
  {
    "start": 6204342,
    "end": 6210430,
    "text": "例えば、x 2のように、gradがあり、テンソルであり、ドット・アイテムで個々の数値を取り出すことができる。"
  },
  {
    "start": 6211410,
    "end": 6220346,
    "text": "基本的にtorchは、テンソルがすべて単一要素テンソルである場合の特別なケースとして、microgradで行ったようなことができる。"
  },
  {
    "start": 6220458,
    "end": 6230450,
    "text": "Pytorchの大きな特徴は、テンソル・オブジェクトを扱うことで、すべてが大幅に効率化されることだ。"
  },
  {
    "start": 6231430,
    "end": 6235554,
    "text": "そうでなければ、私たちが作ったものはPytorchのAPIと非常に一致している。"
  },
  {
    "start": 6235682,
    "end": 6241954,
    "text": "さて、かなり複雑な数式を構築するための機械ができたので、ニューラルネットの構築も始められる。"
  },
  {
    "start": 6242002,
    "end": 6246550,
    "text": "前述したように、ニューラルネットは数学的表現の特定のクラスに過ぎない。"
  },
  {
    "start": 6246990,
    "end": 6253978,
    "text": "そして最終的には、2層、多層パーセプトロンと呼ばれるものを作り上げる。"
  },
  {
    "start": 6254064,
    "end": 6255802,
    "text": "それが何を意味するのか、具体的にお見せしよう。"
  },
  {
    "start": 6255936,
    "end": 6257850,
    "text": "まずは1個のニューロンから始めよう。"
  },
  {
    "start": 6257930,
    "end": 6267194,
    "text": "ここでは、ニューラルネットワーク・モジュールの設計方法においてPytorch APIにも準拠するものを実装する。"
  },
  {
    "start": 6267322,
    "end": 6275694,
    "text": "オートグラッド側でPytorchのAPIと一致させることができることを見たように、ニューラルネットワーク・モジュールでもそれを試してみる。"
  },
  {
    "start": 6275822,
    "end": 6277490,
    "text": "これがクラスニューロンだ。"
  },
  {
    "start": 6278390,
    "end": 6284630,
    "text": "ただ、効率化のために、比較的簡単な部分をコピーペーストするつもりだ。"
  },
  {
    "start": 6285450,
    "end": 6292338,
    "text": "コンストラクタはニューロンへの入力数を受け取る。"
  },
  {
    "start": 6292434,
    "end": 6305610,
    "text": "例えば、このニューロンには3つの入力があり、それらの入力1つ1つに対してマイナス1から1の間の乱数である重みと、このニューロンの全体的なトリガー幸福度をコントロールするバイアスを作成する。"
  },
  {
    "start": 6306510,
    "end": 6313854,
    "text": "では、selfと入力xのdef underscore, underscoreコールを実装しよう。"
  },
  {
    "start": 6313972,
    "end": 6320750,
    "text": "ここでやりたくないのは、w×xにbを足したもので、ここでいうw×xは具体的にはドット積である。"
  },
  {
    "start": 6321330,
    "end": 6326482,
    "text": "まだコールを見ていない人は、とりあえずここで0.0を返しておこう。"
  },
  {
    "start": 6326616,
    "end": 6330674,
    "text": "この仕組みは、例えば2.03.0のようなXを持つことができる。"
  },
  {
    "start": 6330792,
    "end": 6339190,
    "text": "そして、2つの数値からなる2次元のニューロンを初期化し、そのニューロンに2つの数値を入力して出力を得ることができる。"
  },
  {
    "start": 6339690,
    "end": 6344460,
    "text": "したがって、xのnという表記を使うとき、Pythonはcallを使う。"
  },
  {
    "start": 6344910,
    "end": 6347500,
    "text": "現在の呼び出しは0.0を返すだけである。"
  },
  {
    "start": 6350030,
    "end": 6354300,
    "text": "では、このニューロンのフォワードパスを実際にやってみよう。"
  },
  {
    "start": 6354830,
    "end": 6362030,
    "text": "ここでまずやることは、基本的にwの全要素とxの全要素を掛け合わせることだ。"
  },
  {
    "start": 6362100,
    "end": 6364078,
    "text": "ペアで掛け合わせる必要がある。"
  },
  {
    "start": 6364244,
    "end": 6368866,
    "text": "まず最初にやることは、サルタのWとXのジッパーを閉めることだ。"
  },
  {
    "start": 6369048,
    "end": 6377490,
    "text": "のPythonでは、zipは2つのイテレータを受け取り、対応するエントリのタプルを反復処理する新しいイテレータを作成する。"
  },
  {
    "start": 6377830,
    "end": 6383734,
    "text": "例えば、このリストを印刷しても0.0が返されることを示しておこう。"
  },
  {
    "start": 6383772,
    "end": 6398780,
    "text": "ここで、これらのwはxと対になっていることがわかる。"
  },
  {
    "start": 6401390,
    "end": 6414618,
    "text": "W-I-X-Iの場合、WにW、IにXIを掛ける。"
  },
  {
    "start": 6414794,
    "end": 6421986,
    "text": "そして、そのすべてを合計して活性化し、さらにスルタンBを加える。"
  },
  {
    "start": 6422168,
    "end": 6423726,
    "text": "これが列の活性化だ。"
  },
  {
    "start": 6423838,
    "end": 6426526,
    "text": "となれば、当然、非線形性を通す必要がある。"
  },
  {
    "start": 6426638,
    "end": 6431460,
    "text": "これから返すのは第10幕のHと、ここからだ。"
  },
  {
    "start": 6432230,
    "end": 6440710,
    "text": "そして、異なる重みとバイアスを初期化しているため、毎回ニューロンから異なる出力が得られていることがわかる。"
  },
  {
    "start": 6441130,
    "end": 6451386,
    "text": "ここでもう少し効率的にするために、実はsumは2つ目のオプションのパラメータを取る。"
  },
  {
    "start": 6451488,
    "end": 6455630,
    "text": "この和の要素は、ゼロの上に加算される。"
  },
  {
    "start": 6455700,
    "end": 6460560,
    "text": "実際にはセルbから始めて、次のような式にすればいい。"
  },
  {
    "start": 6465410,
    "end": 6468990,
    "text": "であれば、ここでのジェネレーター式はPythonの括弧でなければならない。"
  },
  {
    "start": 6469330,
    "end": 6470400,
    "text": "これでよし。"
  },
  {
    "start": 6473730,
    "end": 6474154,
    "text": "そうだ。"
  },
  {
    "start": 6474212,
    "end": 6476446,
    "text": "これで1つのニューロンを前進させることができる。"
  },
  {
    "start": 6476558,
    "end": 6479246,
    "text": "次はニューロンのレイヤーを定義する。"
  },
  {
    "start": 6479358,
    "end": 6482318,
    "text": "ここにMLPの回路図がある。"
  },
  {
    "start": 6482494,
    "end": 6491122,
    "text": "各層（これは1つの層だが）には、実際にはいくつかのニューロンがあり、それらは互いに接続されていないが、すべてのニューロンが入力に完全に接続されていることがわかる。"
  },
  {
    "start": 6491266,
    "end": 6493414,
    "text": "ニューロンの層とは何か？"
  },
  {
    "start": 6493542,
    "end": 6496170,
    "text": "独立に評価されるニューロンの集合にすぎない。"
  },
  {
    "start": 6496670,
    "end": 6502220,
    "text": "時間の都合上、ここではかなり単純なことをするつもりだ。"
  },
  {
    "start": 6503150,
    "end": 6508790,
    "text": "文字通り、レイヤーはニューロンのリストにすぎない。"
  },
  {
    "start": 6508950,
    "end": 6510734,
    "text": "では、ニューロンの数は？"
  },
  {
    "start": 6510772,
    "end": 6512622,
    "text": "ここでは、それをインプットの論拠とする。"
  },
  {
    "start": 6512676,
    "end": 6514234,
    "text": "レイヤーに何個のニューロンを入れたいか？"
  },
  {
    "start": 6514282,
    "end": 6516346,
    "text": "このレイヤーの出力数。"
  },
  {
    "start": 6516538,
    "end": 6521230,
    "text": "したがって、この与えられた次元数で完全に独立したニューロンを初期化するだけである。"
  },
  {
    "start": 6521310,
    "end": 6526162,
    "text": "私たちはそれを呼び出したら、独自に評価するだけだ。"
  },
  {
    "start": 6526296,
    "end": 6529614,
    "text": "ニューロンの代わりに、ニューロンの層を作ることができる。"
  },
  {
    "start": 6529662,
    "end": 6532182,
    "text": "それらは2次元のニューロンで、3つあることにしよう。"
  },
  {
    "start": 6532316,
    "end": 6537154,
    "text": "ここで、3つの異なるニューロンについて、3つの独立した評価があることがわかる。"
  },
  {
    "start": 6537282,
    "end": 6537960,
    "text": "そうだね。"
  },
  {
    "start": 6538730,
    "end": 6539142,
    "text": "オーケー。"
  },
  {
    "start": 6539196,
    "end": 6544402,
    "text": "最後に、この図を完成させ、多層パーセプトロン（MLP）全体を定義してみましょう。"
  },
  {
    "start": 6544546,
    "end": 6548890,
    "text": "ここでわかるように、MLPでは、これらの層は互いに順次フィード・インしていくだけである。"
  },
  {
    "start": 6549230,
    "end": 6554074,
    "text": "時間の都合上、ここにコードをコピーしておこう。"
  },
  {
    "start": 6554272,
    "end": 6556506,
    "text": "MLPは非常によく似ている。"
  },
  {
    "start": 6556688,
    "end": 6559514,
    "text": "インプットの数は以前と同じだ。"
  },
  {
    "start": 6559632,
    "end": 6565962,
    "text": "ここでは、1つのレイヤーのニューロン数であるn個のアウトを取る代わりに、n個のアウトのリストを取ることにする。"
  },
  {
    "start": 6566026,
    "end": 6570138,
    "text": "このリストには、MLP に必要なすべての層のサイズが定義されています。"
  },
  {
    "start": 6570314,
    "end": 6577650,
    "text": "ここでは、それらをすべてまとめて、これらのサイズの連続するペアを繰り返し、それらのレイヤー・オブジェクトを作成する。"
  },
  {
    "start": 6577800,
    "end": 6580570,
    "text": "ということは、コール関数の中では、ただ順番にコールしていることになる。"
  },
  {
    "start": 6580670,
    "end": 6582566,
    "text": "それはMLPだ。"
  },
  {
    "start": 6582748,
    "end": 6584706,
    "text": "この絵を実際に再現してみよう。"
  },
  {
    "start": 6584738,
    "end": 6589190,
    "text": "入力ニューロンが3つ、4つのレイヤーが2つ、そして出力ユニットが1つ欲しい。"
  },
  {
    "start": 6589770,
    "end": 6593702,
    "text": "私たちは3次元の入力を求めている。"
  },
  {
    "start": 6593766,
    "end": 6595318,
    "text": "これが例だとすると、入力。"
  },
  {
    "start": 6595414,
    "end": 6600454,
    "text": "3つの入力を4つの2つのレイヤーと1つの出力にしたい。"
  },
  {
    "start": 6600582,
    "end": 6602650,
    "text": "これはもちろんMLPである。"
  },
  {
    "start": 6603550,
    "end": 6604490,
    "text": "さあ、行こう。"
  },
  {
    "start": 6604560,
    "end": 6606518,
    "text": "MLPのフォワードパスだ。"
  },
  {
    "start": 6606694,
    "end": 6615310,
    "text": "レイヤーは便利なように常にリストを返すからだ。"
  },
  {
    "start": 6615650,
    "end": 6622222,
    "text": "Len outsが正確に1つの要素であればoutsをゼロで返し、そうでなければfullestを返す。"
  },
  {
    "start": 6622366,
    "end": 6627570,
    "text": "これにより、ニューロンが1つしかない最後のレイヤーで、1つの値だけを取り出すことができる。"
  },
  {
    "start": 6627990,
    "end": 6631238,
    "text": "最後に、xのnの点を描くことができるはずだ。"
  },
  {
    "start": 6631404,
    "end": 6638306,
    "text": "ご想像のとおり、これらの表現は現在、比較的大きな意味を持つようになってきている。"
  },
  {
    "start": 6638498,
    "end": 6647700,
    "text": "これは今定義しているMLP全体であり、1つの出力に至るまですべてである。"
  },
  {
    "start": 6648280,
    "end": 6649076,
    "text": "いいかい？"
  },
  {
    "start": 6649258,
    "end": 6653520,
    "text": "だから、ペンと紙でこれらの表現を区別することはないだろう。"
  },
  {
    "start": 6653600,
    "end": 6662788,
    "text": "マイクログラッドを使えば、これをバックプロパゲートし、すべてのニューロンの重みをバックプロパゲートすることができる。"
  },
  {
    "start": 6662884,
    "end": 6664088,
    "text": "どうなるか見てみよう。"
  },
  {
    "start": 6664174,
    "end": 6668520,
    "text": "では、非常にシンプルなデータ・セットを作ってみよう。"
  },
  {
    "start": 6668670,
    "end": 6670904,
    "text": "このデータセットには4つの例がある。"
  },
  {
    "start": 6671032,
    "end": 6677512,
    "text": "つまり、ニューラルネットには4つの入力が可能で、4つの目標がある。"
  },
  {
    "start": 6677576,
    "end": 6687664,
    "text": "ニューラルネットに、この例を入力したときは1.0、これらの例を入力したときはマイナス1、この例を入力したときは1を代入または出力させたい。"
  },
  {
    "start": 6687782,
    "end": 6692176,
    "text": "これは非常に単純な二値分類器であり、基本的にはニューラルネットである。"
  },
  {
    "start": 6692358,
    "end": 6695776,
    "text": "では、この4つの例についてニューラルネットが現在どう考えているかを考えてみよう。"
  },
  {
    "start": 6695888,
    "end": 6697620,
    "text": "彼らの予想を聞けばいいだけだ。"
  },
  {
    "start": 6698120,
    "end": 6704260,
    "text": "基本的には、軸のxに対してxのnを呼び出すだけでいい。"
  },
  {
    "start": 6705000,
    "end": 6708740,
    "text": "これらの4つの例に対するニューラルネットの出力である。"
  },
  {
    "start": 6708900,
    "end": 6713704,
    "text": "はゼロ91だが、1にしてほしい。"
  },
  {
    "start": 6713822,
    "end": 6715800,
    "text": "我々はこれをもっと高く押し上げるべきだ。"
  },
  {
    "start": 6715870,
    "end": 6717864,
    "text": "これはもっと高くしたい。"
  },
  {
    "start": 6718062,
    "end": 6722250,
    "text": "これはゼロ88と書いてあり、これをマイナス1にしたい。"
  },
  {
    "start": 6722620,
    "end": 6725004,
    "text": "これはゼロ・エイトナであり、マイナス1にしたい。"
  },
  {
    "start": 6725122,
    "end": 6726284,
    "text": "こちらはゼロ8だ。"
  },
  {
    "start": 6726322,
    "end": 6727470,
    "text": "私たちはそれを望んでいる。"
  },
  {
    "start": 6728160,
    "end": 6736160,
    "text": "どのようにニューラルネットを作り、どのように重みを調整すれば、より的確な予測が可能になるのか？"
  },
  {
    "start": 6736660,
    "end": 6745120,
    "text": "これを達成するためにディープラーニングで使用されるトリックは、ニューラルネットの総合的なパフォーマンスを測定する単一の数値を計算することである。"
  },
  {
    "start": 6745190,
    "end": 6747380,
    "text": "この数字1つを損失と呼ぶ。"
  },
  {
    "start": 6747960,
    "end": 6756048,
    "text": "最初の損失は、基本的にニューラルネットのパフォーマンスを測定するために定義する数値である。"
  },
  {
    "start": 6756144,
    "end": 6760772,
    "text": "今、私たちは直感的に、あまりうまくいっていないと感じている。"
  },
  {
    "start": 6760906,
    "end": 6764532,
    "text": "損失が大きくなるので、損失を最小限に抑えたい。"
  },
  {
    "start": 6764676,
    "end": 6769448,
    "text": "特に今回は、平均二乗誤差損失を導入する。"
  },
  {
    "start": 6769624,
    "end": 6781140,
    "text": "これは基本的に、yのグランド・トゥルースとyのアウトプットを、yとyのパンのzipで反復することになる。"
  },
  {
    "start": 6781240,
    "end": 6788736,
    "text": "ジップはそれらのタプルを繰り返し処理する。"
  },
  {
    "start": 6788918,
    "end": 6798324,
    "text": "yのグランドトゥルースとyのアウトプットのそれぞれについて、それらを差し引き、二乗する。"
  },
  {
    "start": 6798522,
    "end": 6800404,
    "text": "まず、これらの損失が何なのかを見てみよう。"
  },
  {
    "start": 6800442,
    "end": 6802340,
    "text": "これらは個々の損失要素である。"
  },
  {
    "start": 6802920,
    "end": 6812010,
    "text": "つまり、基本的には、4つのそれぞれについて、予測と地上真実を引き、それらを2乗している。"
  },
  {
    "start": 6812620,
    "end": 6818250,
    "text": "というのも、これは目標に非常に近いからだ。"
  },
  {
    "start": 6818620,
    "end": 6821230,
    "text": "それを引くと、とても小さな数字になる。"
  },
  {
    "start": 6821840,
    "end": 6825064,
    "text": "ここでは、負のゼロ、1、そして2乗となる。"
  },
  {
    "start": 6825112,
    "end": 6832990,
    "text": "これは、私たちがよりネガティブであろうと、よりポジティブであろうと、常にポジティブな数字が得られるようにするためのものだ。"
  },
  {
    "start": 6834240,
    "end": 6837360,
    "text": "二乗する代わりに、たとえば絶対値をとることもできる。"
  },
  {
    "start": 6837430,
    "end": 6839010,
    "text": "看板を捨てる必要がある。"
  },
  {
    "start": 6839380,
    "end": 6847596,
    "text": "この式は、y outとy ground truthが等しいときだけ正確にゼロになるようにアレンジされていることがわかる。"
  },
  {
    "start": 6847628,
    "end": 6849440,
    "text": "あなたの予想はまさに的中している。"
  },
  {
    "start": 6849520,
    "end": 6850788,
    "text": "あなたはゼロになる。"
  },
  {
    "start": 6850954,
    "end": 6854900,
    "text": "もし予想が的中しなければ、他の数字が出ることになる。"
  },
  {
    "start": 6855050,
    "end": 6856980,
    "text": "例えば、ここでは、我々は大きく外れている。"
  },
  {
    "start": 6857050,
    "end": 6859290,
    "text": "だから、損失はかなり大きい。"
  },
  {
    "start": 6859820,
    "end": 6863896,
    "text": "私たちが不利になればなるほど、損失は大きくなる。"
  },
  {
    "start": 6864078,
    "end": 6867620,
    "text": "我々は高損失を望んでいるのではなく、低損失を望んでいるのだ。"
  },
  {
    "start": 6867780,
    "end": 6874028,
    "text": "つまり、ここでの最終的な損失は、これらの数字の合計だけとなる。"
  },
  {
    "start": 6874194,
    "end": 6880524,
    "text": "これはゼロであるべきであることがわかるだろう。"
  },
  {
    "start": 6880722,
    "end": 6884370,
    "text": "ここで7敗くらいするはずだ。"
  },
  {
    "start": 6884820,
    "end": 6887148,
    "text": "今は損失を最小限に抑えたい。"
  },
  {
    "start": 6887244,
    "end": 6895780,
    "text": "損失が少なければ、すべての予測が目標と等しくなるからだ。"
  },
  {
    "start": 6896280,
    "end": 6898772,
    "text": "最低でもゼロである。"
  },
  {
    "start": 6898906,
    "end": 6903140,
    "text": "これが大きいほど、ニューラルネットの予測は不利になる。"
  },
  {
    "start": 6904040,
    "end": 6910170,
    "text": "さて、もちろん、逆算すると、エンターキーを押したときに何か不思議なことが起こった。"
  },
  {
    "start": 6910540,
    "end": 6920430,
    "text": "もちろん起こった不思議なことは、n層、ニューロン、n層を、例えば第1層のニューロンのようにゼロで見ることができるということだ。"
  },
  {
    "start": 6922480,
    "end": 6928668,
    "text": "MLPにはリストである層があり、各層にはリストであるニューロンがあることを覚えておいてほしい。"
  },
  {
    "start": 6928754,
    "end": 6930628,
    "text": "それが個々の神経細胞を生み出す。"
  },
  {
    "start": 6930744,
    "end": 6932620,
    "text": "それなら、ウェイトがある。"
  },
  {
    "start": 6932780,
    "end": 6935890,
    "text": "したがって、例えば、ウェイトをゼロで見ることができる。"
  },
  {
    "start": 6940180,
    "end": 6942850,
    "text": "おっと、ウェイトとは呼ばない。"
  },
  {
    "start": 6944260,
    "end": 6945356,
    "text": "それが価値だ。"
  },
  {
    "start": 6945478,
    "end": 6949590,
    "text": "今、この値はバックワードパスのためにグラッドも持っている。"
  },
  {
    "start": 6950040,
    "end": 6961092,
    "text": "つまり、この層のこの特定のニューロンのこの特定の重みの勾配が負であるため、損失への影響も負であることがわかる。"
  },
  {
    "start": 6961236,
    "end": 6967850,
    "text": "この層のこのニューロンのウェイトを少し増やせば、損失は減る。"
  },
  {
    "start": 6968780,
    "end": 6973272,
    "text": "私たちは実際に、ニューロンのひとつひとつとその全パラメーターについて、この情報を持っている。"
  },
  {
    "start": 6973336,
    "end": 6977724,
    "text": "ちなみに、引き分けでも負けと見る価値はある。"
  },
  {
    "start": 6977922,
    "end": 6983668,
    "text": "以前、私たちはニューラルネットのニューロン1個のフォワードパスのドロー・ドットを見たが、それはすでに大きな表現だった。"
  },
  {
    "start": 6983784,
    "end": 6985276,
    "text": "この表現は何だろう？"
  },
  {
    "start": 6985388,
    "end": 6992732,
    "text": "私たちは実際にこの4つの例をすべて転送し、その上に平均二乗誤差による損失を乗せている。"
  },
  {
    "start": 6992876,
    "end": 7000656,
    "text": "というわけで、これは本当に巨大なグラフなんだ。"
  },
  {
    "start": 7000768,
    "end": 7004384,
    "text": "私たちが今作り上げたこのグラフは、ある意味過剰だ。"
  },
  {
    "start": 7004512,
    "end": 7009232,
    "text": "1つの例に対してニューラルネットを4回フォワードパスさせるのだから、これは過剰だ。"
  },
  {
    "start": 7009296,
    "end": 7014936,
    "text": "そして、損失が上乗せされ、7.12と感じる損失の値で終わる。"
  },
  {
    "start": 7015118,
    "end": 7027736,
    "text": "この損失は、すべてのフォワードパスを通じて、ニューラルネットのすべての中間値を通じて、もちろん入力である重みのパラメータに至るまで、すべて逆伝播される。"
  },
  {
    "start": 7027928,
    "end": 7032240,
    "text": "これらのウェイト・パラメーターがニューラルネットの入力となる。"
  },
  {
    "start": 7032390,
    "end": 7036704,
    "text": "これらの数値はニューラルネットへの入力である。"
  },
  {
    "start": 7036902,
    "end": 7046176,
    "text": "このあたりを回れば、おそらくこの1.0やこの1.0、あるいは他の例のいくつかが見つかるだろう。"
  },
  {
    "start": 7046278,
    "end": 7048708,
    "text": "をクリックすると、それらすべてにグラデーションがあることがわかる。"
  },
  {
    "start": 7048874,
    "end": 7053380,
    "text": "しかし、このような入力データの勾配は、我々にとってそれほど有益なものではないのだ。"
  },
  {
    "start": 7053530,
    "end": 7058192,
    "text": "それは、入力データが変更できないことを前提としているからだ。"
  },
  {
    "start": 7058336,
    "end": 7059716,
    "text": "それは当然のことだ。"
  },
  {
    "start": 7059818,
    "end": 7061076,
    "text": "つまり固定入力だ。"
  },
  {
    "start": 7061108,
    "end": 7065050,
    "text": "グラデーションがあっても、それを変更したりいじったりするつもりはない。"
  },
  {
    "start": 7065980,
    "end": 7075150,
    "text": "このグラデーションのいくつかは、ニューラルネットワークのパラメーター、WとBのためのもので、もちろん変更したい。"
  },
  {
    "start": 7075760,
    "end": 7084508,
    "text": "それでは、ニューラルネットの全パラメータを集めて、同時に操作できるようにする便利なコードを考えてみよう。"
  },
  {
    "start": 7084604,
    "end": 7090384,
    "text": "そのひとつひとつを、勾配情報に基づいてほんの少しナッジする。"
  },
  {
    "start": 7090582,
    "end": 7094640,
    "text": "ニューラルネットのパラメーターを1つの配列に集めてみよう。"
  },
  {
    "start": 7094800,
    "end": 7106150,
    "text": "サルタBのリストと連結したリストであるサルタWを返すだけのselfのパラメーターを作ってみよう。"
  },
  {
    "start": 7107160,
    "end": 7112120,
    "text": "これは単にリストを返すだけで、リスト＋リストはリストを返すだけである。"
  },
  {
    "start": 7112270,
    "end": 7114452,
    "text": "これがニューロンのパラメーターだ。"
  },
  {
    "start": 7114596,
    "end": 7122396,
    "text": "というのも、pytorchはすべてのnnモジュールにパラメーターを持っていて、まさにここでやっていることをやってくれるからだ。"
  },
  {
    "start": 7122418,
    "end": 7127580,
    "text": "パラメータ・テンソルを返すだけで、それはパラメータ・スカラーだ。"
  },
  {
    "start": 7128320,
    "end": 7133330,
    "text": "さて、レイヤーもモジュールなので、パラメータselfを持つ。"
  },
  {
    "start": 7134100,
    "end": 7141584,
    "text": "基本的に我々がここでやりたいことは、このようなことだ。"
  },
  {
    "start": 7141622,
    "end": 7153380,
    "text": "次に、ソルト・ニューロンのニューロンに対して、ニューロンのパラメータを取得し、paramsを拡張する。"
  },
  {
    "start": 7154200,
    "end": 7159156,
    "text": "これらはこのニューロンのパラメータで、paramsの上に置く。"
  },
  {
    "start": 7159188,
    "end": 7164600,
    "text": "paramsはpieceを拡張し、paramsを返したい。"
  },
  {
    "start": 7165420,
    "end": 7167752,
    "text": "これはコードが多すぎる。"
  },
  {
    "start": 7167806,
    "end": 7184380,
    "text": "実はこれを単純化する方法がある。neuronパラメータにpを指定して、neuron-inselfニューロンをreturn pで返すのだ。"
  },
  {
    "start": 7185700,
    "end": 7195344,
    "text": "Pythonでは、このようにリストをネストして、目的の配列を作ることができる。"
  },
  {
    "start": 7195392,
    "end": 7197248,
    "text": "これらは同一である。"
  },
  {
    "start": 7197424,
    "end": 7201990,
    "text": "これを外して、ここも同じようにしよう。"
  },
  {
    "start": 7204600,
    "end": 7222510,
    "text": "パラメータをselfに定義し、selfのlayerのパラメータをlayerのpのパラメータとして返す。"
  },
  {
    "start": 7223680,
    "end": 7248630,
    "text": "ネットワークを再初期化する必要があるので、ネットワークを再初期化しないように、これをポップ・アウトしておこう。"
  },
  {
    "start": 7249800,
    "end": 7254950,
    "text": "さて、残念ながらネットワークを再初期化しなければならない。"
  },
  {
    "start": 7255640,
    "end": 7258112,
    "text": "新しいAPIをピックアップするために、そうさせてください。"
  },
  {
    "start": 7258176,
    "end": 7265544,
    "text": "これでエンドパラメーターができ、ニューラルネット全体の重みとバイアスになる。"
  },
  {
    "start": 7265742,
    "end": 7277464,
    "text": "このMLPには全部で41のパラメータがあり、損失を再計算すればパラメータを変更できるようになります。"
  },
  {
    "start": 7277512,
    "end": 7288864,
    "text": "ここでは、残念ながら予想も負け方も微妙に異なっているが、それはそれでいい。"
  },
  {
    "start": 7288902,
    "end": 7292924,
    "text": "このニューロンの勾配がわずかにマイナスであることがわかる。"
  },
  {
    "start": 7293052,
    "end": 7297924,
    "text": "今現在のデータを見ることもできる。"
  },
  {
    "start": 7297962,
    "end": 7302500,
    "text": "これがこのニューロンの現在値で、これが損失に対する勾配である。"
  },
  {
    "start": 7303080,
    "end": 7318090,
    "text": "つまり、このニューラルネットの41個のパラメータすべてについて、勾配情報に従ってp個のドットデータを少しずつ変化させるのだ。"
  },
  {
    "start": 7319120,
    "end": 7321950,
    "text": "じゃあ、ここでやることは点、点、点だ。"
  },
  {
    "start": 7322400,
    "end": 7328008,
    "text": "これは基本的に、この勾配降下スキームにおける小さなアップデートになる。"
  },
  {
    "start": 7328104,
    "end": 7337600,
    "text": "勾配降下法では、勾配を損失が大きくなる方向を指すベクトルとして考えている。"
  },
  {
    "start": 7339060,
    "end": 7347328,
    "text": "つまり、勾配降下法では、p個のドットデータを勾配の方向に小さなステップサイズで修正することになる。"
  },
  {
    "start": 7347424,
    "end": 7354100,
    "text": "例えば、0.1がp dot gradのステップサイズというように。"
  },
  {
    "start": 7355000,
    "end": 7355750,
    "text": "そうだね。"
  },
  {
    "start": 7356360,
    "end": 7358840,
    "text": "私たちはここで、いくつかの兆候について考えなければならない。"
  },
  {
    "start": 7358990,
    "end": 7373020,
    "text": "特に、この具体的な例では、このままにしておけば、このニューロンの値は現在、勾配のわずかな量だけ増加していることがわかる。"
  },
  {
    "start": 7373440,
    "end": 7374952,
    "text": "グラデーションはマイナス。"
  },
  {
    "start": 7375096,
    "end": 7378092,
    "text": "このニューロンの値はわずかに下がるだろう。"
  },
  {
    "start": 7378146,
    "end": 7381810,
    "text": "0.84とかそんな感じになる。"
  },
  {
    "start": 7382820,
    "end": 7390240,
    "text": "このニューロンの値が低くなれば、実際に損失が増えることになる。"
  },
  {
    "start": 7390660,
    "end": 7394956,
    "text": "それは、このニューロンの導関数が負だからだ。"
  },
  {
    "start": 7395068,
    "end": 7398804,
    "text": "これを増やせば、損失は減る。"
  },
  {
    "start": 7398922,
    "end": 7402836,
    "text": "それを減らすのではなく、増やすことが私たちの望むことなのだ。"
  },
  {
    "start": 7402938,
    "end": 7405910,
    "text": "基本的に、私たちがここで見逃しているのは、実際にはマイナスの記号が欠けているということだ。"
  },
  {
    "start": 7406440,
    "end": 7410836,
    "text": "またこの別の解釈だが、それは損失を最小限に抑えたいからだ。"
  },
  {
    "start": 7410868,
    "end": 7413704,
    "text": "損失を最大化するのではなく、減少させたいのだ。"
  },
  {
    "start": 7413822,
    "end": 7425064,
    "text": "もう一つの解釈は、先ほど申し上げたように、勾配ベクトル、つまり基本的にはすべての勾配のベクトルが損失を増やす方向を向いていると考えることができます。"
  },
  {
    "start": 7425192,
    "end": 7426556,
    "text": "ならば、それを減らしたい。"
  },
  {
    "start": 7426578,
    "end": 7428856,
    "text": "本当は逆の方向に行きたいんだ。"
  },
  {
    "start": 7429048,
    "end": 7434720,
    "text": "というわけで、損失を最小限に抑えたいのだから、マイナスで正しいことをしているのだと自分を納得させることができる。"
  },
  {
    "start": 7435380,
    "end": 7444100,
    "text": "すべてのパラメーターをほんの少し変えてみると、このデータが少し変化していることがわかるだろう。"
  },
  {
    "start": 7444170,
    "end": 7449492,
    "text": "今、この神経細胞はほんの少し大きな値を示している。"
  },
  {
    "start": 7449626,
    "end": 7453190,
    "text": "ゼロポイント85-4は0.857となった。"
  },
  {
    "start": 7453880,
    "end": 7462756,
    "text": "というのも、このニューロンデータを少し増やすと、勾配に従ってロスが減るからだ。"
  },
  {
    "start": 7462868,
    "end": 7465640,
    "text": "つまり、サイン的には修正されたことになる。"
  },
  {
    "start": 7466140,
    "end": 7474510,
    "text": "もちろん、これらのパラメーターをすべて変更したのだから、損失は少し減っているはずだ。"
  },
  {
    "start": 7475120,
    "end": 7476920,
    "text": "私たちは失ったものを再評価したい。"
  },
  {
    "start": 7477000,
    "end": 7482588,
    "text": "基本的に、これは変わっていないデータ定義なんだ。"
  },
  {
    "start": 7482684,
    "end": 7493920,
    "text": "ここでネットワークのフォワードパスを再計算し、2つの損失値を比較することができる。"
  },
  {
    "start": 7494420,
    "end": 7501348,
    "text": "ここで、損失を再計算すると、新しい損失はこの数字よりわずかに低くなると予想される。"
  },
  {
    "start": 7501434,
    "end": 7504976,
    "text": "今出ているのが4.84よりほんの少し低ければいいのだが..."
  },
  {
    "start": 7505008,
    "end": 7508008,
    "text": "4.36."
  },
  {
    "start": 7508174,
    "end": 7508696,
    "text": "オーケー。"
  },
  {
    "start": 7508798,
    "end": 7514900,
    "text": "ロスが少ないということは、予想が的中しているということだ。"
  },
  {
    "start": 7514980,
    "end": 7519400,
    "text": "今の私たちの予測は、おそらくターゲットにわずかに近い。"
  },
  {
    "start": 7520300,
    "end": 7523916,
    "text": "あとはこのプロセスを繰り返すだけだ。"
  },
  {
    "start": 7524098,
    "end": 7527768,
    "text": "またフォワードパスをしてしまった。"
  },
  {
    "start": 7527944,
    "end": 7529820,
    "text": "今、私たちはそれを逆に失うことができる。"
  },
  {
    "start": 7530240,
    "end": 7533330,
    "text": "これを外して、ステップサイズを測ってみよう。"
  },
  {
    "start": 7534340,
    "end": 7536412,
    "text": "これで少しは損失が減るはずだ。"
  },
  {
    "start": 7536476,
    "end": 7539330,
    "text": "4.36が3.9に"
  },
  {
    "start": 7539700,
    "end": 7540064,
    "text": "そして"
  },
  {
    "start": 7540102,
    "end": 7542832,
    "text": "さて、フォワードパスは終わった。"
  },
  {
    "start": 7542886,
    "end": 7545220,
    "text": "これがバックワードパスのなごりである。"
  },
  {
    "start": 7545640,
    "end": 7551510,
    "text": "これで損失は3.66、3.47となった。"
  },
  {
    "start": 7552280,
    "end": 7553572,
    "text": "おわかりだろう。"
  },
  {
    "start": 7553626,
    "end": 7555076,
    "text": "これを続けるだけだ。"
  },
  {
    "start": 7555178,
    "end": 7557156,
    "text": "これが勾配降下だ。"
  },
  {
    "start": 7557268,
    "end": 7562884,
    "text": "フォワードパス、バックワードパス更新、フォワードパス、バックワードパス更新を繰り返しているだけだ。"
  },
  {
    "start": 7562932,
    "end": 7565476,
    "text": "ニューラルネットは予測を向上させている。"
  },
  {
    "start": 7565668,
    "end": 7576652,
    "text": "ここでypredを見てみると、この値は1に近づいているはずだ。"
  },
  {
    "start": 7576786,
    "end": 7581748,
    "text": "この値はよりプラスに、これらの値はよりマイナスに、そしてこの値もよりプラスになるはずだ。"
  },
  {
    "start": 7581864,
    "end": 7589404,
    "text": "これをあと数回繰り返せば、もう少しスピードアップできるかもしれない。"
  },
  {
    "start": 7589452,
    "end": 7591570,
    "text": "もう少し高い学習率を試してみよう。"
  },
  {
    "start": 7594260,
    "end": 7594764,
    "text": "おっと。"
  },
  {
    "start": 7594812,
    "end": 7595500,
    "text": "よし、行くぞ。"
  },
  {
    "start": 7595510,
    "end": 7611256,
    "text": "ところで、あまり大きなステップを踏もうとすると、自信過剰になってしまうかもしれません。"
  },
  {
    "start": 7611358,
    "end": 7617764,
    "text": "損失関数はあらゆる種類の構造を持っており、私たちが知っているのは、これらすべてのパラメーターの損失に対する極めて局所的な依存性だけである。"
  },
  {
    "start": 7617812,
    "end": 7627070,
    "text": "あまり踏み込み過ぎると、まったく違う部分に踏み込んでしまい、トレーニングが不安定になったり、実際に負けが膨らんだりする可能性がある。"
  },
  {
    "start": 7628080,
    "end": 7630044,
    "text": "これで負けはゼロ4。"
  },
  {
    "start": 7630162,
    "end": 7633344,
    "text": "実際、予想はかなり近いはずだ。"
  },
  {
    "start": 7633462,
    "end": 7634610,
    "text": "見てみよう。"
  },
  {
    "start": 7635060,
    "end": 7639344,
    "text": "ほとんど1、ほとんどマイナス1、ほとんど1。"
  },
  {
    "start": 7639462,
    "end": 7640930,
    "text": "私たちは進み続けることができる。"
  },
  {
    "start": 7641540,
    "end": 7642288,
    "text": "だから"
  },
  {
    "start": 7642454,
    "end": 7644980,
    "text": "そう、後方アップデートだ。"
  },
  {
    "start": 7645640,
    "end": 7646272,
    "text": "おっと。"
  },
  {
    "start": 7646336,
    "end": 7646836,
    "text": "これでよし。"
  },
  {
    "start": 7646858,
    "end": 7651344,
    "text": "スピードが速すぎた。"
  },
  {
    "start": 7651472,
    "end": 7654224,
    "text": "私たちは熱心すぎた。"
  },
  {
    "start": 7654352,
    "end": 7655124,
    "text": "我々は今どこにいるのか？"
  },
  {
    "start": 7655162,
    "end": 7655860,
    "text": "おっと。"
  },
  {
    "start": 7656300,
    "end": 7657096,
    "text": "オーケー。"
  },
  {
    "start": 7657278,
    "end": 7658632,
    "text": "7eマイナス9。"
  },
  {
    "start": 7658686,
    "end": 7664490,
    "text": "これは非常に低損失で、予測は基本的に完璧だ。"
  },
  {
    "start": 7665420,
    "end": 7672888,
    "text": "基本的には、あまりに大きなアップデートをしすぎて、一時的に爆発してしまったんだ。"
  },
  {
    "start": 7672984,
    "end": 7677660,
    "text": "通常、この学習率とその調整は微妙な技術である。"
  },
  {
    "start": 7677730,
    "end": 7679148,
    "text": "学習率を設定したい。"
  },
  {
    "start": 7679234,
    "end": 7682200,
    "text": "低すぎると収束に時間がかかりすぎる。"
  },
  {
    "start": 7682280,
    "end": 7688432,
    "text": "これが高すぎると、全体が不安定になり、損失関数によっては、実際に損失が爆発するかもしれない。"
  },
  {
    "start": 7688566,
    "end": 7692832,
    "text": "ちょうどいいステップサイズを見つけるのは、かなり微妙な技術だ。"
  },
  {
    "start": 7692886,
    "end": 7700900,
    "text": "バニラ勾配降下のようなものを使っていて、たまたまいいところに入ったとき、n個のパラメータを見ることができる。"
  },
  {
    "start": 7702600,
    "end": 7713130,
    "text": "この重みとバイアスを設定することで、私たちのネットワークは、望ましいターゲットを非常に近い形で予測することができる。"
  },
  {
    "start": 7713740,
    "end": 7718170,
    "text": "基本的に、我々はニューラルネットのトレーニングに成功した。"
  },
  {
    "start": 7718780,
    "end": 7723692,
    "text": "では、これをもう少し立派なものにして、実際のトレーニングループを実装してみよう。"
  },
  {
    "start": 7723826,
    "end": 7726056,
    "text": "これが残るデータ定義である。"
  },
  {
    "start": 7726168,
    "end": 7727630,
    "text": "これがフォワードパスだ。"
  },
  {
    "start": 7728320,
    "end": 7735680,
    "text": "kが範囲内にある場合は、何ステップも踏むことになる。"
  },
  {
    "start": 7737620,
    "end": 7741520,
    "text": "まず、フォワードパスをして、ロスを検証する。"
  },
  {
    "start": 7743640,
    "end": 7746000,
    "text": "ニューラルネットをゼロから再初期化しよう。"
  },
  {
    "start": 7746160,
    "end": 7747430,
    "text": "これがそのデータだ。"
  },
  {
    "start": 7748520,
    "end": 7752550,
    "text": "まずフォワードパスを行い、それからバックワードパスを行う。"
  },
  {
    "start": 7759500,
    "end": 7762280,
    "text": "その後、勾配降下法による更新を行う。"
  },
  {
    "start": 7766220,
    "end": 7771660,
    "text": "そうすれば、これを繰り返し、現在のステップ、現在のロスを表示できるはずだ。"
  },
  {
    "start": 7772480,
    "end": 7779630,
    "text": "敗戦の番号のようなものを印字すればいい。"
  },
  {
    "start": 7780480,
    "end": 7781740,
    "text": "次に学習率。"
  },
  {
    "start": 7781810,
    "end": 7783292,
    "text": "0.1は少し小さすぎる。"
  },
  {
    "start": 7783346,
    "end": 7785936,
    "text": "私たちが見た0.1は、ちょっと危険なくらい高すぎる。"
  },
  {
    "start": 7786038,
    "end": 7793200,
    "text": "その中間で、10ステップではなく、例えば20ステップで最適化しよう。"
  },
  {
    "start": 7794520,
    "end": 7801220,
    "text": "このゴミを全部消して、最適化を実行しよう。"
  },
  {
    "start": 7803080,
    "end": 7810410,
    "text": "実際に、より遅く、よりコントロールされた方法で収束し、損失が非常に少なくなっているのがわかるだろう。"
  },
  {
    "start": 7811420,
    "end": 7814890,
    "text": "ワイドレッドはかなりいい出来になると期待している。"
  },
  {
    "start": 7815740,
    "end": 7816810,
    "text": "これでよし。"
  },
  {
    "start": 7819740,
    "end": 7824188,
    "text": "ええと、それで終わり。"
  },
  {
    "start": 7824354,
    "end": 7829004,
    "text": "さて、ちょっと恥ずかしいんだけど、実はここに本当にひどいバグがあるんだ。"
  },
  {
    "start": 7829122,
    "end": 7840588,
    "text": "微妙なバグだし、よくあるバグだし、人生で20回目もやってしまったなんて信じられないよ。"
  },
  {
    "start": 7840684,
    "end": 7846292,
    "text": "ニューラルネットを使った仕事がどんなものか、少しは理解できるだろう。"
  },
  {
    "start": 7846346,
    "end": 7851168,
    "text": "時々、私たちはよくある虫の居所が悪いことがある。"
  },
  {
    "start": 7851264,
    "end": 7863096,
    "text": "実はずいぶん前に、ニューラルネットのよくある間違いをツイートしたことがある。"
  },
  {
    "start": 7863198,
    "end": 7866244,
    "text": "ドットバックの前にグラッドをゼロにするのを忘れている。"
  },
  {
    "start": 7866372,
    "end": 7867450,
    "text": "それは何ですか？"
  },
  {
    "start": 7869420,
    "end": 7878620,
    "text": "基本的に何が起こっているかというと、微妙なバグなのだが、あなたがそれを見たかどうかは分からない。"
  },
  {
    "start": 7879280,
    "end": 7881630,
    "text": "ドットグラッドはゼロから始まる。"
  },
  {
    "start": 7882640,
    "end": 7887600,
    "text": "その後、後方へ移動し、グラデーションを塗りつぶし、データを更新する。"
  },
  {
    "start": 7887670,
    "end": 7891090,
    "text": "私たちは汚物を流さず、そのままにしている。"
  },
  {
    "start": 7891620,
    "end": 7899312,
    "text": "2回目のフォワード・パスで再びバックワードするとき、すべてのバックワード・オペレーションはグラッドにプラス・イコールすることを覚えておいてほしい。"
  },
  {
    "start": 7899456,
    "end": 7904390,
    "text": "だから、これらのグラデーションは加算されるだけで、ゼロにリセットされることはない。"
  },
  {
    "start": 7904920,
    "end": 7907236,
    "text": "基本的に、私たちは卒業をゼロにはしなかった。"
  },
  {
    "start": 7907348,
    "end": 7917770,
    "text": "バックワードする前にgradをゼロにする方法だが、全パラメーターを繰り返し、P gradがゼロに設定されていることを確認する必要がある。"
  },
  {
    "start": 7918620,
    "end": 7922456,
    "text": "コンストラクターと同じようにゼロにリセットする必要がある。"
  },
  {
    "start": 7922648,
    "end": 7927260,
    "text": "覚えておいてほしいのは、このすべてのバリュー・ノードにおいて、gradはゼロにリセットされるということだ。"
  },
  {
    "start": 7927410,
    "end": 7930940,
    "text": "そして、これらのバックワードパスはすべて、そのグラッドにプラス・イコールとなる。"
  },
  {
    "start": 7931280,
    "end": 7938512,
    "text": "バックワードするとき、すべてのグラッドがゼロから始まるように、これらのグラッドをゼロにリセットする必要がある。"
  },
  {
    "start": 7938566,
    "end": 7945408,
    "text": "実際の後方パスでは、損失微分がグラッドに累積される。"
  },
  {
    "start": 7945584,
    "end": 7953024,
    "text": "これはPytorchではゼログラッドであり、我々は少し異なる最適化を得ることになる。"
  },
  {
    "start": 7953152,
    "end": 7954740,
    "text": "ニューラルネットをリセットしよう。"
  },
  {
    "start": 7954890,
    "end": 7956128,
    "text": "データは同じだ。"
  },
  {
    "start": 7956234,
    "end": 7958090,
    "text": "これはもう正しいと思う。"
  },
  {
    "start": 7958460,
    "end": 7963876,
    "text": "もっとゆっくり降下することになる。"
  },
  {
    "start": 7964068,
    "end": 7966292,
    "text": "それでも最終的にはかなりいい結果を残している。"
  },
  {
    "start": 7966356,
    "end": 7972270,
    "text": "このままもう少し続けて、どんどん下げていけばいい。"
  },
  {
    "start": 7974160,
    "end": 7974910,
    "text": "そうだね。"
  },
  {
    "start": 7976160,
    "end": 7979544,
    "text": "以前のものが機能していた唯一の理由は、非常にバグが多いことだ。"
  },
  {
    "start": 7979672,
    "end": 7989136,
    "text": "うまくいった唯一の理由は、これが非常に単純な問題で、ニューラルネットがこのデータに適合するのが非常に簡単だからだ。"
  },
  {
    "start": 7989318,
    "end": 7997350,
    "text": "その結果、勾配が蓄積され、効果的に大きなステップサイズが得られ、収束が非常に速くなった。"
  },
  {
    "start": 7999320,
    "end": 8006536,
    "text": "基本的に、今はロスの値を非常に低くし、ワイプを本当に良いものにするために、さらにステップを踏まなければならない。"
  },
  {
    "start": 8006638,
    "end": 8009640,
    "text": "私たちはもう少し大きなステップに挑戦できる。"
  },
  {
    "start": 8014300,
    "end": 8018010,
    "text": "そう、1マイナス1と1にどんどん近づいていくんだ。"
  },
  {
    "start": 8018480,
    "end": 8029788,
    "text": "なぜなら、コードにバグがたくさんあるかもしれないし、ネットワークが実際に機能するかもしれないからだ。"
  },
  {
    "start": 8029874,
    "end": 8031180,
    "text": "私たちのと同じようにね。"
  },
  {
    "start": 8031330,
    "end": 8043136,
    "text": "もし私たちがもっと複雑な問題を抱えていたら、このバグのせいでロスの最適化がうまくいかなかったかもしれない。"
  },
  {
    "start": 8043318,
    "end": 8046768,
    "text": "では、すべてをまとめて、学んだことをまとめよう。"
  },
  {
    "start": 8046944,
    "end": 8048240,
    "text": "ニューラルネットとは何か？"
  },
  {
    "start": 8048320,
    "end": 8065608,
    "text": "ニューラルネットとは、このような数式、マルチリオ・パーセプトロンの場合はかなり単純な数式で、入力をデータとして受け取り、入力、ニューラルネットの重みとパラメータ、フォワードパスの数式、それに続く損失関数を受け取ります。"
  },
  {
    "start": 8065774,
    "end": 8069252,
    "text": "損失関数は、予測の精度を測定しようとするものである。"
  },
  {
    "start": 8069396,
    "end": 8076364,
    "text": "通常、予測値がターゲットにマッチしているときや、ネットワークが基本的にうまく動作しているときは、損失は少なくなる。"
  },
  {
    "start": 8076482,
    "end": 8083170,
    "text": "損失関数を操作することで、損失が小さいときに、ネットワークがあなたの問題であなたが望むことをするようにする。"
  },
  {
    "start": 8083940,
    "end": 8092636,
    "text": "その後、損失を逆算し、バックプロパゲーションを使って勾配を求め、局所的に損失を減らすためにすべてのパラメーターを調整する方法を知る。"
  },
  {
    "start": 8092828,
    "end": 8096652,
    "text": "となると、勾配降下と呼ばれるプロセスを何度も繰り返さなければならない。"
  },
  {
    "start": 8096796,
    "end": 8100960,
    "text": "勾配情報に従うだけで、損失は最小になる。"
  },
  {
    "start": 8101040,
    "end": 8105910,
    "text": "損失が最小になったとき、ネットワークがあなたの望むことをするように配置される。"
  },
  {
    "start": 8108120,
    "end": 8112536,
    "text": "私たちは神経の塊を持っているだけで、それに任意のことをさせることができる。"
  },
  {
    "start": 8112638,
    "end": 8114650,
    "text": "それがニューラルネットに力を与えている。"
  },
  {
    "start": 8115900,
    "end": 8126184,
    "text": "これは41個のパラメーターを持つ非常に小さなネットワークだが、数十億、現時点ではほぼ数兆のパラメーターを持つ、はるかに複雑なニューラルネットを構築することができる。"
  },
  {
    "start": 8126312,
    "end": 8132384,
    "text": "これは神経組織の塊で、大雑把に言えば、模擬神経組織だ。"
  },
  {
    "start": 8132582,
    "end": 8135584,
    "text": "非常に複雑な問題にも対応できる。"
  },
  {
    "start": 8135702,
    "end": 8156080,
    "text": "例えば、GPTの場合のように、インターネットから大量のテキストを入手し、ニューラルネットに予測させようとしている。"
  },
  {
    "start": 8156160,
    "end": 8157428,
    "text": "それが学習の問題だ。"
  },
  {
    "start": 8157594,
    "end": 8163588,
    "text": "インターネット上でこれを訓練すると、ニューラルネットは実に驚くべき創発特性を持つことがわかった。"
  },
  {
    "start": 8163684,
    "end": 8170484,
    "text": "そのニューラルネットは何千億ものパラメーターを持つが、基本的にはまったく同じ原理で動作する。"
  },
  {
    "start": 8170612,
    "end": 8182264,
    "text": "もちろん、ニューラルネットはもう少し複雑になるが、そうでなければ、勾配の評価はそこにあり、勾配降下はそこにあり、基本的に同じだろう。"
  },
  {
    "start": 8182392,
    "end": 8184780,
    "text": "人々は通常、微妙に異なるアップデートを使用する。"
  },
  {
    "start": 8184860,
    "end": 8188160,
    "text": "これは非常に単純な確率的勾配降下更新である。"
  },
  {
    "start": 8189300,
    "end": 8191484,
    "text": "の場合、損失関数は平均二乗誤差にはならない。"
  },
  {
    "start": 8191532,
    "end": 8195596,
    "text": "彼らは次のトークンを予測するために、クロスエントロピー損失と呼ばれるものを使っているのだろう。"
  },
  {
    "start": 8195708,
    "end": 8196896,
    "text": "もう少し詳細がある。"
  },
  {
    "start": 8196928,
    "end": 8201504,
    "text": "基本的に、ニューラルネットワークのセットアップとニューラルネットワークのトレーニングは同一であり、浸透している。"
  },
  {
    "start": 8201632,
    "end": 8205776,
    "text": "これで、それがボンネットの中でどのように機能するか、直感的に理解できただろう。"
  },
  {
    "start": 8205888,
    "end": 8211880,
    "text": "このビデオの冒頭で、このビデオが終わるころにはマイクログラッドのすべてを理解しているだろうし、徐々にそれを積み上げていくだろうと話した。"
  },
  {
    "start": 8211950,
    "end": 8213784,
    "text": "それを簡単に証明しよう。"
  },
  {
    "start": 8213982,
    "end": 8217368,
    "text": "今日現在microgradにあるすべてのコードを順を追って見ていこうと思う。"
  },
  {
    "start": 8217534,
    "end": 8223320,
    "text": "実は、このビデオをご覧になる頃には、コードの一部が変わっている可能性があります。なぜなら、私はMicrogradの開発を続けるつもりだからです。"
  },
  {
    "start": 8223480,
    "end": 8225084,
    "text": "これまでのところを見てみよう。"
  },
  {
    "start": 8225122,
    "end": 8227176,
    "text": "少なくともinit pyは空っぽだ。"
  },
  {
    "start": 8227288,
    "end": 8231880,
    "text": "エンジン・パイに行くと、ここにあるすべての価値がほとんどわかるはずだ。"
  },
  {
    "start": 8231960,
    "end": 8240370,
    "text": "データ・グラッドの属性があり、後方関数があり、前の子のセットがあり、この値を生成する操作がある。"
  },
  {
    "start": 8240740,
    "end": 8244770,
    "text": "足し算、掛け算、スカラーべき乗。"
  },
  {
    "start": 8245220,
    "end": 8250756,
    "text": "このビデオで使ったten hとは少し違うタイプの非線形性である。"
  },
  {
    "start": 8250938,
    "end": 8258404,
    "text": "どちらも非線形性で、特に10hは今のところマイクログラッドには存在しないが、後で追加するつもりだ。"
  },
  {
    "start": 8258602,
    "end": 8265384,
    "text": "後方へのオペレーションは同じで、その他のオペレーションはすべて、ここでのオペレーションの上に構築されている。"
  },
  {
    "start": 8265502,
    "end": 8267252,
    "text": "価値観は非常にわかりやすいはずだ。"
  },
  {
    "start": 8267316,
    "end": 8269530,
    "text": "このビデオで使われている非直線性を除いては。"
  },
  {
    "start": 8270860,
    "end": 8273864,
    "text": "reluとten hとsigmoidに大差はない。"
  },
  {
    "start": 8273912,
    "end": 8277768,
    "text": "これらの他の非線形性は、すべてほぼ同等であり、mlpsで使用することができる。"
  },
  {
    "start": 8277864,
    "end": 8286812,
    "text": "私がten hを使っているのは、少し滑らかで、reluよりも少し複雑なので、局所的なグラデーションが少し強調されるからだ。"
  },
  {
    "start": 8286876,
    "end": 8289920,
    "text": "デリバティブを使った仕事は、役に立つと思う。"
  },
  {
    "start": 8290660,
    "end": 8293616,
    "text": "Nn Pyはニューラルネットワーク・ライブラリだ。"
  },
  {
    "start": 8293718,
    "end": 8300580,
    "text": "神経層とMLPの実装が同じであることを認識する必要がある。"
  },
  {
    "start": 8300730,
    "end": 8302036,
    "text": "ここにクラスモジュールがある。"
  },
  {
    "start": 8302058,
    "end": 8304096,
    "text": "これらすべてのモジュールの親クラスがある。"
  },
  {
    "start": 8304208,
    "end": 8307956,
    "text": "PytorchにはNnモジュールのクラスがあるからそうしたんだ。"
  },
  {
    "start": 8307988,
    "end": 8310068,
    "text": "だから、これはそのAPIと完全に一致する。"
  },
  {
    "start": 8310164,
    "end": 8314810,
    "text": "PytorchのNnモジュールにはゼログラッドもあるが、ここではそれをリファクタリングした。"
  },
  {
    "start": 8316300,
    "end": 8318552,
    "text": "これでマイクログラッドは終わりだ。"
  },
  {
    "start": 8318606,
    "end": 8330248,
    "text": "基本的には、microgradとPytorchの2つのコードのチャンクを作成し、フォワード・パスとバックワード・パスが同一であることを確認する。"
  },
  {
    "start": 8330424,
    "end": 8336140,
    "text": "少し複雑でない表現と少し複雑な表現では、すべてが一致する。"
  },
  {
    "start": 8336220,
    "end": 8338560,
    "text": "我々は、これらの作戦すべてにおいてピトーチに同意する。"
  },
  {
    "start": 8338900,
    "end": 8346300,
    "text": "最後に、ここにipymbのデモがあるが、この講義で取り上げたデモよりももう少し複雑なバイナリ分類のデモだ。"
  },
  {
    "start": 8346380,
    "end": 8349060,
    "text": "私たちは4つの例というわずかなデータセットしか持っていなかった。"
  },
  {
    "start": 8349560,
    "end": 8354276,
    "text": "ここでは、青い点と赤い点がたくさんある、もう少し複雑な例を見てみよう。"
  },
  {
    "start": 8354378,
    "end": 8360372,
    "text": "我々は、2次元の点を赤か青か区別する2値分類器を再び構築しようとしている。"
  },
  {
    "start": 8360516,
    "end": 8364516,
    "text": "ここはもう少し複雑なMLPで、MLPの規模も大きい。"
  },
  {
    "start": 8364708,
    "end": 8369044,
    "text": "バッチに対応しているため、損失はもう少し複雑だ。"
  },
  {
    "start": 8369172,
    "end": 8374760,
    "text": "というのも、私たちのデータセットは非常に小さかったので、常に4つの例のデータセット全体に対してフォワードパスを行ったからだ。"
  },
  {
    "start": 8374840,
    "end": 8381864,
    "text": "データセットが100万例のような場合、私たちが通常実践しているのは、基本的にランダムなサブセットを選ぶことだ。"
  },
  {
    "start": 8381912,
    "end": 8389170,
    "text": "それをバッチと呼び、そのバッチの前方、後方、更新だけを処理するので、トレーニングセット全体を前方に転送する必要はない。"
  },
  {
    "start": 8389860,
    "end": 8392556,
    "text": "これはバッチ処理に対応している。"
  },
  {
    "start": 8392588,
    "end": 8394960,
    "text": "ここではフォワードパスをする。"
  },
  {
    "start": 8395110,
    "end": 8396732,
    "text": "敗因はもう少し違う。"
  },
  {
    "start": 8396806,
    "end": 8399892,
    "text": "これは、私がここで実施している最大マージン損失である。"
  },
  {
    "start": 8400026,
    "end": 8404436,
    "text": "私たちが使用したのは平均二乗誤差損失で、これは最も単純なものだからだ。"
  },
  {
    "start": 8404618,
    "end": 8406960,
    "text": "バイナリー・クロスエントロピーの損失もある。"
  },
  {
    "start": 8407040,
    "end": 8410760,
    "text": "いずれも2値分類に使用でき、大きな違いはない。"
  },
  {
    "start": 8410830,
    "end": 8416744,
    "text": "これまで見てきた単純な例では、ここでl 2正則化と呼ばれるものが使われている。"
  },
  {
    "start": 8416862,
    "end": 8422664,
    "text": "これはニューラルネットの汎化に関係し、機械学習におけるオーバーフィッティングを抑制する。"
  },
  {
    "start": 8422712,
    "end": 8426700,
    "text": "このビデオではこれらのコンセプトは取り上げなかった。"
  },
  {
    "start": 8426850,
    "end": 8428648,
    "text": "トレーニングループを認識する必要がある。"
  },
  {
    "start": 8428744,
    "end": 8434544,
    "text": "フォワード、ゼログラードでのバックワード、そしてアップデートの繰り返しだ。"
  },
  {
    "start": 8434662,
    "end": 8441516,
    "text": "ここでの更新で、学習率が反復回数の関数としてスケーリングされ、縮小していることに気づくだろう。"
  },
  {
    "start": 8441708,
    "end": 8443900,
    "text": "これは学習率の減衰と呼ばれるものだ。"
  },
  {
    "start": 8443980,
    "end": 8452948,
    "text": "最初のうちは学習率を高くし、最終的にネットワークが安定するにつれて学習率を下げ、最終的に細かいディテールを得る。"
  },
  {
    "start": 8453114,
    "end": 8461290,
    "text": "最終的にニューラルネットの決定面が表示され、データポイントに基づいて赤と青の領域を分離することを学習したことがわかる。"
  },
  {
    "start": 8461900,
    "end": 8463832,
    "text": "これが少し複雑な例だ。"
  },
  {
    "start": 8463886,
    "end": 8467368,
    "text": "デモのipymbでは、自由に行き来することができる。"
  },
  {
    "start": 8467534,
    "end": 8470036,
    "text": "ああ、今日現在、それはマイクログラッドだ。"
  },
  {
    "start": 8470148,
    "end": 8476648,
    "text": "また、Pytorchのようなプロダクション・グレードのライブラリで実際にどのように実装されているかを見てもらうために、実際のものを少しお見せしたいと思います。"
  },
  {
    "start": 8476824,
    "end": 8482868,
    "text": "特に、ピトーチの10時間のバックワードパスを見つけて見せたかった。"
  },
  {
    "start": 8482984,
    "end": 8495392,
    "text": "ここで、マイクログラッドでは、10hの後方パスが1マイナスtの2乗であることがわかる。"
  },
  {
    "start": 8495456,
    "end": 8497956,
    "text": "このようなものを探している。"
  },
  {
    "start": 8498138,
    "end": 8511370,
    "text": "オープンソースのGitHubコードベースを持つPytorchに行き、そのコードの多くに目を通したが、正直なところ15分ほど費やしたが、10個のhを見つけることはできなかった。"
  },
  {
    "start": 8511820,
    "end": 8515812,
    "text": "というのも、これらのライブラリは、残念ながらサイズとエントロピーが大きくなっていくからだ。"
  },
  {
    "start": 8515876,
    "end": 8522696,
    "text": "ten hで検索すると、2800件、406ファイルがヒットする。"
  },
  {
    "start": 8522808,
    "end": 8529836,
    "text": "正直なところ、これらのファイルが何をしているのかわからないし、なぜこれほど多くの10Hについて言及されているのかもわからない。"
  },
  {
    "start": 8529938,
    "end": 8532092,
    "text": "残念ながら、これらのライブラリーはかなり複雑だ。"
  },
  {
    "start": 8532156,
    "end": 8535120,
    "text": "使うものであって、検査するものではない。"
  },
  {
    "start": 8535540,
    "end": 8542448,
    "text": "結局、私は何らかの理由で10Hのバックワードコードを変更しようとする人に出くわした。"
  },
  {
    "start": 8542614,
    "end": 8547280,
    "text": "ここで誰かが、CPUカーネルとcudaカーネルを10時間遡って指摘した。"
  },
  {
    "start": 8548440,
    "end": 8559860,
    "text": "基本的には、PytorchをCPUデバイスで使うかGPUで使うかによって変わってきますが、これらは異なるデバイスです。"
  },
  {
    "start": 8560020,
    "end": 8575540,
    "text": "なぜこんなに大きいかというと、その1、複雑な型を使っている場合、まだ話したことがないが、b float 16という特定のデータ型を使っている場合、そしてそうでない場合、これがカーネルになるからだ。"
  },
  {
    "start": 8575620,
    "end": 8579736,
    "text": "ここでの深い位置には、私たちの後方パスに似たものがある。"
  },
  {
    "start": 8579838,
    "end": 8583250,
    "text": "彼らは1マイナスbの正方形の8倍を持っていた。"
  },
  {
    "start": 8583700,
    "end": 8589356,
    "text": "このbは10個のhの出力に違いない。"
  },
  {
    "start": 8589468,
    "end": 8600390,
    "text": "ここでは、Pytorchの奥深くにあるこの場所で、なぜかバイナリーオペのカーネルの中にあるのを見つけた。"
  },
  {
    "start": 8601160,
    "end": 8606496,
    "text": "それなら、これはGPUカーネルだ。"
  },
  {
    "start": 8606688,
    "end": 8610330,
    "text": "さあ、1行のコードで始めよう。"
  },
  {
    "start": 8610700,
    "end": 8612216,
    "text": "見つけたよ。"
  },
  {
    "start": 8612238,
    "end": 8618184,
    "text": "基本的に、残念ながら、これらのコードベースは非常に大きく、マイクログラッドは非常にシンプルだ。"
  },
  {
    "start": 8618222,
    "end": 8622990,
    "text": "もし実際に本物のものを使いたいのなら、そのコードを見つけるのは難しいだろう。"
  },
  {
    "start": 8623920,
    "end": 8633452,
    "text": "また、Pytorchが、Pytorchに追加したい新しいタイプの関数をレゴブロックとして登録する方法を示している例もお見せしたいと思います。"
  },
  {
    "start": 8633596,
    "end": 8640464,
    "text": "ここで、例えば男女の多項式を3つ足したい場合、こうすることができる。"
  },
  {
    "start": 8640502,
    "end": 8652564,
    "text": "この関数をtorch rgrad関数のサブクラスとして登録し、PyTorchに新しい関数をどのようにフォワードし、どのようにバックワードするかを指示する必要があります。"
  },
  {
    "start": 8652682,
    "end": 8664136,
    "text": "追加したい小さな関数の前方パスさえできれば、そして局所微分がわかっていれば、後方Pytorchで実装されている局所勾配は、関数を通して逆伝播することができる。"
  },
  {
    "start": 8664238,
    "end": 8670856,
    "text": "そうすれば、ピトーチがすでに持っているさまざまなレゴブロックで構成された、より大きなレゴ城のレゴブロックとして使うことができる。"
  },
  {
    "start": 8671038,
    "end": 8673128,
    "text": "ピートーチに伝えるべきことはそれだけだ。"
  },
  {
    "start": 8673144,
    "end": 8674268,
    "text": "すべてがうまくいく。"
  },
  {
    "start": 8674354,
    "end": 8678748,
    "text": "この例に従って、新しいタイプの関数を登録することができる。"
  },
  {
    "start": 8678914,
    "end": 8681544,
    "text": "というのが、この講義で取り上げたかったことのすべてだ。"
  },
  {
    "start": 8681672,
    "end": 8683824,
    "text": "僕と一緒にマイクログラッド作りを楽しんでもらえたらうれしい。"
  },
  {
    "start": 8683862,
    "end": 8693072,
    "text": "このビデオが興味深く、洞察に富んでいると感じていただければ幸いです。"
  },
  {
    "start": 8693206,
    "end": 8703220,
    "text": "また、このビデオに関連した質問ができるディスカッション・フォーラムやディスカッション・グループへのリンクも掲載する予定です。"
  },
  {
    "start": 8703370,
    "end": 8707430,
    "text": "また、よくある質問に答えるフォローアップビデオも作るかもしれない。"
  },
  {
    "start": 8708440,
    "end": 8709668,
    "text": "今のところはそれだけだ。"
  },
  {
    "start": 8709754,
    "end": 8710696,
    "text": "楽しんでいただけたなら幸いだ。"
  },
  {
    "start": 8710718,
    "end": 8715416,
    "text": "もしそうなら、YouTubeがこのビデオをより多くの人に見てもらえるよう、「いいね！」と「購読」をお願いします。"
  },
  {
    "start": 8715598,
    "end": 8716856,
    "text": "今はここまでだ。"
  },
  {
    "start": 8716878,
    "end": 8717850,
    "text": "また後でね"
  },
  {
    "start": 8722420,
    "end": 8723730,
    "text": "問題はここからだ。"
  },
  {
    "start": 8724660,
    "end": 8727250,
    "text": "私たちはDLを知っている。"
  },
  {
    "start": 8728580,
    "end": 8730050,
    "text": "待って、何が問題なの？"
  },
  {
    "start": 8731900,
    "end": 8734132,
    "text": "というのが、この講義で取り上げたかったことのすべてだ。"
  },
  {
    "start": 8734276,
    "end": 8739720,
    "text": "顕微鏡写真のマイクログラブを楽しんでいただけたなら幸いだ。"
  },
  {
    "start": 8741980,
    "end": 8746810,
    "text": "では、乗算もまったく同じことをやってみよう。"
  },
  {
    "start": 8747820,
    "end": 8748760,
    "text": "おっと。"
  },
  {
    "start": 8750780,
    "end": 8751750,
    "text": "そこで何が起こったかは知っている。"
  }
]