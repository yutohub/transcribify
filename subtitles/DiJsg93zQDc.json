[
  {
    "start": 3290,
    "end": 4910,
    "text": "よし、始めよう。"
  },
  {
    "start": 5060,
    "end": 8880,
    "text": "朝一番のトークにお集まりいただき、ありがとうございます。"
  },
  {
    "start": 9330,
    "end": 15390,
    "text": "スタンフォード大学のコンピューターサイエンス教授であるグレッグ・バリアントを紹介できることをうれしく思う。"
  },
  {
    "start": 15730,
    "end": 23406,
    "text": "私の理解では、彼はアルゴリズム、機械学習、統計学、情報理論、そしてそれらすべての組み合わせに取り組んでいる。"
  },
  {
    "start": 23508,
    "end": 27320,
    "text": "今日は、文脈学習について話を聞こう。"
  },
  {
    "start": 28410,
    "end": 30306,
    "text": "紹介してくれて本当にありがとう。"
  },
  {
    "start": 30418,
    "end": 31400,
    "text": "来てくれてありがとう。"
  },
  {
    "start": 34650,
    "end": 44410,
    "text": "具体的な仕事について話すつもりだが、全体的な話としては、具体的な仕事をもっと抽象的な問題に埋め込むつもりだ。"
  },
  {
    "start": 44560,
    "end": 47062,
    "text": "そういう意味では、もっと議論してほしい。"
  },
  {
    "start": 47206,
    "end": 53980,
    "text": "もし私の言うことで何か明確にしておきたいこと、付け加えておきたいことがあれば、遠慮なく私を止めてほしい。"
  },
  {
    "start": 54770,
    "end": 68980,
    "text": "講演での具体的な仕事は、シヴァムやディミトリウスとの共同作業であり、そう、私たちは皆、文脈学習について知っている。"
  },
  {
    "start": 73110,
    "end": 81282,
    "text": "繰り返しになるが、文脈学習の面白いところは、モデルの重みが更新されていないにもかかわらず、何かを学習しているように見えることだ。"
  },
  {
    "start": 81346,
    "end": 81960,
    "text": "そうだろう？"
  },
  {
    "start": 82330,
    "end": 87238,
    "text": "チャットのようなもので、GPCはこれをうまくやることができる。"
  },
  {
    "start": 87324,
    "end": 88054,
    "text": "そうでないかもしれない。"
  },
  {
    "start": 88172,
    "end": 90586,
    "text": "こういう例を思い浮かべるだろう？"
  },
  {
    "start": 90608,
    "end": 95638,
    "text": "9マイナス2イコール1113マイナス7イコール20とか、そんな感じだ。"
  },
  {
    "start": 95734,
    "end": 97260,
    "text": "4マイナス1は？"
  },
  {
    "start": 99150,
    "end": 99658,
    "text": "5人だ。"
  },
  {
    "start": 99744,
    "end": 103614,
    "text": "なるほど、この場合はそう学んだということか。"
  },
  {
    "start": 103652,
    "end": 105840,
    "text": "まあ、マイナスというのは実際にはプラスという意味なんだけどね。"
  },
  {
    "start": 106690,
    "end": 110174,
    "text": "ほとんどの人ができることだ。"
  },
  {
    "start": 110212,
    "end": 114880,
    "text": "なるほど、非常に優れた言語モデルならこれができるかもしれない。"
  },
  {
    "start": 115830,
    "end": 122980,
    "text": "エイブラハム・リンカーンは「尻尾の足をそう呼ぶなら、それは真実だ」と言った。"
  },
  {
    "start": 126250,
    "end": 127000,
    "text": "そうだね。"
  },
  {
    "start": 128810,
    "end": 133106,
    "text": "さて、ではこのようなことを具体的に話していこう。"
  },
  {
    "start": 133138,
    "end": 143180,
    "text": "プロンプトがおかしな仮定の上に条件付けされている場合、それは何を意味するのか？"
  },
  {
    "start": 146510,
    "end": 150018,
    "text": "これらは、文脈学習の例としては伝統的なものだ。"
  },
  {
    "start": 150054,
    "end": 155630,
    "text": "バナナは黄色、ほうれん草は緑色、にんじんはオレンジ色だろう。"
  },
  {
    "start": 157650,
    "end": 159278,
    "text": "こんなこともできる。"
  },
  {
    "start": 159364,
    "end": 161054,
    "text": "このゲームをやろう"
  },
  {
    "start": 161092,
    "end": 165874,
    "text": "犬は5、ネズミは3、アリは1、馬は7。"
  },
  {
    "start": 166072,
    "end": 167170,
    "text": "クジラって何？"
  },
  {
    "start": 168870,
    "end": 169618,
    "text": "9人だ。"
  },
  {
    "start": 169784,
    "end": 170962,
    "text": "ああ、わかった。"
  },
  {
    "start": 171096,
    "end": 172946,
    "text": "ああ、だからこれはサイズに対応しているのかもしれない。"
  },
  {
    "start": 173048,
    "end": 173682,
    "text": "それも可能だ。"
  },
  {
    "start": 173736,
    "end": 176680,
    "text": "もしかしたら、これは長寿とかそういうことに相当するのかもしれない。"
  },
  {
    "start": 179050,
    "end": 180982,
    "text": "人々はこれがかなり得意だ。"
  },
  {
    "start": 181036,
    "end": 182790,
    "text": "子供でも大丈夫だ。"
  },
  {
    "start": 182940,
    "end": 192010,
    "text": "大規模な言語モデル、おそらく非常に優れた言語モデルは、こうしたことのいくつかを簡単に行うことができる。"
  },
  {
    "start": 192990,
    "end": 193402,
    "text": "そして"
  },
  {
    "start": 193456,
    "end": 197530,
    "text": "では、この3つの中で、どういう意味でのイン・コンテキスト・ラーニングなのか？"
  },
  {
    "start": 197600,
    "end": 198698,
    "text": "何かを学ぶ？"
  },
  {
    "start": 198864,
    "end": 201274,
    "text": "まあ、私の意図について何かを学ぶということだね。"
  },
  {
    "start": 201312,
    "end": 212720,
    "text": "このプロンプトを作った私、私の頭の中には何らかのルールがあり、非常に具体的な意味では、モデルは私の頭の中にあるこのもの、私の頭の中にあるこのモデルを学んでいるのだ。"
  },
  {
    "start": 213030,
    "end": 216850,
    "text": "また、ウェイトを更新していない。"
  },
  {
    "start": 219190,
    "end": 219602,
    "text": "オーケー。"
  },
  {
    "start": 219656,
    "end": 227974,
    "text": "それから、コンテクスト・ラーニング（文脈学習）とは、このようなことだけを意味するのではないということも主張したい。"
  },
  {
    "start": 228012,
    "end": 233990,
    "text": "また、私が何かを教えようとして、ただ適当に何かを選んだとする。"
  },
  {
    "start": 234140,
    "end": 236354,
    "text": "空豆は窒素を固定する。"
  },
  {
    "start": 236402,
    "end": 248220,
    "text": "空気中の窒素（n2）をアンモニウム（nh3）に変換して土壌に固定し、窒素を好むが空気から直接摂取できない後続の植物が利用できるようにするのだ。"
  },
  {
    "start": 248670,
    "end": 254080,
    "text": "そう考えると、農家が窒素肥料の購入量を減らすためにできることは何だろうか？"
  },
  {
    "start": 254690,
    "end": 261598,
    "text": "空豆の苗を植えてから他の苗を植えればいい。"
  },
  {
    "start": 261764,
    "end": 268994,
    "text": "私が教えたと思われるこのことに基づいて、私はあなたにもっと深い質問もできるでしょ？"
  },
  {
    "start": 269032,
    "end": 276450,
    "text": "例えば、植物が窒素を好むとして、もし空気から窒素を得られないとしたら、土の中でどのような形で窒素を見つけるのだろうか？"
  },
  {
    "start": 276790,
    "end": 279830,
    "text": "まあ、アンモニウムの形で発見されたのかもしれないね。"
  },
  {
    "start": 280970,
    "end": 288646,
    "text": "それが完全に正しいかどうかはわからないが、これはある種のもので、私があなたに何かを教えたから、あなたはそれを操作して、この関数として自明でないステートメントを考え出すことができる。"
  },
  {
    "start": 288748,
    "end": 290682,
    "text": "これを文脈学習とも呼ぼう。"
  },
  {
    "start": 290816,
    "end": 294780,
    "text": "その過程で、あなたのシナプスはどれも更新されていない。"
  },
  {
    "start": 295230,
    "end": 306910,
    "text": "どのウェイトも変わっていないが、私が言ったことを学んでそれを操ることができれば、まだいくつかできるはずだ。"
  },
  {
    "start": 306980,
    "end": 319060,
    "text": "では、これからお話しする具体的な仕事の根底にある大きな疑問のひとつは、優れた言語モデルは、人間であれコンピューターであれ、文脈学習において本質的にできなければならないのだろうか、というものだ。"
  },
  {
    "start": 320070,
    "end": 330840,
    "text": "どのような優れた言語モデルであれ、話しかけるとうまく返答してくれるようなモデルであれ、このような文脈学習ができる必要があるというのは本当だろうか？"
  },
  {
    "start": 332010,
    "end": 333478,
    "text": "質問はたくさんある。"
  },
  {
    "start": 333644,
    "end": 336360,
    "text": "もしそうなら、すべての優れた言語モデルがそうなのだろうか。"
  },
  {
    "start": 337050,
    "end": 348378,
    "text": "まあ、1つの疑問は、これらの固定ウェイトにどのような実際の学習アルゴリズムがエンコードされているのかということだ。"
  },
  {
    "start": 348544,
    "end": 356910,
    "text": "アルゴリズムはともかく、どのような種類の関数クラスを文脈の中で学ぶことができるのか、あるいは文脈の中で学ぶのか。"
  },
  {
    "start": 358450,
    "end": 365140,
    "text": "それから、少し難しいかもしれないが、もっと興味深い質問もある。"
  },
  {
    "start": 366230,
    "end": 383830,
    "text": "もし、大規模に訓練された言語モデルが、コンテキスト学習で本質的にこのようなことをするのであれば、コンテキスト学習でできる機能クラスはどのようにエンコードされるのか、あるいは訓練コーパスとどのように関連するのか。"
  },
  {
    "start": 388650,
    "end": 394074,
    "text": "では、良い言語モデルとはどのようなものなのか？"
  },
  {
    "start": 394272,
    "end": 395194,
    "text": "私にはできない。"
  },
  {
    "start": 395232,
    "end": 404542,
    "text": "仮に本当に優れた言語モデル、あるいは可能な限り優れた言語モデルがあったとして、それがコンテクスト学習のようなことはできないとして、それはどのようなものだろうか？"
  },
  {
    "start": 404596,
    "end": 406960,
    "text": "恥ずかしくなるほどひどいかどうか？"
  },
  {
    "start": 415190,
    "end": 424050,
    "text": "とすれば、優れた言語モデルは、必ずしも優れた文脈学習能力を持っている必要はない。"
  },
  {
    "start": 424710,
    "end": 430520,
    "text": "まあ、一つの自然なこととして、言語モデルに文脈学習の能力を与えれば、言語モデルは改善されるのだろうか？"
  },
  {
    "start": 431210,
    "end": 435430,
    "text": "もちろん、大規模な言語モデルにこのような能力を与えるにはどうすればいいかという問題もある。"
  },
  {
    "start": 443710,
    "end": 458830,
    "text": "言語モデル、つまり優れた言語モデルは本来、重みを更新したり、実際の処理をしたりすることなく学習できる必要があると考える人がどれくらいいるのか、ちょっと興味があります。"
  },
  {
    "start": 460050,
    "end": 462240,
    "text": "そうではないと思っている人がどれだけいるだろうか？"
  },
  {
    "start": 467270,
    "end": 486470,
    "text": "理論的な観点からすると、実用的な観点からするとかなり興味深い質問なのですが、私たちが何を意味しているのか正確にはわからないのですが、どのような種類のタスクを効果的に解決するためにウェイトの更新が本質的に必要なのでしょうか？"
  },
  {
    "start": 491070,
    "end": 492426,
    "text": "これについては、もう少し詳しく話そう。"
  },
  {
    "start": 492448,
    "end": 498906,
    "text": "結局のところ、人間にとってのアナログとは、学習するために睡眠が必要なタスクとは何かという問題なのだろう。"
  },
  {
    "start": 499088,
    "end": 504094,
    "text": "大雑把に言えば、シナプス的なもので、脳はそれほど大きな働きはしない。"
  },
  {
    "start": 504132,
    "end": 510174,
    "text": "昼間は物事を覚えたりすることができるが、夜になるとシナプスの強度が変わるのかもしれない。"
  },
  {
    "start": 510292,
    "end": 511914,
    "text": "記憶が統合されるときだ。"
  },
  {
    "start": 511962,
    "end": 512730,
    "text": "すべてだ。"
  },
  {
    "start": 512820,
    "end": 522370,
    "text": "コグサイや心理学の文献には、睡眠によってどのような種類の仕事が有益になるかを研究する膨大な量の研究がある。"
  },
  {
    "start": 524230,
    "end": 526402,
    "text": "何が起こっているかは非常に複雑だ。"
  },
  {
    "start": 526546,
    "end": 534630,
    "text": "いろいろなことが起こるが、何かを教わるような実験もたくさんある。"
  },
  {
    "start": 534780,
    "end": 540194,
    "text": "次に、実験に参加した半数の人にリラックスしてもらう。"
  },
  {
    "start": 540322,
    "end": 543242,
    "text": "2時間休憩して、また戻ってくる。"
  },
  {
    "start": 543376,
    "end": 546890,
    "text": "残りの半分は眠らされ、1時間ほど昼寝をさせられる。"
  },
  {
    "start": 547710,
    "end": 551114,
    "text": "そして、彼らが目を覚ました後に何かをする。"
  },
  {
    "start": 551152,
    "end": 559760,
    "text": "ただ休憩したりリラックスしたりするよりも、仮眠をとったほうがはるかに能力向上に効果的な仕事はたくさんある。"
  },
  {
    "start": 560210,
    "end": 562726,
    "text": "記憶の定着に関連する作業だけではない。"
  },
  {
    "start": 562778,
    "end": 575540,
    "text": "人々は記憶の定着のための睡眠の役割をきちんと理解しているようだが、他の種類のこともあまりに良い。"
  },
  {
    "start": 576230,
    "end": 587160,
    "text": "講演のもう少し後で、大規模な言語モデルについて、重みを変化させた方が、特定の内容だけを実行するよりもどれだけ良いのか、というようなことを簡単にお話ししたいと思います。"
  },
  {
    "start": 593870,
    "end": 605120,
    "text": "さて、大規模な言語モデルの話に戻りますが、大規模な言語モデルはコンテキスト学習で本当に性能を発揮できるのかという疑問があると思います。"
  },
  {
    "start": 605890,
    "end": 622238,
    "text": "これがそれほどくだらない質問でないのは、より大きなモデルは文脈学習が得意であるように見えるからであり、より大きなモデルには、より奇妙な微調整や、かなり不透明なプロンプトエンジニアリングがあるからだ。"
  },
  {
    "start": 622334,
    "end": 624660,
    "text": "少なくとも、私は彼らが何をしているのか知らない。"
  },
  {
    "start": 625450,
    "end": 630370,
    "text": "トレーニング・コーパスのことはよく知らない。"
  },
  {
    "start": 630450,
    "end": 634920,
    "text": "どのようなタスクがすでにトレーニング・コーパスに含まれているのか、あるいは含まれていないのかがはっきりしない。"
  },
  {
    "start": 638090,
    "end": 640294,
    "text": "という疑問が残る。"
  },
  {
    "start": 640412,
    "end": 646330,
    "text": "文脈学習におけるこの見かけの能力は、どこまでが単なる幻影なのか、それともトレーニング・コーパスの機能なのか。"
  },
  {
    "start": 647150,
    "end": 656558,
    "text": "少なくとも最初の疑問は、本当の意味で文脈学習を行うモデルを訓練できるかということだ。"
  },
  {
    "start": 656724,
    "end": 660266,
    "text": "ゼロからトレーニングするのだから、隠されたデータセットなどない。"
  },
  {
    "start": 660298,
    "end": 661918,
    "text": "何をトレーニングしているのか、はっきり分かっている。"
  },
  {
    "start": 662004,
    "end": 665394,
    "text": "コンテキスト・ラーニングに微調整はない。"
  },
  {
    "start": 665432,
    "end": 668050,
    "text": "それは迅速なエンジニアリングの機能ではないだろう。"
  },
  {
    "start": 668790,
    "end": 671058,
    "text": "少なくとも、そのためのモデルを訓練することはできないだろうか？"
  },
  {
    "start": 671144,
    "end": 672420,
    "text": "それが最初の質問だ。"
  },
  {
    "start": 674230,
    "end": 680774,
    "text": "では、この質問について説明しよう。"
  },
  {
    "start": 680812,
    "end": 683110,
    "text": "それから、よりレベルの高い質問に戻ろう。"
  },
  {
    "start": 683260,
    "end": 685110,
    "text": "私たちは皆、アンダーワードで話しているのだろうか？"
  },
  {
    "start": 688430,
    "end": 694780,
    "text": "では、ある特定の関数クラスについて、正式にコンテキスト学習を行うとはどういうことだろうか？"
  },
  {
    "start": 697070,
    "end": 699302,
    "text": "まあ、だから、ある程度の配給はあるだろう。"
  },
  {
    "start": 699366,
    "end": 702698,
    "text": "D sub xは例に対する分布になる。"
  },
  {
    "start": 702874,
    "end": 703946,
    "text": "これらの例を考えてみよう。"
  },
  {
    "start": 703978,
    "end": 707040,
    "text": "たぶん、これは単なるベクターで、何か別のものだろう。"
  },
  {
    "start": 707410,
    "end": 715220,
    "text": "私たちは、例をある集合にマッピングする関数についての分布を持つことになる。"
  },
  {
    "start": 715590,
    "end": 720654,
    "text": "そうすれば、ここに例の分布から選んだ例があるように、プロンプトを形成することができる。"
  },
  {
    "start": 720702,
    "end": 730360,
    "text": "まずこの分布から関数を選択し、その関数を使って例、別の例、別のラベル、というようにラベルを描く。"
  },
  {
    "start": 730970,
    "end": 738040,
    "text": "そして、プロンプトの最後にはクエリが表示され、クエリポイントに適用されるこの関数が何なのかを知りたいだけなのだ。"
  },
  {
    "start": 739230,
    "end": 741210,
    "text": "モデルMをトレーニングする。"
  },
  {
    "start": 741280,
    "end": 744540,
    "text": "このモデルが文脈学習でできることがわかる。"
  },
  {
    "start": 744910,
    "end": 755470,
    "text": "この分布とこの分布に関して、この関数のセット、あるいはこの関数のクラスを学習することができる。"
  },
  {
    "start": 756130,
    "end": 772002,
    "text": "このディストリビューションからこのようなプロンプトを作り上げると、それにふさわしいラベルのようなものが手に入る。"
  },
  {
    "start": 772136,
    "end": 783000,
    "text": "これは、文脈の中で学ぶパックのようなものだと思う。"
  },
  {
    "start": 783850,
    "end": 788940,
    "text": "特に、我々のモデルMは固定モデルであり、何もない。"
  },
  {
    "start": 791310,
    "end": 795210,
    "text": "ウェイトはプロンプトを見ながら更新されていく。"
  },
  {
    "start": 799550,
    "end": 807962,
    "text": "さて、これは同じスライドだ。"
  },
  {
    "start": 808106,
    "end": 810222,
    "text": "どのような機能クラスを見るのか？"
  },
  {
    "start": 810276,
    "end": 816114,
    "text": "さて、まずは理論家が最初に書くような非常にシンプルな関数クラスから見ていくことにしよう。"
  },
  {
    "start": 816152,
    "end": 820370,
    "text": "ここではまず、d次元の線形一次関数だけを見ることにする。"
  },
  {
    "start": 820870,
    "end": 844298,
    "text": "また、小さな決定木の集合を学習する小さな学習も見ていくし、他のシンプルな関数クラスもいくつか、より具体的なインスタンス化も行っていく。"
  },
  {
    "start": 844394,
    "end": 845806,
    "text": "モデルを訓練できるか？"
  },
  {
    "start": 845988,
    "end": 859490,
    "text": "変換器やLSDM、あるいはもっと単純なもので、完全に接続されたディープ・ニューラル・ネットワークが、一晩で自然な分布に関する基本的な関数クラスを学習するのかもしれない。"
  },
  {
    "start": 861190,
    "end": 864660,
    "text": "まずはトランスの話から始めよう。"
  },
  {
    "start": 867530,
    "end": 873190,
    "text": "我々はgpt-2スタイルのトランスフォーマー・アーキテクチャーのようなものを考えている。"
  },
  {
    "start": 874170,
    "end": 884010,
    "text": "私たちはさまざまなサイズのモデルを検討したが、実験のほとんどはおよそ2000万個のパラメーターを持つモデルだった。"
  },
  {
    "start": 885310,
    "end": 896010,
    "text": "小型のモデルも大丈夫だが、ある時点で劣化してかなり便利になる。"
  },
  {
    "start": 896170,
    "end": 900670,
    "text": "ラベルや例のラベル、例のラベルにプロンプトを入れるのだ。"
  },
  {
    "start": 901250,
    "end": 909422,
    "text": "ということは、ラベルとなるべきものの埋め込みを見るだけだ。"
  },
  {
    "start": 909486,
    "end": 911940,
    "text": "肝心なのは、ここで何が出てくるかだ。"
  },
  {
    "start": 912630,
    "end": 915010,
    "text": "これはデコーダのみのアーキテクチャのようなものだ。"
  },
  {
    "start": 916010,
    "end": 921510,
    "text": "このラベルは、2つ目の入力から上がってくるものの機能でしかない。"
  },
  {
    "start": 921850,
    "end": 931542,
    "text": "では、敗戦を評価した場合、2つ目のラベルの予測はどの程度可能なのだろうか？"
  },
  {
    "start": 931606,
    "end": 942874,
    "text": "1つの例を示したラベル、2つの例を示したラベル......。"
  },
  {
    "start": 942992,
    "end": 945520,
    "text": "じゃあ、みんなはどうなると思う？"
  },
  {
    "start": 946450,
    "end": 948080,
    "text": "もちろん、学習するよね？"
  },
  {
    "start": 952690,
    "end": 964050,
    "text": "自然な出発点では、例に関する分布は、ただ単に共分散ガウス分布になる。"
  },
  {
    "start": 964890,
    "end": 969698,
    "text": "D次元は一次関数である。"
  },
  {
    "start": 969874,
    "end": 975110,
    "text": "重みベクトルは、これもまたd次元の恒等共分散、ガウシアンから一様に選ぶことにする。"
  },
  {
    "start": 975690,
    "end": 980034,
    "text": "この数字は20次元の線形関数のものだろう。"
  },
  {
    "start": 980082,
    "end": 988140,
    "text": "寸法を使ったスケーリングの法則について話すこともできるけど、たぶんそれはやらない。"
  },
  {
    "start": 990190,
    "end": 998830,
    "text": "これはベイズ最適学習器の経験的なプロットであり、最小二乗を行っているだけである。"
  },
  {
    "start": 1002770,
    "end": 1015082,
    "text": "20個のラベルポイントがコンテキストにあれば、誤差はゼロになり、線形関数を学習したことになる。"
  },
  {
    "start": 1015246,
    "end": 1024440,
    "text": "これは経験的なもので、このメタ学習問題に対して特別に訓練した変換器だからだ。"
  },
  {
    "start": 1024810,
    "end": 1025558,
    "text": "そうだ。"
  },
  {
    "start": 1025724,
    "end": 1027446,
    "text": "これはベースラインだ。"
  },
  {
    "start": 1027478,
    "end": 1035130,
    "text": "私たちは、文脈の中で線形回帰を行う方法を学ぶために、ゼロからトランスフォーマーをトレーニングしています。"
  },
  {
    "start": 1037870,
    "end": 1044154,
    "text": "最小二乗法とは何か、回帰アルゴリズムとは何か、もう少し詳しく話そう。"
  },
  {
    "start": 1044202,
    "end": 1045086,
    "text": "少しね。"
  },
  {
    "start": 1045268,
    "end": 1063662,
    "text": "このオレンジの線は、基本的に青い線の上にあり、これが変圧器が最終的に行うことで、確かにこの例の分布に関しては、基本的に回帰を行っているように見える。"
  },
  {
    "start": 1063726,
    "end": 1064340,
    "text": "そうだね。"
  },
  {
    "start": 1065850,
    "end": 1069000,
    "text": "ベクトルや数値を入力する。"
  },
  {
    "start": 1069610,
    "end": 1070454,
    "text": "そうだね。"
  },
  {
    "start": 1070652,
    "end": 1071400,
    "text": "いいね。"
  },
  {
    "start": 1074650,
    "end": 1075910,
    "text": "何をインプットしますか？"
  },
  {
    "start": 1077070,
    "end": 1080058,
    "text": "私たちはまず、この変圧器をトレーニングした。"
  },
  {
    "start": 1080224,
    "end": 1080650,
    "text": "そうだね。"
  },
  {
    "start": 1080720,
    "end": 1081340,
    "text": "オーケー。"
  },
  {
    "start": 1081790,
    "end": 1083580,
    "text": "前のスライドが正しいのかもしれない。"
  },
  {
    "start": 1085710,
    "end": 1087286,
    "text": "トランスフォーマーをどうやって鍛えるのか？"
  },
  {
    "start": 1087318,
    "end": 1097946,
    "text": "ランダムに初期化し、ランダムなd次元ベクトルからランダムな関数を選ぶ。"
  },
  {
    "start": 1098138,
    "end": 1113794,
    "text": "私たちはこのプロンプトを生成し、これらのxをすべてランダムに選び、そして損失に対してトレーニングアップデートを行う。"
  },
  {
    "start": 1113832,
    "end": 1117078,
    "text": "ここで出てくるのは、このx2のfのようになるはずだ。"
  },
  {
    "start": 1117244,
    "end": 1125000,
    "text": "次の例題では、新しいランダムな関数、クラスから新しいランダムな線形関数、新しいランダムな例題を選びます。"
  },
  {
    "start": 1128410,
    "end": 1129142,
    "text": "これを学べ。"
  },
  {
    "start": 1129196,
    "end": 1130634,
    "text": "数字を入力するだけだ。"
  },
  {
    "start": 1130752,
    "end": 1132122,
    "text": "数字を入力するだけだよ。"
  },
  {
    "start": 1132176,
    "end": 1133366,
    "text": "ゼロからトレーニングしているんだ。"
  },
  {
    "start": 1133398,
    "end": 1136234,
    "text": "言葉を知らず、何も知らない。"
  },
  {
    "start": 1136352,
    "end": 1138954,
    "text": "私たちは、この数字を差し込むだけだ。"
  },
  {
    "start": 1138992,
    "end": 1141420,
    "text": "プロンプトは関数を見つけること。"
  },
  {
    "start": 1142030,
    "end": 1149310,
    "text": "ああ、この予測では、損失は二乗誤差に過ぎないんだ。"
  },
  {
    "start": 1150370,
    "end": 1159250,
    "text": "そして、損失は二乗誤差だ。"
  },
  {
    "start": 1159830,
    "end": 1173000,
    "text": "つまり、ここで出てくるのは、最初のkからマイナス1したラベル付き例とケースラベル付き例の関数で、ここでの損失は、この最後の例のラベルの二乗誤差に過ぎない。"
  },
  {
    "start": 1173530,
    "end": 1177320,
    "text": "このプロセスを通じて、この場合のウェイトは変化しているのか？"
  },
  {
    "start": 1177790,
    "end": 1185450,
    "text": "トレーニング中は変化していくが、評価時にはモデルを修正し、重みを修正する。"
  },
  {
    "start": 1185870,
    "end": 1196862,
    "text": "さて、新しい関数、この分布、そしていくつの例からなる新しいセットを選び、そのプロンプトを作れば、重みは変わらない。"
  },
  {
    "start": 1196916,
    "end": 1206580,
    "text": "プロンプトを表示し、それを変圧器に送り、最後の、あるいはデータポイントのラベルを確認する。"
  },
  {
    "start": 1207270,
    "end": 1214340,
    "text": "これは、あなたが与えたインコンテキストのデータポイントの数の関数として、あなたがどれだけ正確にやっているかを教えてくれます。"
  },
  {
    "start": 1214890,
    "end": 1218498,
    "text": "期待通りだ。"
  },
  {
    "start": 1218594,
    "end": 1224680,
    "text": "ええ、次元ラベルの付いた例を与えると、基本的に線形関数を割り出してくれます。"
  },
  {
    "start": 1225210,
    "end": 1227740,
    "text": "これは単なる教師あり学習ではないのか？"
  },
  {
    "start": 1228750,
    "end": 1235930,
    "text": "たくさんの例を与えて、それから新しい例を与えて、出力を予測させる。"
  },
  {
    "start": 1237550,
    "end": 1238154,
    "text": "いいね。"
  },
  {
    "start": 1238272,
    "end": 1241434,
    "text": "なぜ文脈からそう呼ぶのか？"
  },
  {
    "start": 1241482,
    "end": 1242126,
    "text": "学習。"
  },
  {
    "start": 1242308,
    "end": 1244830,
    "text": "教師あり学習だ。"
  },
  {
    "start": 1244980,
    "end": 1248606,
    "text": "教師あり学習の問題では、線形関数を学習するわけではない。"
  },
  {
    "start": 1248788,
    "end": 1250602,
    "text": "それは回帰を学ぶことだ。"
  },
  {
    "start": 1250746,
    "end": 1251920,
    "text": "そこが違うんだ。"
  },
  {
    "start": 1254690,
    "end": 1259282,
    "text": "通常、一次関数の教師あり学習について考える場合、一次関数は1つである。"
  },
  {
    "start": 1259336,
    "end": 1265042,
    "text": "私はたくさんの例を挙げ、あなたはここでその機能を学ぼうとウェイトを更新している。"
  },
  {
    "start": 1265096,
    "end": 1272114,
    "text": "あなたが学ぼうとしている関数は、ラベル付けされた新しいランダムな線形関数の束を写像する回帰関数である。"
  },
  {
    "start": 1272162,
    "end": 1274086,
    "text": "より大きなクラスだが、監督付き学習であることに変わりはない。"
  },
  {
    "start": 1274188,
    "end": 1275240,
    "text": "ああ、もちろんだ。"
  },
  {
    "start": 1277070,
    "end": 1284410,
    "text": "k個の例を1つのプロンプトのように渡しているのか、それともk個の別々のプロンプトなのか？"
  },
  {
    "start": 1287470,
    "end": 1297038,
    "text": "最後のスライドでは、トレーニングをするときに、各関数、つまり関数をサンプリングして、その関数からプロンプトを1つ作ります。"
  },
  {
    "start": 1297124,
    "end": 1299518,
    "text": "さて、これでk個のトレーニング・データはすべて揃った。"
  },
  {
    "start": 1299604,
    "end": 1301946,
    "text": "それはまるで1つのメタトレーニングポイントのようだ。"
  },
  {
    "start": 1301988,
    "end": 1306610,
    "text": "例えば、1つのファンクションにつき、40回だけ×をするとしよう。"
  },
  {
    "start": 1306760,
    "end": 1307790,
    "text": "それが一つのプロンプトだ。"
  },
  {
    "start": 1307870,
    "end": 1308162,
    "text": "オーケー。"
  },
  {
    "start": 1308216,
    "end": 1313554,
    "text": "そして、新しい関数を組み立て、その関数に向かって新しい40個のxのラベルを作る。"
  },
  {
    "start": 1313672,
    "end": 1314850,
    "text": "それも問題だ。"
  },
  {
    "start": 1314920,
    "end": 1317118,
    "text": "ターゲットはk次元のベクトルのようなものだ。"
  },
  {
    "start": 1317214,
    "end": 1320310,
    "text": "申し訳ないが、出力対象はk次元ベクトルのようなものだ。"
  },
  {
    "start": 1322410,
    "end": 1330780,
    "text": "その次元は、コンテクストxのそれぞれに対して、r1個の数字があると考えればいい。"
  },
  {
    "start": 1336670,
    "end": 1345886,
    "text": "プログレッションの出力全体を予測しようとするのではなく、プログレッションの係数を学習しようとすることに意味があるのだろうか？"
  },
  {
    "start": 1345988,
    "end": 1351006,
    "text": "最後のリニアレイヤーがそうしているのだろうか？"
  },
  {
    "start": 1351188,
    "end": 1352800,
    "text": "なるほど、これはいい質問だ。"
  },
  {
    "start": 1353670,
    "end": 1371510,
    "text": "この係数で表されるような一次関数を学習しなければならないと言うのであれば、それはもっと簡単な問題で、例題と内積したときにこのようなラベルが得られるような係数を見つけろということです。"
  },
  {
    "start": 1373050,
    "end": 1375400,
    "text": "それがここではずっと楽なんだ。"
  },
  {
    "start": 1375770,
    "end": 1378300,
    "text": "どんな奇妙なことでもできる。"
  },
  {
    "start": 1379470,
    "end": 1383900,
    "text": "ええ、もしあなたがただ解凍を始めたいのであれば、それはどのようなアルゴリズムを表しているのですか？"
  },
  {
    "start": 1386670,
    "end": 1399550,
    "text": "このトランスフォームの内部には、一次関数について知っている部分があり、それを試してみて、データと一致したと結論づけることができる。"
  },
  {
    "start": 1399620,
    "end": 1404474,
    "text": "では、どうやって回帰を解決するのか？"
  },
  {
    "start": 1404522,
    "end": 1408706,
    "text": "例えば、ガウスの消去法みたいなことをするんだ。"
  },
  {
    "start": 1408808,
    "end": 1410542,
    "text": "一次関数について知っていますか？"
  },
  {
    "start": 1410606,
    "end": 1412100,
    "text": "そんなところだ。"
  },
  {
    "start": 1412550,
    "end": 1414890,
    "text": "代数学の訓練を受けていたのか？"
  },
  {
    "start": 1415070,
    "end": 1420454,
    "text": "いや、これは文字通り、訓練されただけなんだ。"
  },
  {
    "start": 1420572,
    "end": 1421830,
    "text": "あなたの例もある。"
  },
  {
    "start": 1421980,
    "end": 1424200,
    "text": "それもただのチャットGPTだった。"
  },
  {
    "start": 1425050,
    "end": 1428470,
    "text": "確かに、でもあの建築なんだ。"
  },
  {
    "start": 1428630,
    "end": 1431402,
    "text": "私たちが与えたものでしか訓練されていない。"
  },
  {
    "start": 1431456,
    "end": 1432650,
    "text": "言葉はない。"
  },
  {
    "start": 1436030,
    "end": 1441090,
    "text": "私の疑問は、これが砂糖について学ぶ失禁なのかということだ。"
  },
  {
    "start": 1441270,
    "end": 1449120,
    "text": "トランスフォーマーの代わりに、ディープニューラルネットワークを使ってこのデータセットに適合させる。"
  },
  {
    "start": 1449490,
    "end": 1453146,
    "text": "このディープ・ニューラル・ネットワークはリニア・プレッシャーに耐えられるのか？"
  },
  {
    "start": 1453178,
    "end": 1460660,
    "text": "もしそうなら、よし、わかった、じゃあこの辺で。"
  },
  {
    "start": 1462630,
    "end": 1463746,
    "text": "少し試してみた。"
  },
  {
    "start": 1463768,
    "end": 1467062,
    "text": "完全に接続されたニューラルネットワークの場合は、トレーニングが難しいようだ。"
  },
  {
    "start": 1467196,
    "end": 1471782,
    "text": "lstmsでもやったし、うまくいった。"
  },
  {
    "start": 1471916,
    "end": 1475960,
    "text": "トランスフォーマーに関しては、特別なことは何もないと思う。"
  },
  {
    "start": 1477390,
    "end": 1478700,
    "text": "それはまた今度。"
  },
  {
    "start": 1482350,
    "end": 1482858,
    "text": "オーケー。"
  },
  {
    "start": 1482944,
    "end": 1489914,
    "text": "これは本当に正しいことなのか？"
  },
  {
    "start": 1489952,
    "end": 1493930,
    "text": "あるいは、ただ煙に巻いているだけかもしれない。"
  },
  {
    "start": 1494090,
    "end": 1503454,
    "text": "もし、あなたが合理的なヒューリスティックで偽の方法を作り上げたのであれば、あなたが与えたトレーニングセットから最も近いものを探す、といったようなことができるだろう。"
  },
  {
    "start": 1503572,
    "end": 1505070,
    "text": "これらはひどいものだ。"
  },
  {
    "start": 1505150,
    "end": 1509182,
    "text": "あるいは、ある種の非常に程度の低い平均化アルゴリズムを行っているだけかもしれない。"
  },
  {
    "start": 1509326,
    "end": 1512254,
    "text": "何か根本的なことをやっているように思える。"
  },
  {
    "start": 1512302,
    "end": 1514260,
    "text": "もう少し調べてみよう。"
  },
  {
    "start": 1518150,
    "end": 1518610,
    "text": "オーケー。"
  },
  {
    "start": 1518680,
    "end": 1519402,
    "text": "ただ解決するだけだ。"
  },
  {
    "start": 1519486,
    "end": 1526630,
    "text": "もっと複雑な関数クラスでは、一次関数の代わりにスパース一次関数を使うとしよう。"
  },
  {
    "start": 1527050,
    "end": 1543754,
    "text": "つまり、一次関数があったとして、その座標のうちk個だけが非ゼロだとしたら、必要な例数はk個に次元の対数を掛けた数だけで、正則化回帰の1つであるlasso lのようなものが必要になります。"
  },
  {
    "start": 1543882,
    "end": 1556260,
    "text": "それを利用すれば、驚いたことに、トランスフォーマーはこのようなまばらなものに対しては、基本的に投げ縄と同じくらいうまくいくようだ。"
  },
  {
    "start": 1558870,
    "end": 1560100,
    "text": "ある意味興味深い。"
  },
  {
    "start": 1563350,
    "end": 1566630,
    "text": "真のモデルがスパースであることは伝えない。"
  },
  {
    "start": 1567370,
    "end": 1568840,
    "text": "私たちはこれを与えるだけだ。"
  },
  {
    "start": 1569690,
    "end": 1571842,
    "text": "ラベル付きランダムスパース関数。"
  },
  {
    "start": 1571906,
    "end": 1573590,
    "text": "ラベル付きランダムスパース関数。"
  },
  {
    "start": 1573930,
    "end": 1579850,
    "text": "そして、ラベル付けされた多くの例を与え、新しいランダムなスパース関数を指し示すと、正しいラベルが表示される。"
  },
  {
    "start": 1581470,
    "end": 1593050,
    "text": "不思議なことに、これを訓練すると、損失曲線は長い間平坦になり、その後ジャンプダウンする。"
  },
  {
    "start": 1593130,
    "end": 1598830,
    "text": "その後、本当に長い間横ばいになり、スパース性を利用する方法を見つけ出した途端にジャンプダウンする。"
  },
  {
    "start": 1599570,
    "end": 1606794,
    "text": "ああ、それは二段投げ縄のようなものだ。"
  },
  {
    "start": 1606842,
    "end": 1607358,
    "text": "もしそうなら"
  },
  {
    "start": 1607444,
    "end": 1609418,
    "text": "投げ縄のアイデアは知っているだろう。"
  },
  {
    "start": 1609444,
    "end": 1610638,
    "text": "このレギュラーを与えなければならない。"
  },
  {
    "start": 1610654,
    "end": 1611806,
    "text": "そうしないとオーバーフィットしてしまう。"
  },
  {
    "start": 1611838,
    "end": 1612034,
    "text": "そうだね。"
  },
  {
    "start": 1612072,
    "end": 1615158,
    "text": "大量のデータを投入しない限りはね。"
  },
  {
    "start": 1615164,
    "end": 1615286,
    "text": "そうだね。"
  },
  {
    "start": 1615308,
    "end": 1617030,
    "text": "投げ縄は本当に役に立つ。"
  },
  {
    "start": 1617180,
    "end": 1627910,
    "text": "この場合、回帰アルゴリズムのような正則化器を使わなくても、ゼロを見つけることができるような十分なデータを投入しているのでしょうか？"
  },
  {
    "start": 1629930,
    "end": 1630680,
    "text": "申し訳ない。"
  },
  {
    "start": 1631290,
    "end": 1638982,
    "text": "つまり、我々はこのシステムを訓練し、この文脈におけるxの数が増えるにつれて、その予測における誤差はどのように減少するのだろうか？"
  },
  {
    "start": 1639126,
    "end": 1643026,
    "text": "つまり、この青い線は、回帰を行っただけなら、まだ20次元にいる。"
  },
  {
    "start": 1643078,
    "end": 1645760,
    "text": "20の次元があれば、あなたはそのことを学ぶだろう。"
  },
  {
    "start": 1646210,
    "end": 1649546,
    "text": "この曲線は、レバレッジをかけた場合に得られるものだ。"
  },
  {
    "start": 1649658,
    "end": 1652734,
    "text": "3スパースだったか、4スパースだったか、そんな感じだったかは忘れた。"
  },
  {
    "start": 1652852,
    "end": 1657106,
    "text": "そう、そう、10個の例があればできるんだ。"
  },
  {
    "start": 1657128,
    "end": 1657602,
    "text": "まあね。"
  },
  {
    "start": 1657736,
    "end": 1661742,
    "text": "事前情報を教えていないのに、実際に事前情報を推論しているのはちょっと変でしょう？"
  },
  {
    "start": 1661806,
    "end": 1662420,
    "text": "そうだね。"
  },
  {
    "start": 1664070,
    "end": 1666360,
    "text": "どうしてそうなっているのか、何かお分かりになりますか？"
  },
  {
    "start": 1667770,
    "end": 1669782,
    "text": "そのことについては、すぐに2、3言いたいと思う。"
  },
  {
    "start": 1669836,
    "end": 1689290,
    "text": "つまり、小さな決定木では、Xdboostのような最先端の決定木学習器よりもずっとうまくいっています。"
  },
  {
    "start": 1694210,
    "end": 1698618,
    "text": "関数が、関数クラスのように、ランダムにサンプリングする場合。"
  },
  {
    "start": 1698714,
    "end": 1705714,
    "text": "ランダムに2層のニューラルネットワークを初期化し、それを使ってデータ・ポイントをラベル付けすれば、その関数を学習できる。"
  },
  {
    "start": 1705912,
    "end": 1714370,
    "text": "ここでいう基準線とは、コンテキストに対して勾配降下法で2層ネットワークを学習させる場合のことである。"
  },
  {
    "start": 1714730,
    "end": 1730762,
    "text": "重みを更新しているトランスフォーマーと、それとほぼ同じことをしているトランスフォーマーを、重みが変化していないこのトランスフォーマーと、この関数を学習したい場合にすることを比較しているようなものだ。"
  },
  {
    "start": 1730816,
    "end": 1738326,
    "text": "2層のニューラルネットワークをランダムに初期化し、勾配降下法でこのラベルにマッチするように訓練するのだ。"
  },
  {
    "start": 1738438,
    "end": 1740490,
    "text": "これが参考スライドのようなものだ。"
  },
  {
    "start": 1741310,
    "end": 1747200,
    "text": "ジョナサン、これらの異なるクラスごとに1つのネットワークを訓練したのか、それとも1つなのか？"
  },
  {
    "start": 1747810,
    "end": 1748126,
    "text": "そうだね。"
  },
  {
    "start": 1748148,
    "end": 1752206,
    "text": "それぞれ別々に、ランダムに初期化したんだ。"
  },
  {
    "start": 1752308,
    "end": 1756306,
    "text": "もしそうなら、そんなことをする必要はなかったかもしれないが。"
  },
  {
    "start": 1756408,
    "end": 1758514,
    "text": "これらすべてを1つのネットワークで行うこともできるだろう。"
  },
  {
    "start": 1758632,
    "end": 1759300,
    "text": "そうだ。"
  },
  {
    "start": 1761510,
    "end": 1763540,
    "text": "どうすれば始められるか、もう少し詳しく教えてください。"
  },
  {
    "start": 1763990,
    "end": 1764774,
    "text": "ああ、そうだな。"
  },
  {
    "start": 1764812,
    "end": 1784652,
    "text": "これらは各ノードで、ランダムに選ぶ。"
  },
  {
    "start": 1784796,
    "end": 1786450,
    "text": "これがそうだ。"
  },
  {
    "start": 1787000,
    "end": 1794180,
    "text": "次元空間のまま、ランダムな座標とランダムな閾値を選んで、ランダムな決定木を作る。"
  },
  {
    "start": 1794260,
    "end": 1802760,
    "text": "決定木の分布は、Xgboostが調整されていないという意味で、その分布に合わせたことをしている。"
  },
  {
    "start": 1804640,
    "end": 1813310,
    "text": "それでも、このディストリビューションでも、ただ素朴で貪欲なツリー構築をしているわけではなく、興味深い仕事をしている。"
  },
  {
    "start": 1814000,
    "end": 1815164,
    "text": "これについては、もっと話すことができる。"
  },
  {
    "start": 1815202,
    "end": 1821728,
    "text": "実際、ある種の理論的な見地からは、面白いことをやっていると思う。"
  },
  {
    "start": 1821814,
    "end": 1824656,
    "text": "今、その"
  },
  {
    "start": 1824678,
    "end": 1832500,
    "text": "訓練例、訓練とテストにおける文脈中の例の数は同一である。"
  },
  {
    "start": 1833080,
    "end": 1835408,
    "text": "コンテキストウィンドウのサイズ。"
  },
  {
    "start": 1835584,
    "end": 1836212,
    "text": "そうだね。"
  },
  {
    "start": 1836346,
    "end": 1840340,
    "text": "あなたは一般化についてテストしていない。"
  },
  {
    "start": 1841160,
    "end": 1846820,
    "text": "トレーニングではより少ないサンプルをトレーニングし、テストではより多くのサンプルをテストする。"
  },
  {
    "start": 1848860,
    "end": 1858456,
    "text": "文脈上のXの数は、トランスフォーマーがXの数ごとに機能する方法を持っている。"
  },
  {
    "start": 1858488,
    "end": 1861068,
    "text": "その前に与えられたレッテルをこなそうとしている。"
  },
  {
    "start": 1861154,
    "end": 1867010,
    "text": "その意味では、できるだけ少ない文脈の例で学ぶことに最善を尽くしているのだ。"
  },
  {
    "start": 1870980,
    "end": 1876156,
    "text": "そうだね、もっとたくさんあげれば、僕らのコンテクストウィンドウより大きくなるね。"
  },
  {
    "start": 1876188,
    "end": 1878850,
    "text": "意味不明だ。"
  },
  {
    "start": 1884360,
    "end": 1893030,
    "text": "コンテキストの大きさを5で訓練し、コンテキストの大きさを10でテストして、より良くなった場合、すべての例が与えられる。"
  },
  {
    "start": 1893500,
    "end": 1897688,
    "text": "申し訳ないが、10人というコンテキストのサイズをテストしても意味がない。"
  },
  {
    "start": 1897774,
    "end": 1899480,
    "text": "私たちはこのコンテクストを全部突っ込んでいるんだ。"
  },
  {
    "start": 1899550,
    "end": 1900760,
    "text": "それがインプットだ。"
  },
  {
    "start": 1901260,
    "end": 1903160,
    "text": "こういうものには決まった文脈がある。"
  },
  {
    "start": 1904720,
    "end": 1905950,
    "text": "言いたいことは分かるよ。"
  },
  {
    "start": 1907520,
    "end": 1912220,
    "text": "トランスポートに対する2層ネットワークの規模は？"
  },
  {
    "start": 1913680,
    "end": 1915740,
    "text": "ああ、これは、これはもっと小さいよ。"
  },
  {
    "start": 1919760,
    "end": 1935430,
    "text": "アルゴリズムがs 3dで訓練されたニューラルネットを学習するタスクや関数クラスのようなものを見つけたかどうか興味がある。"
  },
  {
    "start": 1935800,
    "end": 1936212,
    "text": "いいね。"
  },
  {
    "start": 1936266,
    "end": 1940448,
    "text": "じゃあ、lstmsと比較してみるよ。"
  },
  {
    "start": 1940624,
    "end": 1947610,
    "text": "完全に接続されたディープ・ニューラル・ネットワークとの比較をするつもりはないが、この比較はする価値があると思う。"
  },
  {
    "start": 1949100,
    "end": 1964972,
    "text": "さて、非常に手短に説明すると、これは興味深いことで、おそらく退行にとって、カリキュラム学習を行うことはトレーニングを本当にスピードアップさせる。"
  },
  {
    "start": 1965106,
    "end": 1977680,
    "text": "基本的には1次元の例、2次元の例といったように、低次元から始めて、最終的な次元まで次元を積み上げていくのが自然なカリキュラムだ。"
  },
  {
    "start": 1978020,
    "end": 1990020,
    "text": "オレンジの線はカリキュラムなしのトレーニングの損失で、カリキュラムありのオレンジの線はこの青い線である。"
  },
  {
    "start": 1990170,
    "end": 1997320,
    "text": "窪んでしまうのは、低次元のものにとっては簡単な問題だからだ。"
  },
  {
    "start": 1997390,
    "end": 1999450,
    "text": "トレーニングのロスは実際にはもっと少ないはずだ。"
  },
  {
    "start": 1999980,
    "end": 2005916,
    "text": "この数字が上がるということは、カリキュラムが増えるにつれて問題が難しくなっているという事実を反映している。"
  },
  {
    "start": 2006098,
    "end": 2015676,
    "text": "この最初のプロットは、カリキュラムがなくても次元が10であれば、最終的には学ぶことができる。"
  },
  {
    "start": 2015778,
    "end": 2023360,
    "text": "20次元、カリキュラムがないと習得に時間がかかる、30次元、40次元、50次元。"
  },
  {
    "start": 2024740,
    "end": 2029228,
    "text": "もしかしたら、カリキュラムなしで本当に長く走らせないと無理なのかもしれない。"
  },
  {
    "start": 2029414,
    "end": 2029876,
    "text": "そんなところだ。"
  },
  {
    "start": 2029898,
    "end": 2034660,
    "text": "不思議なことに、このカリキュラムはトレーニングをスピードアップさせるが、最終的なパフォーマンスを向上させるわけではない。"
  },
  {
    "start": 2037640,
    "end": 2043024,
    "text": "一般論のようなものだが、最終的なパフォーマンスに支障をきたすことはないのか？"
  },
  {
    "start": 2043072,
    "end": 2044520,
    "text": "引っかかったりしない？"
  },
  {
    "start": 2046700,
    "end": 2058730,
    "text": "プラトー（停滞期）を見たときのような奇妙なことがいくつかあるんですが、ラッソーや学習回帰、スパースティ（疎分散性）などについておっしゃいましたが、プラトーは常に見られるものなのでしょうか、それとも時々見られるものなのでしょうか？"
  },
  {
    "start": 2059660,
    "end": 2065230,
    "text": "トレーニングのスピードを上げることは、学習問題におけるプラトー（停滞期）がいくつあるかわからない場合に役立つのだろうかと思う。"
  },
  {
    "start": 2070260,
    "end": 2079860,
    "text": "数日前、このような大規模な言語モデルは、3桁の数字を正確に掛け算するのが難しいことを知った。"
  },
  {
    "start": 2080360,
    "end": 2086790,
    "text": "線形回帰の数学はどうなっているんだ？"
  },
  {
    "start": 2087640,
    "end": 2090180,
    "text": "まあ、さっきも言ったように、これは3桁の数字を掛け合わせるものなんだ。"
  },
  {
    "start": 2090250,
    "end": 2095944,
    "text": "難しいのは、すべての荷物をどうやって運ぶかを考えるために必要な桁数を与えることだ。"
  },
  {
    "start": 2096142,
    "end": 2101210,
    "text": "これらのネットワークでは、実数を掛け合わせることが許されている。"
  },
  {
    "start": 2102060,
    "end": 2104568,
    "text": "ウェイトが掛け算になり、その繰り返しだ。"
  },
  {
    "start": 2104654,
    "end": 2108670,
    "text": "あなたはここで実際のニューロンに数字を入れた。"
  },
  {
    "start": 2109120,
    "end": 2113640,
    "text": "つまり、入力はベクトルで、これらのレイヤーは行列を掛け合わせる。"
  },
  {
    "start": 2113720,
    "end": 2117150,
    "text": "これは、すべての入力がベクトルであり、数字であるようなものだ。"
  },
  {
    "start": 2119760,
    "end": 2128770,
    "text": "ある数字にはその数字に関連する独特の言葉があるのか、その違いをどう理解するのか。"
  },
  {
    "start": 2131080,
    "end": 2140992,
    "text": "数字の掛け算で難しいのは、人々が数字について、本来言語ではない奇妙な抽象表現を持っているという事実だ。"
  },
  {
    "start": 2141056,
    "end": 2149784,
    "text": "それは、トランスフォーマーがどのように機能するかを焼き付けた表現ではないようなものだ。"
  },
  {
    "start": 2149822,
    "end": 2150970,
    "text": "それが難しいところだ。"
  },
  {
    "start": 2151420,
    "end": 2154090,
    "text": "物を運んだりするときの決まりごとだ。"
  },
  {
    "start": 2157600,
    "end": 2159470,
    "text": "数字を出しているのか？"
  },
  {
    "start": 2162400,
    "end": 2164590,
    "text": "これは何桁の数字ですか？"
  },
  {
    "start": 2164960,
    "end": 2168450,
    "text": "これらはおそらく32ビットで表現された数字だろう。"
  },
  {
    "start": 2170340,
    "end": 2174156,
    "text": "コンピューターは内部で32ビットを掛け合わせる方法を知っている。"
  },
  {
    "start": 2174268,
    "end": 2177072,
    "text": "トランスフォーマーはそれをトークン化する方法を知っている。"
  },
  {
    "start": 2177206,
    "end": 2178396,
    "text": "トークン化はない。"
  },
  {
    "start": 2178508,
    "end": 2179830,
    "text": "そこがポイントだと思う。"
  },
  {
    "start": 2180680,
    "end": 2182310,
    "text": "なんとなくわかる。"
  },
  {
    "start": 2185480,
    "end": 2189472,
    "text": "言語のような抽象的な表現はない。"
  },
  {
    "start": 2189536,
    "end": 2192630,
    "text": "私たちは、そのようなベクトルを与えている。"
  },
  {
    "start": 2197560,
    "end": 2205116,
    "text": "掛け算はこのモデルで許される原始演算なのだろうか？"
  },
  {
    "start": 2205298,
    "end": 2219820,
    "text": "標準的なニューラルネットを考えてみると、線形結合といくつかの非線形活性化関数だけで、その乗算があった。"
  },
  {
    "start": 2220180,
    "end": 2222210,
    "text": "ああ、ありがとう。"
  },
  {
    "start": 2222580,
    "end": 2225532,
    "text": "トランスフォーマーだからですか？"
  },
  {
    "start": 2225676,
    "end": 2229664,
    "text": "いや、そうじゃなくて、完全につながったものには内積もあるんだ。"
  },
  {
    "start": 2229862,
    "end": 2231810,
    "text": "すべてのものには内製品がある。"
  },
  {
    "start": 2236120,
    "end": 2238464,
    "text": "ひとこと"
  },
  {
    "start": 2238512,
    "end": 2241430,
    "text": "これらを訓練するには時間がかかる。"
  },
  {
    "start": 2241800,
    "end": 2244884,
    "text": "どのトレーニングもシングルGPUで1日以上かからなかった。"
  },
  {
    "start": 2244932,
    "end": 2250872,
    "text": "関数クラスからサンプリングされたランダムな関数はそんなに必要ない。"
  },
  {
    "start": 2251006,
    "end": 2262620,
    "text": "たとえ1万個のランダムな線形関数しか見たことがなくても、新しい線形関数に汎化することができる。"
  },
  {
    "start": 2265680,
    "end": 2268210,
    "text": "芹亜が話すのはその一点だ。"
  },
  {
    "start": 2270500,
    "end": 2272160,
    "text": "これはたぶんオチだよね？"
  },
  {
    "start": 2272230,
    "end": 2281580,
    "text": "つまり、関数クラス、線形関数、小さな決定木、小さなニューラルネットワーク、疎な線形関数を学習するモデルを訓練できるかということだ。"
  },
  {
    "start": 2281740,
    "end": 2283350,
    "text": "答えは「イエス」だった。"
  },
  {
    "start": 2284040,
    "end": 2292096,
    "text": "私の主張は、十分な訓練データと十分な大きさのモデルがあれば、ベイズの最適アルゴリズムを学習できるはずだということだ。"
  },
  {
    "start": 2292288,
    "end": 2313208,
    "text": "これは、ベイズ最適アルゴリズムが複雑であっても、たとえラッソが閉形式を持っていなかったり、決定木のアルゴリズムがかなり複雑であったとしても、ベイズが何をするにしても、訓練する対象の分布に関して、ベイズが基本的にベイズ最適に近いことをするはずだという期待です。"
  },
  {
    "start": 2313304,
    "end": 2315070,
    "text": "これが期待すべきことだ。"
  },
  {
    "start": 2315440,
    "end": 2324416,
    "text": "もしベイズの最適値に近いものを学んでいないのであれば、ハイパーパラメーターを調整する必要がある。"
  },
  {
    "start": 2324598,
    "end": 2330384,
    "text": "私の主張は、トランスフォーマーだけでなく、どのようなアーキテクチャーにも当てはまるということだ。"
  },
  {
    "start": 2330432,
    "end": 2332500,
    "text": "これはある意味、期待すべきことだ。"
  },
  {
    "start": 2337000,
    "end": 2339108,
    "text": "これに関する論文は最近もたくさん出ている。"
  },
  {
    "start": 2339194,
    "end": 2349060,
    "text": "これは、ちょうど1カ月前に投稿されたもので、この主張をより厳密にしようとするものだ。"
  },
  {
    "start": 2349220,
    "end": 2355710,
    "text": "つまり、何をするにしても、基本的にベイズ最適化するように訓練すればいいということだ。"
  },
  {
    "start": 2356640,
    "end": 2362540,
    "text": "これらの異なるタスクをすべてこなせるモデルを1つ訓練すれば、ベイズ最適化法によって、いつそれを実行すべきかを判断してくれる。"
  },
  {
    "start": 2362690,
    "end": 2374944,
    "text": "さて、そうなると、本当の疑問は、ベイゾピマルなことをやっているのかもしれないが、学習されたアルゴリズムは優れているのだろうか？"
  },
  {
    "start": 2375142,
    "end": 2382450,
    "text": "いつまで続けるつもりだ？"
  },
  {
    "start": 2383060,
    "end": 2386004,
    "text": "さて、質問だが、学習されたアルゴリズムは良いものだろうか？"
  },
  {
    "start": 2386122,
    "end": 2393944,
    "text": "でも、本当のテストは、アルゴリズムが正しいことをやっているかということなんだ。"
  },
  {
    "start": 2394142,
    "end": 2399400,
    "text": "一つの見方は、これらのアルゴリズムが分布外の例に対してどの程度のパフォーマンスを発揮するかということだ。"
  },
  {
    "start": 2403180,
    "end": 2404216,
    "text": "人々はこれを買うのだろうか？"
  },
  {
    "start": 2404238,
    "end": 2406360,
    "text": "この主張、どう思う？"
  },
  {
    "start": 2406880,
    "end": 2408236,
    "text": "私はそれに戸惑っている。"
  },
  {
    "start": 2408338,
    "end": 2425712,
    "text": "スパース性事前分布を使わずに線形回帰を行ったとき、Leeの2乗回帰とlassoの比較を行ったとき、変換器はこれを見て係数のいくつかがゼロであることを理解することができました。"
  },
  {
    "start": 2425846,
    "end": 2426560,
    "text": "入れろ。"
  },
  {
    "start": 2426630,
    "end": 2436580,
    "text": "回帰がそうでなかったとき、最小二乗回帰はそうではなかった。最小二乗回帰は、そのような事前分布を持たない場合に、私が位相最適と考えるようなものだ。"
  },
  {
    "start": 2437800,
    "end": 2438164,
    "text": "もちろんだ。"
  },
  {
    "start": 2438202,
    "end": 2451240,
    "text": "もし、例の分布が、ラベルが疎な線形関数に対応するラベルベクトルしか与えないのであれば、その分布に関しては、正則化の最適な選択によって、ベイズの最適なものはそうではなくなる。"
  },
  {
    "start": 2452700,
    "end": 2457464,
    "text": "結局、リーの2乗がそれを見つけて、ベイズが最適になるんだろ？"
  },
  {
    "start": 2457502,
    "end": 2459516,
    "text": "バイド、違う。"
  },
  {
    "start": 2459618,
    "end": 2469650,
    "text": "ベイズ最適というのは、ラベル付けされた数、コンテキストの大きさが与えられれば、そのデータ量に最適なベースという意味だと思う。"
  },
  {
    "start": 2470580,
    "end": 2480128,
    "text": "数学的な意味ではなく、3つのラベルが貼られた例があったとして、私が見たことのあるものの分布の条件みたいなものを本当に言っているんだ。"
  },
  {
    "start": 2480214,
    "end": 2481360,
    "text": "何が正しいのか？"
  },
  {
    "start": 2481430,
    "end": 2488676,
    "text": "タダ飯はないとか、最小の記述長とか、シャノンの最適性とか、そういう一般的な議論にすぎない。"
  },
  {
    "start": 2488698,
    "end": 2511790,
    "text": "もしネットワークが事前に同じくらい速いと推測していたとしても、実際にはそうでなかったり、スパースでないと推測していたりする場合、常に最小二乗回帰よりも速く到達することはできない、という中間地点が必要なような気がするんだ。"
  },
  {
    "start": 2512720,
    "end": 2514824,
    "text": "このネットワークだと考えてほしい。"
  },
  {
    "start": 2514872,
    "end": 2518590,
    "text": "今までまばらなものしか見ていないこの分布で訓練されてきた。"
  },
  {
    "start": 2518960,
    "end": 2523136,
    "text": "ある意味では、あなたがまばらなものを与えていることを知っている。"
  },
  {
    "start": 2523238,
    "end": 2526608,
    "text": "このような分配の仕方は、問題点を克服するために学んだことだ。"
  },
  {
    "start": 2526694,
    "end": 2528956,
    "text": "あなたは多くの疎な機能を与えた。"
  },
  {
    "start": 2528988,
    "end": 2531376,
    "text": "機能には機能があることを学んだ。"
  },
  {
    "start": 2531398,
    "end": 2531536,
    "text": "そうだね。"
  },
  {
    "start": 2531558,
    "end": 2534980,
    "text": "濃密な機能を与えれば、それはゴミのようになる。"
  },
  {
    "start": 2535320,
    "end": 2536916,
    "text": "これまで密度の高い機能は見たことがない。"
  },
  {
    "start": 2537018,
    "end": 2538628,
    "text": "これは事前にオーバーファンクションを学習している。"
  },
  {
    "start": 2538714,
    "end": 2538916,
    "text": "オーケー。"
  },
  {
    "start": 2538938,
    "end": 2541852,
    "text": "多くの関数を見てきただけで、関数の事前学習をしてきたのだ。"
  },
  {
    "start": 2541936,
    "end": 2542570,
    "text": "そうだ。"
  },
  {
    "start": 2543660,
    "end": 2553336,
    "text": "この主張が正しいことを証明するためにトランスフォーマーをトレーニングする場合、実際に適切なサイズを与える必要があるのでしょうか？"
  },
  {
    "start": 2553438,
    "end": 2556270,
    "text": "つまり、可能な限りコンテクストのサイズを小さくすること？"
  },
  {
    "start": 2559120,
    "end": 2559484,
    "text": "いや。"
  },
  {
    "start": 2559522,
    "end": 2568424,
    "text": "という主張は、最大のコンテキストが何であれ、それよりも小さいサイズであれば、最初の3つのラベル付き例があれば、基本最適を行うというようなものだ。"
  },
  {
    "start": 2568472,
    "end": 2570770,
    "text": "それは、3つ与えられた中で最適なベース最適なことをしている。"
  },
  {
    "start": 2571300,
    "end": 2573680,
    "text": "この曲線はそれを示しているようなものだ。"
  },
  {
    "start": 2574020,
    "end": 2580208,
    "text": "そう言うことで、今同意する、つまらないことだ、みたいな、本当に迷惑なことを言おうと思っていたんだ。"
  },
  {
    "start": 2580294,
    "end": 2580544,
    "text": "そうだね。"
  },
  {
    "start": 2580582,
    "end": 2582130,
    "text": "ガウス過程ならね。"
  },
  {
    "start": 2582500,
    "end": 2587652,
    "text": "我々は、これらの無限に大きなネットワークは少なくともガウス過程に向かうというこの結果を知っている。"
  },
  {
    "start": 2587706,
    "end": 2589732,
    "text": "ガウス過程は単なる事前河川関数である。"
  },
  {
    "start": 2589786,
    "end": 2590244,
    "text": "その通りだ。"
  },
  {
    "start": 2590282,
    "end": 2591270,
    "text": "本当だ。"
  },
  {
    "start": 2592440,
    "end": 2594950,
    "text": "その前の川の機能を学ぶことができる。"
  },
  {
    "start": 2596060,
    "end": 2606412,
    "text": "明白でない部分は、どのようにして最小値を見つけることができるかということだと思う。"
  },
  {
    "start": 2606546,
    "end": 2606844,
    "text": "そうだね。"
  },
  {
    "start": 2606882,
    "end": 2611150,
    "text": "なぜ、厳密な意味での証明に迷わないのか。"
  },
  {
    "start": 2612640,
    "end": 2614604,
    "text": "確かに明白ではない。"
  },
  {
    "start": 2614802,
    "end": 2617330,
    "text": "という主張だ。"
  },
  {
    "start": 2618500,
    "end": 2622530,
    "text": "それを踏まえた上で、これらのアルゴリズムは本当に優れているのか？"
  },
  {
    "start": 2623060,
    "end": 2627730,
    "text": "任意のファンクションクラスまたは特定のファンクションクラスに対するクレーム。"
  },
  {
    "start": 2629060,
    "end": 2642230,
    "text": "そう、どのようなアーキテクチャのどのような関数クラスであっても、与えられた例が他のようなモデルを予測するトランスフォーマーであるべきだという主張には、これが当てはまる。"
  },
  {
    "start": 2646840,
    "end": 2647216,
    "text": "そうだね。"
  },
  {
    "start": 2647258,
    "end": 2653290,
    "text": "うまくいかなければ、クラウドを大きくし、チェーンを長くし、アマゾンにもっと金を払えばいい。"
  },
  {
    "start": 2654080,
    "end": 2655070,
    "text": "よし、いいぞ。"
  },
  {
    "start": 2657040,
    "end": 2663660,
    "text": "私たちは今、これらのことを変更したと考えて見ていた。"
  },
  {
    "start": 2663730,
    "end": 2666956,
    "text": "今はプロンプトを見ている。"
  },
  {
    "start": 2666988,
    "end": 2671436,
    "text": "我々は、基本的にメジャー・ゼロのサブセットにあるようなものを見ている。"
  },
  {
    "start": 2671468,
    "end": 2680964,
    "text": "私たちは、アルゴリズムが訓練分布の外で何を学習したかを本当に調べている。"
  },
  {
    "start": 2681082,
    "end": 2683764,
    "text": "今はこの話はしないでおこう。"
  },
  {
    "start": 2683802,
    "end": 2687380,
    "text": "これについては、2枚のスライドでお話しすることにしよう。"
  },
  {
    "start": 2690600,
    "end": 2699924,
    "text": "では、トランスフォーマーの代わりにLstmsを使うとしよう。"
  },
  {
    "start": 2700052,
    "end": 2702180,
    "text": "このオレンジの線がLSTMだ。"
  },
  {
    "start": 2702340,
    "end": 2705656,
    "text": "繰り返しになるが、これはあなたがトレーニングするものを配布するためのものだ。"
  },
  {
    "start": 2705758,
    "end": 2710712,
    "text": "と言うかもしれないが、ちょうど20の例がある場合はもう少し悪い。"
  },
  {
    "start": 2710856,
    "end": 2712344,
    "text": "我々は20次元でトレーニングしている。"
  },
  {
    "start": 2712392,
    "end": 2716536,
    "text": "申し訳ないが、一次関数は20次元なので、ここに鋭いキンクがあるはずだ。"
  },
  {
    "start": 2716578,
    "end": 2719648,
    "text": "Lstms、少し悪いけど、そんなに悪くないよ。"
  },
  {
    "start": 2719734,
    "end": 2720736,
    "text": "これに近いね。"
  },
  {
    "start": 2720758,
    "end": 2721840,
    "text": "ベイズが最適。"
  },
  {
    "start": 2725060,
    "end": 2725472,
    "text": "オーケー。"
  },
  {
    "start": 2725526,
    "end": 2739860,
    "text": "Lstmsが実際に何を学習したのか、そのアルゴリズムは何なのかを調べ始めると、それが実際には煙と鏡のようなものであることがわかり始める。"
  },
  {
    "start": 2740440,
    "end": 2742950,
    "text": "これを見るには、次のようにすればいい。"
  },
  {
    "start": 2744280,
    "end": 2747640,
    "text": "我々はノイズのない線形回帰でこれをトレーニングした。"
  },
  {
    "start": 2748140,
    "end": 2754036,
    "text": "少しうるさいラベルを貼ったらどうなるか？"
  },
  {
    "start": 2754228,
    "end": 2759304,
    "text": "つまり、このオレンジ色の曲線はリーの2乗回帰であることがわかる。"
  },
  {
    "start": 2759352,
    "end": 2761368,
    "text": "二重降下現象があることは知っている。"
  },
  {
    "start": 2761464,
    "end": 2761724,
    "text": "そうだろう？"
  },
  {
    "start": 2761762,
    "end": 2772000,
    "text": "線形分類器に従ってラベル付けされた正確なd個の点を与え、ラベルにほんの少しのノイズを加え、線形分類器でフィッティングする場合。"
  },
  {
    "start": 2772980,
    "end": 2777600,
    "text": "この新しい線形分類器は、真の線形分類器とはほとんど相関がない。"
  },
  {
    "start": 2779380,
    "end": 2781970,
    "text": "今私が言ったことを、どれだけの人が本当に解析しているのだろうか？"
  },
  {
    "start": 2783240,
    "end": 2785990,
    "text": "いや、これが二重降下の意味だ。"
  },
  {
    "start": 2787000,
    "end": 2788580,
    "text": "線形分類器を選ぶ。"
  },
  {
    "start": 2790040,
    "end": 2790404,
    "text": "いいね。"
  },
  {
    "start": 2790442,
    "end": 2790788,
    "text": "オーケー。"
  },
  {
    "start": 2790874,
    "end": 2793752,
    "text": "線形モデルを選ぶ。"
  },
  {
    "start": 2793886,
    "end": 2800388,
    "text": "d次元というのは、ランダムに単位ベクトルをd個選ぶということだ。"
  },
  {
    "start": 2800564,
    "end": 2803160,
    "text": "この線形分類器に従ってラベルを付ける。"
  },
  {
    "start": 2803740,
    "end": 2804212,
    "text": "オーケー。"
  },
  {
    "start": 2804286,
    "end": 2807308,
    "text": "これらのラベル付きポイントをフィットさせれば、線形モデルが戻ってくる。"
  },
  {
    "start": 2807394,
    "end": 2810072,
    "text": "D次元のリニア。"
  },
  {
    "start": 2810216,
    "end": 2813660,
    "text": "次に、dラベルに微量のノイズを加える。"
  },
  {
    "start": 2815360,
    "end": 2823132,
    "text": "ラベルはxを変えないが、yはxで評価されるこの一次関数である。"
  },
  {
    "start": 2823196,
    "end": 2830160,
    "text": "これらのラベルにほんの少しノイズを加え、そのノイズのあるラベルを線形関数でフィットさせる。"
  },
  {
    "start": 2830310,
    "end": 2835110,
    "text": "あるいは、これらのd個の新しいラベル付き点を通る一次関数を見つける。"
  },
  {
    "start": 2835960,
    "end": 2840390,
    "text": "この新しい線形関数は、真の線形モデルとどれくらい違うのか？"
  },
  {
    "start": 2841240,
    "end": 2846520,
    "text": "その主張とは、たとえ微量のノイズであっても、これらのものはほとんど無相関になるというものだ。"
  },
  {
    "start": 2849100,
    "end": 2851364,
    "text": "そうだ、これは二重降下だ。"
  },
  {
    "start": 2851412,
    "end": 2852056,
    "text": "私はこれを理解している。"
  },
  {
    "start": 2852078,
    "end": 2853130,
    "text": "見たことがある。"
  },
  {
    "start": 2855520,
    "end": 2856380,
    "text": "何人ですか？"
  },
  {
    "start": 2856450,
    "end": 2859310,
    "text": "これはいい。"
  },
  {
    "start": 2859840,
    "end": 2863048,
    "text": "つまり、基本的には、リグレッション（回帰）を行っていると仮定している。"
  },
  {
    "start": 2863224,
    "end": 2864812,
    "text": "どれだけコンディションが悪いんだ？"
  },
  {
    "start": 2864866,
    "end": 2867600,
    "text": "個のラベルがちょうどある場合。"
  },
  {
    "start": 2868020,
    "end": 2869232,
    "text": "こう考えてみよう。"
  },
  {
    "start": 2869286,
    "end": 2871250,
    "text": "マイナス1点だとする。"
  },
  {
    "start": 2872180,
    "end": 2879892,
    "text": "これでd番目のポイントはdマイナス1ポイントになる。"
  },
  {
    "start": 2879946,
    "end": 2882180,
    "text": "ある一つの方向は、バリエーションがない。"
  },
  {
    "start": 2882680,
    "end": 2888100,
    "text": "このDポイントは、この欠けている方向にほんの少ししか刺さっていない。"
  },
  {
    "start": 2889000,
    "end": 2891110,
    "text": "ここでDマイナス1点。"
  },
  {
    "start": 2893740,
    "end": 2896440,
    "text": "あなたは非常に、ひどく条件づけられている。"
  },
  {
    "start": 2897260,
    "end": 2911180,
    "text": "ランダムな単位ベクトルからd点を選ぶと、共分散の最小固有値は本当に小さくなる。"
  },
  {
    "start": 2911520,
    "end": 2921436,
    "text": "このような回帰曲線は、文字通り、ほんのわずかな、一定のノイズがある場合に得られるものだ。"
  },
  {
    "start": 2921548,
    "end": 2928960,
    "text": "基本的に、これらのノイズの多い点は完璧にフィットするが、線形回帰の真のモデルとの相関はない。"
  },
  {
    "start": 2929560,
    "end": 2936016,
    "text": "トランスフォーマーには、このような二重降下現象がある。"
  },
  {
    "start": 2936048,
    "end": 2937648,
    "text": "私たちはこのような体調不良に見舞われる。"
  },
  {
    "start": 2937824,
    "end": 2938596,
    "text": "どういう意味ですか？"
  },
  {
    "start": 2938618,
    "end": 2953348,
    "text": "つまり、トランスフォーマーは、回帰LSTMと同じように、数値的に非常に不安定なアルゴリズム、非常に繊細なアルゴリズムを学習したことになる。"
  },
  {
    "start": 2953524,
    "end": 2956120,
    "text": "これらは2つの異なるLSTMアーキテクチャである。"
  },
  {
    "start": 2956200,
    "end": 2957740,
    "text": "二重降下はない。"
  },
  {
    "start": 2959280,
    "end": 2960124,
    "text": "どういう意味ですか？"
  },
  {
    "start": 2960162,
    "end": 2965630,
    "text": "つまり、アルゴリズムは、次元が何なのか、dが何なのかを知らないのだ。"
  },
  {
    "start": 2966820,
    "end": 2971676,
    "text": "例の行列を反転させるような複雑なことはしていない。"
  },
  {
    "start": 2971708,
    "end": 2975664,
    "text": "リグレッションのように、数値に非常に敏感なことをやっているわけではない。"
  },
  {
    "start": 2975782,
    "end": 2979584,
    "text": "その代わりに、やらないアルゴリズムとは何か？"
  },
  {
    "start": 2979622,
    "end": 2980540,
    "text": "ダブルディセンド。"
  },
  {
    "start": 2980700,
    "end": 2985056,
    "text": "ポイントの半分に最良のモデルを当てはめ、ポイントの残りの半分に最良のモデルを当てはめ、平均する。"
  },
  {
    "start": 2985088,
    "end": 2986052,
    "text": "これらすべてだ。"
  },
  {
    "start": 2986186,
    "end": 2990224,
    "text": "Dの例もDプラス1の例も関係ない。"
  },
  {
    "start": 2990272,
    "end": 2991024,
    "text": "すべてがスムーズだ。"
  },
  {
    "start": 2991072,
    "end": 2992840,
    "text": "この数値的な不安定さは得られない。"
  },
  {
    "start": 2994140,
    "end": 2999140,
    "text": "より正式な意味では、これらのLSTMはある種の低次関数を学習していると言える。"
  },
  {
    "start": 2999300,
    "end": 3006110,
    "text": "回帰が持つような、条件不一致の例行列を反転させる不安定さはない。"
  },
  {
    "start": 3006800,
    "end": 3008472,
    "text": "これは素晴らしいことだ。"
  },
  {
    "start": 3008536,
    "end": 3010860,
    "text": "我々はlstmsの数値的安定性を持っている。"
  },
  {
    "start": 3013280,
    "end": 3021280,
    "text": "いや、正解が回帰である分布でトレーニングしているのだが、これは数値的に不安定なのだ。"
  },
  {
    "start": 3021780,
    "end": 3033460,
    "text": "トランスフォーマーがこの二重降下を持っているということは、Lstmsがそうでないのに対して、本当に正しいアルゴリズムのようなものをやっている証拠だ。"
  },
  {
    "start": 3035400,
    "end": 3049208,
    "text": "変なプロンプトを出したり、流通から外れたプロンプトを出したりすると、トランスフォーマーは正しいことをする。"
  },
  {
    "start": 3049374,
    "end": 3052100,
    "text": "Lstmsはそれほど強くない。"
  },
  {
    "start": 3052260,
    "end": 3054248,
    "text": "一例かもしれない。"
  },
  {
    "start": 3054334,
    "end": 3055912,
    "text": "仮にXをたくさん与えたとしよう。"
  },
  {
    "start": 3055976,
    "end": 3067096,
    "text": "私たちは20次元空間にいるが、私が与えるXはすべて、数値的に不安定な10次元の亜空間変換器にある。"
  },
  {
    "start": 3067208,
    "end": 3071020,
    "text": "彼らは、これらのことがすべて低次元の亜空間にあることを知っている。"
  },
  {
    "start": 3071600,
    "end": 3074708,
    "text": "Lstmsは、これらのプロンプトがすべてロードの中にあることに気づいていない。"
  },
  {
    "start": 3074744,
    "end": 3076780,
    "text": "これらのxはすべて低次元の部分空間にある。"
  },
  {
    "start": 3076860,
    "end": 3080240,
    "text": "それは悪いパフォーマンスによって現れている。"
  },
  {
    "start": 3082180,
    "end": 3088116,
    "text": "たくさんのベクトルを互いに直交させる必要がある場合を考えてみると、それは非常に高度なことだ。"
  },
  {
    "start": 3088218,
    "end": 3091584,
    "text": "もしあなたのアルゴリズムがそうなら、このような二重降下が起こることになる。"
  },
  {
    "start": 3091712,
    "end": 3094304,
    "text": "トランスフォーマーもある程度はそうなっているようだ。"
  },
  {
    "start": 3094432,
    "end": 3095880,
    "text": "LSPMはそうではない。"
  },
  {
    "start": 3096460,
    "end": 3098820,
    "text": "これらの変圧器には位置エンコーディングがありましたか？"
  },
  {
    "start": 3098900,
    "end": 3099912,
    "text": "彼らには何がある？"
  },
  {
    "start": 3100046,
    "end": 3100756,
    "text": "ポジション的には"
  },
  {
    "start": 3100868,
    "end": 3101240,
    "text": "そうだね。"
  },
  {
    "start": 3101310,
    "end": 3104280,
    "text": "これらは同じ訓練された位置エンコーディングのようなものだ。"
  },
  {
    "start": 3106380,
    "end": 3109340,
    "text": "次元の代用品のようなものだ。"
  },
  {
    "start": 3110480,
    "end": 3114030,
    "text": "これは、どのラベルがどの例に合うかを連想させるためだけのものだ。"
  },
  {
    "start": 3117360,
    "end": 3126772,
    "text": "言っておくが、十分な大きさのモデルであれば、基本的なアルゴリズムを学ぶことが求められるはずだ。"
  },
  {
    "start": 3126856,
    "end": 3130550,
    "text": "このLSTMは十分な大きさではないと言えるのでしょうか？"
  },
  {
    "start": 3131880,
    "end": 3132916,
    "text": "まあ、そんなところかな。"
  },
  {
    "start": 3132938,
    "end": 3136710,
    "text": "これはベイズ最適ではない、と言うかもしれない。"
  },
  {
    "start": 3137240,
    "end": 3156780,
    "text": "このアルゴリズムが、計算時間やトレーニング費用、パラメーターのチューニングにかかる時間といった点で、数値的な影響を受けにくい奇妙なものであることを考えると、配信でうまくいっていることは印象的だ。"
  },
  {
    "start": 3157360,
    "end": 3161950,
    "text": "LSTMを機能させることは、トランスフォーマーを機能させることよりも難しいことだと思う。"
  },
  {
    "start": 3162400,
    "end": 3171324,
    "text": "何かうまくいかないことがあっても、それが不可能だと断言することは難しい。"
  },
  {
    "start": 3171372,
    "end": 3171584,
    "text": "そうだろう？"
  },
  {
    "start": 3171622,
    "end": 3175920,
    "text": "私たちはLSTMSのために懸命に努力した。"
  },
  {
    "start": 3175990,
    "end": 3177708,
    "text": "より深く、より大きく。"
  },
  {
    "start": 3177884,
    "end": 3178610,
    "text": "健康だ。"
  },
  {
    "start": 3182120,
    "end": 3185296,
    "text": "なるほど、小さなLstmsではひどいものだ。"
  },
  {
    "start": 3185408,
    "end": 3188624,
    "text": "これらのLSTMはトランスより10倍大きい。"
  },
  {
    "start": 3188752,
    "end": 3191556,
    "text": "100倍の大きさになれば、もっと良くなる。"
  },
  {
    "start": 3191738,
    "end": 3195160,
    "text": "1000倍長く鍛えれば、もっと良くなる。"
  },
  {
    "start": 3195230,
    "end": 3197290,
    "text": "これらの問題のいくつかは、おそらくなくなるだろう。"
  },
  {
    "start": 3202620,
    "end": 3205720,
    "text": "ああ、ピンクでそう書いてある。"
  },
  {
    "start": 3205800,
    "end": 3206430,
    "text": "そうだね。"
  },
  {
    "start": 3207840,
    "end": 3217180,
    "text": "トランスフォーマーたちは、このような、リップシッツではない正しい機能を習得するのが得意なようだ。"
  },
  {
    "start": 3219280,
    "end": 3221920,
    "text": "彼らは正しいアルゴリズムを持っている。"
  },
  {
    "start": 3222420,
    "end": 3225504,
    "text": "大きな疑問は、彼らのどのような特性がこれを可能にするのか、ということだ。"
  },
  {
    "start": 3225622,
    "end": 3227280,
    "text": "もっと良いアーキテクチャはあるのか？"
  },
  {
    "start": 3230440,
    "end": 3237830,
    "text": "これを簡単に言うなら、最適なアルゴリズムは、その関数として非常に非リップスティッツであるということだと思う。"
  },
  {
    "start": 3239320,
    "end": 3250010,
    "text": "一方、これらのLSCMは、リップスティッツ関数に比例した、より多くのリップスティッツ関数を学習する。"
  },
  {
    "start": 3251340,
    "end": 3252168,
    "text": "これがそうだ。"
  },
  {
    "start": 3252254,
    "end": 3252696,
    "text": "これがそうだ。"
  },
  {
    "start": 3252718,
    "end": 3254704,
    "text": "私たちはそれをうるさいレッテルのように貼る。"
  },
  {
    "start": 3254852,
    "end": 3256316,
    "text": "ちょうどコンテクストの時期だ。"
  },
  {
    "start": 3256338,
    "end": 3258700,
    "text": "彼らは騒音のない環境で訓練を受けてきた。"
  },
  {
    "start": 3261840,
    "end": 3264990,
    "text": "いや、ノイズを与えても、すべてが完璧に見える。"
  },
  {
    "start": 3270420,
    "end": 3271490,
    "text": "あと3分だ。"
  },
  {
    "start": 3271940,
    "end": 3272304,
    "text": "いいね。"
  },
  {
    "start": 3272342,
    "end": 3274304,
    "text": "では、収穫は？"
  },
  {
    "start": 3274342,
    "end": 3274496,
    "text": "そうだね。"
  },
  {
    "start": 3274518,
    "end": 3281590,
    "text": "学習させたモデルが、どのような分布で学習させたモデルに対しても、ほぼベイズ最適な振る舞いをすることを期待したい。"
  },
  {
    "start": 3283160,
    "end": 3289808,
    "text": "訓練分布から大きく外挿が成功すると、興味深いことになる。"
  },
  {
    "start": 3289904,
    "end": 3292970,
    "text": "それがスーリヤが話し始める本当のことだ。"
  },
  {
    "start": 3294140,
    "end": 3299770,
    "text": "では、1つの大きな疑問は、どのようなアーキテクチャが特に成功しているのか、そしてそれはなぜなのか、ということだ。"
  },
  {
    "start": 3307820,
    "end": 3309240,
    "text": "いくつか質問がある。"
  },
  {
    "start": 3309310,
    "end": 3317790,
    "text": "そこで一つ疑問なのだが、仮にこれらのモデルが基本的に最適なことをすると仮定しよう。"
  },
  {
    "start": 3320580,
    "end": 3322370,
    "text": "そこから何が学べるだろうか？"
  },
  {
    "start": 3322980,
    "end": 3334310,
    "text": "では、聴衆の理論家はあまり気にしていないかもしれませんが、分布情報、理論的な質問で、答えが分からなくて、答えを知りたいようなものはありますか？"
  },
  {
    "start": 3335880,
    "end": 3336628,
    "text": "ひとつ質問がある。"
  },
  {
    "start": 3336714,
    "end": 3339460,
    "text": "では、植え込みの徒党について知っている人はどれくらいいるのだろう？"
  },
  {
    "start": 3341880,
    "end": 3349608,
    "text": "ランダムなグラフから徒党を効率的に見つけられるか？"
  },
  {
    "start": 3349774,
    "end": 3352600,
    "text": "効率的なアルゴリズムに必要な徒党の大きさは？"
  },
  {
    "start": 3353980,
    "end": 3357980,
    "text": "この問題は、グラフの分布に関する表現である。"
  },
  {
    "start": 3358720,
    "end": 3366636,
    "text": "もし、私たちがこれらを訓練して最適なことをさせることができると考えるなら、少なくとも私たちはその答えを知ることができるかもしれない。"
  },
  {
    "start": 3366738,
    "end": 3371104,
    "text": "では、実際にどの程度の大きさの徒党を組めば物事を解決できるのか？"
  },
  {
    "start": 3371302,
    "end": 3376976,
    "text": "他にも興味深い情報や理論的なことがたくさんあるが、みんなよく分かっていない。"
  },
  {
    "start": 3377158,
    "end": 3379360,
    "text": "トレース・リコンストラクションについてどれだけの人が知っているだろうか？"
  },
  {
    "start": 3382680,
    "end": 3383284,
    "text": "とにかく？"
  },
  {
    "start": 3383402,
    "end": 3384710,
    "text": "と聞くつもりだった。"
  },
  {
    "start": 3389160,
    "end": 3395956,
    "text": "セオリー・コミュニティが気にするような問題は山ほどあるから、正しい答えを知っておくだけでも役に立つだろう。"
  },
  {
    "start": 3396068,
    "end": 3400410,
    "text": "正解を知るにはいい方法かもしれない。"
  },
  {
    "start": 3403820,
    "end": 3404472,
    "text": "大きな疑問だ。"
  },
  {
    "start": 3404526,
    "end": 3409480,
    "text": "このような学習タスクのために、トランスフォーマーをトレーニングしているのだ。"
  },
  {
    "start": 3409560,
    "end": 3411896,
    "text": "言語モデルへの実際の影響は？"
  },
  {
    "start": 3411928,
    "end": 3414060,
    "text": "言語モデルは回帰するのか？"
  },
  {
    "start": 3415040,
    "end": 3416670,
    "text": "どこまで可能なのか？"
  },
  {
    "start": 3417600,
    "end": 3425120,
    "text": "それから、言語モデルも文脈学習的なものを超えて、他にどのようなアルゴリズムのプリミティブがあるのだろうか？"
  },
  {
    "start": 3425270,
    "end": 3430210,
    "text": "これらの大規模な言語モデルは、バイナリ検索が得意なようだ。"
  },
  {
    "start": 3430740,
    "end": 3447624,
    "text": "チャットGPTで20問遊べば、とてもいいんだ。例えば、1～500,032,000の間の数字を考えているんだけど、それが私の本当の数字より高いか低いかを当てるんだ、と言えば、まさにバイナリサーチをしてくれる。"
  },
  {
    "start": 3447822,
    "end": 3449716,
    "text": "ハードコーディングされているのだろうか？"
  },
  {
    "start": 3449828,
    "end": 3450490,
    "text": "たぶんね。"
  },
  {
    "start": 3451260,
    "end": 3458220,
    "text": "二分探索は、人々が会話の中で使う、かなり基本的なアルゴリズム原始的なもののようだ。"
  },
  {
    "start": 3458720,
    "end": 3481040,
    "text": "もし私が漠然としたことを言ったら、質問をして、バイナリサーチをして、私が言っているソートとは何か、大規模な言語モデルはソートできるのか、この言語モデルはどのような種類のアルゴリズム・プリミティブができるのか、といったような質問をするんだ。"
  },
  {
    "start": 3481190,
    "end": 3485616,
    "text": "もうひとつの疑問は、本当にできない人たちに能力を与えたらどうなるか、ということだ。"
  },
  {
    "start": 3485728,
    "end": 3490080,
    "text": "この能力を与えれば、彼らはどれだけ言語モデリングがうまくなるだろうか？"
  },
  {
    "start": 3490160,
    "end": 3491510,
    "text": "どうやるの？"
  },
  {
    "start": 3491960,
    "end": 3498024,
    "text": "もしかしたら、私はうまくやれるかもしれない。"
  },
  {
    "start": 3498062,
    "end": 3511596,
    "text": "では、これに関連する最後の質問をさせてください。"
  },
  {
    "start": 3511618,
    "end": 3518892,
    "text": "文脈の中で、学習能力とは、なぜ言語モデル、それも大規模な言語モデルが、領域外のプロンプトに対してかなり頑健なのかという、この種のメタ的な疑問でもある。"
  },
  {
    "start": 3519036,
    "end": 3528476,
    "text": "なぜ、ゼロ測度の部分集合を条件にしても、画像分類のような合理的な動作が得られるのでしょうか？"
  },
  {
    "start": 3528588,
    "end": 3533670,
    "text": "このようなことはすべて、領域外のものに対しては非常に脆いということはなんとなくわかっている。"
  },
  {
    "start": 3534600,
    "end": 3537860,
    "text": "この頑強さを生み出す言語固有の要素とは何だろう？"
  },
  {
    "start": 3540120,
    "end": 3549140,
    "text": "プロンプトエンジニアリングや巧妙な接頭辞を追加することで、この種の堅牢性を向上させることができるようだ。"
  },
  {
    "start": 3549960,
    "end": 3551636,
    "text": "両方かもしれない。"
  },
  {
    "start": 3551658,
    "end": 3558600,
    "text": "理論的にも、ある種のソフトセオリーの観点からも、プロンプト・エンジニアリングの力を公式化しようとする価値はあると思う。"
  },
  {
    "start": 3558680,
    "end": 3567330,
    "text": "プロンプト・エンジニアリングというと、言語モデルの少し壊れた部分に応急処置を施すことだと思われがちだが、私はもっと根本的なことだと思う。"
  },
  {
    "start": 3567700,
    "end": 3586784,
    "text": "でも、合理的なプロンプトを追加したり、プロンプトエンジニアリングをして、正しい反応をさせることはできる。"
  },
  {
    "start": 3586912,
    "end": 3598516,
    "text": "月曜日に人類学者が人里離れた村に行き、ある島があるとすると、その島のクマはみんなピンク色をしている、と言ったというバカな話をしたかもしれない。"
  },
  {
    "start": 3598628,
    "end": 3600292,
    "text": "アイヴァンはあの島に住んでいる。"
  },
  {
    "start": 3600436,
    "end": 3607820,
    "text": "アイヴァンがクマを見たら、何色だろう、理屈をこねれば、よし、ピンクに違いない。"
  },
  {
    "start": 3608320,
    "end": 3612952,
    "text": "ある人たちに尋ねると、彼らはこんな推理はしない。"
  },
  {
    "start": 3613096,
    "end": 3620352,
    "text": "70年代から80年代にかけて、人々はこの種の仮定の質問を1世紀にわたって行っていた。"
  },
  {
    "start": 3620486,
    "end": 3621520,
    "text": "名前は忘れた。"
  },
  {
    "start": 3621590,
    "end": 3627248,
    "text": "ハーバード大学の心理学者が、子供たちを対象にこの研究を行った。"
  },
  {
    "start": 3627414,
    "end": 3633924,
    "text": "結局、彼は基本的に、子どもたちは何歳からこういうことをするようになるのか、ということを知りたかったのだ。"
  },
  {
    "start": 3634042,
    "end": 3645736,
    "text": "彼は基本的に、子供たちの間では、彼らが正しいことを言おうが言うまいが、基本的に正しいことをさせることができるということを発見した。"
  },
  {
    "start": 3645838,
    "end": 3656308,
    "text": "よし、ふりをしよう、信じよう、ブラブラしよう、と前置きをすれば、彼らはクマがみんなピンク色の島という不条理な仮定の話をするだろう。"
  },
  {
    "start": 3656404,
    "end": 3660236,
    "text": "そうしないと、何がしたいんだ？"
  },
  {
    "start": 3660258,
    "end": 3661724,
    "text": "こんなばかげたことでコンディションを整えているのか。"
  },
  {
    "start": 3661762,
    "end": 3663100,
    "text": "私にどうしてほしいの？"
  },
  {
    "start": 3663250,
    "end": 3667260,
    "text": "もしそう言うのなら、よし、ふりをしよう。"
  },
  {
    "start": 3667330,
    "end": 3672716,
    "text": "このプロンプト・エンジニアリングという考え方は、単に悪い言語モデルを応急処置するよりもずっと根本的なものだと思う。"
  },
  {
    "start": 3672748,
    "end": 3680630,
    "text": "私は、言葉の仕組み、特にこのようなほとんどゼロに近いものを条件とする場合について、もっと根本的なものだと思う。"
  },
  {
    "start": 3681960,
    "end": 3683140,
    "text": "これで終わりにしよう。"
  },
  {
    "start": 3683290,
    "end": 3684150,
    "text": "ありがとう。"
  },
  {
    "start": 3689080,
    "end": 3691124,
    "text": "グレッグ、素晴らしい話をありがとう。"
  },
  {
    "start": 3691242,
    "end": 3695450,
    "text": "ドリュー、次の講演の準備をしている間に、いくつか質問を受け付けます。"
  },
  {
    "start": 3700220,
    "end": 3701016,
    "text": "そうだね。"
  },
  {
    "start": 3701198,
    "end": 3706330,
    "text": "同じように、LSTMが明らかに正しい学習に失敗している素晴らしい例がある。"
  },
  {
    "start": 3706940,
    "end": 3710984,
    "text": "同じように、LSTMが明らかに正しいアルゴリズムの学習に失敗している例がある。"
  },
  {
    "start": 3711032,
    "end": 3715870,
    "text": "このような例で、トランスフォーマーも悪いことをしているような、クリーンで孤立した例はありますか？"
  },
  {
    "start": 3717380,
    "end": 3717696,
    "text": "たぶんね。"
  },
  {
    "start": 3717718,
    "end": 3722396,
    "text": "lfcmsが納得しているかというと、そうでもないだろう。"
  },
  {
    "start": 3722428,
    "end": 3722720,
    "text": "そうだね。"
  },
  {
    "start": 3722790,
    "end": 3723410,
    "text": "オーケー。"
  },
  {
    "start": 3728860,
    "end": 3729560,
    "text": "そうでもないよ。"
  },
  {
    "start": 3729630,
    "end": 3730250,
    "text": "いや。"
  },
  {
    "start": 3731280,
    "end": 3737404,
    "text": "極端に複雑な意味は探っていない。"
  },
  {
    "start": 3737522,
    "end": 3744536,
    "text": "パリティのような非常に非連続的な関数については、ハーバード大学の研究がある。"
  },
  {
    "start": 3744648,
    "end": 3747264,
    "text": "その場合でも、非常に長い時間トレーニングすることになる。"
  },
  {
    "start": 3747302,
    "end": 3752290,
    "text": "トランスフォーマーに学習させることもできるが、完全に接続されたニューラルネットに学習させることもできる。"
  },
  {
    "start": 3754740,
    "end": 3756384,
    "text": "スーパーハードな機能とは？"
  },
  {
    "start": 3756502,
    "end": 3775080,
    "text": "これは、どのようなタスクに重みの更新が必要なのかという、より一般的な質問と関連しているように思います。"
  },
  {
    "start": 3775740,
    "end": 3779640,
    "text": "勾配降下は、あなたの小さな文脈の例に対するものです。"
  },
  {
    "start": 3780160,
    "end": 3782632,
    "text": "また、微調整のようなものだと考えることもできる。"
  },
  {
    "start": 3782776,
    "end": 3784348,
    "text": "微調整は常に良いことだ。"
  },
  {
    "start": 3784434,
    "end": 3788270,
    "text": "ウェイトを更新できれば、それに越したことはない。"
  },
  {
    "start": 3789040,
    "end": 3791244,
    "text": "でも、そうだね。"
  },
  {
    "start": 3791282,
    "end": 3796184,
    "text": "ウェイトを更新しなければ何も望めないような問題があるのか？"
  },
  {
    "start": 3796312,
    "end": 3799580,
    "text": "それは本当にいい質問だと思うし、本来は計算のようなものだ。"
  },
  {
    "start": 3802560,
    "end": 3803310,
    "text": "ありがとう。"
  },
  {
    "start": 3804840,
    "end": 3813410,
    "text": "いい響きだ。"
  }
]