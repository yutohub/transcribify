[
  {
    "start": 250,
    "end": 878,
    "text": "やあ、みんな。"
  },
  {
    "start": 964,
    "end": 2478,
    "text": "僕のチャンネルにようこそ。"
  },
  {
    "start": 2644,
    "end": 7322,
    "text": "今日は、『ローラ』という非常に影響力のある論文を紹介する。"
  },
  {
    "start": 7466,
    "end": 11950,
    "text": "Loraは、大規模言語モデルの低ランク適応を意味する。"
  },
  {
    "start": 12530,
    "end": 14382,
    "text": "非常に影響力のある論文だ。"
  },
  {
    "start": 14436,
    "end": 17658,
    "text": "たしか2年前にマイクロソフトから発売された。"
  },
  {
    "start": 17834,
    "end": 21902,
    "text": "このビデオでは、ローラとは何なのか、どのように機能するのかを見ていこう。"
  },
  {
    "start": 21956,
    "end": 28920,
    "text": "もちろん、torch以外の外部ライブラリは一切使わずに、ゼロからPytorchに実装する。"
  },
  {
    "start": 29290,
    "end": 31318,
    "text": "行こう"
  },
  {
    "start": 31484,
    "end": 38618,
    "text": "我々は言語モデルの領域にいるが、実際にはLoraはどんなモデルにも適用できる。"
  },
  {
    "start": 38704,
    "end": 45130,
    "text": "実際、後でお見せする私のデモでは、これを非常に単純な分類に適用する。"
  },
  {
    "start": 48110,
    "end": 53322,
    "text": "ローラを研究する前に、そもそもなぜローラが必要なのかを理解する必要がある。"
  },
  {
    "start": 53376,
    "end": 55946,
    "text": "ニューラルネットワークの基本をおさらいしよう。"
  },
  {
    "start": 56058,
    "end": 60734,
    "text": "ある入力があるとする。それは1つの数値かもしれないし、数値のベクトルかもしれない。"
  },
  {
    "start": 60852,
    "end": 65374,
    "text": "とすると、ニューラルネットワークには隠れ層があり、それは通常行列で表される。"
  },
  {
    "start": 65422,
    "end": 70302,
    "text": "そしてもうひとつ隠れ層がある。"
  },
  {
    "start": 70446,
    "end": 72898,
    "text": "ようやく出力が出たね。"
  },
  {
    "start": 73064,
    "end": 76430,
    "text": "通常、ネットワークを訓練するときには、目標も設定する。"
  },
  {
    "start": 76590,
    "end": 81430,
    "text": "私たちがすることは、出力と目標を比較して損失を出すことだ。"
  },
  {
    "start": 81850,
    "end": 87538,
    "text": "最後に、すべての層の各重みに損失をバックプロパゲートする。"
  },
  {
    "start": 87634,
    "end": 91610,
    "text": "この場合、例えば、多くのウェイトを持つ可能性がある。"
  },
  {
    "start": 92030,
    "end": 101680,
    "text": "この層では、重み行列とバイアス行列を持ち、それぞれの重みは損失関数によって修正される。"
  },
  {
    "start": 102050,
    "end": 106960,
    "text": "また、ここではウェイトとバイアスのマトリックスも用意する。"
  },
  {
    "start": 107410,
    "end": 110234,
    "text": "さて、微調整とは何か？"
  },
  {
    "start": 110282,
    "end": 118610,
    "text": "ファインチューニングとは基本的に、事前に学習させたモデルがあり、それを元のモデルが見ていないような他のデータで微調整することを意味する。"
  },
  {
    "start": 118680,
    "end": 123806,
    "text": "例えば、私たちが独自のデータベースを構築している会社で働いているとしよう。"
  },
  {
    "start": 123998,
    "end": 127510,
    "text": "この新しいデータベースは、独自のSQL言語を持っていますね？"
  },
  {
    "start": 127660,
    "end": 134850,
    "text": "多くのプログラミング言語で訓練された事前訓練モデル（GPTとする）をダウンロードした。"
  },
  {
    "start": 135010,
    "end": 147670,
    "text": "このモデルは、ユーザーがデータベースのクエリを作成するのに役立つように、独自のSQL言語で微調整を行いたいと考えています。"
  },
  {
    "start": 147830,
    "end": 157920,
    "text": "以前は、このモデル全体を新しいデータで訓練し、新しいデータを使ってすべての重みを変更していた。"
  },
  {
    "start": 158370,
    "end": 160750,
    "text": "しかし、これには問題がある。"
  },
  {
    "start": 160900,
    "end": 171358,
    "text": "完全なファインチューニングの問題点は、ネットワークを完全に訓練しなければならないことである。"
  },
  {
    "start": 171454,
    "end": 186306,
    "text": "そして、すべての重みに対して逆伝播を実行する必要があり、さらにチェックポイントのためのストレージ要件は高価である。"
  },
  {
    "start": 186418,
    "end": 191574,
    "text": "さらに、オプティマイザーの状態も保存しておけば、例えばアダム・オプティマイザーを使っているとしよう。"
  },
  {
    "start": 191622,
    "end": 198358,
    "text": "各重みのためのアダム・オプティマイザーは、モデルをより良く最適化するために、いくつかの統計も保持している。"
  },
  {
    "start": 198534,
    "end": 200860,
    "text": "我々は多くのデータを保存している。"
  },
  {
    "start": 201170,
    "end": 223474,
    "text": "もし同じベースモデルを使いたいが、2つの異なるデータセットでファインチューニングを行い、基本的に2つの異なるファインチューニング・モデルを持ち、それらを切り替える必要があるとすると、前のモデルをアンロードし、もう一方のファインチューニング・モデルの重みをすべてロードし直す必要があるため、非常にコストがかかる。"
  },
  {
    "start": 223592,
    "end": 227480,
    "text": "モデルのすべての重みメトリクスを置き換える必要がある。"
  },
  {
    "start": 227930,
    "end": 231670,
    "text": "しかし、ローラにはこのような問題に対するより良い解決策がある。"
  },
  {
    "start": 232170,
    "end": 233990,
    "text": "ローラにはこの違いがある。"
  },
  {
    "start": 234060,
    "end": 239866,
    "text": "入力から始めて、事前に訓練したモデルがある。"
  },
  {
    "start": 239968,
    "end": 245338,
    "text": "事前訓練されたモデルとその重みがあり、それを凍結する。"
  },
  {
    "start": 245424,
    "end": 254990,
    "text": "基本的には、Pytorchにこれらの重みを決して触らず、読み取り専用として使用し、これらの重みに対して逆伝播を実行しないように指示する。"
  },
  {
    "start": 255490,
    "end": 262330,
    "text": "次に、もう2つの行列を作り、それぞれ1つずつ、訓練したい行列に対応させる。"
  },
  {
    "start": 262500,
    "end": 275602,
    "text": "基本的にLoraでは、元のモデルの各レイヤーごとに行列bとaを作成する必要はない。"
  },
  {
    "start": 275736,
    "end": 279320,
    "text": "いくつかのレイヤーに適用すればいいだけだ。"
  },
  {
    "start": 279690,
    "end": 284950,
    "text": "この場合、レイヤーは1つだけで、行列bとaを導入するとする。"
  },
  {
    "start": 285100,
    "end": 290266,
    "text": "この行列bとa、そして元の行列wの違いは何ですか？"
  },
  {
    "start": 290368,
    "end": 295130,
    "text": "まず、元の行列がd×kであった次元を見てみよう。"
  },
  {
    "start": 295280,
    "end": 302400,
    "text": "仮にdが1000でkが5000だとする。"
  },
  {
    "start": 303090,
    "end": 311742,
    "text": "2つの新しい行列を作り、掛け合わせると同じ次元になるようにしたい。"
  },
  {
    "start": 311796,
    "end": 321140,
    "text": "d×rにr×kを掛けると、d×kの新しい行列ができる。"
  },
  {
    "start": 321670,
    "end": 331654,
    "text": "内次元は相殺され、rはdやkよりずっと小さくしたいので、rを1に等しくしてもいいだろう。"
  },
  {
    "start": 331852,
    "end": 337718,
    "text": "rを1にすれば、基本的にd×1の行列ができる。"
  },
  {
    "start": 337804,
    "end": 342666,
    "text": "1000×1の行列と、5000×1の行列がある。"
  },
  {
    "start": 342848,
    "end": 363982,
    "text": "この行列のパラメーターの数を比較すると、元の行列wではパラメーターの数pはdにkを掛けた数で、この行列では500万個に相当する。"
  },
  {
    "start": 364046,
    "end": 370050,
    "text": "rが1なら、r×dの行列が1つできる。"
  },
  {
    "start": 370120,
    "end": 382098,
    "text": "1000に5000を足すと、マトリックスには6000の数字しかない。"
  },
  {
    "start": 382274,
    "end": 388150,
    "text": "を掛け合わせても、d×kの行列ができるという利点がある。"
  },
  {
    "start": 388300,
    "end": 397114,
    "text": "もちろん、この行列は元の行列wよりはるかに小さいので、同じ情報を捕らえることはできないと思うかもしれないね。"
  },
  {
    "start": 397232,
    "end": 404798,
    "text": "同じ次元のものを作るとしても、実際には何かをより小さく表現している。"
  },
  {
    "start": 404884,
    "end": 407294,
    "text": "情報を失うかもしれない。"
  },
  {
    "start": 407492,
    "end": 409946,
    "text": "これがローラの考えだ。"
  },
  {
    "start": 410058,
    "end": 420206,
    "text": "実際、ローラの背後にある考え方は、行列Wが多くの重み、つまり我々の目的には実際には意味のない数字を含んでいるということだ。"
  },
  {
    "start": 420318,
    "end": 424258,
    "text": "実際、モデルには何の情報も加えていない。"
  },
  {
    "start": 424344,
    "end": 429042,
    "text": "他のウェイトの組み合わせに過ぎないから、冗長なんだ。"
  },
  {
    "start": 429186,
    "end": 431750,
    "text": "行列W全体は必要ない。"
  },
  {
    "start": 431820,
    "end": 435958,
    "text": "このWの下位の表現を作り、それを微調整すればいい。"
  },
  {
    "start": 436044,
    "end": 439020,
    "text": "このモデルの旅を続けよう。"
  },
  {
    "start": 439630,
    "end": 442470,
    "text": "インクを消させてください。"
  },
  {
    "start": 442550,
    "end": 445820,
    "text": "それでは、2つの行列bとaを作成しよう。"
  },
  {
    "start": 446510,
    "end": 452582,
    "text": "私たちがすることは、同じ次元のもの同士を組み合わせることです。"
  },
  {
    "start": 452646,
    "end": 456862,
    "text": "bにaを掛けると、d×kの次元になる。"
  },
  {
    "start": 456916,
    "end": 458862,
    "text": "元のWと合計することができる。"
  },
  {
    "start": 458996,
    "end": 462922,
    "text": "私たちはアウトプットを出し、そしていつもの目標がある。"
  },
  {
    "start": 462986,
    "end": 469250,
    "text": "損失を計算し、その損失だけを訓練したい行列に逆伝播する。"
  },
  {
    "start": 469320,
    "end": 471570,
    "text": "それがbとaのマトリックスだ。"
  },
  {
    "start": 472230,
    "end": 474286,
    "text": "Wマトリックスには触れない。"
  },
  {
    "start": 474318,
    "end": 481106,
    "text": "訓練前のモデルである元のモデルは凍結され、その重みに触れることはない。"
  },
  {
    "start": 481138,
    "end": 483830,
    "text": "修正するのはbとaのマトリックスだけだ。"
  },
  {
    "start": 484410,
    "end": 485638,
    "text": "どのようなメリットがありますか？"
  },
  {
    "start": 485724,
    "end": 496166,
    "text": "まず第一に、先に見たように、Wマトリックスには例えば500万個のパラメーターがあるため、トレーニングして保存するパラメーターは少なくなる。"
  },
  {
    "start": 496198,
    "end": 501642,
    "text": "オリジナルのものでは、rを5とした場合、パラメータは全部で30,000しかない。"
  },
  {
    "start": 501696,
    "end": 511738,
    "text": "また、パラメータが元の1％以下であれば、ほとんどのパラメータについて勾配を評価する必要がないため、必要な記憶容量が少なくて済み、逆伝播も速くなる。"
  },
  {
    "start": 511914,
    "end": 521474,
    "text": "例えば、SQL用とJavaScriptコード生成用の2つの異なるモデルがあるとします。"
  },
  {
    "start": 521592,
    "end": 526822,
    "text": "この2つのマトリックスを切り替える場合は、リロードするだけでいい。"
  },
  {
    "start": 526876,
    "end": 530162,
    "text": "Wマトリックスは触っていないので、リロードする必要はない。"
  },
  {
    "start": 530226,
    "end": 537160,
    "text": "を使用しても、元の事前学習済みモデルと変わらない。"
  },
  {
    "start": 539630,
    "end": 549530,
    "text": "なぜこのようなことができるかというと、論文にも書かれていることだが、事前に訓練されたモデルには、彼らが見たことがあるということだ。"
  },
  {
    "start": 549600,
    "end": 555418,
    "text": "直感的には、実際の次元よりも小さい興味深い次元を持っている。"
  },
  {
    "start": 555594,
    "end": 563738,
    "text": "これにヒントを得て、彼らは適応の間、重みの更新も本質的に低いランクを持つという仮説を立てた。"
  },
  {
    "start": 563914,
    "end": 569586,
    "text": "行列のランクが意味するものは、基本的には、後で実際に例を挙げて説明する。"
  },
  {
    "start": 569768,
    "end": 583046,
    "text": "基本的には、多数のベクトル、列ベクトルからなる行列があると想像してください。行列のランクは、互いに線形独立なベクトルの数です。"
  },
  {
    "start": 583148,
    "end": 587320,
    "text": "そのどれかを直線的に組み合わせて別のものを作ることはできない。"
  },
  {
    "start": 588270,
    "end": 595980,
    "text": "これはまた、他の列を線形結合することで得られるので、どれだけの列が冗長であるかを示している。"
  },
  {
    "start": 597150,
    "end": 605186,
    "text": "この論文で彼らが意味しているのは、W行列が実際にはランク不足であるということである。"
  },
  {
    "start": 605238,
    "end": 607146,
    "text": "つまり、フルランクではないということだ。"
  },
  {
    "start": 607178,
    "end": 611982,
    "text": "の寸法は1000×1000かもしれないが、実際のランクは10かもしれない。"
  },
  {
    "start": 612116,
    "end": 616420,
    "text": "実際、10×10の指標を使えば、ほとんどの情報を把握することができる。"
  },
  {
    "start": 616790,
    "end": 624050,
    "text": "このランク削減の考え方は、例えば圧縮アルゴリズムなど、多くの場面で使われている。"
  },
  {
    "start": 624870,
    "end": 635110,
    "text": "ここでは、ランキングとメトリック分解の数学について復習し、PytorchでのLoraの実装を確認する。"
  },
  {
    "start": 636170,
    "end": 637846,
    "text": "ここで切り替えよう。"
  },
  {
    "start": 638028,
    "end": 639740,
    "text": "まずはここに行こう。"
  },
  {
    "start": 640190,
    "end": 651374,
    "text": "メトリックス分解の非常に簡単な例と、行列がどのようにランク不足になりうるか、そしてどのようにすれば情報のほとんどをとらえたより小さな行列を作ることができるかをお見せします。"
  },
  {
    "start": 651492,
    "end": 657150,
    "text": "まずは非常にシンプルなライブラリGeorgeとnumpyをインポートしてみよう。"
  },
  {
    "start": 657570,
    "end": 663946,
    "text": "では、ここで人為的にランク不足の10×10マトリックスを作成しよう。"
  },
  {
    "start": 663978,
    "end": 668514,
    "text": "実際のランクは2で、ランク不足になるように作っている。"
  },
  {
    "start": 668552,
    "end": 672900,
    "text": "この行列が10×10であっても、100個の数字があることがわかる。"
  },
  {
    "start": 675190,
    "end": 680466,
    "text": "この行列のランクは実際には2であり、numpyを使って評価することができる。"
  },
  {
    "start": 680578,
    "end": 684040,
    "text": "この行列のランクが実際には2であることがわかるだろう。"
  },
  {
    "start": 685210,
    "end": 700442,
    "text": "つまり、特異値分解を意味するSVDと呼ばれるアルゴリズムを使って分解すれば、usとvという3つの行列が生成され、それを掛け合わせるとwになる。"
  },
  {
    "start": 700576,
    "end": 705754,
    "text": "の次元は、ランクによってもっと小さくなる。"
  },
  {
    "start": 705802,
    "end": 719790,
    "text": "ここでrは元の行列のランクを示し、これらの行列の最初のR列だけを取れば、元の行列の情報のほとんどを捕らえることができる。"
  },
  {
    "start": 719950,
    "end": 722658,
    "text": "それを単純な方法で視覚化することができる。"
  },
  {
    "start": 722824,
    "end": 730390,
    "text": "ローラの場合と同じように、この分解を使ってb行列とa行列を計算するのだ。"
  },
  {
    "start": 730730,
    "end": 737462,
    "text": "を見ると、もともと10×10であったW行列の下側の表現を作成したことがわかる。"
  },
  {
    "start": 737516,
    "end": 742780,
    "text": "ここで、10×2と2×10の2つの行列、1つはb、もう1つはaを作成した。"
  },
  {
    "start": 743470,
    "end": 750230,
    "text": "私たちがすることは、いくつかの入力、仮にxと呼ぶことにする、そしていくつかのバイアスを取ることである。"
  },
  {
    "start": 750390,
    "end": 756058,
    "text": "Wの元の行列（10×10の行列）を使って出力を計算する。"
  },
  {
    "start": 756154,
    "end": 766130,
    "text": "にxを掛け、バイアスを加え、さらに分解の結果であるb行列とa行列を使って出力を計算する。"
  },
  {
    "start": 766710,
    "end": 774174,
    "text": "ローラにxを掛けたものにバイアスを加えたのと同じように、Bにaを掛けたものを使ってYプライムを計算した。"
  },
  {
    "start": 774302,
    "end": 782162,
    "text": "Bとaが実際にはもっと少ない要素しか持っていなくても、出力は同じであることがわかる。"
  },
  {
    "start": 782226,
    "end": 786226,
    "text": "この場合、名前を変えるのを忘れていた。"
  },
  {
    "start": 786258,
    "end": 804510,
    "text": "これはbとa、オーケー、bとa、オーケー、で、私が示したいのは、これは承認されていないのですが、実はこのW行列を人為的に作り、ランク不足を人為的に作ったからです。"
  },
  {
    "start": 804930,
    "end": 808480,
    "text": "実はこのコードはどこかから持ってきたんだ。"
  },
  {
    "start": 811570,
    "end": 822574,
    "text": "このアイデアは、同じ入力に対して同じ出力を生成できる、より小さなマトリックスを持つことができるということである。"
  },
  {
    "start": 822702,
    "end": 826030,
    "text": "ご覧のように、bとaの要素が組み合わされている。"
  },
  {
    "start": 826110,
    "end": 843500,
    "text": "b行列とa行列を合わせた要素数は40であるのに対し、元の行列では100要素あった。それでも、同じ与えられた入力に対して同じ出力が得られるのだから、bとaがWの最も重要な情報のほとんどを捉えていたことになる。"
  },
  {
    "start": 844030,
    "end": 845770,
    "text": "では、ローラのところに行こう。"
  },
  {
    "start": 846430,
    "end": 848762,
    "text": "では、ロラをステップ・バイ・ステップで実装していこう。"
  },
  {
    "start": 848896,
    "end": 851786,
    "text": "これから行うのは、分類作業だ。"
  },
  {
    "start": 851818,
    "end": 865374,
    "text": "というのも、ある特定の桁に対する性能があまり良くないことがわかったからだ。"
  },
  {
    "start": 865412,
    "end": 867746,
    "text": "その1つだけを微調整したい。"
  },
  {
    "start": 867928,
    "end": 881430,
    "text": "我々はLoraを使い、Loraで微調整を行う場合、実際には非常に少数のパラメーターを修正していることを示す。"
  },
  {
    "start": 881580,
    "end": 882454,
    "text": "始めよう。"
  },
  {
    "start": 882572,
    "end": 884642,
    "text": "通常のライブラリをインポートする。"
  },
  {
    "start": 884706,
    "end": 888078,
    "text": "torchとmultipotlib。"
  },
  {
    "start": 888194,
    "end": 889434,
    "text": "実際には必要ないだろう。"
  },
  {
    "start": 889472,
    "end": 892220,
    "text": "プログレスバーを可視化するTQDM。"
  },
  {
    "start": 892670,
    "end": 898490,
    "text": "常に同じ結果を返すように、決定論的にするんだ。"
  },
  {
    "start": 898910,
    "end": 901502,
    "text": "データセットをロードする。"
  },
  {
    "start": 901556,
    "end": 905678,
    "text": "すでにトーチビジョンに統合されているので、大した問題ではない。"
  },
  {
    "start": 905844,
    "end": 913530,
    "text": "ローダーを作成する際、数字を分類するための最適化されていないニューラルネットワークを作成する。"
  },
  {
    "start": 913610,
    "end": 916974,
    "text": "基本的に、これはタスクのための非常に大きなネットワークである。"
  },
  {
    "start": 917022,
    "end": 919938,
    "text": "そんなに大きなネットワークは必要ないが、具体的なものにしたい。"
  },
  {
    "start": 920024,
    "end": 926900,
    "text": "パラメータを節約していることを示したいので、わざとこんなに大きくしたんだ。"
  },
  {
    "start": 927290,
    "end": 930582,
    "text": "私はリッチボーイ・ネットと呼んでいる。"
  },
  {
    "start": 930636,
    "end": 932806,
    "text": "効率なんてどうでもいいでしょ？"
  },
  {
    "start": 932988,
    "end": 938686,
    "text": "これは3つの線形層からなる非常に単純なネットワークで、ルーの活性化を持つ。"
  },
  {
    "start": 938738,
    "end": 947546,
    "text": "最後のレイヤーは、基本的に数字を0、1、2、9までのカテゴリーに分類するだけである。"
  },
  {
    "start": 947728,
    "end": 951850,
    "text": "このネットワークを作成し、amnestで訓練する。"
  },
  {
    "start": 951930,
    "end": 959790,
    "text": "を1エポックだけ実行し、分類のためのMNIstの単純な訓練だけを行う。"
  },
  {
    "start": 960930,
    "end": 977490,
    "text": "なぜなら、ローラが元の重みを変更していないことを証明するために、このコピーが必要になるからだ。"
  },
  {
    "start": 979430,
    "end": 985890,
    "text": "学習済みのモデルをテストして、精度をチェックすることもできる。"
  },
  {
    "start": 985970,
    "end": 994694,
    "text": "テストしてみると、精度は非常に高いが、9という数字については、他の数字に比べて精度が低いことがわかる。"
  },
  {
    "start": 994742,
    "end": 998300,
    "text": "特に9という数字を微調整したい。"
  },
  {
    "start": 998670,
    "end": 999034,
    "text": "オーケー。"
  },
  {
    "start": 999072,
    "end": 1005386,
    "text": "ローラ、実は論文の中で、大規模な言語モデルを使ってファインチューニングをしているんだ。"
  },
  {
    "start": 1005418,
    "end": 1008400,
    "text": "だから私はアムネストとこのとてもシンプルな例を使っているんだ。"
  },
  {
    "start": 1009570,
    "end": 1013042,
    "text": "とにかく、もっと微調整したい桁が1つある。"
  },
  {
    "start": 1013096,
    "end": 1013700,
    "text": "そうだろう？"
  },
  {
    "start": 1014230,
    "end": 1022962,
    "text": "微調整をする前に、ここで作成したネットワークにどれだけのパラメーターがあるか、視覚化してみよう。"
  },
  {
    "start": 1023016,
    "end": 1023620,
    "text": "ネット。"
  },
  {
    "start": 1024310,
    "end": 1035634,
    "text": "レイヤー1の重みとバイアス、レイヤー2の重みとバイアス、レイヤー3の重みとバイアス。"
  },
  {
    "start": 1035682,
    "end": 1040330,
    "text": "合計で2,807,010のパラメータがある。"
  },
  {
    "start": 1041310,
    "end": 1043398,
    "text": "では、ローラを紹介しよう。"
  },
  {
    "start": 1043574,
    "end": 1050800,
    "text": "そこでローラはa、bという2つの行列を導入する。"
  },
  {
    "start": 1051250,
    "end": 1067634,
    "text": "これらの行列のサイズは、元のウェイトがd×kであれば、bはd×r、aはr×kである。"
  },
  {
    "start": 1067752,
    "end": 1071220,
    "text": "私はただ、フィーチャー・インとフィーチャー・アウトと呼んでいるだけだ。"
  },
  {
    "start": 1071750,
    "end": 1078210,
    "text": "論文では、b行列をゼロで初期化し、a行列をランダムなガウスで初期化すると書かれている。"
  },
  {
    "start": 1078290,
    "end": 1080920,
    "text": "ここでもそうしている。"
  },
  {
    "start": 1081530,
    "end": 1083842,
    "text": "そして、スケールパラメーターも導入している。"
  },
  {
    "start": 1083906,
    "end": 1093046,
    "text": "これは論文の4.1節にあるもので、基本的に項目の尺度を変えずに順位を変えることができる。"
  },
  {
    "start": 1093158,
    "end": 1095014,
    "text": "私はアルファを使うだけだ。"
  },
  {
    "start": 1095062,
    "end": 1101738,
    "text": "アルファが固定されているのは、同じモデルを違うランクで試したいからかもしれない。"
  },
  {
    "start": 1101834,
    "end": 1106560,
    "text": "この場合、縮尺の代わりに、数字の縮尺を同じにすることができる。"
  },
  {
    "start": 1108050,
    "end": 1111338,
    "text": "もしLoraが有効なら、ウェイトメトリクスが欲しい。"
  },
  {
    "start": 1111354,
    "end": 1122130,
    "text": "というのも、論文でもバイアスのマトリックスに対してではなく、重みに対してのみ実行されているからだ。"
  },
  {
    "start": 1123430,
    "end": 1128742,
    "text": "Lauraが有効な場合、重み行列はxとなる。"
  },
  {
    "start": 1128796,
    "end": 1134882,
    "text": "元の重さにbを加えたものにaを掛けたものである。"
  },
  {
    "start": 1134946,
    "end": 1137218,
    "text": "これも論文で紹介されている。"
  },
  {
    "start": 1137404,
    "end": 1142940,
    "text": "を掛けるのではなく、基本的にはwとする。"
  },
  {
    "start": 1143550,
    "end": 1152686,
    "text": "元のネットワークと同じようにxにwを掛ける代わりに、wにbとaを掛けたものを掛ける。"
  },
  {
    "start": 1152788,
    "end": 1154414,
    "text": "これは論文に書かれている。"
  },
  {
    "start": 1154532,
    "end": 1156480,
    "text": "ここで見ることができる。"
  },
  {
    "start": 1157410,
    "end": 1158718,
    "text": "降りよう。"
  },
  {
    "start": 1158884,
    "end": 1160542,
    "text": "ここに書いてある。"
  },
  {
    "start": 1160676,
    "end": 1181442,
    "text": "xにwだけを掛けるのではなく、このデルタwを掛けるのだ。これは、微調整のために重みがどれだけ動いたかを示すもので、bにaを掛けたものだ。"
  },
  {
    "start": 1181506,
    "end": 1187570,
    "text": "このパラメタリゼーションを追加するために、Pytorchの特別な関数、pytorch parameterizationを使っています。"
  },
  {
    "start": 1187650,
    "end": 1193660,
    "text": "どのように機能するのか、もっと詳しく知りたい方は、こちらがリンク先だが、簡単に紹介しよう。"
  },
  {
    "start": 1194110,
    "end": 1204606,
    "text": "パラメータ化とは基本的に、線形1層の重み行列を、この場合はこの関数で置き換えることができるようにすることである。"
  },
  {
    "start": 1204788,
    "end": 1214594,
    "text": "ニューラルネットワークがウェイト行列にアクセスしたいときは、直接ウェイト行列にアクセスするのではなく、この関数にアクセスする。"
  },
  {
    "start": 1214792,
    "end": 1219410,
    "text": "この関数が基本的にローラのパラメータ化である。"
  },
  {
    "start": 1219830,
    "end": 1231190,
    "text": "重みの行列を求めると、この関数が呼び出され、元の重みが返される。"
  },
  {
    "start": 1231690,
    "end": 1236614,
    "text": "スパイトーチはその仕事を続ける。"
  },
  {
    "start": 1236652,
    "end": 1239702,
    "text": "をx倍するだけである。"
  },
  {
    "start": 1239756,
    "end": 1247180,
    "text": "実際には、ウェイトは元のウェイトに、論文に従ってこの方法で組み合わせたbとaを加えたものになる。"
  },
  {
    "start": 1247870,
    "end": 1253486,
    "text": "enabledプロパティを変更することで、各レイヤーのローラを簡単に有効/無効にすることができる。"
  },
  {
    "start": 1253588,
    "end": 1255360,
    "text": "ここで見ることができる。"
  },
  {
    "start": 1255810,
    "end": 1259674,
    "text": "有効になっていれば、b行列とa行列を使用する。"
  },
  {
    "start": 1259722,
    "end": 1262906,
    "text": "無効になっている場合は、オリジナルのウェイトのみを使用する。"
  },
  {
    "start": 1263098,
    "end": 1263930,
    "text": "それを可能にすれば"
  },
  {
    "start": 1264010,
    "end": 1268062,
    "text": "基本的には、ウェイトの微調整も可能にするということだ。"
  },
  {
    "start": 1268126,
    "end": 1269074,
    "text": "それを無効にすれば"
  },
  {
    "start": 1269112,
    "end": 1272100,
    "text": "モデルは訓練前のモデルと同じように振る舞うはずだ。"
  },
  {
    "start": 1273030,
    "end": 1276590,
    "text": "ローラによって追加されたパラメータを可視化することもできる。"
  },
  {
    "start": 1276750,
    "end": 1279010,
    "text": "パラメータはいくつ追加されたのか？"
  },
  {
    "start": 1279090,
    "end": 1284754,
    "text": "元のレイヤー、1、2、3では、ウェイトとバイアスしかなかった。"
  },
  {
    "start": 1284882,
    "end": 1288422,
    "text": "ローラAマトリックスとローラBマトリックスもある。"
  },
  {
    "start": 1288486,
    "end": 1294780,
    "text": "私は1ランクを選んだ。"
  },
  {
    "start": 1298670,
    "end": 1304814,
    "text": "重み行列が1000×784なので、行列Bは1000×1である。"
  },
  {
    "start": 1304852,
    "end": 1312778,
    "text": "1000×1×1×784でウェイト行列の次元は同じになる。"
  },
  {
    "start": 1312874,
    "end": 1314862,
    "text": "私たちはすべてのレイヤーにそうしている。"
  },
  {
    "start": 1315006,
    "end": 1321490,
    "text": "ローラなしのオリジナルモデルでは、2,807,010個のパラメーターがあった。"
  },
  {
    "start": 1322070,
    "end": 1332550,
    "text": "ローラ行列を加えると、2,813,804個のパラメータがあるが、そのうちの6000個しかない。"
  },
  {
    "start": 1332620,
    "end": 1338790,
    "text": "ローラが紹介した選手は実際にトレーニングされるが、他の選手はトレーニングされない。"
  },
  {
    "start": 1339210,
    "end": 1343110,
    "text": "そのためには、既知のローラのパラメーターをフリーズさせる。"
  },
  {
    "start": 1343270,
    "end": 1346758,
    "text": "ここでは、パラメーターをフリーズさせるコードを作成した。"
  },
  {
    "start": 1346854,
    "end": 1350540,
    "text": "そのためにrequire scrudをfalseに設定しただけだ。"
  },
  {
    "start": 1352750,
    "end": 1356942,
    "text": "そこで私たちが行うのは、9という数字に限ってモデルを微調整することだ。"
  },
  {
    "start": 1356996,
    "end": 1366718,
    "text": "というのも、ここでお見せしたように、私たちはもともと9という数字の精度を上げたいのであって、それ以外のことで微調整をすることはない。"
  },
  {
    "start": 1366804,
    "end": 1379410,
    "text": "すべての数字について訓練された事前訓練モデルがありますが、今度は9という数字についてのみ微調整を行い、9という数字の精度を向上させ、他の数字の精度を低下させることを期待します。"
  },
  {
    "start": 1379570,
    "end": 1381160,
    "text": "ここに戻ろう。"
  },
  {
    "start": 1381930,
    "end": 1383030,
    "text": "私はそれを訓練している。"
  },
  {
    "start": 1383100,
    "end": 1394220,
    "text": "このモデルを微調整するのは9桁の数字だけで、モデルをあまり変えたくないので100バッチだけ行う。"
  },
  {
    "start": 1394830,
    "end": 1397962,
    "text": "トレーニングでやっている。"
  },
  {
    "start": 1398096,
    "end": 1405226,
    "text": "ということで、基本的には、フリーズしたパラメーターが微調整によって変化していないことをお見せしたい。"
  },
  {
    "start": 1405258,
    "end": 1414350,
    "text": "凍結されたパラメータはこれであり、ここでモデルをプレトレーニングした後に保存したオリジナルの重みと同じである。"
  },
  {
    "start": 1414420,
    "end": 1416282,
    "text": "ここではオリジナルのパラメーターを保存する。"
  },
  {
    "start": 1416346,
    "end": 1424354,
    "text": "私たちは実際にクローンを作ってみた。"
  },
  {
    "start": 1424552,
    "end": 1443802,
    "text": "そして、ローラを有効にして、ウェイトにアクセスすると、Pytorchは実際にウェイトを元のウェイトに置き換え、さらにここで定義した式に従って、bにaを掛けたものにスケールを掛けたものに置き換えます。"
  },
  {
    "start": 1443936,
    "end": 1454074,
    "text": "pytorchがウェイト行列にアクセスしようとするたびに、実際にこの関数が実行され、この関数は元のウェイトにbを掛け合わせたものにスケールを掛けたものを返します。"
  },
  {
    "start": 1454202,
    "end": 1455742,
    "text": "これがここで起きていることだ。"
  },
  {
    "start": 1455796,
    "end": 1463978,
    "text": "ローラを有効にした場合、ローラを無効にした場合、パラメータ化を無効にしているので、元のウェイトが返されるだけだ。"
  },
  {
    "start": 1464154,
    "end": 1465522,
    "text": "なぜこのようなことが起こるのか？"
  },
  {
    "start": 1465576,
    "end": 1469970,
    "text": "というのも、ここではローラが無効になったら、元のウエイトを返せばいいと言ったからだ。"
  },
  {
    "start": 1471750,
    "end": 1476994,
    "text": "今できることは、ローラを有効にしてモデルをテストすることだ。"
  },
  {
    "start": 1477192,
    "end": 1480982,
    "text": "9という数字がより良いパフォーマンスをしていることがわかる。"
  },
  {
    "start": 1481116,
    "end": 1484690,
    "text": "もちろん、他の桁に関する情報も失った。"
  },
  {
    "start": 1484850,
    "end": 1490154,
    "text": "Loraを無効にすると、モデルは訓練前のモデルとまったく同じ動作をする。"
  },
  {
    "start": 1490272,
    "end": 1498410,
    "text": "微調整なしで、この数値は訓練前のモデルと同じであることがわかる。"
  },
  {
    "start": 1498480,
    "end": 1507454,
    "text": "ナンバーゼロは33のカウントを間違えており、D 9107のカウントを間違えていた。"
  },
  {
    "start": 1507492,
    "end": 1512238,
    "text": "Lauraを無効にすると、モデルは訓練前のモデルとまったく同じ動作をする。"
  },
  {
    "start": 1512324,
    "end": 1520180,
    "text": "Lauraを有効にすると、微調整されたモデルのように振る舞うメトリクスbとaが導入される。"
  },
  {
    "start": 1520790,
    "end": 1526782,
    "text": "ローラの一番いいところは、オリジナルのウェイトに手を加えなかったことだ。"
  },
  {
    "start": 1526846,
    "end": 1534146,
    "text": "を変更したのはb行列とa行列だけであり、その次元はw行列に比べてはるかに小さい。"
  },
  {
    "start": 1534178,
    "end": 1544966,
    "text": "このファインチューン・モデルを保存する場合、200万個ではなく6794個を保存すればいい。"
  },
  {
    "start": 1545078,
    "end": 1560526,
    "text": "このモデルの多くのバージョンを微調整することができ、このパラメータ化でb行列とw行列を変更するだけで、簡単に切り替えることができる。"
  },
  {
    "start": 1560708,
    "end": 1562750,
    "text": "これがロラの力だ。"
  },
  {
    "start": 1563730,
    "end": 1568974,
    "text": "私は理論的でありながら実用的なビデオを作るよう心がけているので、私のビデオが分かりやすかったことを願う。"
  },
  {
    "start": 1569102,
    "end": 1575860,
    "text": "もう少しうまく説明したいことがあれば、コメントで教えてほしい。"
  },
  {
    "start": 1576390,
    "end": 1589426,
    "text": "私のアカウントにあるpytorchloraのリポジトリを使って、ランキングのサイズを変えてみたり、モデルを変えてみたりして遊んでみてください。"
  },
  {
    "start": 1589458,
    "end": 1590520,
    "text": "とても簡単だ。"
  },
  {
    "start": 1590890,
    "end": 1604138,
    "text": "pytorchのパラメータ化機能も読むことをお勧めします。異なる種類のパラメータ化を導入するのはとても簡単ですし、ニューラルネットワークのパラメータ化で遊ぶこともできます。"
  },
  {
    "start": 1604314,
    "end": 1609182,
    "text": "このビデオを楽しんでいただけたなら幸いです。"
  },
  {
    "start": 1609236,
    "end": 1613580,
    "text": "機械学習とディープラーニングに関するビデオをもっと見るには、私のチャンネルに戻って来てください。"
  }
]