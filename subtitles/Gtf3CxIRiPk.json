[
  {
    "start": 250,
    "end": 2398,
    "text": "これはLLMの世界のすべてを変える。"
  },
  {
    "start": 2484,
    "end": 6906,
    "text": "LLMとは大規模言語モデルの略で、基本的にはディープラーニングモデルである。"
  },
  {
    "start": 7018,
    "end": 9802,
    "text": "ディープラーニングモデルとは、深層ニューラルネットワークのことだ。"
  },
  {
    "start": 9866,
    "end": 15146,
    "text": "複数のニューラルネットワークを深く積み重ねることで、最終的な出力が得られる。"
  },
  {
    "start": 15178,
    "end": 17226,
    "text": "どのように最終的なアウトプットを出すのですか？"
  },
  {
    "start": 17338,
    "end": 23242,
    "text": "これらのディープニューラルネットワークは重みを持ち、これらの重みは行列の乗算のように掛け合わされる。"
  },
  {
    "start": 23306,
    "end": 27042,
    "text": "突然変異の多いハイパフォーマンス・コンピューティング環境で起こることだ。"
  },
  {
    "start": 27106,
    "end": 30262,
    "text": "これが、ディープ・ニューラル・ネットワークを機能させる方法だ。"
  },
  {
    "start": 30316,
    "end": 40350,
    "text": "というのも、NvidiaのGPUはCUDAを搭載しており、高性能な行列乗算をサポートしているからだ。"
  },
  {
    "start": 40450,
    "end": 45846,
    "text": "gpuを必要としない世界、高性能な行列乗算を必要としない世界を想像してみてほしい。"
  },
  {
    "start": 46038,
    "end": 47450,
    "text": "そんなことが可能なのか？"
  },
  {
    "start": 47600,
    "end": 50538,
    "text": "質問の答えは \"可能 \"だ。"
  },
  {
    "start": 50704,
    "end": 52298,
    "text": "実際、それは可能だ。"
  },
  {
    "start": 52384,
    "end": 58250,
    "text": "また、性能面でも現在の大規模言語モデルと同等になるだろう。"
  },
  {
    "start": 58330,
    "end": 63626,
    "text": "それが、今日取り上げる1ビットllmsの時代なのだ。"
  },
  {
    "start": 63738,
    "end": 68130,
    "text": "大きな言語モデルはすべて1.58ビットである。"
  },
  {
    "start": 68630,
    "end": 70254,
    "text": "タイトルは少し分かりにくいかもしれない。"
  },
  {
    "start": 70302,
    "end": 73666,
    "text": "タイトルと論文の内容を説明しよう。"
  },
  {
    "start": 73768,
    "end": 80822,
    "text": "まず第一に、一般的に大きな言語モデルがこのように見える場合、それは32ビットモデルか16ビットモデルのどちらかである。"
  },
  {
    "start": 80876,
    "end": 81718,
    "text": "どういう意味ですか？"
  },
  {
    "start": 81804,
    "end": 89570,
    "text": "つまり、モデルのすべてのパラメータは32ビットのfloatまたは16ビットのfloatで表現される。"
  },
  {
    "start": 89650,
    "end": 93306,
    "text": "floatをご存じない方は、floatは小数点以下の値でしかありません。"
  },
  {
    "start": 93408,
    "end": 97482,
    "text": "整数とは、正負の符号を持つ整数にほかならない。"
  },
  {
    "start": 97536,
    "end": 101914,
    "text": "プラス1マイナス10、プラス2、プラス3、プラス4。"
  },
  {
    "start": 101952,
    "end": 103434,
    "text": "これはすべて整数である。"
  },
  {
    "start": 103562,
    "end": 109690,
    "text": "0234 5.5678.7013 これはすべてフロート。"
  },
  {
    "start": 109770,
    "end": 112718,
    "text": "異なる浮動小数点は異なる精度レベルを持つ。"
  },
  {
    "start": 112804,
    "end": 117570,
    "text": "32ビット浮動小数点か16ビット浮動小数点かのどちらかである。"
  },
  {
    "start": 117720,
    "end": 120334,
    "text": "このように、私たちは異なる精度を持つ。"
  },
  {
    "start": 120462,
    "end": 125714,
    "text": "現在のディープラーニング・モデル、つまり大規模な言語モデルは次のようなものだ。"
  },
  {
    "start": 125832,
    "end": 135766,
    "text": "もしそうだとしたら、あるいは彼らがここでやったことは、この行列の掛け算や行列をこのように変形させたということだ。"
  },
  {
    "start": 135868,
    "end": 141722,
    "text": "彼らはマイナス10にできるすべてを表現している。"
  },
  {
    "start": 141856,
    "end": 146906,
    "text": "それこそが、このビットネットB 1.58モデルなのだ。"
  },
  {
    "start": 147008,
    "end": 159854,
    "text": "モデル上のすべてのパラメータは、32ビットの浮動小数点や16ビットの浮動小数点として表現されるのではなく、ゼロ、1マイナス1の3値として表現される。"
  },
  {
    "start": 159892,
    "end": 162366,
    "text": "プラス10、マイナス1"
  },
  {
    "start": 162468,
    "end": 164366,
    "text": "彼らはどうやってこのアイデアを思いついたのだろう？"
  },
  {
    "start": 164468,
    "end": 165806,
    "text": "彼らはこう考えた。"
  },
  {
    "start": 165908,
    "end": 167230,
    "text": "そんなことはない。"
  },
  {
    "start": 167300,
    "end": 174274,
    "text": "マイクロソフトの別の研究者たちが最初にやったことだが、もしかしたら、これと重なる部分があるかもしれない。"
  },
  {
    "start": 174312,
    "end": 178578,
    "text": "その後、彼らは1ビット・トランスフォーマーというものを作った。"
  },
  {
    "start": 178674,
    "end": 187810,
    "text": "今回の論文では、すべてが浮動小数点値ではなく、プラス1、マイナス1、ゼロで表現されている。"
  },
  {
    "start": 187890,
    "end": 200394,
    "text": "前回の論文、1ビット変換器の論文では、マイクロソフトと中国科学院大学が、すべてを1プラス1かマイナス1にした。"
  },
  {
    "start": 200432,
    "end": 204890,
    "text": "だから1ビットのトランスフォーマーモデル、あるいは1ビットのウェイトと呼ばれるのだ。"
  },
  {
    "start": 204970,
    "end": 207726,
    "text": "ここでは、すべてのパラメータは1ビットで表現されている。"
  },
  {
    "start": 207828,
    "end": 212878,
    "text": "私たちが入手した新しい論文では、すべてのパラメーターはプラス1からマイナス10で表されている。"
  },
  {
    "start": 212964,
    "end": 218226,
    "text": "つまり、1ビットではなく1.58ビットなのだ。"
  },
  {
    "start": 218328,
    "end": 223570,
    "text": "彼らは、ゼロを加えることで学習能力の面で有利になると言う。"
  },
  {
    "start": 223990,
    "end": 228102,
    "text": "ここで何が起こっているかというと、単純に、このようなマトリックスを手に入れたということだ。"
  },
  {
    "start": 228156,
    "end": 229766,
    "text": "このようなマトリックスがもう一つある。"
  },
  {
    "start": 229868,
    "end": 237458,
    "text": "通常、現在のディープラーニング・ニューラルネットワークでは、行列の掛け算と足し算が必要だ。"
  },
  {
    "start": 237634,
    "end": 239078,
    "text": "これをドット積と呼ぶ。"
  },
  {
    "start": 239164,
    "end": 242250,
    "text": "掛け算、足し算、掛け算、足し算、掛け算、足し算。"
  },
  {
    "start": 242320,
    "end": 243962,
    "text": "それが常態化している。"
  },
  {
    "start": 244016,
    "end": 249942,
    "text": "重みと入力が乗算され、それが加算される。"
  },
  {
    "start": 250006,
    "end": 251686,
    "text": "これが掛け算され、足し算される。"
  },
  {
    "start": 251718,
    "end": 259006,
    "text": "yはWの関数に等しく、x wは重みであり、これはすでにモデルの一部である。"
  },
  {
    "start": 259188,
    "end": 263454,
    "text": "入力とは、あなたがモデル内部に与えるものであり、それによって計算が行われる。"
  },
  {
    "start": 263492,
    "end": 266726,
    "text": "必要であれば、追加の教材を提供することもできる。"
  },
  {
    "start": 266778,
    "end": 269778,
    "text": "ディープラーニングをどう理解するか。"
  },
  {
    "start": 269864,
    "end": 273890,
    "text": "技術的には、モデルの重量と入力xがあることがわかる。"
  },
  {
    "start": 273960,
    "end": 275266,
    "text": "これがすべてだ。"
  },
  {
    "start": 275368,
    "end": 284930,
    "text": "ここで何が変わるかというと、浮動小数点値の代わりにプラス1マイナス10、つまり3進数値、この場合は整数を使うことになる。"
  },
  {
    "start": 285090,
    "end": 290198,
    "text": "つまり、足し算だけでいいし、掛け算は完全に無視できるということだ。"
  },
  {
    "start": 290294,
    "end": 296774,
    "text": "掛け算で済むということは、掛け算は必要ないということだ。"
  },
  {
    "start": 296822,
    "end": 299670,
    "text": "英語では掛け算は必要ない。"
  },
  {
    "start": 299750,
    "end": 302682,
    "text": "もし、あなたが \"do away \"と言うなら、私は \"do away \"と言うべきだと思う。"
  },
  {
    "start": 302736,
    "end": 307754,
    "text": "乗算をなくせば、GPUのようなハードウェアは必要ないということだ。"
  },
  {
    "start": 307802,
    "end": 310746,
    "text": "実際、完全に新しい種類のハードウェアを作ることができる。"
  },
  {
    "start": 310858,
    "end": 313870,
    "text": "Grokがやっているようなことがその例だ。"
  },
  {
    "start": 313940,
    "end": 321598,
    "text": "Grokがまさにこのようなことをしているとは思わないが、サポートし、最適化され、高度に最適化された別の種類のハードウェアを持つことができる。"
  },
  {
    "start": 321694,
    "end": 325678,
    "text": "このような計算のために、GPUはなぜこれほど普及したのか？"
  },
  {
    "start": 325774,
    "end": 332338,
    "text": "Gpusはゲーマーに使われ、Gpusはマイナーに使われていたが、GPUはアクセラレーション・コンピューティングのおかげで大人気となった。"
  },
  {
    "start": 332434,
    "end": 336338,
    "text": "なぜかジェンセン・ファンは大当たりし、宝くじを手にした。"
  },
  {
    "start": 336434,
    "end": 343366,
    "text": "そのCUDAに対応したNvidiaのGPUは、行列の掛け算であるディープラーニングに高度に最適化された場所となった。"
  },
  {
    "start": 343478,
    "end": 350854,
    "text": "これは、行列の掛け算を必要としない新しいハードウェアを作るのに役立つ。"
  },
  {
    "start": 350902,
    "end": 357854,
    "text": "つまり、ハードウェア面での技術革新には大きな可能性があり、ソフトウェア面での技術革新にも大きなチャンスがあるということだ。"
  },
  {
    "start": 357892,
    "end": 360238,
    "text": "最適化アルゴリズムの側面も大きい。"
  },
  {
    "start": 360324,
    "end": 362170,
    "text": "それがこの論文で証明されていることだ。"
  },
  {
    "start": 362250,
    "end": 375426,
    "text": "この論文は、ビットネットアーキテクチャ（この場合はb 1.58）ができれば、LLMのすべてのパラメータ、つまりウェイトは3値として定義され、マイナス10になると言っている。"
  },
  {
    "start": 375528,
    "end": 379434,
    "text": "精度が高ければ高いほど、性能は全精度に一致する。"
  },
  {
    "start": 379502,
    "end": 394166,
    "text": "FP 16浮動小数点16またはb浮動小数点16トランスフォーマーLLMと同じモデルサイズで、例えばここで70億のパラメーターを取れば、70億のパラメーターを取れば、7億のパラメーターを取れば、7億のパラメーターを取れば、一方はビットネットで、もう一方はトランスフォーマーだ。"
  },
  {
    "start": 394198,
    "end": 397850,
    "text": "性能は同じである。"
  },
  {
    "start": 397920,
    "end": 413838,
    "text": "最大の利点は、レイテンシー、メモリをどれだけ速く供給できるか、ラムが必要とするメモリ量、スループット、1秒間に生成できるトークン数、そして最も重要なのはエネルギー消費量である。"
  },
  {
    "start": 413924,
    "end": 429518,
    "text": "結局のところ、より深いところでは、1.58ビットLLMは、これらのモデルをどのようにスケールアップできるかについて、まったく新しいスケーリング法則を定義しており、高性能でコスト効率もよい新世代のLLMをトレーニングするための新しいレシピを定義している。"
  },
  {
    "start": 429614,
    "end": 432950,
    "text": "これはまた、新しいハードウェアに道を譲ることもできる。"
  },
  {
    "start": 433020,
    "end": 437734,
    "text": "彼らが何をしたかを手短にお見せするために、この特別な点をご理解いただけたと思う。"
  },
  {
    "start": 437932,
    "end": 443594,
    "text": "私が言ったように、彼らはこれらのものをすべて取り出して、その値を四捨五入して、マイナス10にしたんだ。"
  },
  {
    "start": 443712,
    "end": 449270,
    "text": "これらは、前回紹介した1ビット・トランスフォーマーの利点をすべて備えている。"
  },
  {
    "start": 449350,
    "end": 454158,
    "text": "この論文には有益な情報がたくさんあると思うので、別途取り上げるかもしれない。"
  },
  {
    "start": 454324,
    "end": 456014,
    "text": "この論文を見逃すなんて信じられない。"
  },
  {
    "start": 456052,
    "end": 457786,
    "text": "これは10月17日のことだった。"
  },
  {
    "start": 457898,
    "end": 464442,
    "text": "これは、1ビットのビットネットのように、その特定の論文のアーキテクチャを保持している。"
  },
  {
    "start": 464506,
    "end": 470802,
    "text": "唯一の変更点は、2つの値から1つを引いた値（1ビット値）の代わりに、0が追加されたことだ。"
  },
  {
    "start": 470856,
    "end": 472754,
    "text": "つまり1.58ビットになる。"
  },
  {
    "start": 472872,
    "end": 474686,
    "text": "これには2つの利点がある。"
  },
  {
    "start": 474798,
    "end": 488086,
    "text": "ひとつは、特徴フィルタリングを可能にすること、そしてもうひとつは、LLMが首尾一貫したテキストを生成できるかどうかを測定するために使われる指標である、パープレキシティ（perplexity）という観点から、学習するモデルの質を高めることである。"
  },
  {
    "start": 488188,
    "end": 497114,
    "text": "また、下流のタスク、たとえば質問と回答、何かを理解する、最終的なタスクにおいても、より良いパフォーマンスを発揮する。"
  },
  {
    "start": 497232,
    "end": 504974,
    "text": "これはllama LLMというモデルで、llamaベースのアーキテクチャ、大規模な言語モデルです。"
  },
  {
    "start": 505012,
    "end": 506846,
    "text": "文字通り、ラマモデルと比較しているのだ。"
  },
  {
    "start": 506948,
    "end": 519986,
    "text": "実際、物事を平等に、フェアプレーにするために、彼らが行ったのは、モデルを完全に取り込み、赤いパジャマのデータセットでゼロからトレーニングすることだった。"
  },
  {
    "start": 520008,
    "end": 525666,
    "text": "彼らはラマと同じようなデータセットを使い、比較した。"
  },
  {
    "start": 525688,
    "end": 533046,
    "text": "ラマLLM、7億パラメータ、13億パラメータ、30億パラメータモデルを比較した場合。"
  },
  {
    "start": 533148,
    "end": 538162,
    "text": "3つの異なる場面で、戸惑いがほぼ同じであることがわかるだろう。"
  },
  {
    "start": 538226,
    "end": 541322,
    "text": "あなたは当惑を見ることができ、当惑は小さいほど良い。"
  },
  {
    "start": 541376,
    "end": 543478,
    "text": "戸惑いはほとんど同じである。"
  },
  {
    "start": 543574,
    "end": 549046,
    "text": "実際、最大の30億パラメータモデルであるLama LLMよりも当惑度は低い。"
  },
  {
    "start": 549078,
    "end": 553338,
    "text": "私はいつも不思議に思っていたのだが、ほら、当惑は次の言葉を予測するための良い尺度だ。"
  },
  {
    "start": 553434,
    "end": 564670,
    "text": "というのも、最近では、質問と回答のようなタスク、テキストの分類、テキストの要約など、多くの下流タスクにモデルを使いたいからです。"
  },
  {
    "start": 564740,
    "end": 572750,
    "text": "彼らはまた、hellaswag arcのような多くのメトリクスと比較した。"
  },
  {
    "start": 572830,
    "end": 593978,
    "text": "平均的に見れば、つまり、個々の数字を見てもわかるが、平均的に見れば、ビットネットbの158億モデル、30億パラメーターモデル、39億パラメーターモデルを見れば、30億パラメーターモデルの大台に乗り、同等のラマ変圧器モデルよりスコアが上だ。"
  },
  {
    "start": 594144,
    "end": 596086,
    "text": "ほら、これはまだ変圧器だ。"
  },
  {
    "start": 596278,
    "end": 598138,
    "text": "トランスフォーマーモデルではない。"
  },
  {
    "start": 598224,
    "end": 599674,
    "text": "これはまだトランスフォーマーモデルだ。"
  },
  {
    "start": 599712,
    "end": 607706,
    "text": "というのも、1ビットモデルを見ればわかることだが、彼らは文字通りトランスフォーマーのアーキテクチャを採用したのだ。"
  },
  {
    "start": 607818,
    "end": 611038,
    "text": "あの変圧器の構造では、彼らはすべてを奪ってしまった。"
  },
  {
    "start": 611204,
    "end": 618766,
    "text": "ただひとつ、彼らは従来の行列乗算の代わりにビット線形を使っている。"
  },
  {
    "start": 618798,
    "end": 627234,
    "text": "行列の掛け算が必要なところでは、行列の掛け算を必要としないビットリニアが使われている。"
  },
  {
    "start": 627272,
    "end": 629010,
    "text": "これはまだ変圧器だ。"
  },
  {
    "start": 629090,
    "end": 634406,
    "text": "ラマもトランスフォーマーであることに変わりはないが、ウェイトの中の数字の表し方がまったく変わってしまう。"
  },
  {
    "start": 634428,
    "end": 639898,
    "text": "ディープラーニングの部分を見つけるための計算方法がまったく変わってしまう。"
  },
  {
    "start": 639984,
    "end": 642710,
    "text": "困惑の中で良い点を取る。"
  },
  {
    "start": 642790,
    "end": 646790,
    "text": "アークやLSwaGのような下流タスクでは好成績を収めている。"
  },
  {
    "start": 646950,
    "end": 650182,
    "text": "最大のポイントは正確さだけではない。"
  },
  {
    "start": 650246,
    "end": 654634,
    "text": "レイテンシーを生み出す時間が改善される。"
  },
  {
    "start": 654762,
    "end": 661834,
    "text": "2.7倍の向上、2.4倍の向上、スループットの向上が見られる。"
  },
  {
    "start": 661882,
    "end": 664930,
    "text": "スループットは9倍もある。"
  },
  {
    "start": 665080,
    "end": 671700,
    "text": "700億のパラメータ・モデルで1秒間に2977トークンを生成できる。"
  },
  {
    "start": 672230,
    "end": 676338,
    "text": "これは、現在グロックがプロダクションで行っていることをはるかに上回るものだ。"
  },
  {
    "start": 676514,
    "end": 682082,
    "text": "これは高度な、言ってみればハードウェアを必要としない。"
  },
  {
    "start": 682146,
    "end": 686166,
    "text": "これはとてもシンプルなことなので、少なくとも私はそう理解している。"
  },
  {
    "start": 686348,
    "end": 689798,
    "text": "GPUのようなハードウェアは必要ない。"
  },
  {
    "start": 689894,
    "end": 695450,
    "text": "これによって、スループット（1秒あたりのトークン数）が飛躍的に向上すると思う。"
  },
  {
    "start": 695600,
    "end": 703622,
    "text": "Grokがミクスチャーのために与えている毎秒500トークンのために、人々は狂喜乱舞していると思う。"
  },
  {
    "start": 703686,
    "end": 705534,
    "text": "卵70億個のパラメーターモデル。"
  },
  {
    "start": 705652,
    "end": 714862,
    "text": "これは、このリャマ・アーキテクチャー・モデルの9倍以上となる。"
  },
  {
    "start": 714916,
    "end": 716462,
    "text": "これは大きいと思う。"
  },
  {
    "start": 716516,
    "end": 718466,
    "text": "この他にも興味深いディテールがたくさんある。"
  },
  {
    "start": 718488,
    "end": 726002,
    "text": "詳細は割愛するが、これで、どこでモデルを走らせることができるか、どんなハードウェアでモデルを走らせることができるかについて、大きな可能性が生まれた。"
  },
  {
    "start": 726056,
    "end": 729474,
    "text": "例えば、モバイルエッジのように、異なるタイプのハードウェアだ。"
  },
  {
    "start": 729522,
    "end": 732850,
    "text": "BBC Fourのビットキットを見たことがあるかどうかわからないけど。"
  },
  {
    "start": 732930,
    "end": 735698,
    "text": "可能性は計り知れないと思う。"
  },
  {
    "start": 735794,
    "end": 747050,
    "text": "モデルをスケールアップする方法、LLMのスケーリングロス、モデルを実行する方法、スループット、私はゲームが完全に変わると思う。"
  },
  {
    "start": 747120,
    "end": 748810,
    "text": "それがこのビデオを作った理由のひとつだ。"
  },
  {
    "start": 748880,
    "end": 755978,
    "text": "コメント欄で、1ビットまたは1.58ビットのLLMという全く新しい方向性についてどう思うか教えてほしい。"
  },
  {
    "start": 756154,
    "end": 757578,
    "text": "一応の認識をしておいてほしい。"
  },
  {
    "start": 757674,
    "end": 762954,
    "text": "例えば、16ビット精度のLLMモデルを持っているとしよう。"
  },
  {
    "start": 763002,
    "end": 771994,
    "text": "40億のパラメーターを持つモデルなら、40億のパラメーターはすべて16ビットの浮動小数点値で表現されなければならない。"
  },
  {
    "start": 772122,
    "end": 780710,
    "text": "つまり、現在必要な16倍のハードウェアが何であれ、1.58ビットになったことで、よりシンプルに減らすことができる。"
  },
  {
    "start": 780860,
    "end": 790406,
    "text": "このようなアーキテクチャーを使うだけで、1514倍、あるいは8倍もの大幅な時間短縮が可能になる。"
  },
  {
    "start": 790428,
    "end": 796038,
    "text": "この特別なスペースで人々が何をしているかを見るのは、とてもエキサイティングなことだと思う。"
  },
  {
    "start": 796124,
    "end": 796950,
    "text": "また別のビデオで会おう。"
  },
  {
    "start": 797020,
    "end": 797650,
    "text": "ハッピー・プロンプト。"
  }
]