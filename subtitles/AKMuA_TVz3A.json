[
  {
    "start": 330,
    "end": 1950,
    "text": "引用数がある。"
  },
  {
    "start": 2020,
    "end": 3066,
    "text": "これは6桁だ。"
  },
  {
    "start": 3098,
    "end": 8302,
    "text": "41万9,000人以上、そしてそれ以上だ。"
  },
  {
    "start": 8356,
    "end": 12590,
    "text": "だから、イリヤがllmsと未来について何を語るのか、とても楽しみだ。"
  },
  {
    "start": 12740,
    "end": 13518,
    "text": "持ち去れ。"
  },
  {
    "start": 13604,
    "end": 13902,
    "text": "オーケー。"
  },
  {
    "start": 13956,
    "end": 20880,
    "text": "皆さん、こんにちは。"
  },
  {
    "start": 21250,
    "end": 22830,
    "text": "紹介してくれてありがとう。"
  },
  {
    "start": 23490,
    "end": 24640,
    "text": "いつかはね。"
  },
  {
    "start": 25370,
    "end": 29154,
    "text": "ウメッシュからこのイベントに誘われたとき、私は本当に興奮した。"
  },
  {
    "start": 29202,
    "end": 33414,
    "text": "講演者リストを見て、素晴らしい、行って何か話そう、と思ったんだ。"
  },
  {
    "start": 33612,
    "end": 42380,
    "text": "つまり、OpenAIで行っている技術的な仕事の多くは、実際に話すことができるということです。"
  },
  {
    "start": 43550,
    "end": 48666,
    "text": "今、何を話せばいいのか、本当に頭を悩ませていた。"
  },
  {
    "start": 48768,
    "end": 60558,
    "text": "だから、少し前から、私はAIアライメントの研究に全力を注いでいる。"
  },
  {
    "start": 60724,
    "end": 65220,
    "text": "じゃあ、これは次のトークに使うとして、このトークには何がいいだろう？"
  },
  {
    "start": 65670,
    "end": 68580,
    "text": "あることを思いついた。"
  },
  {
    "start": 69670,
    "end": 88860,
    "text": "何年も前、つまり2016年にOpenAIで行った非常に古い結果についてお話ししましょう。"
  },
  {
    "start": 89390,
    "end": 91260,
    "text": "私はそれを分かち合おうと思った。"
  },
  {
    "start": 91710,
    "end": 98780,
    "text": "この時点で、皆さんはそれらを明白に見つけることができるかもしれないが、もしかしたらすべてではないかもしれない。"
  },
  {
    "start": 99150,
    "end": 102160,
    "text": "少なくとも、面白いと思ってもらえる可能性は少しはある。"
  },
  {
    "start": 102690,
    "end": 106480,
    "text": "期待値を控えめに設定し、できればそれを上回りたい。"
  },
  {
    "start": 107890,
    "end": 115970,
    "text": "では、教師なし学習の理論、そんなものが存在するのだろうか？"
  },
  {
    "start": 119190,
    "end": 125140,
    "text": "さて、教師なし学習について話す前に、学習全般について話したい。"
  },
  {
    "start": 125910,
    "end": 129318,
    "text": "学習とは何か？"
  },
  {
    "start": 129404,
    "end": 133080,
    "text": "なぜ学習が必要なのか、なぜコンピューターが学習できなければならないのか？"
  },
  {
    "start": 133850,
    "end": 139914,
    "text": "今、私たちはニューラルネットワークが学習するという当たり前の事実に慣れきっている。"
  },
  {
    "start": 140032,
    "end": 143094,
    "text": "なぜ彼らは数学が好きなのか？"
  },
  {
    "start": 143142,
    "end": 144042,
    "text": "なぜそうしなければならないのか？"
  },
  {
    "start": 144096,
    "end": 152030,
    "text": "なぜデータには、機械学習モデルが捉えることのできる規則性があるのだろうか？"
  },
  {
    "start": 152610,
    "end": 154640,
    "text": "だから、それは明白な質問ではない。"
  },
  {
    "start": 155250,
    "end": 170500,
    "text": "機械学習において、何年も前に複数の人々によって行われた重要な概念的進歩のひとつに、教師あり学習の発見と定式化がある。"
  },
  {
    "start": 171190,
    "end": 175670,
    "text": "パック学習や統計的学習理論という名前で呼ばれている。"
  },
  {
    "start": 176410,
    "end": 188570,
    "text": "教師あり学習のいいところは、学習が成功しなければならない数学的条件を正確に与えてくれることだ。"
  },
  {
    "start": 190910,
    "end": 214898,
    "text": "あるデータ分布のデータがあれば、低い訓練損失を達成することができ、自由度の数が訓練セットよりも小さければ、低いテスト誤差を達成することができ、それが保証されると言われる。"
  },
  {
    "start": 215064,
    "end": 228082,
    "text": "という数学的条件があれば、学習誤差の少ない関数を見つけることができ、学習は成功する。"
  },
  {
    "start": 228226,
    "end": 235846,
    "text": "そうだ、これは理屈で説明できる非常に理にかなった数学的なことだ。"
  },
  {
    "start": 236028,
    "end": 238198,
    "text": "だから教師あり学習は簡単なのだ。"
  },
  {
    "start": 238364,
    "end": 242922,
    "text": "そうしたら、単純だと思っていた定理が全部出てきた。"
  },
  {
    "start": 243056,
    "end": 244570,
    "text": "エレガントだと思った。"
  },
  {
    "start": 245070,
    "end": 247034,
    "text": "こんな定理を見たことがあるだろう。"
  },
  {
    "start": 247072,
    "end": 248122,
    "text": "もし、あなたが自分のものを持っているなら。"
  },
  {
    "start": 248256,
    "end": 252314,
    "text": "基本的には、知っていれば、ああ、あれね、となるようなものだ。"
  },
  {
    "start": 252432,
    "end": 259242,
    "text": "もしあなたがそれを知らなければ、30秒で説明することはできないだろう。"
  },
  {
    "start": 259306,
    "end": 260890,
    "text": "どれも複雑なことではない。"
  },
  {
    "start": 261050,
    "end": 277894,
    "text": "つまり、関数クラスにいくつかの関数がある場合、少なくとも1つの関数について、訓練誤差がテスト誤差から遠くなる確率がある、という小さな証明がある。"
  },
  {
    "start": 277932,
    "end": 279954,
    "text": "これはすべて、3行分の計算だ。"
  },
  {
    "start": 280082,
    "end": 282920,
    "text": "この3行の数学で、教師あり学習のすべてを証明することができる。"
  },
  {
    "start": 283770,
    "end": 285000,
    "text": "それはいいね。"
  },
  {
    "start": 285450,
    "end": 286678,
    "text": "とてもいいね。"
  },
  {
    "start": 286844,
    "end": 292822,
    "text": "スーパーバイズド・ラーニングは、比較的よく理解されている。"
  },
  {
    "start": 292966,
    "end": 295910,
    "text": "なぜ成功しなければならないかは分かっている。"
  },
  {
    "start": 295990,
    "end": 305360,
    "text": "私たちは、大規模な教師あり学習のデータセットを収集し、モデルが改良され続けることを完全に確信することができる。"
  },
  {
    "start": 307570,
    "end": 311760,
    "text": "という話だ。"
  },
  {
    "start": 312850,
    "end": 318178,
    "text": "そうそう、この結果について非常に重要なことを言い忘れていた。"
  },
  {
    "start": 318264,
    "end": 322402,
    "text": "テスト分布と訓練分布は同じでなければならない。"
  },
  {
    "start": 322536,
    "end": 328630,
    "text": "もし同じであれば、教師あり学習の理論が機能し、成功することになる。"
  },
  {
    "start": 329690,
    "end": 332230,
    "text": "概念的には些細なことだ。"
  },
  {
    "start": 332890,
    "end": 348490,
    "text": "なぜ教師あり学習がうまくいくのか、なぜ音声認識がうまくいくのか、なぜ画像分類がうまくいくのか、その答えがある。"
  },
  {
    "start": 348990,
    "end": 352410,
    "text": "ここはとてもいい。"
  },
  {
    "start": 352480,
    "end": 356018,
    "text": "VCの次元について少し横槍を入れたい。"
  },
  {
    "start": 356134,
    "end": 359514,
    "text": "そんなことを気にする人たちには、ごく一部の人たちかもしれない。"
  },
  {
    "start": 359562,
    "end": 363200,
    "text": "次の30秒間をボーッとしたいのであれば、ご自由にどうぞ。"
  },
  {
    "start": 365090,
    "end": 374430,
    "text": "そのため、統計的学習理論に関する多くの著作は、VC次元を重要な要素として強調している。"
  },
  {
    "start": 375850,
    "end": 390410,
    "text": "実際、VC次元が発明された唯一の理由は、無限の精度を持つパラメーターを扱えるようにするためだった。"
  },
  {
    "start": 391550,
    "end": 396406,
    "text": "VC次元は、パラメーターのような精度を無限の精度で扱うために考案された。"
  },
  {
    "start": 396438,
    "end": 400090,
    "text": "線形分類器があれば、すべてのパラメータは無限の精度を持つ。"
  },
  {
    "start": 401410,
    "end": 408110,
    "text": "しかしもちろん、現実にはすべての浮動小数点は有限精度であり、その精度は低下している。"
  },
  {
    "start": 411650,
    "end": 425134,
    "text": "コンピュータが実装する関数の数は実際には少なく、この小さな数式に戻すことができる。"
  },
  {
    "start": 425272,
    "end": 426742,
    "text": "これはクールだと思う。"
  },
  {
    "start": 426796,
    "end": 431830,
    "text": "証明の線が少ないので、これは魅力的だと思う。"
  },
  {
    "start": 435210,
    "end": 438860,
    "text": "では、教師なし学習について話そう。"
  },
  {
    "start": 439710,
    "end": 443722,
    "text": "まず、教師なし学習とは何か？"
  },
  {
    "start": 443856,
    "end": 446938,
    "text": "教師あり学習とはどのようなものですか？"
  },
  {
    "start": 446944,
    "end": 452720,
    "text": "ああ、こうしてくれ、ああしてくれというデータがある。"
  },
  {
    "start": 453090,
    "end": 454586,
    "text": "トレーニングのミスはよくやっている。"
  },
  {
    "start": 454618,
    "end": 456080,
    "text": "トレーニングの誤差は少ない。"
  },
  {
    "start": 456530,
    "end": 464258,
    "text": "あなたは、関数クラスのパラメーターの自由度よりも多くのトレーニングデータを持っている。"
  },
  {
    "start": 464344,
    "end": 466130,
    "text": "ビットのような自由度。"
  },
  {
    "start": 467110,
    "end": 469646,
    "text": "そうすれば、教師あり学習は成功する。"
  },
  {
    "start": 469678,
    "end": 471010,
    "text": "教師なし学習とは何か？"
  },
  {
    "start": 471080,
    "end": 474786,
    "text": "教師なし学習について何が言える？"
  },
  {
    "start": 474968,
    "end": 481858,
    "text": "少なくとも私は、教師なし学習について納得のいく説明を見たことがない。"
  },
  {
    "start": 481954,
    "end": 483426,
    "text": "それを数学的に推論するには？"
  },
  {
    "start": 483458,
    "end": 490262,
    "text": "私たちは直感的にそれを推論することができるが、数学的に、そして何らかの文脈でそれを推論することができるだろうか？"
  },
  {
    "start": 490326,
    "end": 493754,
    "text": "教師なし学習の古い夢とは？"
  },
  {
    "start": 493792,
    "end": 497590,
    "text": "ところで、この夢は達成されたが、それは経験的に達成された。"
  },
  {
    "start": 497750,
    "end": 514320,
    "text": "経験的な結果をほんの少し超えることができないだろうか。例えば、何も言われずにただ画像を見たり、ただテキストを見たりしても、どういうわけかデータの中に存在する真の隠された構造を発見することができる。"
  },
  {
    "start": 516790,
    "end": 518260,
    "text": "なぜそうしなければならないのか？"
  },
  {
    "start": 518870,
    "end": 520100,
    "text": "そうすべきなのか？"
  },
  {
    "start": 520550,
    "end": 522340,
    "text": "私たちはそれを期待すべきなのだろうか？"
  },
  {
    "start": 523830,
    "end": 528690,
    "text": "教師あり学習の保証と似たようなものはないだろう。"
  },
  {
    "start": 528850,
    "end": 533302,
    "text": "教師あり学習が保証するのは、そう、トレーニングの誤差を少なくすれば、学習が進むということだ。"
  },
  {
    "start": 533356,
    "end": 534600,
    "text": "大きな成功を収めるだろう。"
  },
  {
    "start": 535850,
    "end": 539660,
    "text": "教師なし学習では、このようにはならないようだ。"
  },
  {
    "start": 540670,
    "end": 546940,
    "text": "教師なし学習については、ボルトマン・マシンですでに長い間議論されていた。"
  },
  {
    "start": 547390,
    "end": 551318,
    "text": "教師なし学習も小規模ではうまくいかなかった。"
  },
  {
    "start": 551494,
    "end": 554698,
    "text": "ノイジー・ノート・エンコーダーのように、古いアイデアはそこにあった。"
  },
  {
    "start": 554874,
    "end": 560446,
    "text": "覚えている人は覚えていると思うが、バートやその中の拡散モデルのようなものだ。"
  },
  {
    "start": 560468,
    "end": 565090,
    "text": "それはほんの小さなひねりであり、あらゆる時代の言語モデルなのだ。"
  },
  {
    "start": 565160,
    "end": 568754,
    "text": "彼らはまた、当時としてはクールなサンプルを作成した。"
  },
  {
    "start": 568792,
    "end": 574980,
    "text": "その教師なし学習の性能は、今日のものほど素晴らしいものではなかった。"
  },
  {
    "start": 575830,
    "end": 581314,
    "text": "なぜ混乱するのか？"
  },
  {
    "start": 581362,
    "end": 583926,
    "text": "教師なし学習はどのように機能するのか？"
  },
  {
    "start": 583948,
    "end": 593702,
    "text": "ある種の再構成誤差を最適化しようとか、ある種のノイズ除去誤差を最適化しようとか、ある種の自己教師付き学習誤差を最適化しようとかね。"
  },
  {
    "start": 593766,
    "end": 596954,
    "text": "1つの目的を最適化するんだろう？"
  },
  {
    "start": 596992,
    "end": 598314,
    "text": "ああ、そうだ。"
  },
  {
    "start": 598432,
    "end": 600410,
    "text": "あなたは別の目的に関心がある。"
  },
  {
    "start": 601310,
    "end": 610618,
    "text": "ということは、教師なし学習で良い結果が得られると期待する理由がないということではないのか？"
  },
  {
    "start": 610634,
    "end": 618130,
    "text": "というか、経験的にそれを得ることはできるが、その謎のレベルはかなり高い、と私は主張する。"
  },
  {
    "start": 618470,
    "end": 621230,
    "text": "まったく、まったく近寄りがたい現象のように思える。"
  },
  {
    "start": 621310,
    "end": 625762,
    "text": "ある目的を最適化する一方で、別の目的にも気を配っている。"
  },
  {
    "start": 625906,
    "end": 628390,
    "text": "それがどうしてマジックになるんだ？"
  },
  {
    "start": 631770,
    "end": 644140,
    "text": "ところで、もうひとつは、まあ、90％しか真実ではないことを言おうと思っているんだけどね。"
  },
  {
    "start": 645470,
    "end": 647174,
    "text": "教師なし学習はそうではない。"
  },
  {
    "start": 647302,
    "end": 652126,
    "text": "入力分布の構造を学べばいいんだ。"
  },
  {
    "start": 652228,
    "end": 660030,
    "text": "では、一様分布から学習している場合はどうなるかというと、教師なし学習アルゴリズムはすべて失敗する。"
  },
  {
    "start": 661810,
    "end": 663460,
    "text": "それをどう考えるべきか？"
  },
  {
    "start": 664870,
    "end": 666420,
    "text": "どう言えばいいのだろう？"
  },
  {
    "start": 668230,
    "end": 669502,
    "text": "仮定する必要があるのか？"
  },
  {
    "start": 669566,
    "end": 670958,
    "text": "どんな仮定ですか？"
  },
  {
    "start": 671134,
    "end": 680102,
    "text": "教師なし学習についての考え方を紹介したいと思います。"
  },
  {
    "start": 680236,
    "end": 681734,
    "text": "もしかしたら、あなたも面白いと思うかもしれない。"
  },
  {
    "start": 681772,
    "end": 682840,
    "text": "見てみよう。"
  },
  {
    "start": 684570,
    "end": 703280,
    "text": "教師なし学習を行う方法のひとつを紹介したい。教師なし学習を行う方法が主流になることがなかったため、必ずしも広く知られているわけではないが、教師あり学習と同様に、機能しなければならないというクールな特徴がある。"
  },
  {
    "start": 704130,
    "end": 715300,
    "text": "入力にラベルが与えられていないにもかかわらず、教師なし学習がうまくいく保証はあるのだろうか？"
  },
  {
    "start": 715910,
    "end": 717490,
    "text": "配給マッチング。"
  },
  {
    "start": 719750,
    "end": 721330,
    "text": "配給マッチング。"
  },
  {
    "start": 722390,
    "end": 723918,
    "text": "ディストリビューション・マッチングとは？"
  },
  {
    "start": 724014,
    "end": 727640,
    "text": "例えば、XとYのデータがある。"
  },
  {
    "start": 728250,
    "end": 730662,
    "text": "データソース、両者の間に対応関係はない。"
  },
  {
    "start": 730716,
    "end": 742300,
    "text": "2つのデータソース、データソースx、次にデータソース、y言語、1つの言語、2つのテキスト、スピーチ、これらの間に対応関係がありません。"
  },
  {
    "start": 744190,
    "end": 746250,
    "text": "この基準を見てみよう。"
  },
  {
    "start": 746750,
    "end": 754720,
    "text": "xのfの分布がyの分布に類似するような関数fを求めよ。"
  },
  {
    "start": 755410,
    "end": 757598,
    "text": "これはfに対する制約である。"
  },
  {
    "start": 757764,
    "end": 768558,
    "text": "例えば機械翻訳や音声認識の場合、この制約は意味があるかもしれない。"
  },
  {
    "start": 768734,
    "end": 772690,
    "text": "長い文章を書けば、そう言えるかもしれない。"
  },
  {
    "start": 774070,
    "end": 784690,
    "text": "そのような分布になるような関数があると言うなら、英語の文の分布を取り、関数fを適用すると、フランス語の文の分布に非常に似たものが得られる。"
  },
  {
    "start": 784770,
    "end": 794300,
    "text": "よし、fに関する真の制約を見つけたぞ。"
  },
  {
    "start": 794990,
    "end": 802474,
    "text": "xの次元数とyの次元数が十分に高ければ、多くの制約が与えられることになる。"
  },
  {
    "start": 802602,
    "end": 806960,
    "text": "実際、その情報からFをほぼ完全に回復させることができるかもしれない。"
  },
  {
    "start": 808530,
    "end": 819860,
    "text": "これは教師あり学習の一例であり、教師なし学習でも、教師あり学習と同じ意味で機能することが保証されている。"
  },
  {
    "start": 822230,
    "end": 828550,
    "text": "また、小さな単純暗号のような置換暗号もこの枠組みに当てはまるだろう。"
  },
  {
    "start": 829770,
    "end": 830822,
    "text": "それが問題なんだ。"
  },
  {
    "start": 830876,
    "end": 831720,
    "text": "わかったよ。"
  },
  {
    "start": 832090,
    "end": 855200,
    "text": "教師なし学習について、数学的に意味のあることが言えるかもしれない。"
  },
  {
    "start": 856450,
    "end": 862754,
    "text": "このセットアップの特徴は、まだ少し人工的だが、まだ現実味があるということだ。"
  },
  {
    "start": 862792,
    "end": 864946,
    "text": "機械学習のセットアップはこうではない。"
  },
  {
    "start": 865128,
    "end": 872770,
    "text": "私たちが教師なし学習について考えたいように、それもいいのではないだろうか？"
  },
  {
    "start": 872840,
    "end": 878840,
    "text": "では、私が言いたかったことの神話をお見せしよう。"
  },
  {
    "start": 879530,
    "end": 890806,
    "text": "教師なし学習を教師あり学習と同等に考え、数学的に何をするのか？"
  },
  {
    "start": 890918,
    "end": 897210,
    "text": "教師なし学習が救助のための優れた圧縮であると、どうすれば確信できるのだろうか？"
  },
  {
    "start": 898830,
    "end": 901446,
    "text": "明らかに、よく知られている。"
  },
  {
    "start": 901558,
    "end": 907146,
    "text": "明らかにと言うべきではないが、圧縮が予測であることはよく知られている。"
  },
  {
    "start": 907258,
    "end": 910314,
    "text": "すべてのコンプレッサーはプレディショナーにも、またその逆も可能だ。"
  },
  {
    "start": 910442,
    "end": 914450,
    "text": "すべての圧縮機とすべての予測機の間には1対1の対応がある。"
  },
  {
    "start": 915910,
    "end": 928354,
    "text": "しかし、教師なし学習について考えるという目的においては、圧縮という言語にはいくつかの利点があると私は主張したい。"
  },
  {
    "start": 928482,
    "end": 930520,
    "text": "少なくとも私にとってはそうだった。"
  },
  {
    "start": 930970,
    "end": 932760,
    "text": "おそらく、あなたにとってもそうだろう。"
  },
  {
    "start": 936090,
    "end": 941670,
    "text": "次のような思考実験を考えてみよう。"
  },
  {
    "start": 941830,
    "end": 944650,
    "text": "この思考実験が最も重要なスライドだ。"
  },
  {
    "start": 945870,
    "end": 965700,
    "text": "大きな巨大ハードディスクに2つのデータセット、xとyがあり、2つのデータセット、2つのファイルがあり、データを取り込んで圧縮されたオブジェクトを出力する非常に優れた圧縮アルゴリズムcがあるとする。"
  },
  {
    "start": 967990,
    "end": 975938,
    "text": "例えば、xとyを連結して圧縮する場合、2つのデータセットを取って連結し、コンプレッサーに送る。"
  },
  {
    "start": 976034,
    "end": 977160,
    "text": "どうなるのだろうか？"
  },
  {
    "start": 978170,
    "end": 979318,
    "text": "まあ、見てみよう。"
  },
  {
    "start": 979484,
    "end": 979766,
    "text": "何が？"
  },
  {
    "start": 979788,
    "end": 985240,
    "text": "特に重要なのは、十分に優れたコンプレッサーは何をするのかということだ。"
  },
  {
    "start": 987390,
    "end": 997450,
    "text": "私の答えは、非常に直感的なものだが、xの中に存在するパターンを使ってyを圧縮し、その逆もしかり。"
  },
  {
    "start": 998510,
    "end": 1004246,
    "text": "予測についても同じ主張ができるが、圧縮について言う方がなぜか直感的だ。"
  },
  {
    "start": 1004358,
    "end": 1007278,
    "text": "なぜそうなのかはわからないが、私はそうだと思う。"
  },
  {
    "start": 1007444,
    "end": 1008910,
    "text": "それが手がかりだ。"
  },
  {
    "start": 1009250,
    "end": 1028066,
    "text": "もし圧縮が十分で、本当に優れた圧縮コンプレッサーのようなものであれば、巨大なファイルを連結したときの圧縮率は、2つのファイルを別々に圧縮したときよりも悪くならないはずだ。"
  },
  {
    "start": 1028258,
    "end": 1037790,
    "text": "連結によって得られた追加の圧縮は、コンプレッサーが気づいたある種の共有構造である。"
  },
  {
    "start": 1037970,
    "end": 1041450,
    "text": "コンプレッサーが優れていればいるほど、より多くの共有構造を抽出することができる。"
  },
  {
    "start": 1041870,
    "end": 1046060,
    "text": "このギャップが共有構造、あるいはアルゴリズム的な相互情報である。"
  },
  {
    "start": 1048510,
    "end": 1050202,
    "text": "面白いだろ？"
  },
  {
    "start": 1050336,
    "end": 1052014,
    "text": "私が何を言いたいかわかるだろう。"
  },
  {
    "start": 1052132,
    "end": 1054506,
    "text": "Yはあなたが監督したタスクのデータである。"
  },
  {
    "start": 1054618,
    "end": 1056682,
    "text": "Xはあなたの監督されていないタスクである。"
  },
  {
    "start": 1056826,
    "end": 1064420,
    "text": "突然、yを助けようとするxのパターンについて、数学的な理由や情報を持っている。"
  },
  {
    "start": 1066310,
    "end": 1069810,
    "text": "分配マッチングがどのように一般化されているのかにも注目してほしい。"
  },
  {
    "start": 1070390,
    "end": 1096060,
    "text": "xが言語1でyが言語2であるような分布マッチングのケースで、一方の分布を他方の分布に変換する単純な関数fが存在することを知っているとする。"
  },
  {
    "start": 1100690,
    "end": 1102080,
    "text": "とてもクールだと思う。"
  },
  {
    "start": 1102770,
    "end": 1104190,
    "text": "私たちは輪を閉じた。"
  },
  {
    "start": 1107330,
    "end": 1109214,
    "text": "では、どのように正式化するのか？"
  },
  {
    "start": 1109332,
    "end": 1112080,
    "text": "教師なし学習の形式化とは？"
  },
  {
    "start": 1113650,
    "end": 1115220,
    "text": "できるかどうかやってみよう。"
  },
  {
    "start": 1116310,
    "end": 1119182,
    "text": "MLアルゴリズムを考えてみよう。"
  },
  {
    "start": 1119326,
    "end": 1125990,
    "text": "ところで、以下では少し杜撰になるが、圧縮と予測を同じ意味で使うことにする。"
  },
  {
    "start": 1127290,
    "end": 1129510,
    "text": "機械学習アルゴリズムがあるとする。"
  },
  {
    "start": 1129580,
    "end": 1136038,
    "text": "これはアルゴリズムaで、yを圧縮してxにアクセスしようとする。"
  },
  {
    "start": 1136204,
    "end": 1140666,
    "text": "Xはファイル番号1、Yはファイル番号2である。"
  },
  {
    "start": 1140848,
    "end": 1149082,
    "text": "機械学習アルゴリズム、つまりコンプレッサーにyを圧縮させたい。"
  },
  {
    "start": 1149216,
    "end": 1151440,
    "text": "目標はyを可能な限り圧縮することだ。"
  },
  {
    "start": 1152370,
    "end": 1157178,
    "text": "私たちは、この特別なアルゴリズムを使って後悔していることは何かと自問することができる。"
  },
  {
    "start": 1157354,
    "end": 1159040,
    "text": "何が言いたいかわかる？"
  },
  {
    "start": 1160530,
    "end": 1162250,
    "text": "何に対する後悔？"
  },
  {
    "start": 1162420,
    "end": 1174580,
    "text": "私が十分に良い仕事をすれば、後悔が少ないということは、このラベルのないデータから得られる可能性のあるすべての助けを得たということだ。"
  },
  {
    "start": 1175110,
    "end": 1179106,
    "text": "このラベルのないデータは可能な限り私を助けてくれたし、悪い気はしない。"
  },
  {
    "start": 1179128,
    "end": 1179734,
    "text": "悪いとは思っていない。"
  },
  {
    "start": 1179772,
    "end": 1186022,
    "text": "私は、より優れた圧縮アルゴリズムを持つ他の誰かが使うことができたであろう予測値を、テーブルの上に残してしまったような気がしてならない。"
  },
  {
    "start": 1186156,
    "end": 1187400,
    "text": "そういうことだ。"
  },
  {
    "start": 1189370,
    "end": 1198890,
    "text": "特に、ラベリングされていないデータの中に、自分のタスクに役立つ情報があれば、夜も安心して眠ることができる。"
  },
  {
    "start": 1200750,
    "end": 1202750,
    "text": "私ほどいい仕事をした者はいない。"
  },
  {
    "start": 1202820,
    "end": 1206080,
    "text": "私は、ラベルのないデータから恩恵を受けるという最高の仕事をしてきた。"
  },
  {
    "start": 1207810,
    "end": 1211550,
    "text": "これは教師なし学習を考えるための一歩だと思う。"
  },
  {
    "start": 1211700,
    "end": 1216958,
    "text": "教師なしデータセットが実際に有用かどうかはわからない。"
  },
  {
    "start": 1217054,
    "end": 1218174,
    "text": "超便利かもしれない。"
  },
  {
    "start": 1218222,
    "end": 1222370,
    "text": "答えがあるかもしれないし、まったく役に立たないかもしれないし、一様分布かもしれない。"
  },
  {
    "start": 1223590,
    "end": 1229974,
    "text": "もし後悔の少ない教師なし学習アルゴリズムがあれば、それが最初のケースか2番目のケースかを言うことができる。"
  },
  {
    "start": 1230092,
    "end": 1238540,
    "text": "私はベストを尽くしたと思っているし、ラベルのないデータから利益を得ることにベストを尽くしたと思っている。"
  },
  {
    "start": 1240670,
    "end": 1245734,
    "text": "さて、少し曖昧な理論ランドに寄り道してみたい。"
  },
  {
    "start": 1245862,
    "end": 1255706,
    "text": "コルモゴロフ複雑性は、究極の圧縮機として、究極の後悔の少ないアルゴリズムを与えてくれます。"
  },
  {
    "start": 1255818,
    "end": 1258382,
    "text": "私が何を言いたいのか、すぐにわかるだろう。"
  },
  {
    "start": 1258516,
    "end": 1263870,
    "text": "では、コマゴロフについてだが、まず、ここでコマゴロフの複雑性を知っている人はいるだろうか？"
  },
  {
    "start": 1264930,
    "end": 1266606,
    "text": "オーケー、約50％だ。"
  },
  {
    "start": 1266788,
    "end": 1267520,
    "text": "そうだね。"
  },
  {
    "start": 1269170,
    "end": 1273250,
    "text": "コモゴロフの複雑性は、1分で簡単に説明できる類のものだ。"
  },
  {
    "start": 1274230,
    "end": 1275846,
    "text": "そうしよう"
  },
  {
    "start": 1276028,
    "end": 1290246,
    "text": "僕がデータを渡すか、あるいは君が僕にデータを渡して、僕がそれを圧縮して、可能な限り最短のプログラムを渡すとしよう。"
  },
  {
    "start": 1290358,
    "end": 1295900,
    "text": "存在するプログラムの中で、実行すればデータを出力する最短のプログラム。"
  },
  {
    "start": 1297390,
    "end": 1298730,
    "text": "誤字がある。"
  },
  {
    "start": 1300350,
    "end": 1301290,
    "text": "誤字。"
  },
  {
    "start": 1304290,
    "end": 1305760,
    "text": "その通りだ。"
  },
  {
    "start": 1306850,
    "end": 1308080,
    "text": "そうだね。"
  },
  {
    "start": 1310370,
    "end": 1313860,
    "text": "これはxを出力する最短プログラムの長さである。"
  },
  {
    "start": 1314550,
    "end": 1315300,
    "text": "そうだ。"
  },
  {
    "start": 1316150,
    "end": 1332054,
    "text": "直感的に、このコンプレッサーがかなり優秀であることがわかるだろう。この定理を証明するのも実に簡単で、証明しやすいというか、実感しやすい。"
  },
  {
    "start": 1332172,
    "end": 1335170,
    "text": "一度それを感じれば、それを証明するのは簡単だと信じてくれるだろう。"
  },
  {
    "start": 1335330,
    "end": 1346074,
    "text": "コルマゴロフ・コンプレッサーを使って弦楽器を圧縮する場合、基本的に圧縮の質について後悔することはほとんどないと言えるでしょう。"
  },
  {
    "start": 1346272,
    "end": 1347690,
    "text": "この結果は証明できる。"
  },
  {
    "start": 1347840,
    "end": 1361450,
    "text": "これは、文字列xとデータセット・データベースがあった場合、xを出力する最短のプログラムが、コンプレッサーが必要とする出力よりも短いというものだ。"
  },
  {
    "start": 1361610,
    "end": 1375122,
    "text": "しかしまあ、あなたのコンプレッサーはデータを圧縮し、さらに何文字のコードであろうと、ちょっとした用語を加えたのだから、直感的にコンプレッサーを実装する必要があり、それがいかに理にかなっているかわかるだろう。"
  },
  {
    "start": 1375256,
    "end": 1376654,
    "text": "シミュレーションの議論。"
  },
  {
    "start": 1376782,
    "end": 1377074,
    "text": "そうだろう？"
  },
  {
    "start": 1377112,
    "end": 1378046,
    "text": "シミュレーションの議論。"
  },
  {
    "start": 1378078,
    "end": 1380918,
    "text": "このコンプレッサーはすごいんだ。"
  },
  {
    "start": 1381004,
    "end": 1383960,
    "text": "クールと言いたいところだが、コンピューター・プログラムは付いているのか？"
  },
  {
    "start": 1384650,
    "end": 1386934,
    "text": "このコンピュータープログラムをkに渡してくれる？"
  },
  {
    "start": 1387132,
    "end": 1393370,
    "text": "Kはコンプレッサーを作動させ、コンピュータープログラムを作動させる。"
  },
  {
    "start": 1393710,
    "end": 1399430,
    "text": "詳細は省くが、複雑さのコルマゴールを感じてもらえたと思う。"
  },
  {
    "start": 1399510,
    "end": 1403450,
    "text": "コンプレッサーのコルマゴールは、他のコンピュータープログラムをシミュレートしたり、他のコンプレッサーをシミュレートすることができる。"
  },
  {
    "start": 1403530,
    "end": 1405322,
    "text": "これが計算できない理由でもある。"
  },
  {
    "start": 1405466,
    "end": 1411550,
    "text": "すべてのコンピュータープログラムをシミュレートする自由を大いに感じているからだ。"
  },
  {
    "start": 1413510,
    "end": 1416670,
    "text": "現存する最高のコンプレッサーだ。"
  },
  {
    "start": 1416830,
    "end": 1420466,
    "text": "私たちは教師なし学習のための優れた圧縮について話していた。"
  },
  {
    "start": 1420568,
    "end": 1425534,
    "text": "ここで、アコルマゴロフの複雑性を一般化してみよう。"
  },
  {
    "start": 1425582,
    "end": 1435456,
    "text": "アコルマゴロフは、サイドの情報をより詳細に使用することを認めている。"
  },
  {
    "start": 1435568,
    "end": 1441092,
    "text": "この点は重要なので、何度も繰り返して言う。"
  },
  {
    "start": 1441226,
    "end": 1446868,
    "text": "明らかにコモゴロフ・コンプレッサーは計算できない。"
  },
  {
    "start": 1446964,
    "end": 1450676,
    "text": "全体的なプログラムを検索するようなものだからね。"
  },
  {
    "start": 1450788,
    "end": 1462376,
    "text": "100層からなるニューラルネットのパラメータに対してSGDを実行すると、一定のメモリ量とステップ数を持つコンピューター上で自動的にプログラム検索を行うようなものだということをご存知ですか？"
  },
  {
    "start": 1462488,
    "end": 1468930,
    "text": "ニューラルネットをミクロ、ミクロ、ミクロにフィットさせるようなものだ。"
  },
  {
    "start": 1469300,
    "end": 1471920,
    "text": "フィーリングが似ているんだ。"
  },
  {
    "start": 1473300,
    "end": 1475010,
    "text": "魔法みたいでしょ？"
  },
  {
    "start": 1477320,
    "end": 1479216,
    "text": "ニューラルネットワークは小さなプログラムをシミュレートすることができる。"
  },
  {
    "start": 1479248,
    "end": 1480304,
    "text": "小さなコンピューターだ。"
  },
  {
    "start": 1480352,
    "end": 1481216,
    "text": "それらは回路だ。"
  },
  {
    "start": 1481328,
    "end": 1483536,
    "text": "回路はコンピューターであり、計算機である。"
  },
  {
    "start": 1483648,
    "end": 1502810,
    "text": "SGDはプログラムを検索し、ディープラーニングはすべてSGDの奇跡の上に成り立っている。"
  },
  {
    "start": 1503820,
    "end": 1509100,
    "text": "したがって、コルモボール・コンプレッサーのミニチュアを計算することができる。"
  },
  {
    "start": 1511360,
    "end": 1515020,
    "text": "シミュレーションの議論はここでも当てはまる。"
  },
  {
    "start": 1515090,
    "end": 1524108,
    "text": "より優れたニューラルネットワーク・アーキテクチャーを設計しようとしたことがあるかどうかはわからないが、より優れたニューラルネットワーク・アーキテクチャーを見つけるのは難しいということがわかるだろう。"
  },
  {
    "start": 1524204,
    "end": 1528576,
    "text": "このコネクションを追加して、あのコネクションを追加して、あれもこれも変更しよう、と。"
  },
  {
    "start": 1528678,
    "end": 1530864,
    "text": "シミュレーションはなぜ難しいのか？"
  },
  {
    "start": 1530912,
    "end": 1536768,
    "text": "というのも、新しいアーキテクチャは、そうでない場合を除き、古いアーキテクチャによってかなりストレートにシミュレートできるからだ。"
  },
  {
    "start": 1536864,
    "end": 1538080,
    "text": "それはまれなケースだ。"
  },
  {
    "start": 1538160,
    "end": 1543748,
    "text": "まれに、小さなRNNからトランスフォーマーに切り替えたときなど、大きな改善が見られることがある。"
  },
  {
    "start": 1543844,
    "end": 1547690,
    "text": "RNNには隠れ状態というボトルネックがある。"
  },
  {
    "start": 1548060,
    "end": 1550404,
    "text": "トランスフォーマーの実装には苦労している。"
  },
  {
    "start": 1550532,
    "end": 1559070,
    "text": "しかし、もし非常に大きな隠れ状態を持つRNNを設計する方法が見つかっていたら、おそらくまた変圧器と同じように使えるようになるだろう。"
  },
  {
    "start": 1560160,
    "end": 1562252,
    "text": "これがそのリンクだ。"
  },
  {
    "start": 1562306,
    "end": 1570800,
    "text": "フォーマルな土地からニューラルネットワークの土地へと、どのように切り替わっていくのかが見えてくる。"
  },
  {
    "start": 1572660,
    "end": 1577570,
    "text": "教師なし学習の解決策としての条件付き共通複雑度。"
  },
  {
    "start": 1578740,
    "end": 1586452,
    "text": "似たような定理で、xが与えられたときのYのkを定義するつもりはない。"
  },
  {
    "start": 1586506,
    "end": 1592950,
    "text": "xを探索することが許されるなら、Yを出力する最短プログラムのようなものだ。"
  },
  {
    "start": 1593660,
    "end": 1597768,
    "text": "これは、xを探索することが許された場合にYを出力する最短プログラムの長さである。"
  },
  {
    "start": 1597854,
    "end": 1599496,
    "text": "同じ結果を証明できる。"
  },
  {
    "start": 1599678,
    "end": 1605244,
    "text": "そう、これが教師なし学習に対する解決策なのだ。"
  },
  {
    "start": 1605442,
    "end": 1613070,
    "text": "それを使えば、教師なし学習を自分よりうまくやる人はいないとわかっているので、とてもとても安全に、夜ぐっすり眠ることができる。"
  },
  {
    "start": 1613600,
    "end": 1614350,
    "text": "そうだろう？"
  },
  {
    "start": 1615040,
    "end": 1616216,
    "text": "文字通りの意味だ。"
  },
  {
    "start": 1616338,
    "end": 1622000,
    "text": "これは教師なし学習の究極の後悔の少ない解決策だが、計算不可能であることを除けば。"
  },
  {
    "start": 1622340,
    "end": 1624400,
    "text": "便利なフレームワークだとは思う。"
  },
  {
    "start": 1625140,
    "end": 1628230,
    "text": "ここでは例ではなく、データセットを条件とする。"
  },
  {
    "start": 1629400,
    "end": 1634564,
    "text": "これはyを予測するためにxからすべての値を抽出する。"
  },
  {
    "start": 1634682,
    "end": 1635636,
    "text": "データセット"
  },
  {
    "start": 1635738,
    "end": 1637670,
    "text": "データセットであって、例ではない。"
  },
  {
    "start": 1638280,
    "end": 1640744,
    "text": "これが教師なし学習の解決策だ。"
  },
  {
    "start": 1640942,
    "end": 1641690,
    "text": "完了した。"
  },
  {
    "start": 1642620,
    "end": 1643370,
    "text": "成功だ。"
  },
  {
    "start": 1646780,
    "end": 1665264,
    "text": "この条件付きコルモゴロフ複雑性というのは、コンプレッサーがあるものを圧縮しようとする一方で、別のものにもアクセスできるという話ですね。"
  },
  {
    "start": 1665462,
    "end": 1679580,
    "text": "これは、ビッグデータセットのフィッティングを気にする場合、機械学習の文脈では少々不自然である。"
  },
  {
    "start": 1679660,
    "end": 1685748,
    "text": "ビッグデータセットのコンディショニングに本当に良い方法はない、と言ってもいいと思う。"
  },
  {
    "start": 1685834,
    "end": 1690120,
    "text": "大きなデータセットを適合させることはできても、それを条件づけることはできない。"
  },
  {
    "start": 1690540,
    "end": 1711980,
    "text": "この結果は、もしあなたが教師付きタスクyに関する予測を行うことに関心があるのなら、xとyの連結を圧縮するだけの古き良きコルモラフ・コンプレッサーを使っても、条件付きコンプレッサーを使うのと同じくらい良い結果が得られるということを示している。"
  },
  {
    "start": 1712400,
    "end": 1721664,
    "text": "私が今言ったことには、さらに詳細やいくつかの微妙な点があり、それについて興味のある人がいれば、喜んでオフラインで話すつもりだ。"
  },
  {
    "start": 1721782,
    "end": 1732550,
    "text": "前のスライドでは、教師なし学習を解決するために条件付きクロマゴロフ・コンプレッサーを使うことができると言いましたが、この条件付きクロマゴロフ・コンプレッサーは、教師なし学習を解決するために条件付きクロマゴロフ・コンプレッサーを使うことができます。"
  },
  {
    "start": 1733320,
    "end": 1736432,
    "text": "つまり、普段使っているクロマゴロフ・コンプレッサーも使えるということだ。"
  },
  {
    "start": 1736496,
    "end": 1745050,
    "text": "すべてのデータを放り込み、すべてのファイルを取り出し、それらを連結し、圧縮するだけで、監督されたタスク、つまりあなたが気にかけているタスクについて、素晴らしい予測をすることができる。"
  },
  {
    "start": 1746700,
    "end": 1748888,
    "text": "なぜそうなのかについては、いくつかの直感がある。"
  },
  {
    "start": 1748974,
    "end": 1753412,
    "text": "この結果は、実は少しトリッキーであることを証明しているようなものだ。"
  },
  {
    "start": 1753556,
    "end": 1754890,
    "text": "私はやらない。"
  },
  {
    "start": 1756860,
    "end": 1762300,
    "text": "教師なし学習の解決策は、カンマグラフの複雑さをコモグラフ圧縮機にすべて委ねることだ。"
  },
  {
    "start": 1764000,
    "end": 1773564,
    "text": "最後に、このような関節の圧迫は、オーバーフィットをしなければ最大限の可能性があることを述べておこう。"
  },
  {
    "start": 1773692,
    "end": 1780736,
    "text": "データセットがある場合、パラメータが与えられたときの尤度の合計が、データセットを圧縮するコストとなる。"
  },
  {
    "start": 1780838,
    "end": 1783680,
    "text": "また、パラメータを圧縮するためのコストも必要だ。"
  },
  {
    "start": 1783760,
    "end": 1793000,
    "text": "2つのデータ・セットを圧縮したい場合は、トレーニング・セットとデータ・セットにポイントを追加して、その合計に項を追加すればいい。"
  },
  {
    "start": 1793660,
    "end": 1804216,
    "text": "この連結、つまり結合圧縮は、機械学習の文脈ではとても自然なことで、だからこそ、条件付きコモド複雑性があるように、わざわざ言う価値があったのだ。"
  },
  {
    "start": 1804248,
    "end": 1814204,
    "text": "しかし、レギュラーを使うことで、すべてを圧縮することができる。"
  },
  {
    "start": 1814242,
    "end": 1816028,
    "text": "ホモグロブの複雑さも機能する。"
  },
  {
    "start": 1816194,
    "end": 1818812,
    "text": "だから、これはエレガントだと思う。"
  },
  {
    "start": 1818876,
    "end": 1826896,
    "text": "というのも、目を凝らせば、ニューラルネットワークが何をしているのかが説明できる。"
  },
  {
    "start": 1826998,
    "end": 1832276,
    "text": "大きなニューラルネットワークのsgdは、我々の大きなプログラム検索なんだ。"
  },
  {
    "start": 1832458,
    "end": 1836870,
    "text": "より大きなニューラルネットワークは、コルモゴロフ・コンプレッサーにどんどん近づいていく。"
  },
  {
    "start": 1837480,
    "end": 1846484,
    "text": "コルモゴロフ・コンプレッサーの近寄りがたいアイデアに近づくからだ。"
  },
  {
    "start": 1846532,
    "end": 1856380,
    "text": "私たちは、より大きなニューラルネットを訓練すればするほど、より大きなニューラルネットを訓練すればするほど、予測値を抽出することに関する後悔が少なくなることを望んでいる。"
  },
  {
    "start": 1858080,
    "end": 1867180,
    "text": "さて、これがGPTモデルに適用される方法だが、私はこの理論がGPTモデルにも適用されると主張する。"
  },
  {
    "start": 1867340,
    "end": 1879484,
    "text": "GPTモデルで少し厄介なのは、圧縮やスーパーバイザーに言及することなく、その動作の理論を説明できることだ。"
  },
  {
    "start": 1879532,
    "end": 1882752,
    "text": "いや、それはインターネット上のテキストの条件付き配信に過ぎない、と言うだけだ。"
  },
  {
    "start": 1882896,
    "end": 1883732,
    "text": "融合学習。"
  },
  {
    "start": 1883786,
    "end": 1886272,
    "text": "繰り返されるパターンがある文書を想像してみてほしい。"
  },
  {
    "start": 1886336,
    "end": 1887990,
    "text": "このパターンはおそらく続くだろう。"
  },
  {
    "start": 1888520,
    "end": 1892484,
    "text": "GPTモデルは直感的に説明できる。"
  },
  {
    "start": 1892532,
    "end": 1899396,
    "text": "少なくとも、彼らの核融合の挙動は、この理論を暗示することなく間違いなく説明できる。"
  },
  {
    "start": 1899588,
    "end": 1903772,
    "text": "だから、それがいいと思ったんだ。"
  },
  {
    "start": 1903826,
    "end": 1907820,
    "text": "この理論を直接的に証明するものは他にあるのだろうか？"
  },
  {
    "start": 1911040,
    "end": 1913496,
    "text": "ビジョンのような別の領域を見つけることはできないのだろうか？"
  },
  {
    "start": 1913688,
    "end": 1916080,
    "text": "ビジョンにはピクセルがあるのだから。"
  },
  {
    "start": 1917860,
    "end": 1922210,
    "text": "ピクセルの上でこれを行うことで、教師なし学習がうまくいくことを示せますか？"
  },
  {
    "start": 1923860,
    "end": 1926130,
    "text": "答えはイエスだ。"
  },
  {
    "start": 1926740,
    "end": 1934580,
    "text": "これはIGPTと呼ばれるもので、高価な概念実証だ。"
  },
  {
    "start": 1935480,
    "end": 1937456,
    "text": "実践的な手順という意味ではなかった。"
  },
  {
    "start": 1937488,
    "end": 1943336,
    "text": "これは、本当に優れた次のステップの予測ツールがあれば、教師なし学習がうまくいくことを示した論文という意味だ。"
  },
  {
    "start": 1943438,
    "end": 1944964,
    "text": "画像領域で証明された。"
  },
  {
    "start": 1945012,
    "end": 1945780,
    "text": "画像のドメイン。"
  },
  {
    "start": 1945860,
    "end": 1948456,
    "text": "そこで、私はそれを綴ることにする。"
  },
  {
    "start": 1948558,
    "end": 1954940,
    "text": "画像をピクセルの列に変換する。"
  },
  {
    "start": 1955280,
    "end": 1964708,
    "text": "各ピクセルに強度の離散値を与え、同じ変換器で次のピクセルを予測するだけでよい。"
  },
  {
    "start": 1964904,
    "end": 1965920,
    "text": "それだけだ。"
  },
  {
    "start": 1966070,
    "end": 1972480,
    "text": "BErtとは異なり、次のトークンを予測するだけなので、尤度が最大になり、圧縮される。"
  },
  {
    "start": 1976420,
    "end": 1979350,
    "text": "すぐに結果が出るようなものだ。"
  },
  {
    "start": 1981720,
    "end": 1983204,
    "text": "これはCFR10の結果である。"
  },
  {
    "start": 1983242,
    "end": 1984752,
    "text": "さまざまなサイズのモデルがある。"
  },
  {
    "start": 1984816,
    "end": 1986244,
    "text": "これが彼らの次のステップだ。"
  },
  {
    "start": 1986282,
    "end": 1990752,
    "text": "教師なし学習タスクにおけるピクセル予測タスクの予測精度。"
  },
  {
    "start": 1990816,
    "end": 1992784,
    "text": "これがリニアプローブの精度である。"
  },
  {
    "start": 1992832,
    "end": 1993408,
    "text": "リニアプローブ。"
  },
  {
    "start": 1993424,
    "end": 1998984,
    "text": "ニューラルネットの中のあるレイヤーを選ぶと、最適なレイヤーが線形分類器に当てはまり、その結果を見ることができる。"
  },
  {
    "start": 1999102,
    "end": 2002164,
    "text": "きれいなカーブを描くと、だんだん似てくる。"
  },
  {
    "start": 2002302,
    "end": 2003628,
    "text": "あなたが望んでいるようなものだ。"
  },
  {
    "start": 2003714,
    "end": 2005310,
    "text": "まるで効いているみたいだ。"
  },
  {
    "start": 2005760,
    "end": 2011804,
    "text": "次の単語予測、次のピックと同じ種類のピクセル予測。"
  },
  {
    "start": 2011842,
    "end": 2022450,
    "text": "単なるピクセルの予測ではなく、次のピクセルの予測が、より良い教師なし学習につながるのだ。"
  },
  {
    "start": 2023940,
    "end": 2025424,
    "text": "とてもクールだと思った。"
  },
  {
    "start": 2025542,
    "end": 2026576,
    "text": "試してみた。"
  },
  {
    "start": 2026598,
    "end": 2035780,
    "text": "私たちはそれをさまざまな程度にスケールアップし、実際に、私たちが近づいたところではそれなりにうまく学習することを示した。"
  },
  {
    "start": 2035850,
    "end": 2042368,
    "text": "私たちの教師なし学習と、imagenetのその日最高の教師なし学習との間のギャップは、完全に一致しなかった。"
  },
  {
    "start": 2042464,
    "end": 2056172,
    "text": "というのも、こういったものは大きな高解像度の画像を使うのに、私たちは64×64の小さな画像を巨大なトランスフォーマーと一緒に使っているからだ。"
  },
  {
    "start": 2056226,
    "end": 2059164,
    "text": "今の基準からすれば小さなものだが、当時としては巨大なものだった。"
  },
  {
    "start": 2059282,
    "end": 2060860,
    "text": "60億のパラメータ。"
  },
  {
    "start": 2062080,
    "end": 2074210,
    "text": "大きな画像データセットに対して教師なし次ピクセル予測を行い、imagenet上で線形プローブをフィットさせ、強力な結果を得る。"
  },
  {
    "start": 2075940,
    "end": 2076652,
    "text": "CFAR。"
  },
  {
    "start": 2076716,
    "end": 2077328,
    "text": "そうだね。"
  },
  {
    "start": 2077494,
    "end": 2081350,
    "text": "素晴らしいことのひとつは、これでCFAR10で99％を獲得したことだ。"
  },
  {
    "start": 2081960,
    "end": 2083190,
    "text": "クールだったよ。"
  },
  {
    "start": 2086040,
    "end": 2090070,
    "text": "2020年、違う時代だったけど、クールだったよ。"
  },
  {
    "start": 2092380,
    "end": 2096360,
    "text": "最後に、線形表現について2、3のコメントをしておこう。"
  },
  {
    "start": 2099180,
    "end": 2107068,
    "text": "私が圧縮理論を好きなのは、教師なし学習について厳密に考えることができないということが長い間気になっていたからだ。"
  },
  {
    "start": 2107154,
    "end": 2110056,
    "text": "私は今、少なくとも部分的にはできると主張している。"
  },
  {
    "start": 2110168,
    "end": 2114430,
    "text": "それでも、たくさんの手を振ることは必要だが、以前よりは少し減ったかもしれない。"
  },
  {
    "start": 2114960,
    "end": 2118444,
    "text": "には、なぜ表現が線形分離可能でなければならないのかが書かれていない。"
  },
  {
    "start": 2118492,
    "end": 2120690,
    "text": "なぜリニアプローブが起こるのかは書かれていない。"
  },
  {
    "start": 2121700,
    "end": 2128056,
    "text": "直線的な表現は常に起こるものであり、それが形成される理由は深く深遠なものでなければならない。"
  },
  {
    "start": 2128108,
    "end": 2141796,
    "text": "私が面白いと思ったのは、次のピクセル予測モデルの自己回帰モデルは、バートよりも線形表現が優れているようだということです。"
  },
  {
    "start": 2141988,
    "end": 2146692,
    "text": "青のように、青の精度はバート対自己回帰的である。"
  },
  {
    "start": 2146836,
    "end": 2148168,
    "text": "それがなぜなのかは分からない。"
  },
  {
    "start": 2148254,
    "end": 2150004,
    "text": "というか、推測はできる。"
  },
  {
    "start": 2150052,
    "end": 2160140,
    "text": "いくつかの推測はあるが、なぜそのような直線的な表現が形成されるのか、もっと理解を深めてもいいと思う。"
  },
  {
    "start": 2161440,
    "end": 2163884,
    "text": "そして、ああ、これで終わりだ。"
  },
  {
    "start": 2164002,
    "end": 2165520,
    "text": "ご清聴ありがとうございました。"
  },
  {
    "start": 2172420,
    "end": 2174572,
    "text": "推測を述べていただけますか？"
  },
  {
    "start": 2174716,
    "end": 2183168,
    "text": "ええ、基本的には、次のピクセルを予測するということは、前のすべてのピクセルから次のピクセルを予測するということだと思います。"
  },
  {
    "start": 2183264,
    "end": 2186768,
    "text": "長期的な構造を見る必要がある。"
  },
  {
    "start": 2186944,
    "end": 2194716,
    "text": "バートでは、ベクターがあり、ピクセルのトークンの25％を落としたとする。"
  },
  {
    "start": 2194768,
    "end": 2202008,
    "text": "この場合、どのような予測も、過去と未来を少し見ることで、かなりうまくいく。"
  },
  {
    "start": 2202174,
    "end": 2207400,
    "text": "難しい予測作業をすべて取り除くか、あるいははるかに簡単にする。"
  },
  {
    "start": 2207480,
    "end": 2214652,
    "text": "次の画素予測における最も難しい予測タスクは、誕生予測のケースにおける最も難しい予測タスクよりもずっと難しい。"
  },
  {
    "start": 2214706,
    "end": 2223996,
    "text": "この議論は、それを検証するための実験をデザインしようとすれば、検証することさえできるかもしれない。"
  },
  {
    "start": 2224018,
    "end": 2226060,
    "text": "でも、それは推測の域を出ない。"
  },
  {
    "start": 2227000,
    "end": 2232900,
    "text": "次ピクセル予測で、よりロバストな2次元バージョンはありますか？"
  },
  {
    "start": 2235240,
    "end": 2237220,
    "text": "より強固になった。"
  },
  {
    "start": 2239420,
    "end": 2252536,
    "text": "ニューラルネットワークを、さまざまな入力に確率を割り当てる確率論的モデルに変えるようなことなら、何でもできる。"
  },
  {
    "start": 2252568,
    "end": 2254636,
    "text": "そうなのか？"
  },
  {
    "start": 2254818,
    "end": 2261800,
    "text": "次のトークンを予測するもうひとつの大きな方法は、拡散モデルである。"
  },
  {
    "start": 2261960,
    "end": 2264544,
    "text": "拡散モデルもそうではない。"
  },
  {
    "start": 2264582,
    "end": 2273980,
    "text": "高画質画像ジェネレーターで使われている拡散モデルは、入力の尤度を最大化するものではない。"
  },
  {
    "start": 2274060,
    "end": 2275164,
    "text": "彼らには別の目的がある。"
  },
  {
    "start": 2275212,
    "end": 2280448,
    "text": "最も独創的な定式化は、尤度の最大化である。"
  },
  {
    "start": 2280624,
    "end": 2283780,
    "text": "ところで、拡散モデルは反論である。"
  },
  {
    "start": 2286120,
    "end": 2297320,
    "text": "拡散モデルもまた、birdがそうでないのと同じ理由で、xトークン予測モデルよりも表現が悪くなるはずだと私は主張したい。"
  },
  {
    "start": 2297740,
    "end": 2304590,
    "text": "このことがさらに、私の中では、リニアな表現が形成されるに至った原因についての謎を深めている。"
  },
  {
    "start": 2307920,
    "end": 2309212,
    "text": "ああ、話してくれてありがとう。"
  },
  {
    "start": 2309266,
    "end": 2315896,
    "text": "私はクロモグラフの複雑さとニューラルネットワークのアナロジーが好きだ。"
  },
  {
    "start": 2316008,
    "end": 2321420,
    "text": "類似性がないように思えるのは、ニューラルネットワークにはトレーニングダイナミクスがあることだ。"
  },
  {
    "start": 2321500,
    "end": 2330656,
    "text": "コンピュータのプログラムをすべて取り上げるようなことをすれば、よりメモリレスで複雑なものになる。"
  },
  {
    "start": 2330848,
    "end": 2332244,
    "text": "データの順番は関係ない。"
  },
  {
    "start": 2332282,
    "end": 2335008,
    "text": "ニューラルネットでは明らかにそうだ。"
  },
  {
    "start": 2335184,
    "end": 2342250,
    "text": "トレーニングの初期に学習された単純なヒューリスティックや特徴が、トレーニングの後半になっても残っているのだろう。"
  },
  {
    "start": 2343100,
    "end": 2349160,
    "text": "だから、この写真の文脈でそれをどう考えるか、あなたは考えていると思う。"
  },
  {
    "start": 2349580,
    "end": 2352488,
    "text": "そう、つまり、この例えは完璧ではないんだ。"
  },
  {
    "start": 2352574,
    "end": 2353368,
    "text": "その通りだ。"
  },
  {
    "start": 2353454,
    "end": 2359688,
    "text": "あなたの説明では、この例えが最も破綻しているのは、検索手順だ。"
  },
  {
    "start": 2359784,
    "end": 2360092,
    "text": "そうだろう？"
  },
  {
    "start": 2360146,
    "end": 2361564,
    "text": "これこそ、あなたが暗示していることだ。"
  },
  {
    "start": 2361602,
    "end": 2363624,
    "text": "データの順番は重要だ。"
  },
  {
    "start": 2363752,
    "end": 2364140,
    "text": "なぜですか？"
  },
  {
    "start": 2364210,
    "end": 2367180,
    "text": "なぜなら、私たちは弱い探索手順を使っているからだ。"
  },
  {
    "start": 2368000,
    "end": 2374830,
    "text": "Colmagorvコンプレッサーは、毎回ゼロからすべてを列挙するという無限にコストのかかる検索手順を使用する。"
  },
  {
    "start": 2376240,
    "end": 2402080,
    "text": "だから、この特別な例えには注意が必要だ。"
  },
  {
    "start": 2402500,
    "end": 2405460,
    "text": "万能ではないことは明らかだ。"
  },
  {
    "start": 2405880,
    "end": 2413830,
    "text": "教師なし学習がどこから来たのかを説明することには価値があると思う。"
  },
  {
    "start": 2419740,
    "end": 2430296,
    "text": "圧縮と次のビット予測の間を行ったり来たりすることは、教師なし学習と教師あり学習を関連づけることにつながる。"
  },
  {
    "start": 2430478,
    "end": 2443324,
    "text": "クリプトパシーから話を戻すと、この種の理論は80年代までさかのぼる。"
  },
  {
    "start": 2443452,
    "end": 2450210,
    "text": "予測できるアルゴリズムがあるのなら、できるアルゴリズムがあることになる。"
  },
  {
    "start": 2451300,
    "end": 2452688,
    "text": "暗号はその逆だ。"
  },
  {
    "start": 2452774,
    "end": 2459910,
    "text": "比較のしようがない、だからしようがないと言っているのだろう。"
  },
  {
    "start": 2461800,
    "end": 2466100,
    "text": "見分けがつくというこの考え方は、どのようなトランスフォームが可能なのだろうか。"
  },
  {
    "start": 2466170,
    "end": 2468360,
    "text": "それは自然なことだろうか？"
  },
  {
    "start": 2469740,
    "end": 2476120,
    "text": "質問は理解できたと思う。"
  },
  {
    "start": 2476270,
    "end": 2488392,
    "text": "私はあなたがおっしゃった分野にはあまり詳しくないので言い換えますが、あなたは、もし分布を区別することができれば、その区別可能性を使って予測することができると言っているのですね。"
  },
  {
    "start": 2488536,
    "end": 2495070,
    "text": "という質問だろう。"
  },
  {
    "start": 2498820,
    "end": 2502704,
    "text": "そこで質問なのだが、あなたは自分の言っていることをどの程度予測できるのだろうか？"
  },
  {
    "start": 2502742,
    "end": 2503908,
    "text": "少しは予測できる？"
  },
  {
    "start": 2503994,
    "end": 2505510,
    "text": "それがあなたの興味ですか？"
  },
  {
    "start": 2506600,
    "end": 2509910,
    "text": "例えば、次のビットがどうなるかを予測することができる。"
  },
  {
    "start": 2512200,
    "end": 2528192,
    "text": "そう、つまり、関連するものをひとつ挙げるとすれば、エネルギー・ベースのモデルだ。"
  },
  {
    "start": 2528356,
    "end": 2544652,
    "text": "エネルギー・ベースのモデルは、ニューラルネットワークを確率分布に変えるもうひとつの方法を提供する。"
  },
  {
    "start": 2544716,
    "end": 2546672,
    "text": "であれば、それらすべてを正規化する。"
  },
  {
    "start": 2546806,
    "end": 2555920,
    "text": "特にエネルギーに基づくモデルに関しては、分布の比率がエネルギーの違いに対応するような気がする。"
  },
  {
    "start": 2556070,
    "end": 2558308,
    "text": "もしかしたら、あなたが言っていることと何か関係があるのかもしれない。"
  },
  {
    "start": 2558474,
    "end": 2565780,
    "text": "あなたがおっしゃったことについては、おそらく的確なコメントにはなっていないと思いますが、残念ながら、これ以上付け加えることはないと思います。"
  },
  {
    "start": 2571100,
    "end": 2575560,
    "text": "VCディメンジョンの名誉を守るために、ちょっとだけ言葉を添えておこうかな。"
  },
  {
    "start": 2577180,
    "end": 2582316,
    "text": "2007年、私は量子状態のパック学習に関する定理を持っていた。"
  },
  {
    "start": 2582418,
    "end": 2598720,
    "text": "これはまさに、無限精度を気にしなくても、状態空間を離散化しても、VC次元やファットチャタリング次元などを見ると、仮説クラスの大きさの対数よりも指数関数的に小さくなる例でした。"
  },
  {
    "start": 2598870,
    "end": 2602400,
    "text": "それでもBCディメンションが欲しいケースはあると思う。"
  },
  {
    "start": 2603240,
    "end": 2604710,
    "text": "クールな例だ。"
  },
  {
    "start": 2608840,
    "end": 2613472,
    "text": "私はあなたの表記に完全に従ったわけではない。"
  },
  {
    "start": 2613616,
    "end": 2618168,
    "text": "キャピタルXは分布データからのサンプルである。"
  },
  {
    "start": 2618254,
    "end": 2631084,
    "text": "トランスフォーマーのSGDは、一度に1つのサンプルしか与えられないので、圧縮に最適なプログラムとは言えないかもしれません。"
  },
  {
    "start": 2631122,
    "end": 2631580,
    "text": "その通りだ。"
  },
  {
    "start": 2631650,
    "end": 2633468,
    "text": "もうひとつ、私たちが想定していることがある。"
  },
  {
    "start": 2633554,
    "end": 2648530,
    "text": "トレーニングの総損失に目を向けると、変換器である必要はないが、データに対数の確率を割り当てるニューラルネットワークがあるとする。"
  },
  {
    "start": 2649060,
    "end": 2666100,
    "text": "大量の訓練事例があれば、ニューラルネットを走らせることができ、いくつかのニューラルネットがあれば、それぞれの事例について対数確率を計算し、その合計をとれば、そのニューラルネットがデータセット全体に割り当てる対数確率になる。"
  },
  {
    "start": 2666250,
    "end": 2668384,
    "text": "さて、ニューラルネットワークとは何か。"
  },
  {
    "start": 2668522,
    "end": 2678990,
    "text": "この特殊な定式化によって、ニューラルネットワークは、データの順序における時間的またはいかなる種類の構造にも明示的に気づくことができなくなる。"
  },
  {
    "start": 2680320,
    "end": 2682076,
    "text": "私は今でも、そう言うことに意味があると主張している。"
  },
  {
    "start": 2682098,
    "end": 2698480,
    "text": "そう、データセットの全ロック確率を計算すれば、負の対数確率がわかる。文字通り、このニューラルネットワークをコンプレッサーとして使った場合と比較して、このデータセットを圧縮するのに必要なビット数がわかる。"
  },
  {
    "start": 2702740,
    "end": 2713108,
    "text": "教師なし学習を理解し、動機づけるためのフレームワークとして圧縮を主張するのであればね。"
  },
  {
    "start": 2713274,
    "end": 2728596,
    "text": "というのも、次単語予測はどんなテキストタスクでも次単語予測に変換できるからです。"
  },
  {
    "start": 2728708,
    "end": 2733644,
    "text": "つまり教師なし学習は、表面的にはテキストに対する教師あり学習と同じなのだ。"
  },
  {
    "start": 2733762,
    "end": 2738952,
    "text": "そこで、次のピクセルの予測を立てることができない画像GPTに目を向ける。"
  },
  {
    "start": 2739016,
    "end": 2740920,
    "text": "できるかもしれないが、できないとだけ言っておこう。"
  },
  {
    "start": 2741000,
    "end": 2747170,
    "text": "とすれば、教師なし学習を定式化するのに圧縮が良い方法であることを示す方法として、線形表現を使うことができる。"
  },
  {
    "start": 2747540,
    "end": 2754464,
    "text": "その場合、非常に効果的なコンプレッサーが存在するが、そのコンプレッサーは有益なリニア表現を与えてくれない。"
  },
  {
    "start": 2754592,
    "end": 2770084,
    "text": "教師なし学習と教師あり学習が表面的には同じではないが、圧縮が優れた教師なし目的であることを示すために、コンプレッサーが効果的な線形表現を与える必要がないケースはあるのだろうか？"
  },
  {
    "start": 2770212,
    "end": 2777050,
    "text": "そう、この質問については思うところがある。"
  },
  {
    "start": 2778720,
    "end": 2785000,
    "text": "まず第一に、リニア表現、優れたリニア表現はボーナスだと言っておこう。"
  },
  {
    "start": 2785160,
    "end": 2791320,
    "text": "この議論では、だからリニアな表現が出てくるべきだ、とは一言も言っていない。"
  },
  {
    "start": 2791480,
    "end": 2804408,
    "text": "しかし、私は、関節の圧縮は、HGDという悪い探索アルゴリズムを使ったハチャメチャな近似微調整のようなものだから、理論的には良い微調整が生まれるはずだと主張したい。"
  },
  {
    "start": 2804604,
    "end": 2818340,
    "text": "今、私たちは、これらの古い実験から得られた証拠から、バートが画像に対して実行された場合、次のピクセル予測よりも悪い線形表現を学習することを示唆していることを知っている。"
  },
  {
    "start": 2818780,
    "end": 2821156,
    "text": "おそらく拡散モデルも同じだろう。"
  },
  {
    "start": 2821188,
    "end": 2822580,
    "text": "その可能性は高いと思う。"
  },
  {
    "start": 2822740,
    "end": 2827872,
    "text": "拡散モデルの微調整がどのように比較されるのか、非常に興味深いと思う。"
  },
  {
    "start": 2827956,
    "end": 2829292,
    "text": "もしかしたら、もう誰かが知っているかもしれない。"
  },
  {
    "start": 2829426,
    "end": 2841984,
    "text": "ここで、教師なし学習に対するあなたのアイデアを復活させることはできないだろうか。"
  },
  {
    "start": 2842022,
    "end": 2852784,
    "text": "パット・ラーニングではカバーしきれない、教師あり学習から得られる知見もありそうだ。"
  },
  {
    "start": 2852822,
    "end": 2856784,
    "text": "私はここで、より多くの洞察が得られると思う。"
  },
  {
    "start": 2856822,
    "end": 2862004,
    "text": "ここで得られるかもしれない特別な洞察は、ファンクションクラスに対する洞察であり、望ましいファンクションクラスに対する洞察である。"
  },
  {
    "start": 2862042,
    "end": 2865284,
    "text": "例えば、ニューラルネットを何層にもしたいだろう？"
  },
  {
    "start": 2865322,
    "end": 2871268,
    "text": "レイヤーが多ければ、より広い、より大きなニューラルネットになる。"
  },
  {
    "start": 2871284,
    "end": 2878844,
    "text": "つまり、本質的には、この分野ですでに発見されているようなことだが、教師あり学習では、数パラメーター以上の例は必要ないかもしれない。"
  },
  {
    "start": 2878882,
    "end": 2880380,
    "text": "これが説明かもしれない。"
  },
  {
    "start": 2884400,
    "end": 2884924,
    "text": "その通りだ。"
  },
  {
    "start": 2884962,
    "end": 2889176,
    "text": "これが例えば、この理論の弱点かもしれない。"
  },
  {
    "start": 2889208,
    "end": 2893552,
    "text": "ところで、この理論には実用上大きな弱点がある。計算コストを無視していることだ。"
  },
  {
    "start": 2893606,
    "end": 2894992,
    "text": "それは情報だけに焦点を当てている。"
  },
  {
    "start": 2895126,
    "end": 2899424,
    "text": "皆さんはユニバーサル・トランスをご存知だろうか？"
  },
  {
    "start": 2899622,
    "end": 2901276,
    "text": "基本的には変圧器だ。"
  },
  {
    "start": 2901308,
    "end": 2903280,
    "text": "各層で同じ重みを使う。"
  },
  {
    "start": 2903940,
    "end": 2911190,
    "text": "素晴らしいアイデアだが、多くのパラメーターを持ちたければ、その分多くの計算が必要になる。"
  },
  {
    "start": 2912120,
    "end": 2914548,
    "text": "コンピュートコストは無視できない。"
  },
  {
    "start": 2914714,
    "end": 2916616,
    "text": "これは計算コストを無視している。"
  },
  {
    "start": 2916798,
    "end": 2921000,
    "text": "計算コストを無視すれば、どのように進めるかのレシピが得られる。"
  },
  {
    "start": 2925500,
    "end": 2931640,
    "text": "自己回帰モデル、特に人気分布の重要性は何だと思いますか？"
  },
  {
    "start": 2931720,
    "end": 2936510,
    "text": "鳥は最大尤度のトレーニングをしているようなものだと考えればいいんだろ？"
  },
  {
    "start": 2938320,
    "end": 2941010,
    "text": "例えば、鳥からサンプリングする方法を見つけることができる。"
  },
  {
    "start": 2942180,
    "end": 2948080,
    "text": "確かに拡散モデルは最尤モデルに設定することができる。"
  },
  {
    "start": 2949220,
    "end": 2967152,
    "text": "ということは、この理論が予測する拡散モデルもまた、同じように偉大なことをすることが可能なはずである。"
  },
  {
    "start": 2967216,
    "end": 2974504,
    "text": "自己回帰モデルでも拡散モデルでも、同じことが言える。"
  },
  {
    "start": 2974622,
    "end": 2978096,
    "text": "自己回帰モデルはシンプルで便利だ。"
  },
  {
    "start": 2978228,
    "end": 2984190,
    "text": "もしかしたら、エネルギーベースのモデルはもっとすごいことをするかもしれないが、その観点からすれば、どれも同じだ。"
  },
  {
    "start": 2986400,
    "end": 2996320,
    "text": "現時点ではGPT4が最も優れたコンプレッサーのようで、おそらく最大のモデルだと思われる。"
  },
  {
    "start": 2996470,
    "end": 2999872,
    "text": "一方では、よりうまく圧縮することができる。"
  },
  {
    "start": 2999926,
    "end": 3004500,
    "text": "一方、コンプレッサー自体のサイズは大きくなっている。"
  },
  {
    "start": 3005240,
    "end": 3006164,
    "text": "それともあれか？"
  },
  {
    "start": 3006202,
    "end": 3013584,
    "text": "モ複雑性と呼ばれる理論の観点からは、実は必ずしもそうではない。"
  },
  {
    "start": 3013712,
    "end": 3034270,
    "text": "まあ、つまり、重要なのは、そのようなものの費用、あなたが本当に、あなたが本当に望んでいるものは、そのようなもののためだということだ。"
  },
  {
    "start": 3034880,
    "end": 3043100,
    "text": "というのも、この理論では、固定されたデータセットを圧縮したいと言うからだ。"
  },
  {
    "start": 3043170,
    "end": 3052012,
    "text": "GPTモデルの訓練方法では、大きな訓練セットがあり、次にテストセットがある。"
  },
  {
    "start": 3052156,
    "end": 3061072,
    "text": "テストセットが無限で、テストセットを圧縮することに関心がある場合、このテストセットのサイズがはるかに大きい限り、コンプレッサーのサイズは気にならない。"
  },
  {
    "start": 3061136,
    "end": 3070520,
    "text": "これもまた、例えるならディス・アナロジーのようなもので、より明確にするためには、もう少し注意深く考えることが有効だろう。"
  },
  {
    "start": 3071740,
    "end": 3077240,
    "text": "独立した検証テストを使えばいいのだろうか？"
  },
  {
    "start": 3078880,
    "end": 3080028,
    "text": "それでいいのか？"
  },
  {
    "start": 3080194,
    "end": 3081052,
    "text": "いいことだよ。"
  },
  {
    "start": 3081106,
    "end": 3082510,
    "text": "それはいいことだ。"
  },
  {
    "start": 3083520,
    "end": 3094784,
    "text": "つまり、シングルエポックの場合も同じだと思うんだ。"
  },
  {
    "start": 3094822,
    "end": 3106276,
    "text": "シングル・エポックの場合であれば、訓練しながらログ・プローブを計算すればよい。"
  },
  {
    "start": 3106298,
    "end": 3108870,
    "text": "シングルリポの場合、状況はよく似ていると思う。"
  },
  {
    "start": 3111960,
    "end": 3129672,
    "text": "ただ、先月ACM誌のfindingsに掲載された論文で、より簡単な圧縮文字列を使用したものがありました。"
  },
  {
    "start": 3129816,
    "end": 3135288,
    "text": "2つの文字列を個別に連結し、コンピュータの距離にする。"
  },
  {
    "start": 3135464,
    "end": 3139710,
    "text": "Gzipの出力をフルに使っているのはそのためだ。"
  },
  {
    "start": 3141440,
    "end": 3146336,
    "text": "ええ、それについての私の唯一のコメントは、Gzipはテキストの圧縮にあまり強くないということです。"
  },
  {
    "start": 3146518,
    "end": 3152172,
    "text": "ある程度のことは可能だということを示していると思う。"
  },
  {
    "start": 3152236,
    "end": 3157750,
    "text": "本当に肉厚なものはすべて、最後の部分を絞るときに起こるんだ。"
  },
  {
    "start": 3158200,
    "end": 3162420,
    "text": "コンセプトの証明のおもちゃのように機能するようだ。"
  },
  {
    "start": 3163880,
    "end": 3169510,
    "text": "バグがあったようで、今は機能していない。"
  },
  {
    "start": 3173020,
    "end": 3179020,
    "text": "ジェイコブはカリキュラム効果について、ニューラルネットには現れるが、ニューラルネットには現れないことを指摘した。"
  },
  {
    "start": 3179090,
    "end": 3181976,
    "text": "あなたには見えないが、カマルゴラの複雑さ。"
  },
  {
    "start": 3182168,
    "end": 3189980,
    "text": "ちょうど昼食時に、カリキュラムの効果が実際にどの程度重要なのか、現在の経験的状況はどうなのか、という話をしていたところだ。"
  },
  {
    "start": 3190050,
    "end": 3191790,
    "text": "私たちは誰もその答えを知らなかった。"
  },
  {
    "start": 3192480,
    "end": 3200428,
    "text": "まあ、そんな感じだ。"
  },
  {
    "start": 3200514,
    "end": 3206800,
    "text": "ちょっとね、どっちが簡単かっていうと、こっちの方が簡単なんだ。"
  },
  {
    "start": 3206960,
    "end": 3215804,
    "text": "私たちは、私たちが使っているアーキテクチャのようなトランスフォーマーを比較的簡単に最適化できるようにするために、現場として多くの仕事をしてきた。"
  },
  {
    "start": 3215872,
    "end": 3224216,
    "text": "良い初期化を見つけ、良いハイパーパラメータを見つけ、トレーニングができるだけ簡単になるようにアーキテクチャを変更した。"
  },
  {
    "start": 3224398,
    "end": 3234540,
    "text": "トレーニングの最適化問題が簡単であればあるほど、カリキュラムの影響を受けにくくなる。"
  },
  {
    "start": 3235440,
    "end": 3251728,
    "text": "例えば、ニューラルチューリングマシンのようなエキゾチックなアーキテクチャーをトレーニングしていた人たちがいて、それは本当に複雑なものだった。"
  },
  {
    "start": 3251894,
    "end": 3258020,
    "text": "すぐに全データを与えると失敗してしまうからだ。"
  },
  {
    "start": 3262120,
    "end": 3262870,
    "text": "そうだね。"
  },
  {
    "start": 3263720,
    "end": 3265990,
    "text": "イリヤにもう一度お礼を言おう。"
  },
  {
    "start": 3278320,
    "end": 3304070,
    "text": "ああ、つまり、そういうことだ。"
  },
  {
    "start": 3326760,
    "end": 3428570,
    "text": "そういうものだと思う。"
  }
]