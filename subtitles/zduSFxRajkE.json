[
  {
    "start": 90,
    "end": 750,
    "text": "皆さん、こんにちは。"
  },
  {
    "start": 900,
    "end": 5354,
    "text": "このビデオでは、大規模な言語モデルにおけるトークン化のプロセスを取り上げたいと思います。"
  },
  {
    "start": 5482,
    "end": 12474,
    "text": "私が悲しい顔をしているのがお分かりだろう。トークン化は、大規模な言語モデルを扱う上で最も苦手な部分だからだ。"
  },
  {
    "start": 12522,
    "end": 20238,
    "text": "残念ながら、かなり毛深く、厄介で、注意すべき隠れたフットガンがたくさんあるため、ある程度詳細に理解する必要がある。"
  },
  {
    "start": 20324,
    "end": 24990,
    "text": "大規模な言語モデルにおける多くの問題は、一般的にトークン化にまでさかのぼる。"
  },
  {
    "start": 25450,
    "end": 27030,
    "text": "トークン化とは何ですか？"
  },
  {
    "start": 27450,
    "end": 30870,
    "text": "さて、前回のビデオではGPTをゼロから構築してみよう。"
  },
  {
    "start": 31290,
    "end": 36546,
    "text": "実はトークン化はすでにやっていたのだが、非常に素朴で単純なバージョンのトークン化だった。"
  },
  {
    "start": 36738,
    "end": 46620,
    "text": "この動画のGoogle Colabにアクセスすると、ここにトレーニングセットがロードされているのがわかります。"
  },
  {
    "start": 47230,
    "end": 50294,
    "text": "さて、シェイクスピアのデータセットは、最初はただの大きな文字列である。"
  },
  {
    "start": 50342,
    "end": 52014,
    "text": "Pythonではただのテキストだ。"
  },
  {
    "start": 52132,
    "end": 56378,
    "text": "では、どうやってテキストを大規模な言語モデルに差し込むのか？"
  },
  {
    "start": 56554,
    "end": 64906,
    "text": "この場合、この文字列で出現する可能性のある65文字の語彙を作成した。"
  },
  {
    "start": 65018,
    "end": 68626,
    "text": "これらは可能性のあるキャラクターで、65人いることがわかった。"
  },
  {
    "start": 68728,
    "end": 78070,
    "text": "そして、ありとあらゆる文字、小さな文字列の一部をトークン、つまり整数に変換するためのルックアップテーブルを作成した。"
  },
  {
    "start": 78650,
    "end": 82182,
    "text": "例えば、ここではhiという文字列をトークン化した。"
  },
  {
    "start": 82316,
    "end": 84950,
    "text": "この一連のトークンを受け取った。"
  },
  {
    "start": 85530,
    "end": 91654,
    "text": "ここでは、データセットの最初の1000文字を取り出し、トークンにエンコードした。"
  },
  {
    "start": 91782,
    "end": 97446,
    "text": "これはキャラクターレベルなので、1000個のトークンを連続で受け取った。"
  },
  {
    "start": 97638,
    "end": 100410,
    "text": "トークン1847、その他"
  },
  {
    "start": 100990,
    "end": 108670,
    "text": "後で、これらのトークンを言語モデルに差し込む方法は、埋め込みテーブルを使うことであることを見た。"
  },
  {
    "start": 109330,
    "end": 115758,
    "text": "つまり、65個のトークンがあるとすると、埋め込みテーブルは65行になる。"
  },
  {
    "start": 115934,
    "end": 120526,
    "text": "大雑把に言えば、すべてのトークンに関連する整数を取っている。"
  },
  {
    "start": 120638,
    "end": 131210,
    "text": "それをこのテーブルのルックアップとして使い、対応する行を抜き出している。この行は、逆伝播を使ってトレーニングするための、トレーニング可能なパラメーターだ。"
  },
  {
    "start": 131310,
    "end": 134786,
    "text": "これが、トランスフォーマーに供給されるベクトルである。"
  },
  {
    "start": 134978,
    "end": 138310,
    "text": "これが、トランスフォーマーがすべてのトークンを認識する方法だ。"
  },
  {
    "start": 138970,
    "end": 144694,
    "text": "ここでは、文字レベルのトークナイザーという非常に素朴なトークン化処理をしていた。"
  },
  {
    "start": 144822,
    "end": 154590,
    "text": "実際には、アート言語モデルの代わりに、残念ながら、もっと複雑なスキームを使ってトークンボキャブラリーを構築している。"
  },
  {
    "start": 155250,
    "end": 159262,
    "text": "私たちが扱っているのはキャラクターレベルではなく、チャンクレベルだ。"
  },
  {
    "start": 159396,
    "end": 179522,
    "text": "これらの文字チャンクが構築される方法には、例えばバイトペアエンコーディングアルゴリズムのようなアルゴリズムが使われます。このビデオの中で詳しく説明するつもりですが、大規模言語モデルの文脈におけるトークン化のメカニズムとしてバイトレベルエンコーディングを紹介した論文を簡単にお見せしたいと思います。"
  },
  {
    "start": 179666,
    "end": 182294,
    "text": "それはおそらくGPT-2ペーパーだと思う。"
  },
  {
    "start": 182412,
    "end": 192010,
    "text": "入力表現のセクションまでスクロールすると、ここでトークン化、トークン化に持たせたいプロパティの種類が説明されています。"
  },
  {
    "start": 192160,
    "end": 206090,
    "text": "ここでは、50,257個の可能なトークンの語彙があり、コンテキストのサイズが1024個のトークンになるようなトークナイザーを作ると結論付けている。"
  },
  {
    "start": 206170,
    "end": 217134,
    "text": "トランスフォーマー・ニューラル・ネットワークのアテンション層では、すべてのトークンがシーケンスの前のトークンにアテンションしており、最大1024個のトークンを見ることになる。"
  },
  {
    "start": 217262,
    "end": 225294,
    "text": "トークンは、いわば大きな言語モデルの原子のようなもので、すべてがトークン単位で構成されている。"
  },
  {
    "start": 225342,
    "end": 226802,
    "text": "すべてはトークンのためだ。"
  },
  {
    "start": 226866,
    "end": 234390,
    "text": "トークン化とは、文字列やテキストをトークンの列に変換するプロセスであり、その逆もまた同様である。"
  },
  {
    "start": 234730,
    "end": 241210,
    "text": "トークンで検索すると63件ヒットする。"
  },
  {
    "start": 241550,
    "end": 244118,
    "text": "トークンが再び浸透しているからだ。"
  },
  {
    "start": 244214,
    "end": 248220,
    "text": "ここでは、2兆トークンのデータでトレーニングしたなどと述べている。"
  },
  {
    "start": 249310,
    "end": 251862,
    "text": "では、独自のトークナイザーを作ってみよう。"
  },
  {
    "start": 251926,
    "end": 259422,
    "text": "幸いなことに、バイトベアリングコーディングのアルゴリズムはそれほど複雑ではない。"
  },
  {
    "start": 259556,
    "end": 271170,
    "text": "コードに入る前に、トークン化から生じる複雑さを簡単に説明したい。なぜこのようなことをするのか、なぜこのようなことをするのか、なぜこのようなことが重要なのかを十分に理解してもらいたいからだ。"
  },
  {
    "start": 271590,
    "end": 278486,
    "text": "トークン化は、大規模な言語モデルにおける多くの奇妙さの核心である。"
  },
  {
    "start": 278668,
    "end": 290170,
    "text": "ニューラル・ネットワーク・アーキテクチャや大規模な言語モデル自体の問題に見える問題の多くは、実はトークン化の問題であり、根本的にはそこに行き着く。"
  },
  {
    "start": 290320,
    "end": 298806,
    "text": "もしあなたが、大規模な言語モデルがスペリングタスクを簡単にこなせないという問題に気づいているなら、それはたいていトークン化が原因だ。"
  },
  {
    "start": 298998,
    "end": 304110,
    "text": "単純な文字列処理は、大規模な言語モデルがネイティブに実行するのは難しいかもしれない。"
  },
  {
    "start": 304690,
    "end": 309710,
    "text": "どの英語圏の言語も、もっとひどい結果になることがある。"
  },
  {
    "start": 310210,
    "end": 312810,
    "text": "時々llmsは単純な計算が苦手なことがある。"
  },
  {
    "start": 312890,
    "end": 315650,
    "text": "また、トークン化のトレースも可能である。"
  },
  {
    "start": 316550,
    "end": 323250,
    "text": "GPT-2は特に、トークン化のために、Pythonの将来のバージョンよりもかなり多くの問題を抱えていただろう。"
  },
  {
    "start": 323670,
    "end": 324722,
    "text": "他にも多くの問題がある。"
  },
  {
    "start": 324776,
    "end": 327142,
    "text": "もしかしたら、末尾の空白について奇妙な警告を見たことがあるかもしれない。"
  },
  {
    "start": 327196,
    "end": 328760,
    "text": "これはトークン化の問題だ。"
  },
  {
    "start": 330570,
    "end": 341374,
    "text": "ソリッド・ゴールド・マジックアープとは何かとGPTに質問したら、LLMは完全に発狂し、まったく関係のない余談を始めるだろう。"
  },
  {
    "start": 341522,
    "end": 344618,
    "text": "もしかしたら、構造化データではJSonよりもYamlを使うように言われているかもしれない。"
  },
  {
    "start": 344704,
    "end": 346570,
    "text": "そのすべてがトークン化と関係している。"
  },
  {
    "start": 346910,
    "end": 350074,
    "text": "基本的にトークン化は多くの問題の核心である。"
  },
  {
    "start": 350272,
    "end": 361870,
    "text": "ビデオの最後には、またこれらの話に戻りますが、今は少し飛ばして、このウェブアプリ、ticktokenizer Versailleアプリに行きましょう。"
  },
  {
    "start": 362020,
    "end": 363406,
    "text": "ここにロードしてある。"
  },
  {
    "start": 363508,
    "end": 374370,
    "text": "このウェブ・アプリで気に入っているのは、トークン化がブラウザのJavaScriptでライブで実行されることだ。"
  },
  {
    "start": 375110,
    "end": 383042,
    "text": "ここでは、左側に表示されているのが文字列で、右側に表示されているのが、現在GPT-2トークナイザーを使用している文字列です。"
  },
  {
    "start": 383186,
    "end": 394006,
    "text": "ここに貼り付けた文字列は、現在300のトークンにトークン化されている。"
  },
  {
    "start": 394198,
    "end": 404240,
    "text": "たとえば、この単語のトークン化は、トークン30,642と1634の2つのトークンになった。"
  },
  {
    "start": 404930,
    "end": 409994,
    "text": "トークン・スペースはトークン318である。"
  },
  {
    "start": 410122,
    "end": 421010,
    "text": "また、ここにはスペースや改行文字がありますが、わかりやすくするために非表示にすることができます。"
  },
  {
    "start": 422470,
    "end": 430754,
    "text": "トークン・スペースatはトークン379、トークン・スペースtheは262など。"
  },
  {
    "start": 430882,
    "end": 435510,
    "text": "ここで、スペースがトークンのチャンクの一部であることに気づくだろう。"
  },
  {
    "start": 436810,
    "end": 442858,
    "text": "さて、これは私たちの英語の文章がどのように分かれたかというようなもので、それはそれで良いことだと思う。"
  },
  {
    "start": 443024,
    "end": 445606,
    "text": "さて、ここで算数を入れてみた。"
  },
  {
    "start": 445718,
    "end": 453754,
    "text": "トークン127がプラスされ、トークン6、スペース6の後に77が続いていることがわかる。"
  },
  {
    "start": 453872,
    "end": 465518,
    "text": "ここで起こっているのは、127は1つのトークンとしてラージ言語モデルに入力されるが、677という数字は実際には2つの別々のトークンとして入力されるということだ。"
  },
  {
    "start": 465694,
    "end": 473582,
    "text": "そのため、大規模な言語モデルはそれを考慮し、ネットワーク内で正しく処理しなければならない。"
  },
  {
    "start": 473726,
    "end": 476942,
    "text": "ここでは、804は2つのトークンに分割される。"
  },
  {
    "start": 477006,
    "end": 478702,
    "text": "これはすべて完全に恣意的なものだ。"
  },
  {
    "start": 478846,
    "end": 484914,
    "text": "ここに4桁の数字の別の例がある。"
  },
  {
    "start": 484962,
    "end": 488118,
    "text": "複数桁のトークンを持つこともある。"
  },
  {
    "start": 488214,
    "end": 492774,
    "text": "トークンの数だけ桁があることもあるし、すべてが恣意的なんだ。"
  },
  {
    "start": 492822,
    "end": 496906,
    "text": "トークナイザーから出てくる、もうひとつの例を挙げよう。"
  },
  {
    "start": 497088,
    "end": 499766,
    "text": "糸を引く卵がある。"
  },
  {
    "start": 499958,
    "end": 502750,
    "text": "これが2つのトークンになったことがわかるだろう。"
  },
  {
    "start": 503170,
    "end": 510138,
    "text": "なぜか、卵を持っていると言うと、それがスペースエッグの場合、トークンが2つになってしまうんだ。"
  },
  {
    "start": 510314,
    "end": 511706,
    "text": "すみません、トークンは1つです。"
  },
  {
    "start": 511818,
    "end": 521502,
    "text": "文頭のjust egg単体では2トークンだが、ここではスペース・エッグとして、まったく同じ文字列がいきなり1トークンになる。"
  },
  {
    "start": 521646,
    "end": 522340,
    "text": "いいかい？"
  },
  {
    "start": 522870,
    "end": 525758,
    "text": "ここで、小文字のeggは1つのトークンであることがわかった。"
  },
  {
    "start": 525854,
    "end": 527814,
    "text": "特に、色が違うことに注目してほしい。"
  },
  {
    "start": 527852,
    "end": 529266,
    "text": "これは別のトークンだ。"
  },
  {
    "start": 529378,
    "end": 530946,
    "text": "これは大文字と小文字を区別する。"
  },
  {
    "start": 531058,
    "end": 535170,
    "text": "もちろん、資本の卵も別のトークンだろう。"
  },
  {
    "start": 535250,
    "end": 538930,
    "text": "また、これは任意の2トークンである。"
  },
  {
    "start": 539090,
    "end": 550118,
    "text": "同じコンセプト・エッグでも、それが文頭にあるか、文末にあるか、小文字か大文字か、あるいは混在しているかによって、基本的にはすべてまったく異なるトークンになり、異なるIDになる。"
  },
  {
    "start": 550214,
    "end": 568660,
    "text": "言語モデルは、学習対象となるすべてのインターネットテキストから得られた生データから、これらのテキストが実際にはすべてまったく同じ概念であることを学習しなければならない。そして、ニューラルネットワークのパラメータでこれらのテキストをグループ化し、データのパターンに基づいて、これらのテキストがすべて非常に類似していることを理解しなければならない。"
  },
  {
    "start": 570550,
    "end": 577854,
    "text": "このデモの後、OpenAIのチャチ・バトから韓国語で紹介がある。"
  },
  {
    "start": 577982,
    "end": 581698,
    "text": "マナソ、パンガワヤなどだ。"
  },
  {
    "start": 581874,
    "end": 583138,
    "text": "これは韓国語だ。"
  },
  {
    "start": 583234,
    "end": 593198,
    "text": "ここに書いたのは、チャチBtでは英語以外の言語が若干不利に働くことに気づくからだ。"
  },
  {
    "start": 593314,
    "end": 599194,
    "text": "その理由のひとつは、もちろん、chachiptのトレーニング・データセットが、英語の方が他のものよりもはるかに大きいからだ。"
  },
  {
    "start": 599312,
    "end": 604166,
    "text": "これは、大規模な言語モデルそのものだけでなく、トークナイザーについても同じことが言える。"
  },
  {
    "start": 604278,
    "end": 610442,
    "text": "トークン・サイザーを訓練するとき、同様に訓練セットがあることがわかるだろう。"
  },
  {
    "start": 610586,
    "end": 616690,
    "text": "結局のところ、英語のトークンはもっと長くなるということだ。"
  },
  {
    "start": 617430,
    "end": 619250,
    "text": "どう言えばいいんだ？"
  },
  {
    "start": 619320,
    "end": 624946,
    "text": "英語で一つの文章があったとして、それをトークン化すると、10個のトークンだとか、そういうことがわかるかもしれない。"
  },
  {
    "start": 625048,
    "end": 636200,
    "text": "その文章を韓国語や日本語などに翻訳してみると、通常、使用されるトークンの数がはるかに多いことがわかる。"
  },
  {
    "start": 636650,
    "end": 640054,
    "text": "まったく同じことに、もっとたくさんのトークンを使っている。"
  },
  {
    "start": 640252,
    "end": 644774,
    "text": "これは、すべてのドキュメントのシーケンス長を肥大化させる。"
  },
  {
    "start": 644902,
    "end": 646902,
    "text": "より多くのトークンを使うことになる。"
  },
  {
    "start": 646966,
    "end": 656298,
    "text": "ということは、トランスフォーマーの注意の中で、これらのトークンが互いにアテンションしようとするとき、そのトランスフォーマーのコンテキストの最大長を使い果たすことになる。"
  },
  {
    "start": 656394,
    "end": 663290,
    "text": "だから基本的に、英語以外の文章はトランスフォーマーの視点から引き伸ばされる。"
  },
  {
    "start": 663370,
    "end": 669074,
    "text": "これは、トークナイザーとトークン化そのものに使われるトレーニングセットに関係しています。"
  },
  {
    "start": 669272,
    "end": 677640,
    "text": "英語では、より大きなトークンとより大きなグループが作成され、英語以外のテキストには小さな境界線がたくさんできる。"
  },
  {
    "start": 679210,
    "end": 683670,
    "text": "これを英語に訳すと、かなり少ないトークンになるだろう。"
  },
  {
    "start": 684170,
    "end": 688862,
    "text": "最後の例は、fizzbuzzを実行するためのPythonの小さなスニペットだ。"
  },
  {
    "start": 689026,
    "end": 695414,
    "text": "注目してほしいのは、これらの個々のスペースはすべて別のトークンだということだ。"
  },
  {
    "start": 695542,
    "end": 697420,
    "text": "形だけの220だ。"
  },
  {
    "start": 697870,
    "end": 704410,
    "text": "2222-2222の後にスペースを入れる。"
  },
  {
    "start": 704570,
    "end": 713834,
    "text": "ここで起こっているのは、トランスフォーマーがこのテキストを消費したり作成しようとしたりするときに、これらのスペースをすべて個別に処理する必要があるということだ。"
  },
  {
    "start": 713882,
    "end": 718510,
    "text": "これらはすべて、順番にトランス全体に1つずつ送り込まれる。"
  },
  {
    "start": 718590,
    "end": 722626,
    "text": "だから、このような形で形骸化するのは非常にもったいない。"
  },
  {
    "start": 722808,
    "end": 726878,
    "text": "その結果、GPT-2はPythonと相性が悪い。"
  },
  {
    "start": 726974,
    "end": 730054,
    "text": "コーディングや言語モデルそのものとは関係ない。"
  },
  {
    "start": 730172,
    "end": 743478,
    "text": "ただ、Pythonでスペースを使ったインデントを多用すると、テキストが肥大化してしまい、シーケンスの長さが足りなくなってしまうんだ。"
  },
  {
    "start": 743574,
    "end": 745194,
    "text": "大雑把に言って、何が起こっているんだ？"
  },
  {
    "start": 745232,
    "end": 746422,
    "text": "無駄が多すぎる。"
  },
  {
    "start": 746486,
    "end": 748522,
    "text": "トークンのスペースを取りすぎている。"
  },
  {
    "start": 748656,
    "end": 751526,
    "text": "ここで上にスクロールして、トークナイザーを変更することもできる。"
  },
  {
    "start": 751638,
    "end": 756874,
    "text": "ここで、GPT-2トークナイザーはこの文字列のトークン数を300にすることに注意してください。"
  },
  {
    "start": 756922,
    "end": 761434,
    "text": "ここでは、GPTの4つのトークナイザーであるCl 100kベースに変更することができます。"
  },
  {
    "start": 761562,
    "end": 764270,
    "text": "トークン数が185に減っていることがわかる。"
  },
  {
    "start": 764420,
    "end": 769422,
    "text": "を全く同じ文字列に置き換えた場合、トークンの数はほぼ同じになる。"
  },
  {
    "start": 769566,
    "end": 778462,
    "text": "大雑把に言えば、これはGPT 4トークナイザーのトークン数がGPT-2トークナイザーのトークン数のおよそ2倍だからです。"
  },
  {
    "start": 778526,
    "end": 781366,
    "text": "私たちはおよそ5万ドルからおよそ10万ドルになった。"
  },
  {
    "start": 781548,
    "end": 788466,
    "text": "同じテキストが半分のトークンに収まるのだから。"
  },
  {
    "start": 788578,
    "end": 793050,
    "text": "これは、トランスへの入力がより密になる。"
  },
  {
    "start": 793630,
    "end": 799274,
    "text": "トランスフォーマーでは、すべてのトークンは、その前に注目するトークンの数に限りがある。"
  },
  {
    "start": 799392,
    "end": 809038,
    "text": "つまり、この変更によって、次に予測すべきトークンのコンテキストとして、およそ2倍のテキストを見ることができるようになったのだ。"
  },
  {
    "start": 809204,
    "end": 819822,
    "text": "もちろん、トークンの数を増やすだけで、無限に良くなるわけではない。トークンの数を増やせば増やすほど、エンベッディングテーブルは大きくなっていくからだ。"
  },
  {
    "start": 819886,
    "end": 825122,
    "text": "また、出力では次のトークンを予測しようとしており、そこにはソフト・マックスがある。"
  },
  {
    "start": 825176,
    "end": 836790,
    "text": "これについては後で詳しく説明するつもりだが、ボキャブラリーにちょうどいい数のトークンがあって、すべてが適切な密度で、なおかつかなり効率的なスイートスポットがどこかにある。"
  },
  {
    "start": 837370,
    "end": 845994,
    "text": "さて、GPTの4つのトークナイザーについて特に注目していただきたいのは、Pythonのホワイトスペースの扱いが大幅に改善されたことです。"
  },
  {
    "start": 846112,
    "end": 858314,
    "text": "ここにある4つのスペースは、ここにある3つのスペースに対して1つのトークンとして表現されているのがわかるだろう。そして、トークンのスペースifとここにある7つのスペースは、すべて1つのトークンにまとめられている。"
  },
  {
    "start": 858442,
    "end": 861210,
    "text": "私たちはPythonを表現する方法をより効率的にしている。"
  },
  {
    "start": 861290,
    "end": 870366,
    "text": "これはOpenAIがGPT 4トークナイザーを設計する際に意図的に選択したもので、より多くの空白を1文字にまとめている。"
  },
  {
    "start": 870478,
    "end": 880386,
    "text": "こうすることで、Pythonの密度が濃くなり、シーケンスの次のトークンを予測しようとするときに、その前のより多くのコードに注目できるようになる。"
  },
  {
    "start": 880578,
    "end": 896390,
    "text": "GPT-2からGPT-4へのPythonコーディング能力の向上は、言語モデルやアーキテクチャ、最適化の詳細の問題だけではありません。"
  },
  {
    "start": 896550,
    "end": 898700,
    "text": "さて、それではコードを書き始めよう。"
  },
  {
    "start": 899230,
    "end": 904874,
    "text": "私たちがやりたいことは、文字列を言語モデルに送り込むことだ。"
  },
  {
    "start": 905002,
    "end": 911882,
    "text": "そのためには、文字列を何らかの方法でトークン化し、一定の語彙で整数にする必要がある。"
  },
  {
    "start": 912026,
    "end": 920046,
    "text": "そして、その整数を用いてベクターのルックアップテーブルをルックアップし、そのベクターをトランスフォーマーに入力する。"
  },
  {
    "start": 920238,
    "end": 927726,
    "text": "もちろん、単純な英語のアルファベットをサポートしたいだけでなく、さまざまな種類の言語をサポートしたいからだ。"
  },
  {
    "start": 927838,
    "end": 931382,
    "text": "韓国語で「こんにちは」。"
  },
  {
    "start": 931516,
    "end": 937510,
    "text": "また、インターネット上で見かけるような特殊文字、例えばモグなどもサポートしたい。"
  },
  {
    "start": 938170,
    "end": 942650,
    "text": "このテキストをどうやってトランスフォーマーに送り込むのか？"
  },
  {
    "start": 943070,
    "end": 945210,
    "text": "ところで、この文章は何なんだ？"
  },
  {
    "start": 945280,
    "end": 945990,
    "text": "パイソンで？"
  },
  {
    "start": 946070,
    "end": 954240,
    "text": "Pythonの文字列のドキュメントを見れば、文字列はUnicodeコードポイントの不変シーケンスであることがわかる。"
  },
  {
    "start": 954850,
    "end": 957120,
    "text": "じゃあ、ユニコードのコードポイントって何？"
  },
  {
    "start": 957730,
    "end": 959390,
    "text": "ウィキペディアを見ればいい。"
  },
  {
    "start": 959730,
    "end": 966660,
    "text": "Unicodeコードポイントは、Unicode標準の一部としてUnicodeコンソーシアムによって定義されています。"
  },
  {
    "start": 967030,
    "end": 972706,
    "text": "というのは、今現在、およそ15万字の定義に過ぎないということだ。"
  },
  {
    "start": 972888,
    "end": 978478,
    "text": "大雑把に言えば、それらの文字がどのような形をしていて、どのような整数がそれらの文字を表しているかということだ。"
  },
  {
    "start": 978574,
    "end": 983334,
    "text": "今現在、161のスクリプトで15万文字と書いてある。"
  },
  {
    "start": 983452,
    "end": 986994,
    "text": "ここを下にスクロールすると、スタンダードが非常に生きていることがわかる。"
  },
  {
    "start": 987122,
    "end": 989406,
    "text": "最新の規格である15.1は2023年9月。"
  },
  {
    "start": 989408,
    "end": 1001046,
    "text": "基本的にこれは、例えば異なるスクリプトにまたがるすべてのキャラクターなど、たくさんの種類のキャラクターを定義するための方法だ。"
  },
  {
    "start": 1001238,
    "end": 1007418,
    "text": "Pythonのord関数を使えば、1つの文字からUnicodeのコードポイントにアクセスすることができます。"
  },
  {
    "start": 1007514,
    "end": 1016820,
    "text": "例えば、hのorを渡すと、1文字hのユニコード・ポイントが104であることがわかる。"
  },
  {
    "start": 1019030,
    "end": 1020974,
    "text": "これは任意に複雑にすることができる。"
  },
  {
    "start": 1021022,
    "end": 1026840,
    "text": "この絵文字のコードポイントは128,000である。"
  },
  {
    "start": 1027290,
    "end": 1032806,
    "text": "あるいは、ウンを取って50,000とすることもできる。"
  },
  {
    "start": 1032988,
    "end": 1038858,
    "text": "ここで注意してほしいのは、文字列をここに差し込むことはできないということだ。"
  },
  {
    "start": 1038944,
    "end": 1043930,
    "text": "ユニコードのコードポイント文字を1つだけ受け取り、それが整数であることを教えてくれる。"
  },
  {
    "start": 1044830,
    "end": 1051866,
    "text": "このようにして、特定の文字列のすべての文字とそのコードポイントを調べることができる。"
  },
  {
    "start": 1051968,
    "end": 1058080,
    "text": "この文字列のxをxの順に並べると、このエンコーディングになる。"
  },
  {
    "start": 1058450,
    "end": 1063546,
    "text": "さて、ここで我々はすでに生のコードポイントを整数に変えている。"
  },
  {
    "start": 1063658,
    "end": 1068186,
    "text": "トークン化せずに、単純に整数を使えばいいのでは？"
  },
  {
    "start": 1068228,
    "end": 1071858,
    "text": "なぜ、これをそのままネイティブに使えないのか？"
  },
  {
    "start": 1072024,
    "end": 1075874,
    "text": "その理由のひとつは、もちろん、その場合の語彙がかなり長くなるからだ。"
  },
  {
    "start": 1075992,
    "end": 1081990,
    "text": "Unicodeの場合、これは15万ものコードポイントからなる語彙である。"
  },
  {
    "start": 1082140,
    "end": 1087826,
    "text": "それよりも心配なのは、ユニコードの標準が非常に生きていて、変化し続けていることだ。"
  },
  {
    "start": 1087938,
    "end": 1092730,
    "text": "だから、私たちが直接使いたいと思うような安定した表現ではない。"
  },
  {
    "start": 1093070,
    "end": 1095594,
    "text": "そういった理由から、もう少し良いものが必要なのだ。"
  },
  {
    "start": 1095712,
    "end": 1098390,
    "text": "より良いものを見つけるために、私たちはエンコーディングに目を向ける。"
  },
  {
    "start": 1098550,
    "end": 1107550,
    "text": "ウィキペディアのページを見ると、ユニコード・コンソーシアムはUTF 8、UTF 16、UTF 32の3種類のエンコーディングを定義している。"
  },
  {
    "start": 1107700,
    "end": 1115650,
    "text": "これらのエンコーディングは、ユニコード・テキストをバイナリ・データまたはバイト・ストリームに変換する方法です。"
  },
  {
    "start": 1115990,
    "end": 1118260,
    "text": "UTF 8が圧倒的に多い。"
  },
  {
    "start": 1118710,
    "end": 1120418,
    "text": "これはUTF 8のページである。"
  },
  {
    "start": 1120504,
    "end": 1130306,
    "text": "さて、このウィキペディアのページは実際にはかなり長いのだが、我々の目的にとって重要なのは、UTF8はすべてのコードポイントを受け取り、それをストリームに変換するということだ。"
  },
  {
    "start": 1130418,
    "end": 1133282,
    "text": "このバイトストリームは1～4バイトである。"
  },
  {
    "start": 1133346,
    "end": 1135154,
    "text": "これは可変長エンコーディングである。"
  },
  {
    "start": 1135282,
    "end": 1141900,
    "text": "スキーマによれば、ユニコード・ポイントによって、各コード・ポイントに対して1バイトから4バイトのバイトを持つことになる。"
  },
  {
    "start": 1142270,
    "end": 1147370,
    "text": "さらにUTF 8、UTF 16、UTF 32がある。"
  },
  {
    "start": 1147440,
    "end": 1153398,
    "text": "UTF 32は可変長ではなく固定長であるため良いが、他にも多くの欠点がある。"
  },
  {
    "start": 1153584,
    "end": 1161278,
    "text": "これら3つのエンコーディングの長所と短所の全種類は、このビデオの範囲を超えている。"
  },
  {
    "start": 1161364,
    "end": 1169102,
    "text": "私はただ、このブログ記事が面白かったことを指摘したいだけだ。このブログ記事の最後にも、かなり役に立つ参考文献がいくつもある。"
  },
  {
    "start": 1169246,
    "end": 1185750,
    "text": "そのうちのひとつがUTFエイト・エブリウェア・マニフェストで、このマニフェストには、UTFエイトが他のエンコーディングよりも格段に好まれ、はるかに優れている理由、そしてインターネット上でUTFエイトがより多く使用されている理由が書かれている。"
  },
  {
    "start": 1186410,
    "end": 1196140,
    "text": "主な利点のひとつは、UTF 8だけが、より単純なASCIIエンコーディングと後方互換性があることです。"
  },
  {
    "start": 1196590,
    "end": 1199114,
    "text": "このビデオで詳細を説明するつもりはない。"
  },
  {
    "start": 1199312,
    "end": 1205886,
    "text": "UTF 8エンコーディングが好きなことは言うまでもない。"
  },
  {
    "start": 1205908,
    "end": 1214766,
    "text": "これをUTF 8にエンコードすると、pythonの文字列クラスはドット・エンコードを持っていて、例えばUTF 8のエンコードを与えることができる。"
  },
  {
    "start": 1214948,
    "end": 1223342,
    "text": "なぜなら、これはbytesオブジェクトとしてのbytesだからである。"
  },
  {
    "start": 1223406,
    "end": 1231110,
    "text": "個人的には、リストを通してエンコードするのが好きだ。"
  },
  {
    "start": 1231530,
    "end": 1237682,
    "text": "これは、UTF 8エンコーディングに従ってこの文字列を表す生のバイトである。"
  },
  {
    "start": 1237826,
    "end": 1239762,
    "text": "UTF 16にも注目しよう。"
  },
  {
    "start": 1239906,
    "end": 1241910,
    "text": "少し違ったバイトストリームが得られる。"
  },
  {
    "start": 1242250,
    "end": 1245462,
    "text": "ここで、UTF 16の欠点のひとつが見えてくる。"
  },
  {
    "start": 1245526,
    "end": 1249066,
    "text": "ゼロ・サムシング、ゼロ・サムシング、ゼロ・サムシングがあるじゃないか。"
  },
  {
    "start": 1249168,
    "end": 1252106,
    "text": "ちょっと無駄なエンコーディングだということが分かってきた。"
  },
  {
    "start": 1252218,
    "end": 1261998,
    "text": "確かに、単純なアスキー文字や英字の場合、ここではゼロ・サムシング、ゼロ・サムシングという構造があるだけで、決していいものではない。"
  },
  {
    "start": 1262164,
    "end": 1263774,
    "text": "UTF 32も同じ。"
  },
  {
    "start": 1263892,
    "end": 1267998,
    "text": "これを拡大すると、このエンコーディングの無駄が見えてくる。"
  },
  {
    "start": 1268014,
    "end": 1274770,
    "text": "私たちの目的からすると、ゼロの後に何かが続くことが多いので、これは望ましくない。"
  },
  {
    "start": 1275750,
    "end": 1281506,
    "text": "この目的のためには、UTF 8にこだわりたい。"
  },
  {
    "start": 1281698,
    "end": 1291690,
    "text": "しかし、UTF8を素朴に使うだけなら、これらはバイト・ストリームなので、語彙の長さは256しかないことになる。"
  },
  {
    "start": 1292110,
    "end": 1295002,
    "text": "この語彙のサイズはとてもとても小さい。"
  },
  {
    "start": 1295136,
    "end": 1303870,
    "text": "これを素朴に使うと、すべてのテキストが非常に長いバイト列にわたって引き伸ばされることになる。"
  },
  {
    "start": 1304290,
    "end": 1314006,
    "text": "つまり、エンベデッドテーブルが小さくなり、最終レイヤーの予測が非常に小さくなるということだ。"
  },
  {
    "start": 1314058,
    "end": 1315682,
    "text": "我々のシークエンスは非常に長い。"
  },
  {
    "start": 1315816,
    "end": 1337666,
    "text": "計算上の理由から、変換器でサポートできる注意のコンテキストの長さはかなり限られている。"
  },
  {
    "start": 1337858,
    "end": 1343062,
    "text": "Utf8エンコーディングの生のバイトは使いたくない。"
  },
  {
    "start": 1343126,
    "end": 1352614,
    "text": "ハイパーパラメーターとして調整できる、より大きな語彙サイズをサポートできるようにしたいが、これらの文字列のエンコーディングはUtf 8にこだわりたい。"
  },
  {
    "start": 1352742,
    "end": 1354094,
    "text": "どうする？"
  },
  {
    "start": 1354212,
    "end": 1362494,
    "text": "もちろん、その答えはバイトペア・エンコーディング・アルゴリズムに頼ることになる。"
  },
  {
    "start": 1362612,
    "end": 1363854,
    "text": "それについてはまた後ほど。"
  },
  {
    "start": 1363892,
    "end": 1373950,
    "text": "私は、生のバイト列を言語モデルに送り込むことができれば、それ以上の喜びはないだろうという事実を簡単に述べておきたい。"
  },
  {
    "start": 1374030,
    "end": 1378690,
    "text": "実際、昨年の夏には、その可能性に関する論文が発表されている。"
  },
  {
    "start": 1378840,
    "end": 1391034,
    "text": "というのも、前述したように、シーケンスが非常に長くなるため、注目度が非常に高くなるという問題が出てくるからだ。"
  },
  {
    "start": 1391232,
    "end": 1399446,
    "text": "そこでこの論文では、生のバイトを送り込むことを可能にするトランスフォーマーの階層構造を提案している。"
  },
  {
    "start": 1399558,
    "end": 1406058,
    "text": "最後に彼らは、これらの結果を合わせて、トークン化不要の自己回帰系列モデリングが大規模で実行可能であることが証明された、と述べている。"
  },
  {
    "start": 1406154,
    "end": 1408526,
    "text": "トークン化が無料になれば、本当に素晴らしいことだ。"
  },
  {
    "start": 1408628,
    "end": 1412010,
    "text": "バイトストリームを直接モデルに送り込むだけだ。"
  },
  {
    "start": 1412090,
    "end": 1418314,
    "text": "残念ながら、まだ十分な数のグループによって、十分な規模で証明されたとは言えない。"
  },
  {
    "start": 1418442,
    "end": 1421890,
    "text": "誰かがこのようなことを思いつくことを願っている。"
  },
  {
    "start": 1421960,
    "end": 1429342,
    "text": "このため、言語モデルに直接入力することはできず、バイペア符号化アルゴリズムを使って圧縮する必要がある。"
  },
  {
    "start": 1429406,
    "end": 1430470,
    "text": "どうなるか見てみよう。"
  },
  {
    "start": 1430540,
    "end": 1438614,
    "text": "先に述べたように、バイト・ペア・エンコーディングのアルゴリズムはそれほど複雑ではないし、基本的な考え方に関しては、ウィキペディアのページが実に参考になる。"
  },
  {
    "start": 1438812,
    "end": 1441554,
    "text": "我々がやっているのは、ある種の入力シーケンスだ。"
  },
  {
    "start": 1441682,
    "end": 1447770,
    "text": "例えば、ここにはABCとDという4つの要素しかない。"
  },
  {
    "start": 1447840,
    "end": 1451980,
    "text": "バイトの代わりに、4つのボキャブラリーがあったとしよう。"
  },
  {
    "start": 1452910,
    "end": 1455166,
    "text": "シーケンスが長すぎるので圧縮したい。"
  },
  {
    "start": 1455268,
    "end": 1473102,
    "text": "最も頻繁に出現するトークンのペアを繰り返し見つけ、そのペアを特定したら、そのペアを新しいトークンに置き換えて語彙に追加する。"
  },
  {
    "start": 1473246,
    "end": 1484534,
    "text": "例えば、ここではaaというバイト・ペアが最も頻繁に出現するので、新しいトークンを作り、それをcapital zと呼ぶことにして、aaが出現するたびにzで置き換える。"
  },
  {
    "start": 1484732,
    "end": 1487400,
    "text": "今、ここには2つのZがある。"
  },
  {
    "start": 1487790,
    "end": 1507018,
    "text": "ここでは、語彙サイズ4の11文字のシーケンスを、9個のトークンのみのシーケンスに変換している。"
  },
  {
    "start": 1507194,
    "end": 1509470,
    "text": "またこのプロセスを繰り返すことができる。"
  },
  {
    "start": 1509620,
    "end": 1516398,
    "text": "もう一度シーケンスを見て、最も頻度の高いトークンのペアを特定する。"
  },
  {
    "start": 1516574,
    "end": 1518580,
    "text": "それが今、アブになったとしよう。"
  },
  {
    "start": 1519030,
    "end": 1523170,
    "text": "では、Abをyと呼ぶ新しいトークンで置き換えよう。"
  },
  {
    "start": 1523320,
    "end": 1527862,
    "text": "yはabとなり、Abのすべての出現箇所がyに置き換えられる。"
  },
  {
    "start": 1527996,
    "end": 1529880,
    "text": "結局、こうなる。"
  },
  {
    "start": 1530490,
    "end": 1543306,
    "text": "今、私たちのシークエンスには123-4567文字しかないが、4つの語彙要素や5つの語彙要素だけでなく、6つの語彙要素がある。"
  },
  {
    "start": 1543488,
    "end": 1556318,
    "text": "最後のラウンドでは、再び配列に目を通し、フレーズzy、またはペアzyが最も一般的であることを見つけ、それをもう1度別の文字（仮にxとする）に置き換える。"
  },
  {
    "start": 1556404,
    "end": 1561722,
    "text": "XはZyであり、Zyの出現箇所をすべて置き換えると、次のようになる。"
  },
  {
    "start": 1561866,
    "end": 1583400,
    "text": "基本的に、このプロセスを経た後、11個のトークンで構成され、語彙の長さが4であったものが、12345個のトークンで構成され、語彙の長さが7になる。"
  },
  {
    "start": 1583930,
    "end": 1589314,
    "text": "このようにして、新しいトークンを鋳造するときにシーケンスを繰り返し圧縮することができる。"
  },
  {
    "start": 1589442,
    "end": 1594054,
    "text": "バイトシーケンスから始めるのとまったく同じ方法で。"
  },
  {
    "start": 1594182,
    "end": 1607870,
    "text": "語彙のサイズは256だが、これからこれらを調べて、最も多く出現するバイト・ペアを見つけ、新しいトークンの鋳造を繰り返し、語彙に追加し、何かを置き換える。"
  },
  {
    "start": 1608020,
    "end": 1620526,
    "text": "このようにして、圧縮されたトレーニングデータセットと、任意のシーケンスをこの語彙を使ってエンコードし、デコードして文字列に戻すアルゴリズムが完成する。"
  },
  {
    "start": 1620638,
    "end": 1622546,
    "text": "では、そのすべてを実行に移そう。"
  },
  {
    "start": 1622728,
    "end": 1624034,
    "text": "私がしたことはこうだ。"
  },
  {
    "start": 1624152,
    "end": 1630582,
    "text": "このブログの最初の段落をコピーペーストしてテキストにしたんだ。"
  },
  {
    "start": 1630716,
    "end": 1633240,
    "text": "これはとても長いセリフだ。"
  },
  {
    "start": 1634170,
    "end": 1639078,
    "text": "さて、トークンを取得するには、先ほど述べたように、テキストをUTF 8にエンコードするだけだ。"
  },
  {
    "start": 1639244,
    "end": 1644090,
    "text": "この時点でのトークンは、生のバイトのシングル・ストリームとなる。"
  },
  {
    "start": 1644510,
    "end": 1656922,
    "text": "Pythonで操作したり視覚化したりするのが簡単になるように、バイト・オブジェクトの代わりにすべてのバイトを整数に変換してリストを作ります。"
  },
  {
    "start": 1657066,
    "end": 1658414,
    "text": "そのすべてを印刷します。"
  },
  {
    "start": 1658452,
    "end": 1668020,
    "text": "これは元の段落で、長さは533コードポイントである。"
  },
  {
    "start": 1668550,
    "end": 1678298,
    "text": "この時点で長さが616バイト、つまり616トークンであることがわかる。"
  },
  {
    "start": 1678414,
    "end": 1690518,
    "text": "これがなぜ多いかというと、単純なASCII文字、つまりシンプルな文字の多くは1バイトになるだけだが、ユニコードのより複雑な文字の多くは4バイトまでの複数バイトになるからだ。"
  },
  {
    "start": 1690604,
    "end": 1692780,
    "text": "だから、我々はそのサイズを拡大している。"
  },
  {
    "start": 1693630,
    "end": 1703594,
    "text": "さて、アルゴリズムの最初のステップとしてやりたいことは、ここを繰り返し、最も頻繁に出現するバイトのペアを見つけることだ。"
  },
  {
    "start": 1703712,
    "end": 1711598,
    "text": "もしあなたが副業でノートを作っているのなら、基本的にリンクをクリックしてこのノートを見つけ、その機能を自分で書いてみることをお勧めする。"
  },
  {
    "start": 1711764,
    "end": 1716138,
    "text": "そうでなければ、まずここに来て、最も一般的なペアを見つける関数を実装するつもりだ。"
  },
  {
    "start": 1716234,
    "end": 1717682,
    "text": "よし、それで思いついたのがこれだ。"
  },
  {
    "start": 1717736,
    "end": 1721374,
    "text": "これを実装する方法はいろいろあるが、私はgetstatsという関数を呼んでいる。"
  },
  {
    "start": 1721422,
    "end": 1723118,
    "text": "これは整数のリストを期待する。"
  },
  {
    "start": 1723214,
    "end": 1726606,
    "text": "基本的には辞書を使ってカウントを記録している。"
  },
  {
    "start": 1726718,
    "end": 1733240,
    "text": "ということは、これはリストの連続する要素を反復するパイソン的な方法である。"
  },
  {
    "start": 1733610,
    "end": 1739122,
    "text": "ということで、ここではすべてのペアを1ずつインクリメントして記録している。"
  },
  {
    "start": 1739186,
    "end": 1743914,
    "text": "ここにあるすべてのトークンでこれを呼び出すと、スタッツはここに出てくる。"
  },
  {
    "start": 1744032,
    "end": 1750620,
    "text": "これが辞書で、キーは連続する要素のタプルで、これがカウントである。"
  },
  {
    "start": 1750990,
    "end": 1764190,
    "text": "少し良い方法で印刷するために、私が好きな方法のひとつで、ここで少し複合的に印刷します。"
  },
  {
    "start": 1764350,
    "end": 1768340,
    "text": "辞書に対して呼び出された項目は、キー値のペアを返す。"
  },
  {
    "start": 1768870,
    "end": 1777798,
    "text": "なぜなら、値キーのリストであれば、ソートを呼び出すことができるからだ。"
  },
  {
    "start": 1777884,
    "end": 1789254,
    "text": "デフォルトでは、Pythonは最初の要素を使用します。この場合、タプルが与えられていれば、値でソートし、降順になるように反転してそれを表示します。"
  },
  {
    "start": 1789292,
    "end": 1797162,
    "text": "基本的には、101コンマ32が最もよく出現する連続ペアのようで、20回出現している。"
  },
  {
    "start": 1797296,
    "end": 1798426,
    "text": "私たちはそれを再確認することができる。"
  },
  {
    "start": 1798448,
    "end": 1799978,
    "text": "それは理にかなっている。"
  },
  {
    "start": 1800064,
    "end": 1809066,
    "text": "100と132を検索すると、このペアが20回出現していることがわかる。"
  },
  {
    "start": 1809258,
    "end": 1816190,
    "text": "そのペアが何なのかを知りたい場合は、PythonではOrdの反対語であるcharを使うことができる。"
  },
  {
    "start": 1816270,
    "end": 1824546,
    "text": "これをユニコードのコード・ポイントにすると、101と32になり、これがeとスペースであることがわかる。"
  },
  {
    "start": 1824728,
    "end": 1830630,
    "text": "つまり、多くの単語がeで終わっている。"
  },
  {
    "start": 1830700,
    "end": 1832806,
    "text": "エスパスを例に挙げよう。"
  },
  {
    "start": 1832988,
    "end": 1834534,
    "text": "ここではそれがたくさん起こっている。"
  },
  {
    "start": 1834572,
    "end": 1836450,
    "text": "これが最も一般的なペアである。"
  },
  {
    "start": 1836610,
    "end": 1841638,
    "text": "さて、最も一般的なペアを特定したので、このシーケンスを反復処理したい。"
  },
  {
    "start": 1841814,
    "end": 1846186,
    "text": "256というIDで新しいトークンを鋳造するんだよね？"
  },
  {
    "start": 1846288,
    "end": 1849514,
    "text": "なぜなら、これらのトークンは現在、ゼロから2 5 5まであるからだ。"
  },
  {
    "start": 1849632,
    "end": 1858254,
    "text": "新しいトークンを作成すると、idは256となり、このリスト全体を繰り返し処理することになる。"
  },
  {
    "start": 1858372,
    "end": 1861870,
    "text": "101コンマ32を見るたびに。"
  },
  {
    "start": 1861940,
    "end": 1864594,
    "text": "それを2、5、6と入れ替えるんだ。"
  },
  {
    "start": 1864792,
    "end": 1869570,
    "text": "今すぐそれを実行に移そう。"
  },
  {
    "start": 1869720,
    "end": 1874718,
    "text": "ノートを汚さないために、最初にコメントした。"
  },
  {
    "start": 1874904,
    "end": 1880146,
    "text": "これはパイソンで最高順位のペアを取得するいい方法だ。"
  },
  {
    "start": 1880258,
    "end": 1887960,
    "text": "基本的には、このディクショナリの統計に対してmaxを呼び出しており、これにより最大のキーが返される。"
  },
  {
    "start": 1888490,
    "end": 1891430,
    "text": "となると、問題はキーの順位だ。"
  },
  {
    "start": 1891590,
    "end": 1904358,
    "text": "キーをランク付けする関数を提供することができ、その関数は単なるstats getで、stats getは基本的に値を返すので、値によってランク付けし、最大のキーを取得する。"
  },
  {
    "start": 1904464,
    "end": 1910000,
    "text": "100と132を統合するには、先ほど見たように101と32のコンマが必要だ。"
  },
  {
    "start": 1910850,
    "end": 1914558,
    "text": "これは私が書いた関数だが、これにもいろいろなバージョンがある。"
  },
  {
    "start": 1914724,
    "end": 1922290,
    "text": "IDのリストと置き換えたいペアを受け取り、そのペアを新しいインデックスiDXに置き換える。"
  },
  {
    "start": 1923030,
    "end": 1924878,
    "text": "IDを反復する。"
  },
  {
    "start": 1924974,
    "end": 1927870,
    "text": "ペアを見つけたら、IDXと交換する。"
  },
  {
    "start": 1928030,
    "end": 1935000,
    "text": "この新しいリストを作成し、ゼロから始めて、このリスト全体を左から右に順次見ていく。"
  },
  {
    "start": 1935610,
    "end": 1940550,
    "text": "ここでは、現在の位置とペアが等しいかどうかをチェックしている。"
  },
  {
    "start": 1942090,
    "end": 1944406,
    "text": "ここでは、ペアが一致するかどうかをチェックしている。"
  },
  {
    "start": 1944518,
    "end": 1956478,
    "text": "このリストの一番右の要素にいるときに、一番最後の位置で圏外にならないようにするためだ。"
  },
  {
    "start": 1956564,
    "end": 1959210,
    "text": "そうでなければ、out of boundsエラーとなる。"
  },
  {
    "start": 1959290,
    "end": 1962714,
    "text": "私たちは、最後の要素にならないようにしなければならない。"
  },
  {
    "start": 1962842,
    "end": 1965538,
    "text": "というのは嘘だろう。"
  },
  {
    "start": 1965704,
    "end": 1974066,
    "text": "一致するものがあれば、その置換インデックスを新しいリストに追加し、位置を2つインクリメントする。"
  },
  {
    "start": 1974088,
    "end": 1986022,
    "text": "マッチするペアが見つからなければ、その位置の要素をコピーして1つインクリメントし、これを返す。"
  },
  {
    "start": 1986156,
    "end": 1987702,
    "text": "とても小さなおもちゃの例だ。"
  },
  {
    "start": 1987756,
    "end": 1998378,
    "text": "566791のリストがあり、67を9に置き換えたい場合、これを呼び出せば、要求通りの結果が得られる。"
  },
  {
    "start": 1998464,
    "end": 2001740,
    "text": "ここでは6 7が9 9に置き換えられている。"
  },
  {
    "start": 2002430,
    "end": 2008298,
    "text": "では、トークンを受け取る実際のユースケースのために、このコメントを解除しておこう。"
  },
  {
    "start": 2008394,
    "end": 2013970,
    "text": "ここでトップ・ペアを取り、2 5 6に置き換えてトークン2を得たい。"
  },
  {
    "start": 2014040,
    "end": 2016930,
    "text": "これを実行すると、次のようになる。"
  },
  {
    "start": 2018070,
    "end": 2027106,
    "text": "以前はこのリストに長さ616があったことを思い出してほしい。"
  },
  {
    "start": 2027208,
    "end": 2027522,
    "text": "そうだろう？"
  },
  {
    "start": 2027576,
    "end": 2031910,
    "text": "これは20回発生するため、理にかなっている。"
  },
  {
    "start": 2032250,
    "end": 2037800,
    "text": "さらに、私たちはここで2、5、6を見つけることができる。"
  },
  {
    "start": 2038330,
    "end": 2042066,
    "text": "さらに、ダブルチェックで、100と132は出てこないはずだ。"
  },
  {
    "start": 2042188,
    "end": 2044762,
    "text": "これはオリジナルの配列で、たくさんある。"
  },
  {
    "start": 2044816,
    "end": 2047962,
    "text": "の配列には100と132はない。"
  },
  {
    "start": 2048096,
    "end": 2053290,
    "text": "この1つのペアのマージに成功したので、あとはこれを繰り返すだけだ。"
  },
  {
    "start": 2053360,
    "end": 2057486,
    "text": "もう一度シーケンスを確認し、最も一般的なペアを見つけ、それを置き換える。"
  },
  {
    "start": 2057588,
    "end": 2062586,
    "text": "では、これらの関数を使ったwhileループを書いてみよう。"
  },
  {
    "start": 2062698,
    "end": 2065018,
    "text": "何回やるんだ？"
  },
  {
    "start": 2065124,
    "end": 2066306,
    "text": "まあ、それは完全に私たち次第だ。"
  },
  {
    "start": 2066328,
    "end": 2074286,
    "text": "ハイパーパラメーターとして、ステップ数が多ければ多いほど、語彙は増え、シーケンスは短くなる。"
  },
  {
    "start": 2074398,
    "end": 2078586,
    "text": "実際には、私たちが最もうまくいくスイートスポットがある。"
  },
  {
    "start": 2078718,
    "end": 2083746,
    "text": "これはハイパーパラメーターのようなもので、これを調整することで良い語彙サイズを見つけることができる。"
  },
  {
    "start": 2083858,
    "end": 2087634,
    "text": "一例として、GPT4は現在およそ10万トークンを使用している。"
  },
  {
    "start": 2087762,
    "end": 2091450,
    "text": "現在のところ、妥当な数字だと思う。"
  },
  {
    "start": 2091520,
    "end": 2093286,
    "text": "その代わり、それらは大規模な言語モデルである。"
  },
  {
    "start": 2093398,
    "end": 2098454,
    "text": "では、これらのステップを繰り返しながら、すべてをまとめていくことにしよう。"
  },
  {
    "start": 2098582,
    "end": 2110462,
    "text": "さて、whileループに入る前に、もうひとつセルを追加したい。"
  },
  {
    "start": 2110596,
    "end": 2115822,
    "text": "基本的には、より長いテキストを使用することで、バイト・ペアの代表的な統計を取ることができる。"
  },
  {
    "start": 2115886,
    "end": 2120340,
    "text": "その方が長い文章になるので、より賢明な結果が得られるだけだ。"
  },
  {
    "start": 2121270,
    "end": 2127342,
    "text": "ここに生のテキストがあるので、UTF 8エンコーディングを使ってバイトにエンコードする。"
  },
  {
    "start": 2127486,
    "end": 2135720,
    "text": "では、前回と同じように、Pythonではこれを整数のリストに変えているだけだ。"
  },
  {
    "start": 2136170,
    "end": 2143558,
    "text": "そこで、ループの中で実際にマージを行うために私が考えたコードがこれだ。"
  },
  {
    "start": 2143734,
    "end": 2146630,
    "text": "この2つの関数は、上で説明したものと同じである。"
  },
  {
    "start": 2146710,
    "end": 2151020,
    "text": "ここに掲載したのは、あくまでも参考にしてもらうためだ。"
  },
  {
    "start": 2151550,
    "end": 2153894,
    "text": "この2つは同一である。"
  },
  {
    "start": 2153942,
    "end": 2156106,
    "text": "そして、これが私が追加した新しいコードである。"
  },
  {
    "start": 2156298,
    "end": 2162238,
    "text": "まず最初にしたいことは、トークナイザーに持たせたい最終的な語彙のサイズを決めることだ。"
  },
  {
    "start": 2162324,
    "end": 2167150,
    "text": "前述したように、これはハイパーパラメーターであり、最高のパフォーマンスに応じて何らかの方法で設定する。"
  },
  {
    "start": 2167310,
    "end": 2179906,
    "text": "というのも、生バイトのトークンがすでに256個あるからだ。"
  },
  {
    "start": 2180098,
    "end": 2186966,
    "text": "276に到達するためには、20の新しいトークンを追加するために20回のマージを行う必要がある。"
  },
  {
    "start": 2187068,
    "end": 2190840,
    "text": "これはPythonでリストのコピーを作成する一つの方法である。"
  },
  {
    "start": 2191370,
    "end": 2198022,
    "text": "トークンのリストをリストでラップすることで、Pythonは個々の要素からなる新しいリストを作成します。"
  },
  {
    "start": 2198086,
    "end": 2199770,
    "text": "これは単なるコピー操作である。"
  },
  {
    "start": 2200770,
    "end": 2203994,
    "text": "そして、ここではマージ辞書を作成している。"
  },
  {
    "start": 2204122,
    "end": 2211930,
    "text": "このマージ辞書は、基本的に子1、子2の新しいトークンへのマッピングを維持する。"
  },
  {
    "start": 2212090,
    "end": 2222078,
    "text": "つまり、これから構築するのはマージのバイナリツリーなのだが、実はこれはツリーではない。"
  },
  {
    "start": 2222174,
    "end": 2231238,
    "text": "私たちの場合、まず一番下の葉から始める。これは個々のバイトで、256個のトークンをスタートさせ、一度に2個ずつマージしていく。"
  },
  {
    "start": 2231324,
    "end": 2237910,
    "text": "だから、これらの要素を融合させることで、木というより森になる。"
  },
  {
    "start": 2238250,
    "end": 2244646,
    "text": "20のマージについて、最もよく発生するペアを見つけよう。"
  },
  {
    "start": 2244758,
    "end": 2247834,
    "text": "そのために新しいトークン整数を鋳造するつもりだ。"
  },
  {
    "start": 2247952,
    "end": 2249482,
    "text": "ここではゼロから始める。"
  },
  {
    "start": 2249536,
    "end": 2259210,
    "text": "2、5、6から開始し、それをマージすると表示し、そのペアのすべてを新しく鋳造されたトークンに置き換える。"
  },
  {
    "start": 2259370,
    "end": 2265650,
    "text": "この整数のペアが新しい整数に統合されたことを記録する。"
  },
  {
    "start": 2266390,
    "end": 2270290,
    "text": "これを実行すると、次のような出力が得られる。"
  },
  {
    "start": 2272070,
    "end": 2282582,
    "text": "例えば、最初のマージは以前とまったく同じで、100と132のトークンが新しいトークン、2、5、6にマージされた。"
  },
  {
    "start": 2282716,
    "end": 2289122,
    "text": "ここで、個々のトークン101と32は、結合後もシーケンス中に出現する可能性があることに留意されたい。"
  },
  {
    "start": 2289186,
    "end": 2293274,
    "text": "それが2つ5つ6つとなるのは、正確に連続して発生した場合だけだ。"
  },
  {
    "start": 2293312,
    "end": 2302710,
    "text": "さて、ここで特に注目していただきたいのは、新しく鋳造されたトークンである2 5 6トークンもまた、統合の対象であるということだ。"
  },
  {
    "start": 2302790,
    "end": 2309200,
    "text": "ここでの20回目の合流は、2-5-6と2-5-9が合流し、2-7-5となった。"
  },
  {
    "start": 2309730,
    "end": 2315482,
    "text": "トークンを置き換えるたびに、トークンは次の反復ラウンドでマージの対象となる。"
  },
  {
    "start": 2315626,
    "end": 2325794,
    "text": "そのため、個々のツリーではなく、小さなバイナリー・フォレストのようなものを構築している。"
  },
  {
    "start": 2325922,
    "end": 2329480,
    "text": "特に、私たちはこのトークン・リストからスタートした。"
  },
  {
    "start": 2330090,
    "end": 2340914,
    "text": "24,000バイトでスタートし、20回マージした結果、19,000トークンしかない。"
  },
  {
    "start": 2341042,
    "end": 2346058,
    "text": "したがって、単純に両者を割った圧縮比はおよそ1.27となる。"
  },
  {
    "start": 2346144,
    "end": 2351150,
    "text": "これが、たった20回のマージでこのテキストを圧縮できた量だ。"
  },
  {
    "start": 2351970,
    "end": 2358000,
    "text": "もちろん、語彙を増やせば増やすほど、圧縮率は高くなる。"
  },
  {
    "start": 2360210,
    "end": 2360862,
    "text": "最後に"
  },
  {
    "start": 2360996,
    "end": 2365614,
    "text": "言うなれば、トークナイザーのトレーニングのようなものだ。"
  },
  {
    "start": 2365732,
    "end": 2376498,
    "text": "さて、私が言いたかったことのひとつは、この図で説明できるかもしれませんが、トークナイザーは大規模な言語モデル自体とはまったく別のオブジェクトだということです。"
  },
  {
    "start": 2376664,
    "end": 2381522,
    "text": "この講義では、LLM自体にはあまり触れず、トークナイザーをトレーニングするだけです。"
  },
  {
    "start": 2381586,
    "end": 2384550,
    "text": "これは通常、完全に別の前処理段階である。"
  },
  {
    "start": 2384700,
    "end": 2391162,
    "text": "大規模な言語モデルが潜在的に異なるトレーニングセットを持つように、トークナイザーは独自のトレーニングセットを持つ。"
  },
  {
    "start": 2391296,
    "end": 2395290,
    "text": "トークナイザーには、トークナイザーを訓練するための文書の訓練セットがあります。"
  },
  {
    "start": 2395630,
    "end": 2403298,
    "text": "そして、このトークナイザーの語彙を訓練するために、上で見たようなバイトペアリング・エンコーディング・アルゴリズムを実行している。"
  },
  {
    "start": 2403494,
    "end": 2405022,
    "text": "は独自のトレーニングセットを持っている。"
  },
  {
    "start": 2405076,
    "end": 2408750,
    "text": "これは、最初に一度だけ実行する前処理段階である。"
  },
  {
    "start": 2409810,
    "end": 2413530,
    "text": "トークナイザーはバイパス符号化アルゴリズムを使って学習されます。"
  },
  {
    "start": 2413690,
    "end": 2422126,
    "text": "トークナイザーが学習され、語彙とマージができれば、エンコードとデコードの両方ができるようになる。"
  },
  {
    "start": 2422238,
    "end": 2424066,
    "text": "この2本の矢印だ。"
  },
  {
    "start": 2424168,
    "end": 2431542,
    "text": "トークナイザーは、先ほど見たように、Unicodeのコードポイントの並びである生テキスト間の翻訳レイヤーです。"
  },
  {
    "start": 2431676,
    "end": 2440200,
    "text": "生テキストをトークン列に変換することもできるし、逆にトークン列を生テキストに変換することもできる。"
  },
  {
    "start": 2441690,
    "end": 2449178,
    "text": "さて、トークナイザーの訓練とマージができたところで、エンコードとデコードのステップに移ろう。"
  },
  {
    "start": 2449264,
    "end": 2453838,
    "text": "テキストをくれたら、ここにトークンがあるし、逆にトークンをくれたら、ここにテキストがある。"
  },
  {
    "start": 2454004,
    "end": 2472862,
    "text": "そして、言語モデルはその後にステップ2として学習される。一般的に、最先端のアプリケーションでは、言語モデルの学習データをすべて取得し、それをトークナイザーに通して、すべてを大量のトークン列に変換する。"
  },
  {
    "start": 2472926,
    "end": 2474610,
    "text": "そうすれば、生テキストを捨てることができる。"
  },
  {
    "start": 2474680,
    "end": 2478986,
    "text": "トークン自体はディスクに保存されている。"
  },
  {
    "start": 2479038,
    "end": 2482646,
    "text": "これは、大規模な言語モデルが学習時に実際に読み取っているものである。"
  },
  {
    "start": 2482748,
    "end": 2487030,
    "text": "これは、1つの大規模な前処理段階として取ることができる1つのアプローチだ。"
  },
  {
    "start": 2488810,
    "end": 2493334,
    "text": "ああ、基本的に一番伝えたいことは、これは完全に別のステージだということだ。"
  },
  {
    "start": 2493382,
    "end": 2496090,
    "text": "通常、独自の全トレーニングセットを持っている。"
  },
  {
    "start": 2496240,
    "end": 2500474,
    "text": "これらのトレーニングセットは、トークナイザーと大規模言語モデルで異なるものにしたいかもしれません。"
  },
  {
    "start": 2500592,
    "end": 2511406,
    "text": "例えば、トークン・サイザーをトレーニングする場合、先ほど述べたように、私たちは英語のテキストのパフォーマンスだけを気にしているわけではありません。"
  },
  {
    "start": 2511508,
    "end": 2535720,
    "text": "というのも、トークナイザーの学習セットに含まれるさまざまな言語の量によって、トークン空間におけるこの種のデータの密度が決まるからです。"
  },
  {
    "start": 2536090,
    "end": 2549530,
    "text": "ということは、直感的に言えば、ある程度の量のデータを追加した場合、例えば、トークン・サイザーのトレーニング・セットに大量の日本語データを追加した場合、より多くの日本語トークンがマージされ、その結果、日本語のシーケンスが短くなるということです。"
  },
  {
    "start": 2549870,
    "end": 2557200,
    "text": "これは、トークン空間で作業できるコンテキストの長さが有限である大規模言語モデルにとって有益なことだ。"
  },
  {
    "start": 2557570,
    "end": 2559022,
    "text": "それが理解できればいいのだが......。"
  },
  {
    "start": 2559156,
    "end": 2562042,
    "text": "次はエンコードとデコードについて説明しよう。"
  },
  {
    "start": 2562106,
    "end": 2563882,
    "text": "これでトークナイザーのトレーニングが完了した。"
  },
  {
    "start": 2563946,
    "end": 2568046,
    "text": "マージはできたが、次はエンコードとデコードをどうするか？"
  },
  {
    "start": 2568158,
    "end": 2571666,
    "text": "では、まずデコーディングから始めよう。"
  },
  {
    "start": 2571768,
    "end": 2577106,
    "text": "トークン列が与えられたら、トークナイザーを通してpython文字列オブジェクトを返してみましょう。"
  },
  {
    "start": 2577208,
    "end": 2578580,
    "text": "生テキスト。"
  },
  {
    "start": 2578950,
    "end": 2581190,
    "text": "これが実装したい機能だ。"
  },
  {
    "start": 2581610,
    "end": 2584850,
    "text": "整数のリストが与えられたので、Pythonの文字列を返します。"
  },
  {
    "start": 2584930,
    "end": 2587238,
    "text": "もしよろしければ、ご自分でこの機能を実装してみてください。"
  },
  {
    "start": 2587324,
    "end": 2588418,
    "text": "楽しい練習だよ。"
  },
  {
    "start": 2588514,
    "end": 2591210,
    "text": "そうでなければ、私自身の解決策を貼り付け始めるつもりだ。"
  },
  {
    "start": 2592110,
    "end": 2594778,
    "text": "やり方はいろいろある。"
  },
  {
    "start": 2594944,
    "end": 2596186,
    "text": "これが一つの方法だ。"
  },
  {
    "start": 2596368,
    "end": 2600990,
    "text": "vocabと呼ぶ前処理変数のようなものを作ろうと思う。"
  },
  {
    "start": 2601890,
    "end": 2611226,
    "text": "vocabは、トークンIDからそのトークンのバイトオブジェクトへのマッピング、またはPythonの辞書です。"
  },
  {
    "start": 2611418,
    "end": 2623234,
    "text": "まず、0から2、5、5までのトークンの生バイトから始め、次にすべてのマージの順番に進み、ここで足し算をすることで、このボキャブラリー・リストに入力する。"
  },
  {
    "start": 2623352,
    "end": 2629766,
    "text": "これは基本的に、最初の子の後に続く2番目の子のバイト表現である。"
  },
  {
    "start": 2629868,
    "end": 2631714,
    "text": "これらはバイトオブジェクトであることを忘れないでほしい。"
  },
  {
    "start": 2631762,
    "end": 2635234,
    "text": "この追加は、2つのバイトオブジェクトの追加である。"
  },
  {
    "start": 2635282,
    "end": 2636706,
    "text": "単なる連結だ。"
  },
  {
    "start": 2636898,
    "end": 2638440,
    "text": "それがここで得られるものだ。"
  },
  {
    "start": 2639610,
    "end": 2653046,
    "text": "Pythonの辞書をドット・アイテムを使って反復しているのだが、マージ辞書にアイテムを挿入した順番通りに実行することが重要なのだ。"
  },
  {
    "start": 2653158,
    "end": 2656510,
    "text": "幸運なことに、python 3.7からは、これが保証されている。"
  },
  {
    "start": 2656580,
    "end": 2662698,
    "text": "Python 3.7以前では、この繰り返しは、マージに要素を挿入する方法に関して順番が狂っていたかもしれません。"
  },
  {
    "start": 2662794,
    "end": 2667620,
    "text": "これはうまくいかなかったかもしれないが、我々は最新のPythonを使っているので大丈夫だ。"
  },
  {
    "start": 2668310,
    "end": 2674130,
    "text": "そしてここで、IDが与えられたら、まずトークンを取得する。"
  },
  {
    "start": 2675910,
    "end": 2682706,
    "text": "この実装方法は、すべてのIDを繰り返し、ボキャブを使ってバイトを検索している。"
  },
  {
    "start": 2682818,
    "end": 2689414,
    "text": "これは、Pythonでこれらのバイトを連結してトークンを生成する方法のひとつだ。"
  },
  {
    "start": 2689542,
    "end": 2692918,
    "text": "ということは、この時点でのトークンは生のバイトということになる。"
  },
  {
    "start": 2693094,
    "end": 2698822,
    "text": "Utf8を使ってデコードし、パイソンの文字列に戻さなければならない。"
  },
  {
    "start": 2698966,
    "end": 2704570,
    "text": "以前は文字列オブジェクトのドット・エンコードを呼び出してバイトを取得していたが、今回はその逆だ。"
  },
  {
    "start": 2704650,
    "end": 2712640,
    "text": "バイトを受け取り、バイト・オブジェクトのデコードを呼び出してPythonの文字列を取得し、テキストを返す。"
  },
  {
    "start": 2714210,
    "end": 2716690,
    "text": "これが私たちにできることだ。"
  },
  {
    "start": 2716760,
    "end": 2723278,
    "text": "さて、実はこれには私が実装した方法に問題があり、実際にエラーを投げてしまう可能性がある。"
  },
  {
    "start": 2723374,
    "end": 2731458,
    "text": "なぜこのコードが、一連のIDを差し込むとエラーになるのかを考えてみよう。"
  },
  {
    "start": 2731474,
    "end": 2732790,
    "text": "それは不運だ。"
  },
  {
    "start": 2733450,
    "end": 2735094,
    "text": "この問題を実証してみよう。"
  },
  {
    "start": 2735212,
    "end": 2742940,
    "text": "97のようなものを解読しようとすると、ここにaという文字が返ってくる。"
  },
  {
    "start": 2743470,
    "end": 2761834,
    "text": "128を単一要素としてデコードしようとすると、トークン128は文字列やPythonオブジェクトのユニコードデコードエラーutfA can't decode byte zero x 80となる。"
  },
  {
    "start": 2761882,
    "end": 2762782,
    "text": "どういう意味ですか？"
  },
  {
    "start": 2762916,
    "end": 2770402,
    "text": "さて、この意味を理解するためには、先ほど簡単に紹介したUTF 8のページに戻る必要がある。"
  },
  {
    "start": 2770536,
    "end": 2775666,
    "text": "基本的に、UTfの8バイトが取る特定のスキーマがある。"
  },
  {
    "start": 2775848,
    "end": 2785446,
    "text": "特に、ユニコード文字のいくつかについてマルチバイトオブジェクトを持つ場合、エンコーディングがどのように機能するかについて、特別なエンベロープを持たなければならない。"
  },
  {
    "start": 2785628,
    "end": 2789618,
    "text": "つまり、ここで起きているのは無効なスタートバイトである。"
  },
  {
    "start": 2789714,
    "end": 2795850,
    "text": "それは、2進数表現である128が、1の後にすべてのゼロが続くからだ。"
  },
  {
    "start": 2796190,
    "end": 2805454,
    "text": "というのも、1の後に0が続くのは、いわばどのルールにも当てはまらないからだ。"
  },
  {
    "start": 2805572,
    "end": 2809038,
    "text": "これは無効な開始バイト（バイト1）である。"
  },
  {
    "start": 2809204,
    "end": 2816738,
    "text": "この1つの後に1が続き、その後に0が続き、そしてここにユニコードの内容を余すことなく入れなければならない。"
  },
  {
    "start": 2816904,
    "end": 2822206,
    "text": "基本的に、我々はUTf8の規格に正確に準拠していないので、これを解読することはできない。"
  },
  {
    "start": 2822318,
    "end": 2832450,
    "text": "従って、これを修正する方法は、パイソンのバイトデコード関数でこのエラー等号を使用することである。"
  },
  {
    "start": 2832610,
    "end": 2834738,
    "text": "デフォルトのエラーは厳格である。"
  },
  {
    "start": 2834834,
    "end": 2840098,
    "text": "有効なUTF 8バイトエンコーディングでない場合はエラーをスローします。"
  },
  {
    "start": 2840194,
    "end": 2843462,
    "text": "エラー処理に関しては、いろいろなことが考えられる。"
  },
  {
    "start": 2843606,
    "end": 2850082,
    "text": "特にstrictの代わりにreplaceに変更してみよう。"
  },
  {
    "start": 2850246,
    "end": 2854554,
    "text": "この特別なマーカーに置き換わる。"
  },
  {
    "start": 2854682,
    "end": 2856490,
    "text": "これが置換文字である。"
  },
  {
    "start": 2856650,
    "end": 2862400,
    "text": "エラーはreplaceに等しい。"
  },
  {
    "start": 2863990,
    "end": 2869186,
    "text": "基本的に、すべてのバイト列がUTf8として有効なわけではない。"
  },
  {
    "start": 2869368,
    "end": 2881958,
    "text": "たとえば、大規模な言語モデルがトークンを不適切な方法で予測した場合、有効なUTF8に該当しない可能性があり、デコードできなくなります。"
  },
  {
    "start": 2882124,
    "end": 2892042,
    "text": "標準的なプラクティスは、基本的にerrors equal replaceを使うことであり、これはOpenAIのコードにも見られることだ。"
  },
  {
    "start": 2892176,
    "end": 2902086,
    "text": "基本的に、出力にこのような文字が表示されるときはいつも、何かが間違っていて、LM出力が有効でなかった場合である。"
  },
  {
    "start": 2902278,
    "end": 2904286,
    "text": "じゃあ、次は逆を行こう。"
  },
  {
    "start": 2904388,
    "end": 2911230,
    "text": "文字列が与えられたら、それをトークンにエンコードする。"
  },
  {
    "start": 2911970,
    "end": 2915426,
    "text": "これは、私たちが関心を抱いている関数のシグネチャーである。"
  },
  {
    "start": 2915608,
    "end": 2919966,
    "text": "これは基本的に、トークンの整数のリストを表示する。"
  },
  {
    "start": 2920078,
    "end": 2924686,
    "text": "楽しい練習がしたいのなら、自分でもやってみるといい。"
  },
  {
    "start": 2924878,
    "end": 2927830,
    "text": "ここで一時停止してください。そうでなければ、私のソリューションを入れ始めます。"
  },
  {
    "start": 2928810,
    "end": 2930870,
    "text": "これにはいろいろなやり方がある。"
  },
  {
    "start": 2930940,
    "end": 2936520,
    "text": "これは私が思いついた方法のひとつだ。"
  },
  {
    "start": 2937210,
    "end": 2944854,
    "text": "まず最初にやることは、テキストをUTF 8にエンコードして生のバイトを得ることだ。"
  },
  {
    "start": 2944982,
    "end": 2951398,
    "text": "そして、前回と同じように、bytesオブジェクトでlistを呼び出し、それらのバイトの整数のリストを取得する。"
  },
  {
    "start": 2951574,
    "end": 2955338,
    "text": "これが開始トークンであり、シーケンスの生のバイトである。"
  },
  {
    "start": 2955514,
    "end": 2965690,
    "text": "もちろん、上記のマージ辞書によれば、そしてこれがマージであったことを思い出せば、いくつかのバイトはこのルックアップに従ってマージされるかもしれない。"
  },
  {
    "start": 2965850,
    "end": 2972350,
    "text": "それに加えて、マージは上から下へと作られており、これはマージに何かを挿入する順番のようなものであることを覚えておいてほしい。"
  },
  {
    "start": 2972510,
    "end": 2983830,
    "text": "というのも、例えば、このマージは、ここでマージされた2つの5と6に依存しているからだ。"
  },
  {
    "start": 2983980,
    "end": 2987366,
    "text": "上から下に順番に行くしかない。"
  },
  {
    "start": 2987388,
    "end": 3009070,
    "text": "これから何かをマージするのであれば、何度かマージすることになると思うので、while trueを実行し、これに従ってマージしていい連続したバイトのペアを見つけたい。"
  },
  {
    "start": 3009970,
    "end": 3020078,
    "text": "getstatsは基本的に、トークンのシーケンスの中ですべてのペアが何回出現するかをカウントし、それを辞書として返してくれる。"
  },
  {
    "start": 3020254,
    "end": 3027950,
    "text": "辞書は、すべての異なるバイトペアから、それらが出現する回数へのマッピングであった。"
  },
  {
    "start": 3028030,
    "end": 3028660,
    "text": "そうだね。"
  },
  {
    "start": 3029430,
    "end": 3036450,
    "text": "この時点では、これらのペアが連続する中で何回現れるかを気にする必要はない。"
  },
  {
    "start": 3036530,
    "end": 3039682,
    "text": "だから、基本的にはこの辞書のキーだけを使うつもりだ。"
  },
  {
    "start": 3039746,
    "end": 3043900,
    "text": "私が気にするのは、合併候補の集合だけだ。"
  },
  {
    "start": 3044590,
    "end": 3049030,
    "text": "ここで、ループのこの段階でマージするペアを特定したい。"
  },
  {
    "start": 3049190,
    "end": 3050186,
    "text": "私たちは何を望んでいるのか？"
  },
  {
    "start": 3050208,
    "end": 3063390,
    "text": "マージ・ディクショナリーの中で最もインデックスが小さいペア、あるいはstatsの中のキーを見つけたい。"
  },
  {
    "start": 3063890,
    "end": 3070580,
    "text": "また、これを実装する方法はいろいろあるが、ここではちょっと派手なことをやってみよう。"
  },
  {
    "start": 3072150,
    "end": 3077938,
    "text": "Pythonでイテレータに対してminを呼び出すときに、min over an iteratorを使うつもりだ。"
  },
  {
    "start": 3078034,
    "end": 3079666,
    "text": "これが辞書だ。"
  },
  {
    "start": 3079778,
    "end": 3083282,
    "text": "Pythonでこの辞書のキーを反復処理する。"
  },
  {
    "start": 3083426,
    "end": 3103440,
    "text": "トークンの中にある連続したペアの中から、最小値を持つペアを選びます。minはキーを取り、minを実行するための値を返す関数を与えます。"
  },
  {
    "start": 3103970,
    "end": 3112750,
    "text": "私たちが気にしているのは、マージしてそのペアのインデックスを取得することだ。"
  },
  {
    "start": 3113670,
    "end": 3125442,
    "text": "基本的に、統計の中のどのペアについても、そのペアがどのようなインデックスを持っているかをマージで調べ、最小の数字を持つペアを取得したい。"
  },
  {
    "start": 3125576,
    "end": 3130290,
    "text": "例えば、101と32のペアがあれば、絶対にそのペアを手に入れたい。"
  },
  {
    "start": 3130450,
    "end": 3132454,
    "text": "ここで特定して返却したい。"
  },
  {
    "start": 3132492,
    "end": 3136070,
    "text": "が発生すれば、100と132になる。"
  },
  {
    "start": 3136570,
    "end": 3150762,
    "text": "ここでfloat infをフォールバックとして入れているのは、get関数で、基本的にマージに含まれないペアを考慮する場合、そのペアはマージされる資格がないからですね。"
  },
  {
    "start": 3150816,
    "end": 3163790,
    "text": "トークン列の中に、マージするペアでない、マージできないペアがある場合、それは実際にはここには存在せず、インデックスを持たず、マージできない。"
  },
  {
    "start": 3163940,
    "end": 3171300,
    "text": "ここで無限大がいいのは、最小値を算出するときに候補リストに入らないことが確実に保証されるからだ。"
  },
  {
    "start": 3172790,
    "end": 3174500,
    "text": "これもひとつの方法だ。"
  },
  {
    "start": 3174870,
    "end": 3182130,
    "text": "基本的に、長い話を短くすると、これはトークンに出現する最も適格なマージ候補のペアを返す。"
  },
  {
    "start": 3182290,
    "end": 3189354,
    "text": "ここでひとつ注意しなければならないのは、この関数が次のような失敗をする可能性があるということだ。"
  },
  {
    "start": 3189472,
    "end": 3198134,
    "text": "マージするものがないのであれば、マージで満たされるものはもう何もない。"
  },
  {
    "start": 3198182,
    "end": 3199110,
    "text": "合併するものは何もない。"
  },
  {
    "start": 3199190,
    "end": 3206590,
    "text": "そしてそのペアが、スタッツの一番最初の要素になると思う。"
  },
  {
    "start": 3207250,
    "end": 3217902,
    "text": "このペアは実際にはマージ可能なペアではなく、これらのペアはすべてマージ基準に対してfloat Infと評価されるため、任意にstats内部の最初のペアになるだけである。"
  },
  {
    "start": 3218046,
    "end": 3222142,
    "text": "というのも、もうペアを組むことがないからだ。"
  },
  {
    "start": 3222206,
    "end": 3229314,
    "text": "もしこのペアが返されたマージになければ、これは何もマージするものがなかったというシグナルである。"
  },
  {
    "start": 3229442,
    "end": 3231250,
    "text": "もう1組も合併できない。"
  },
  {
    "start": 3231330,
    "end": 3233080,
    "text": "その場合は脱退する。"
  },
  {
    "start": 3234970,
    "end": 3236630,
    "text": "それ以外は合併できない。"
  },
  {
    "start": 3238810,
    "end": 3249660,
    "text": "ところで、これはPythonで一生懸命やっているようなものだが、本当は、ここで一番低いインデックスでマージできるペアを見つけようとしているだけなのだ。"
  },
  {
    "start": 3250450,
    "end": 3257470,
    "text": "さて、もし最も低いインデックスでマージの内側にあるペアが見つかったら、それをマージすることができる。"
  },
  {
    "start": 3257620,
    "end": 3268414,
    "text": "そのペアのマージ辞書を調べてインデックスを探し、そのインデックスにマージする。"
  },
  {
    "start": 3268542,
    "end": 3274350,
    "text": "トークン・イコールを行い、元のトークンを置き換える。"
  },
  {
    "start": 3274430,
    "end": 3278786,
    "text": "ペア・ペアをインデックスIDXに置き換える。"
  },
  {
    "start": 3278898,
    "end": 3284322,
    "text": "これは、pairが出現するたびにIDXに置き換えられた新しいトークンのリストを返します。"
  },
  {
    "start": 3284386,
    "end": 3289334,
    "text": "我々はマージを行なっており、最終的に何もマージできなくなるまでこれを続けるつもりだ。"
  },
  {
    "start": 3289382,
    "end": 3293210,
    "text": "ここに出てきて、脱出して、ここでトークンを返すだけだ。"
  },
  {
    "start": 3294190,
    "end": 3296874,
    "text": "それが実装だと思う。"
  },
  {
    "start": 3296912,
    "end": 3298394,
    "text": "これで問題なく走れることを願っている。"
  },
  {
    "start": 3298432,
    "end": 3299020,
    "text": "クールだ。"
  },
  {
    "start": 3300830,
    "end": 3302906,
    "text": "ああ、これは合理的に見える。"
  },
  {
    "start": 3302938,
    "end": 3305674,
    "text": "例えば、32はASCIIではスペースである。"
  },
  {
    "start": 3305722,
    "end": 3306720,
    "text": "それがここにある。"
  },
  {
    "start": 3308290,
    "end": 3310494,
    "text": "これはうまくいったようだ。"
  },
  {
    "start": 3310612,
    "end": 3313294,
    "text": "さて、それでは少なくともこのセクションは終わりにしよう。"
  },
  {
    "start": 3313412,
    "end": 3318514,
    "text": "これはまだ正しい実装ではないことを指摘したかったのだが、それは特殊なケースを省いているからだ。"
  },
  {
    "start": 3318632,
    "end": 3322990,
    "text": "特に、これを行おうとするとエラーになる。"
  },
  {
    "start": 3323150,
    "end": 3330790,
    "text": "問題は、1文字だけ、あるいは空の文字列しかない場合、statsは空になり、minの内部で問題が発生することだ。"
  },
  {
    "start": 3330940,
    "end": 3341014,
    "text": "トークンのLenが2つ以上であれば、この問題に対処できる。"
  },
  {
    "start": 3341062,
    "end": 3342058,
    "text": "我々はただ戻るだけだ。"
  },
  {
    "start": 3342224,
    "end": 3344380,
    "text": "そうすれば、そのケースは解決する。"
  },
  {
    "start": 3345390,
    "end": 3346090,
    "text": "オーケー。"
  },
  {
    "start": 3346240,
    "end": 3349786,
    "text": "次に、私たちのためにいくつかのテストケースを用意しました。"
  },
  {
    "start": 3349888,
    "end": 3354330,
    "text": "まず、次のことを確認しておこう。"
  },
  {
    "start": 3354990,
    "end": 3360782,
    "text": "ある文字列をエンコードしてデコードし直すと、同じ文字列が返ってくると思うだろう？"
  },
  {
    "start": 3360916,
    "end": 3362670,
    "text": "それはすべての弦に言えることですか？"
  },
  {
    "start": 3366250,
    "end": 3367462,
    "text": "ここではそうだ。"
  },
  {
    "start": 3367516,
    "end": 3369880,
    "text": "一般的にはそうだろうと思う。"
  },
  {
    "start": 3370970,
    "end": 3387770,
    "text": "というのも、前述したように、すべてのトークン・シーケンスが有効なUTF、8種類のバイト・ストリームであるとは限らないからだ。"
  },
  {
    "start": 3388670,
    "end": 3391126,
    "text": "これは一方向にしか進まない。"
  },
  {
    "start": 3391318,
    "end": 3394170,
    "text": "その方向性については、ここで確認することができる。"
  },
  {
    "start": 3394240,
    "end": 3401646,
    "text": "トレーニング・テキスト（トークナイザーをトレーニングするためのテキスト）を使えば、エンコードとデコードを行ったときに同じものが返ってくることを確認できる。"
  },
  {
    "start": 3401828,
    "end": 3403630,
    "text": "ここでいくつかの検証データを取ってみた。"
  },
  {
    "start": 3403700,
    "end": 3406866,
    "text": "このウェブページにアクセスして、いくつかのテキストを手に入れたんだ。"
  },
  {
    "start": 3406968,
    "end": 3411780,
    "text": "これはトークナイザーが見たことのないテキストであり、これも動作することを確認できる。"
  },
  {
    "start": 3412390,
    "end": 3412754,
    "text": "オーケー。"
  },
  {
    "start": 3412792,
    "end": 3415490,
    "text": "これは、これが正しく実施されたことを確信させてくれる。"
  },
  {
    "start": 3415910,
    "end": 3419090,
    "text": "以上が、バイト・ペア符号化アルゴリズムの基本である。"
  },
  {
    "start": 3419170,
    "end": 3423474,
    "text": "トークナイザーを訓練する方法を見た。"
  },
  {
    "start": 3423602,
    "end": 3431530,
    "text": "このトークナイザーのパラメーターは、まさにこのマージの辞書であり、基本的には生のバイトの上に小さなバイナリー・フォレストを作る。"
  },
  {
    "start": 3432430,
    "end": 3438678,
    "text": "このマージ・テーブルがあれば、生テキストとトークン列のエンコードとデコードが可能になる。"
  },
  {
    "start": 3438854,
    "end": 3442102,
    "text": "これがトークナイザーの最もシンプルな設定だ。"
  },
  {
    "start": 3442246,
    "end": 3448270,
    "text": "しかし、これからやることは、最先端のローンチ言語モデルと、それらが使用するトークナイザーの種類を見ていくことだ。"
  },
  {
    "start": 3448340,
    "end": 3451022,
    "text": "この図式が急速に複雑化していくのがわかるだろう。"
  },
  {
    "start": 3451156,
    "end": 3456834,
    "text": "この複雑化の詳細をひとつずつ見ていくことにしよう。"
  },
  {
    "start": 3457032,
    "end": 3459774,
    "text": "まずはGPTシリーズを見てみよう。"
  },
  {
    "start": 3459902,
    "end": 3467718,
    "text": "特に、ここにGPT-2の論文がありますが、この論文は2019年頃のものなので、5年前のものです。"
  },
  {
    "start": 3467884,
    "end": 3471026,
    "text": "入力表現までスクロールしてみよう。"
  },
  {
    "start": 3471138,
    "end": 3474806,
    "text": "ここではGPD 2で使用しているトークナイザーについて話している。"
  },
  {
    "start": 3474988,
    "end": 3479722,
    "text": "この文章はかなり読みやすいので、ぜひ一度立ち止まって読んでみてほしい。"
  },
  {
    "start": 3479856,
    "end": 3488342,
    "text": "ここで彼らは、utf8エンコーディングのバイトレベル表現にバイトペアエンコーディングアルゴリズムを使用する動機付けをしている。"
  },
  {
    "start": 3488486,
    "end": 3492734,
    "text": "ここで彼らはモチベーションを高め、ボキャブラリーの大きさなどについて話す。"
  },
  {
    "start": 3492932,
    "end": 3498254,
    "text": "さて、ここまではすべてこれまで取材してきた通りだが、このあたりから物事が出発する。"
  },
  {
    "start": 3498452,
    "end": 3503214,
    "text": "彼らが言及したのは、我々が行ってきたような素朴なアルゴリズムを適用しているわけではないということだ。"
  },
  {
    "start": 3503332,
    "end": 3505826,
    "text": "特に、モチベーションを高める例を挙げよう。"
  },
  {
    "start": 3506008,
    "end": 3508178,
    "text": "dogのような一般的な単語があるとする。"
  },
  {
    "start": 3508344,
    "end": 3515986,
    "text": "何が起こるかというと、犬はもちろん、文章中に非常に頻繁に登場し、例としてあらゆる種類の句読点のすぐ隣に登場する。"
  },
  {
    "start": 3516088,
    "end": 3520562,
    "text": "ドック、ドット、ドック、エクスクラメーションマーク、ドック、クエスチョンマークなど。"
  },
  {
    "start": 3520706,
    "end": 3529670,
    "text": "素朴に、BPアルゴリズムがこれらを単一のトークンに統合できると想像するかもしれませんが、そうすると、句読点が少し異なるだけのdogと同じようなトークンがたくさんできてしまいます。"
  },
  {
    "start": 3529830,
    "end": 3532566,
    "text": "だから、クラスタ化すべきでないものをクラスタ化しているように感じる。"
  },
  {
    "start": 3532598,
    "end": 3535450,
    "text": "あなたは意味論と句読点を組み合わせている。"
  },
  {
    "start": 3536350,
    "end": 3538662,
    "text": "これは最適ではない。"
  },
  {
    "start": 3538726,
    "end": 3541558,
    "text": "確かに、これは最適ではないとも言われている。"
  },
  {
    "start": 3541574,
    "end": 3543162,
    "text": "いくつかの実験によると、このような結果が出ている。"
  },
  {
    "start": 3543306,
    "end": 3551600,
    "text": "彼らがやりたいのは、マニュアル的な方法でトップダウンを行い、ある種のキャラクターを決して一緒にしてはいけないと強制することだ。"
  },
  {
    "start": 3552530,
    "end": 3557390,
    "text": "彼らは、バイトペアエンコーディングアルゴリズムの上に、これらのマージルールを強制したいのだ。"
  },
  {
    "start": 3557550,
    "end": 3563874,
    "text": "彼らのコードを見て、実際にどのようにこれを実施しているのか、また実際にどのようなマージを行なっているのかを見てみよう。"
  },
  {
    "start": 3564072,
    "end": 3569170,
    "text": "GPT-2のタブはGitHubのOpenAIの下にあります。"
  },
  {
    "start": 3569330,
    "end": 3573510,
    "text": "ソースに行くとエンコーダーpyがある。"
  },
  {
    "start": 3573850,
    "end": 3580882,
    "text": "なぜなら、これはトークナイザーであり、トークナイザーはエンコードもデコードもできるからだ。"
  },
  {
    "start": 3581026,
    "end": 3585238,
    "text": "エンコーダーと呼ばれるのはちょっと気が引けるが、これはトークナイザーだ。"
  },
  {
    "start": 3585414,
    "end": 3589114,
    "text": "ここで起こっていることはたくさんあるので、ある時点で詳しく説明するつもりだ。"
  },
  {
    "start": 3589232,
    "end": 3592540,
    "text": "今はこの部分に集中したい。"
  },
  {
    "start": 3592910,
    "end": 3597840,
    "text": "ここでは、非常に複雑に見えるレジXパターンを作っている。"
  },
  {
    "start": 3598530,
    "end": 3606340,
    "text": "これは、テキストのどの部分が絶対に合併されないかというルールを強制するための核となる部分である。"
  },
  {
    "start": 3606790,
    "end": 3615394,
    "text": "なぜなら、Pythonのreモジュールであるimport reではなく、import reg X as reだからだ。"
  },
  {
    "start": 3615512,
    "end": 3621750,
    "text": "reg XはPythonパッケージで、pip install reg Xでインストールできる。"
  },
  {
    "start": 3621820,
    "end": 3623400,
    "text": "それはもう少し強力だ。"
  },
  {
    "start": 3625850,
    "end": 3633530,
    "text": "このパターンが何をやっているのか、そしてなぜこのパターンが実際に彼らが求めている分離をやっているのかを見てみよう。"
  },
  {
    "start": 3633680,
    "end": 3640326,
    "text": "では、このパターンをJupyterノートブックにコピー・ペーストした。"
  },
  {
    "start": 3640518,
    "end": 3649118,
    "text": "彼らのコードとまったく同じように、私たちはこのパターンに対して、興味のある任意の文字列のすべてを再検索する。"
  },
  {
    "start": 3649284,
    "end": 3658706,
    "text": "これはGPT-2のようなLlMに入力するトークンにエンコードしたい文字列だ。"
  },
  {
    "start": 3658888,
    "end": 3663010,
    "text": "さて、re Findallはこのパターンを使って文字列とのマッチングを試みる。"
  },
  {
    "start": 3664790,
    "end": 3676680,
    "text": "この仕組みは、文字列の左から右へと進み、パターンにマッチしようとしている。"
  },
  {
    "start": 3677210,
    "end": 3687638,
    "text": "このパターンを見て、まずこれが生の文字列であることに注目してほしい。"
  },
  {
    "start": 3687734,
    "end": 3690490,
    "text": "これは文字列そのものではなく、パターンそのものである。"
  },
  {
    "start": 3690560,
    "end": 3691180,
    "text": "そうだね。"
  },
  {
    "start": 3692190,
    "end": 3695194,
    "text": "多くのオーズで構成されていることに注目してほしい。"
  },
  {
    "start": 3695242,
    "end": 3696554,
    "text": "この縦棒が見えるだろうか？"
  },
  {
    "start": 3696602,
    "end": 3698640,
    "text": "これらはレギュレーションXのオーズである。"
  },
  {
    "start": 3699170,
    "end": 3704362,
    "text": "だから、このパターンを左から右へ進み、どこにいても文字列と照合しようとする。"
  },
  {
    "start": 3704516,
    "end": 3707826,
    "text": "我々はハローを持っているので、それに合わせようと思っている。"
  },
  {
    "start": 3707928,
    "end": 3720134,
    "text": "まあ、アポストロフィsでもアポストロフィtでもなく、これらのどれでもない。"
  },
  {
    "start": 3720252,
    "end": 3721750,
    "text": "lのpとは？"
  },
  {
    "start": 3721900,
    "end": 3725400,
    "text": "それは、私が見つけたいくつかの文書に書かれている。"
  },
  {
    "start": 3726650,
    "end": 3728520,
    "text": "他のソースもあるかもしれない。"
  },
  {
    "start": 3729450,
    "end": 3737930,
    "text": "LのPは文字で、あらゆる言語のあらゆる種類の文字であり、helloはh e lloなどの文字で構成されている。"
  },
  {
    "start": 3738270,
    "end": 3741466,
    "text": "任意スペースの後に文字が続く。"
  },
  {
    "start": 3741498,
    "end": 3748666,
    "text": "1つ以上の文字がhelloにマッチするが、空白は文字ではないのでマッチは終了する。"
  },
  {
    "start": 3748778,
    "end": 3755454,
    "text": "そこから、再び文字列とのマッチングを試みる新たな試みが始まる。"
  },
  {
    "start": 3755652,
    "end": 3763378,
    "text": "ここからは、まったく同じポイントに再び到達するまで、これらすべてをスキップすることになる。"
  },
  {
    "start": 3763464,
    "end": 3768322,
    "text": "これは、オプションのスペースに続いて、1つまたは複数の文字が続くので、マッチする。"
  },
  {
    "start": 3768466,
    "end": 3774760,
    "text": "これを実行すると、helloとspace worldという2つの要素からなるリストが得られる。"
  },
  {
    "start": 3775610,
    "end": 3777046,
    "text": "お元気ですか？"
  },
  {
    "start": 3777068,
    "end": 3780380,
    "text": "さらに文字を増やせば、このようになる。"
  },
  {
    "start": 3780990,
    "end": 3783386,
    "text": "さて、これは何をしているのか、なぜ重要なのか。"
  },
  {
    "start": 3783568,
    "end": 3791994,
    "text": "トークン化のために文字列を直接エンコードする代わりに、まず文字列を分割する。"
  },
  {
    "start": 3792112,
    "end": 3796046,
    "text": "実際にコードを見ていくときに、もう少し詳しく説明しよう。"
  },
  {
    "start": 3796228,
    "end": 3804338,
    "text": "つまり、テキストをこのようなテキストのリストに分割するのだ。"
  },
  {
    "start": 3804504,
    "end": 3812990,
    "text": "このリストのすべての要素は、トーケナイザーによって個別に処理され、その処理結果はすべて単純に連結される。"
  },
  {
    "start": 3813150,
    "end": 3814840,
    "text": "こんにちは。"
  },
  {
    "start": 3815210,
    "end": 3816920,
    "text": "ああ、どうだったかな。"
  },
  {
    "start": 3817450,
    "end": 3819398,
    "text": "こんにちは、お元気ですか？"
  },
  {
    "start": 3819484,
    "end": 3821046,
    "text": "リストには5つの要素がある。"
  },
  {
    "start": 3821148,
    "end": 3830566,
    "text": "これらはすべて、独立して、テキストからトークン列へと進み、トークン列が連結される。"
  },
  {
    "start": 3830598,
    "end": 3832140,
    "text": "すべてが統合されるんだ。"
  },
  {
    "start": 3832590,
    "end": 3839066,
    "text": "大雑把に言えば、このリストの要素間のマージだけを見つけるということだ。"
  },
  {
    "start": 3839168,
    "end": 3843470,
    "text": "これらの要素のひとつひとつを個別にマージすることしか考えられない。"
  },
  {
    "start": 3844050,
    "end": 3854318,
    "text": "これらの要素すべてについて個別に可能な限りの結合を行った後、そのすべての結果を連結によって結合する。"
  },
  {
    "start": 3854494,
    "end": 3865654,
    "text": "ということは、基本的に、あなたが効果的にやっていることは、このeとこのスペースをマージすることはないということだ。"
  },
  {
    "start": 3865772,
    "end": 3873000,
    "text": "ということは、エスパスを合併させるつもりはないということか？"
  },
  {
    "start": 3873450,
    "end": 3882790,
    "text": "基本的に、このreg xパターンを使ってテキストをチャンクアップすることは、いくつかのマージが起こらないように強制する一つの方法に過ぎない。"
  },
  {
    "start": 3882940,
    "end": 3892478,
    "text": "このテキストをさらに詳しく見ていくと、このテキストが高いレベルでやろうとしていることは、文字や数字、句読点などをまたいでマージしないようにすることであることがわかるだろう。"
  },
  {
    "start": 3892564,
    "end": 3894382,
    "text": "それがどのように機能するのか、もう少し詳しく見てみよう。"
  },
  {
    "start": 3894436,
    "end": 3895102,
    "text": "続けよう。"
  },
  {
    "start": 3895156,
    "end": 3897470,
    "text": "これでnのpが揃った。"
  },
  {
    "start": 3897540,
    "end": 3904250,
    "text": "ドキュメントを見ると、nのpはどんなスクリプトでもあらゆる種類の数値文字である。"
  },
  {
    "start": 3904330,
    "end": 3905454,
    "text": "それは数字だ。"
  },
  {
    "start": 3905652,
    "end": 3909506,
    "text": "オプションでスペースと数字があり、それらは分離される。"
  },
  {
    "start": 3909608,
    "end": 3911502,
    "text": "文字と数字が分離されている。"
  },
  {
    "start": 3911566,
    "end": 3913122,
    "text": "そうしたら、ハロー、ワールド。"
  },
  {
    "start": 3913176,
    "end": 3914706,
    "text": "1、2、3、元気かい？"
  },
  {
    "start": 3914888,
    "end": 3920598,
    "text": "なぜなら、1はもう文字ではなく、数字だからだ。"
  },
  {
    "start": 3920684,
    "end": 3924550,
    "text": "このグループはそのためのマッチングを行い、我々はそれを別個のものとして獲得する。"
  },
  {
    "start": 3926590,
    "end": 3928442,
    "text": "このアポストロフィの働きを見てみよう。"
  },
  {
    "start": 3928496,
    "end": 3939660,
    "text": "ここで、アポストロフィvを例にとると、ここでのアポストロフィは文字でも数字でもない。"
  },
  {
    "start": 3940350,
    "end": 3945440,
    "text": "helloはマッチングを止め、次にthisとthatを正確にマッチさせる。"
  },
  {
    "start": 3945810,
    "end": 3948640,
    "text": "それは別のものとして出てくるだろう。"
  },
  {
    "start": 3949090,
    "end": 3951406,
    "text": "なぜここでアポストロフィを使うのか？"
  },
  {
    "start": 3951508,
    "end": 3957758,
    "text": "正直なところ、これらは一般的によく使われるアポストロフィーのようなものだと思う。"
  },
  {
    "start": 3957934,
    "end": 3966210,
    "text": "ユニコードのアポストロフィがあるとどうなるか、お見せしましょう。"
  },
  {
    "start": 3966290,
    "end": 3972550,
    "text": "例えば、家を持っている場合、このマッチングによって、その家は切り離される。"
  },
  {
    "start": 3972890,
    "end": 3979578,
    "text": "ユニコードのアポストロフィをこのように使うと、突然うまくいかなくなる。"
  },
  {
    "start": 3979744,
    "end": 3983020,
    "text": "だから、このアポストロフィーは、これからは独自のものになる。"
  },
  {
    "start": 3983550,
    "end": 3992590,
    "text": "つまり、このアポストロフィーの種類は基本的にハードコーディングされている。"
  },
  {
    "start": 3993010,
    "end": 3997706,
    "text": "これに加えて、GPT-2のドキュメントを見ることができる。"
  },
  {
    "start": 3997738,
    "end": 4002078,
    "text": "ここでは、パターンを定義する際に、無視するケースを追加すべきだと言っている。"
  },
  {
    "start": 4002164,
    "end": 4005342,
    "text": "BPマージは、短縮形の大文字バージョンで起こることがある。"
  },
  {
    "start": 4005486,
    "end": 4010590,
    "text": "彼らが指摘しているのは、これがどのようにアポストロフィと小文字になっているかということだ。"
  },
  {
    "start": 4010750,
    "end": 4019190,
    "text": "まあ、大文字と小文字を無視していないので、このルールでは大文字の場合はアポストロフィーは分離されない。"
  },
  {
    "start": 4019690,
    "end": 4023960,
    "text": "家はこんな感じだろう。"
  },
  {
    "start": 4024570,
    "end": 4032380,
    "text": "もし私が家を建てたとしたら、もし私が大文字なら、突然アポストロフィがひとりでに出てくることに気づくだろう。"
  },
  {
    "start": 4033150,
    "end": 4039782,
    "text": "の場合、トークン化は大文字と小文字で異なる動作をし、これらのアポストロフィを矛盾なく分離する。"
  },
  {
    "start": 4039846,
    "end": 4045470,
    "text": "すごくグロくて、ちょっと気持ち悪いけど、そういうものなんだ。"
  },
  {
    "start": 4045620,
    "end": 4049706,
    "text": "さて、ではアポストロフィーの表現をたくさんマッチさせた後に戻ってこよう。"
  },
  {
    "start": 4049818,
    "end": 4053310,
    "text": "ところで、ここでのもう一つの問題は、これらがかなり言語特異的であるということだ。"
  },
  {
    "start": 4053380,
    "end": 4059742,
    "text": "例えば、すべての言語がアポストロフィを使うか使わないかは知らないが、それは一貫性のないトークン化だろう。"
  },
  {
    "start": 4059806,
    "end": 4060626,
    "text": "その結果だ。"
  },
  {
    "start": 4060808,
    "end": 4067074,
    "text": "そして文字を一致させようとし、次に数字を一致させようとし、それでもうまくいかなければここに戻る。"
  },
  {
    "start": 4067272,
    "end": 4074774,
    "text": "つまり、これもまた、任意のスペース、文字数でもスペースでもないもの、そしてそのうちの1つ以上、ということだ。"
  },
  {
    "start": 4074892,
    "end": 4080540,
    "text": "これは、文字でも数字でもなく、大雑把に言えば句読点をマッチさせようとしているのだ。"
  },
  {
    "start": 4080910,
    "end": 4083386,
    "text": "このグループはそのきっかけを作ろうとする。"
  },
  {
    "start": 4083408,
    "end": 4095454,
    "text": "このようにすると、この部分は文字でも数字でもなく、実際にここに引っかかって、独自のグループになる。"
  },
  {
    "start": 4095572,
    "end": 4097390,
    "text": "私たちは句読点を区切りました。"
  },
  {
    "start": 4098210,
    "end": 4101434,
    "text": "最後に、これも少しわかりにくい。"
  },
  {
    "start": 4101562,
    "end": 4108770,
    "text": "これはホワイトスペースにマッチしているが、これはREG Xのネガティブ・ルックアヘッド・アサーションを使用している。"
  },
  {
    "start": 4108920,
    "end": 4115090,
    "text": "これは、最後の空白文字を含まず、空白文字までをマッチングさせるものである。"
  },
  {
    "start": 4115910,
    "end": 4117080,
    "text": "なぜこれが重要なのか？"
  },
  {
    "start": 4117690,
    "end": 4119238,
    "text": "これはかなり微妙だと思う。"
  },
  {
    "start": 4119324,
    "end": 4123270,
    "text": "空白が常に単語の頭に含まれているのがわかるだろう。"
  },
  {
    "start": 4123420,
    "end": 4127074,
    "text": "スペースR、スペースU、等々。"
  },
  {
    "start": 4127202,
    "end": 4129260,
    "text": "ここにたくさんのスペースがあるとする。"
  },
  {
    "start": 4130270,
    "end": 4137420,
    "text": "ここで何が起こるかというと、最後の文字を含めないまでの空白がこれに引っかかるということだ。"
  },
  {
    "start": 4137790,
    "end": 4148286,
    "text": "そうすることで、最後の文字までのスペースが分離され、最後の文字がここに来てスペースUと結合できるようになる。"
  },
  {
    "start": 4148468,
    "end": 4152378,
    "text": "それがいいのは、スペースUが共通のトークンだからだ。"
  },
  {
    "start": 4152474,
    "end": 4156258,
    "text": "余分なスペースがなければ、スペースUがある。"
  },
  {
    "start": 4156344,
    "end": 4162706,
    "text": "トークンを追加しても、スペースを追加しても、スペースUはあるが、余分な空白がある。"
  },
  {
    "start": 4162888,
    "end": 4170098,
    "text": "基本的に、GPT-2トークナイザーは、文字や数字に空白があることを非常に好みます。"
  },
  {
    "start": 4170194,
    "end": 4172614,
    "text": "これは一貫していることだ。"
  },
  {
    "start": 4172812,
    "end": 4174246,
    "text": "そのためにあるんだ。"
  },
  {
    "start": 4174348,
    "end": 4179558,
    "text": "そして最後のフォールバックは空白文字である。"
  },
  {
    "start": 4179734,
    "end": 4189770,
    "text": "というのは、もしそれが捕らえられなかったら、これは末尾のスペースなどを捕らえるだろう。"
  },
  {
    "start": 4189920,
    "end": 4192554,
    "text": "ここでもうひとつ、実際の例を示したい。"
  },
  {
    "start": 4192672,
    "end": 4199086,
    "text": "パイソンコードの一部であるこの文字列があり、それを分割しようとすると、このような出力が得られる。"
  },
  {
    "start": 4199268,
    "end": 4207380,
    "text": "というのも、カテゴリーの種類が変わるたびに、かなり頻繁に分割しているからだ。"
  },
  {
    "start": 4208310,
    "end": 4211170,
    "text": "これらの要素の中で合併が起こることはない。"
  },
  {
    "start": 4211830,
    "end": 4214498,
    "text": "それがここにあるものだ。"
  },
  {
    "start": 4214664,
    "end": 4226498,
    "text": "トークナイザーを訓練するために、OpenAIはこれを使ってテキストをチャンクに分割し、すべてのチャンクの中でBPEアルゴリズムだけを実行したと思うかもしれない。"
  },
  {
    "start": 4226674,
    "end": 4228458,
    "text": "というのは正確ではない。"
  },
  {
    "start": 4228544,
    "end": 4230166,
    "text": "その理由は以下の通りである。"
  },
  {
    "start": 4230278,
    "end": 4232954,
    "text": "ここにスペースがあることに注目してほしい。"
  },
  {
    "start": 4233152,
    "end": 4240470,
    "text": "これらのスペースは、最終的には要素全体となるが、OpenAIによって実際にマージされることはない。"
  },
  {
    "start": 4240550,
    "end": 4250960,
    "text": "というのも、まったく同じチャンクをティック・トークナイザーにコピー・ペーストしてみると、すべてのスペースが独立したまま、トークン220になっていることがわかるからだ。"
  },
  {
    "start": 4251810,
    "end": 4257342,
    "text": "OpenAIのある時点で、これらのスペースは決して統合されないというルールが施行されたと思う。"
  },
  {
    "start": 4257486,
    "end": 4265122,
    "text": "つまり、チャンキングとBPEに加えて、OpenAIが明確にしていない追加ルールがある。"
  },
  {
    "start": 4265256,
    "end": 4268370,
    "text": "現在、GPT-2トークナイザーのトレーニングコードは公開されていない。"
  },
  {
    "start": 4268450,
    "end": 4272118,
    "text": "私たちが持っているのは、すでにお見せしたコードだけです。"
  },
  {
    "start": 4272204,
    "end": 4277058,
    "text": "彼らが公開したこのコードは、トークンの推論コードに過ぎない。"
  },
  {
    "start": 4277154,
    "end": 4278614,
    "text": "これはトレーニングコードではない。"
  },
  {
    "start": 4278652,
    "end": 4281398,
    "text": "テキストを渡してトークナイザーを訓練することはできない。"
  },
  {
    "start": 4281494,
    "end": 4288938,
    "text": "これは推論コードで、上で説明したマージを新しいテキストに適用している。"
  },
  {
    "start": 4289104,
    "end": 4298014,
    "text": "だから、OpenAIがどのようにトークナイザーを訓練したのか、正確にはわからない。"
  },
  {
    "start": 4298212,
    "end": 4305982,
    "text": "次に紹介したいのは、OpenAIの公式トークン化ライブラリであるTicktokenライブラリだ。"
  },
  {
    "start": 4306126,
    "end": 4314174,
    "text": "これはtick token Pipで、tick tokenをインストールすると、トークン推論ができるようになります。"
  },
  {
    "start": 4314302,
    "end": 4317750,
    "text": "これもトレーニングコードではなく、トークン化のための推論コードに過ぎない。"
  },
  {
    "start": 4318890,
    "end": 4321222,
    "text": "どのように使うのか、お見せしたかったのです。"
  },
  {
    "start": 4321276,
    "end": 4326418,
    "text": "非常にシンプルで、これを実行するとGPT-2トークンかGPT 4トークンが得られるだけだ。"
  },
  {
    "start": 4326514,
    "end": 4328978,
    "text": "これはGPT 4で使用されるトークナイザーです。"
  },
  {
    "start": 4329164,
    "end": 4332966,
    "text": "特に、GPT-2のホワイトスペースがマージされていないことがわかる。"
  },
  {
    "start": 4332998,
    "end": 4340042,
    "text": "GPT 4では、これらの白いスペースはマージされる。"
  },
  {
    "start": 4340106,
    "end": 4343870,
    "text": "GPT4まで下がれば、それらは統合される。"
  },
  {
    "start": 4346130,
    "end": 4354370,
    "text": "GPTの4つのトークナイザーでは、テキストのチャンクアップに使用する正規表現が変更された。"
  },
  {
    "start": 4354520,
    "end": 4368438,
    "text": "Ticktokenライブラリにアクセスして、Ticktoken X OpenAI publicというファイルを開くと、OpenAIが管理しているさまざまなトークナイザーの定義がここにあります。"
  },
  {
    "start": 4368604,
    "end": 4373698,
    "text": "そのため、推論を行うためには、必然的にストリングの詳細を公表しなければならなかった。"
  },
  {
    "start": 4373874,
    "end": 4381866,
    "text": "これはGPT-2ですでに見た文字列で、少し違うが、実際にはここで説明したものと同じである。"
  },
  {
    "start": 4381968,
    "end": 4385558,
    "text": "私たちが説明したこのパターンは、このパターンと同等である。"
  },
  {
    "start": 4385654,
    "end": 4387990,
    "text": "こちらは実行速度が少し速いだけだ。"
  },
  {
    "start": 4388150,
    "end": 4392154,
    "text": "ここでは少し定義が異なるが、それ以外は同じである。"
  },
  {
    "start": 4392352,
    "end": 4402430,
    "text": "少し特殊なトークンに入りますが、cl 100 kまでスクロールすると、これはGPT 4トークナイザーで、パターンが変わっているのがわかります。"
  },
  {
    "start": 4403250,
    "end": 4409620,
    "text": "これは、他の特別なトークンの束に加え、大きな変化のようなものだ。"
  },
  {
    "start": 4410230,
    "end": 4416030,
    "text": "正直なところ、これには心が麻痺してしまうからだ。"
  },
  {
    "start": 4416110,
    "end": 4422006,
    "text": "チャットGPTと正規表現のドキュメントを引っ張り出してきて、とにかく一通りやってみることをお勧めする。"
  },
  {
    "start": 4422108,
    "end": 4432554,
    "text": "主な変更点は、第一に、このIが見えると思うが、これは大文字と小文字を区別しないマッチを意味する。"
  },
  {
    "start": 4432672,
    "end": 4446298,
    "text": "だから、さっきの、ああ、大文字を使うべきだった、というコメントは、基本的には、アポストロフィーのs、アポストロフィーのD、アポストロフィーのMなどをマッチングさせることになる。"
  },
  {
    "start": 4446474,
    "end": 4449370,
    "text": "小文字と大文字の両方でマッチさせるつもりだ。"
  },
  {
    "start": 4449530,
    "end": 4450810,
    "text": "それは直った。"
  },
  {
    "start": 4450970,
    "end": 4455570,
    "text": "ホワイトスペースの処理にはいろいろな種類があるが、その詳細については割愛する。"
  },
  {
    "start": 4455720,
    "end": 4462740,
    "text": "そしてもうひとつ、数字が一致するとき、1つから3つの数字しか一致しないことにお気づきだろう。"
  },
  {
    "start": 4463110,
    "end": 4473430,
    "text": "3桁以上の数字が統合されることはなく、統合されるのは3桁までの数字だけである。"
  },
  {
    "start": 4473850,
    "end": 4480170,
    "text": "これは、非常に長い数字列のトークンを防ぐために行われた変更のひとつだ。"
  },
  {
    "start": 4480990,
    "end": 4488182,
    "text": "なぜなら、どれも文書化されておらず、パターンがわかっているだけだからだ。"
  },
  {
    "start": 4488326,
    "end": 4494238,
    "text": "ああ、それはそれとして、GPT4が行った変更の一部だ。"
  },
  {
    "start": 4494324,
    "end": 4498640,
    "text": "もちろん、語彙のサイズはおよそ50kからおよそ100kになった。"
  },
  {
    "start": 4499250,
    "end": 4505490,
    "text": "次に、OpenAIがリリースしたGPT-2エンコーダpyを簡単に説明します。"
  },
  {
    "start": 4506390,
    "end": 4509102,
    "text": "これは、すでに簡単に紹介したファイルだ。"
  },
  {
    "start": 4509246,
    "end": 4515480,
    "text": "さて、このファイルはかなり短いので、現時点では比較的理解しやすいはずだ。"
  },
  {
    "start": 4516970,
    "end": 4525126,
    "text": "ここでは一番下から、エンコーダーJsOnとボキャブラリーBPEの2つのファイルを読み込み、軽い処理を行っている。"
  },
  {
    "start": 4525148,
    "end": 4528358,
    "text": "そして、トークナイザーであるこのエンコーダー・オブジェクトを呼び出す。"
  },
  {
    "start": 4528534,
    "end": 4536940,
    "text": "もしこの2つのファイルを検査したいのであれば、以下のようなコードを使えばいい。"
  },
  {
    "start": 4538270,
    "end": 4541614,
    "text": "この2つのファイルをダウンロードすることができますので、よろしければご覧ください。"
  },
  {
    "start": 4541732,
    "end": 4548190,
    "text": "このエンコーダーは、彼らのコードでは \"エンコーダー \"と呼ばれているが、我々のボキャブラリーとまったく同じものである。"
  },
  {
    "start": 4548530,
    "end": 4561310,
    "text": "このボキャブラリー・オブジェクトを使えば、非常に効率的にデコードすることができる。"
  },
  {
    "start": 4561470,
    "end": 4570658,
    "text": "私たちのボキャブラリーはまさに彼らのエンコーダーであり、そして彼らのボキャブラリーBPEは、紛らわしいことに実は私たちのマージなのだ。"
  },
  {
    "start": 4570834,
    "end": 4578410,
    "text": "彼らのBPEマージは、ボキャブラリーBPE内のデータに基づいているが、結局は我々のマージと同等である。"
  },
  {
    "start": 4578830,
    "end": 4588918,
    "text": "つまり、マージ変数とボキャブ変数だ。"
  },
  {
    "start": 4589014,
    "end": 4596510,
    "text": "この2つの変数を使うだけで、トークナイザーを表すことができ、このトークナイザーを学習させれば、エンコードもデコードもできる。"
  },
  {
    "start": 4596930,
    "end": 4610078,
    "text": "さて、openeiがここでやっていることの中で、実は少しわかりにくいのは、このエンコーダーとデコーダーの他に、バイト・エンコーダーとバイト・デコーダーというものもあるということだ。"
  },
  {
    "start": 4610254,
    "end": 4618318,
    "text": "というのは、残念ながら、ただ単に実装のディテールが些細なことであり、実際には深い意味も面白みもない。"
  },
  {
    "start": 4618344,
    "end": 4620086,
    "text": "それについての議論は省略する。"
  },
  {
    "start": 4620188,
    "end": 4631782,
    "text": "オープニング・アドがここでやっていることは、理由はよくわからないが、エンコードとデコードができるトークナイザーだけでなく、トークナイザーと連続的に使用されるまったく別のレイヤーをここに追加しているのだ。"
  },
  {
    "start": 4631926,
    "end": 4638534,
    "text": "だから、まずバイトエンコードをしてからエンコードし、次にデコードをしてからバイトデコードする。"
  },
  {
    "start": 4638662,
    "end": 4644542,
    "text": "それがループで、ただ連続して積み重なっているだけで、それほど面白くはない。"
  },
  {
    "start": 4644596,
    "end": 4647086,
    "text": "私はそれをカバーしないし、あなたが望むなら、あなたはそれを踏み抜くことができる。"
  },
  {
    "start": 4647268,
    "end": 4653006,
    "text": "そうでなければ、バイト・エンコーダーとバイト・デコーダーを無視すれば、このファイルはアルゴリズム的に非常に馴染みのあるものになる。"
  },
  {
    "start": 4653108,
    "end": 4656802,
    "text": "ここで重要なのは、BPE機能と呼ばれるものだ。"
  },
  {
    "start": 4656936,
    "end": 4667906,
    "text": "このループは、私たちのwhileループとよく似ていて、ダイアグラムを特定しようとしている。"
  },
  {
    "start": 4668008,
    "end": 4672066,
    "text": "そして、ここでも私たちと同じように、このペアをマージしようとするforループがある。"
  },
  {
    "start": 4672258,
    "end": 4676838,
    "text": "彼らはすべてのシークエンスに目を通し、ペアを見つけるたびにマージする。"
  },
  {
    "start": 4677004,
    "end": 4682026,
    "text": "彼らはそれを、本文中の合併の可能性がなくなるまで繰り返す。"
  },
  {
    "start": 4682128,
    "end": 4683846,
    "text": "それがこのファイルの核心だ。"
  },
  {
    "start": 4683958,
    "end": 4687642,
    "text": "私たちが実装したように、エンコードとデコードの関数がある。"
  },
  {
    "start": 4687776,
    "end": 4696954,
    "text": "手短に言えば、この時点で皆さんに受け取っていただきたいのは、残念ながら、彼らのコードは少々乱雑ではあるが、アルゴリズム的には我々が上記で構築したものと同じだということだ。"
  },
  {
    "start": 4697082,
    "end": 4706290,
    "text": "上記で作り上げたものは、実際にBPEトークナイザーを構築し、それを訓練し、エンコードとデコードの両方を行うために必要なアルゴリズムだと理解していただければと思います。"
  },
  {
    "start": 4706710,
    "end": 4710030,
    "text": "次の話題は、特別なトークンについてである。"
  },
  {
    "start": 4710190,
    "end": 4724310,
    "text": "生バイトとBPEのマージから来るトークンに加えて、データの異なる部分を区切ったり、トークン・ストリームの特別な構造を作るために導入したりする、あらゆる種類のトークンを挿入することができる。"
  },
  {
    "start": 4724650,
    "end": 4732982,
    "text": "OpenAIのGPD 2のこのエンコーダー・オブジェクトを見てください。"
  },
  {
    "start": 4733126,
    "end": 4737820,
    "text": "長さが50,257であることにお気づきだろう。"
  },
  {
    "start": 4739790,
    "end": 4741530,
    "text": "先ほども言ったように、マッピングだ。"
  },
  {
    "start": 4741690,
    "end": 4744122,
    "text": "私たちのボキャブラリーのマッピングとは逆転している。"
  },
  {
    "start": 4744186,
    "end": 4750480,
    "text": "私たちのボキャブラリーは整数から文字列へ、そして文字列は驚くべき理由もなくその逆を行く。"
  },
  {
    "start": 4751330,
    "end": 4755810,
    "text": "ここで注目すべきは、このマッピングテーブルが50,257であることだ。"
  },
  {
    "start": 4755960,
    "end": 4757794,
    "text": "その数字はどこから来ているのか？"
  },
  {
    "start": 4757992,
    "end": 4760366,
    "text": "トークンはどこにある？"
  },
  {
    "start": 4760398,
    "end": 4763730,
    "text": "前述したように、256バイトのトークンがある。"
  },
  {
    "start": 4765190,
    "end": 4771778,
    "text": "その後、オープニングは実際に5万回のマージを行ったので、それが他のトークンになる。"
  },
  {
    "start": 4771874,
    "end": 4774694,
    "text": "これは50,256になる。"
  },
  {
    "start": 4774892,
    "end": 4777366,
    "text": "57番目のトークンとは？"
  },
  {
    "start": 4777478,
    "end": 4785740,
    "text": "基本的に特別なトークンが1つあり、その特別なトークンはend of textと呼ばれる。"
  },
  {
    "start": 4786350,
    "end": 4790010,
    "text": "これは特別なトークンで、最後のトークンだ。"
  },
  {
    "start": 4790350,
    "end": 4794480,
    "text": "このトークンは、トレーニングセット内のドキュメントを区切るために使用されます。"
  },
  {
    "start": 4794930,
    "end": 4801070,
    "text": "学習データを作成するとき、すべての文書があり、それらをトークン化し、トークンのストリームを得ます。"
  },
  {
    "start": 4801410,
    "end": 4804866,
    "text": "トークンは0から50,256までしかない。"
  },
  {
    "start": 4804888,
    "end": 4814126,
    "text": "そして、それらのドキュメントの間に特別なテキスト終了トークンを置き、そのトークンをドキュメント間に挿入する。"
  },
  {
    "start": 4814318,
    "end": 4819714,
    "text": "これは、文書が終了したという言語モデルへのシグナルとして使っている。"
  },
  {
    "start": 4819762,
    "end": 4823750,
    "text": "以下は、前回の文書とは無関係になる。"
  },
  {
    "start": 4824090,
    "end": 4826678,
    "text": "とはいえ、言語モデルはデータからこれを学習しなければならない。"
  },
  {
    "start": 4826844,
    "end": 4834074,
    "text": "このトークンは通常、その前に何があったのか、その前に何があったのかという記憶を消去することを意味することを学ぶ必要がある。"
  },
  {
    "start": 4834112,
    "end": 4843470,
    "text": "このトークンは、実は次に来るものにとって有益な情報ではないが、言語モデルがこれを学習することを期待している。"
  },
  {
    "start": 4843970,
    "end": 4850862,
    "text": "これはGPTからトークナイザーへのコードで、以前から遊んでいたものだ。"
  },
  {
    "start": 4850996,
    "end": 4852222,
    "text": "ここに追加できるよね？"
  },
  {
    "start": 4852276,
    "end": 4854130,
    "text": "こんにちは、お元気ですか？"
  },
  {
    "start": 4854200,
    "end": 4855678,
    "text": "トークンが違うんだ。"
  },
  {
    "start": 4855774,
    "end": 4859780,
    "text": "テキストの末尾に \"end \"を付けるとどうなるか見てみよう。"
  },
  {
    "start": 4860390,
    "end": 4868626,
    "text": "私が完成させるまで、これらはすべて異なるトークン、テキストの終わり、まだフリップトークンであったことがわかるだろう。"
  },
  {
    "start": 4868658,
    "end": 4872566,
    "text": "それを終えると、いきなり50,256トークンになる。"
  },
  {
    "start": 4872588,
    "end": 4879750,
    "text": "これが機能するのは、実際にBPEの合併を経ていないからだ。"
  },
  {
    "start": 4879910,
    "end": 4888650,
    "text": "その代わり、トークンを実際に出力するコードには、特殊なトークンを扱うための特別なケースの指示がある。"
  },
  {
    "start": 4889310,
    "end": 4894794,
    "text": "エンコーダPYには、このような特殊トークンの取り扱いに関する特別な指示は見られなかった。"
  },
  {
    "start": 4894842,
    "end": 4896142,
    "text": "そこにはない。"
  },
  {
    "start": 4896276,
    "end": 4909026,
    "text": "Rubyで実装されたTicktokenライブラリに行けば、このような特殊なトークンを登録、作成、ボキャブラリーに追加し、それを探すという、あらゆる種類の特殊なケースの取り扱いがある。"
  },
  {
    "start": 4909048,
    "end": 4915586,
    "text": "このような特別なトークンを見るたびに、実際にやってきて、その特別なトークンを入れ替える。"
  },
  {
    "start": 4915778,
    "end": 4920630,
    "text": "これらは、バイトコーディングの典型的なアルゴリズムから外れている。"
  },
  {
    "start": 4921450,
    "end": 4940874,
    "text": "これらの特別なトークンは、基本的にシーケンス内の次のトークンを予測する基本的な言語モデリングだけでなく、特に後の微調整段階やチャットGBDのようなすべての側面に至るまで、広範囲に使用されます。"
  },
  {
    "start": 4941002,
    "end": 4953490,
    "text": "このティック・トークナイザーのページを更新すると、ここにあるデフォルトの例では、基本モデルのエンコーダーではなく、微調整されたモデルのようなトークナイザーが使われています。"
  },
  {
    "start": 4953910,
    "end": 4962718,
    "text": "例えば、GPT3.5のターボ・スキームを使った場合、ここにあるのはすべて特別なトークンで、イム・スタート、イム・エンドなどだ。"
  },
  {
    "start": 4962894,
    "end": 4967160,
    "text": "ちなみに、これはイマジナリーモノローグの略で、アンダースコアのスタートだ。"
  },
  {
    "start": 4967690,
    "end": 4971638,
    "text": "すべてのメッセージに始まりと終わりがあるのがわかるだろう。"
  },
  {
    "start": 4971724,
    "end": 4981434,
    "text": "これらの会話を区切り、メッセージの流れを把握するために、他にも多くのトークンが使用されている。"
  },
  {
    "start": 4981632,
    "end": 4990602,
    "text": "ティック・トークン・ライブラリーに戻り、一番下までスクロールすると、ティック・トークンを拡張する方法と、ティック・トークンを作成する方法について書かれています。"
  },
  {
    "start": 4990656,
    "end": 4996958,
    "text": "基本的には、GPT 4を使用しているため、CL 100Kのベーストークンをフォークすることができます。"
  },
  {
    "start": 4997044,
    "end": 4999706,
    "text": "例えば、さらに特別なトークンを追加して拡張することができる。"
  },
  {
    "start": 4999738,
    "end": 5000686,
    "text": "これらは完全にあなた次第だ。"
  },
  {
    "start": 5000708,
    "end": 5005214,
    "text": "任意のトークンを考え出し、後から新しいidで追加することができる。"
  },
  {
    "start": 5005342,
    "end": 5011890,
    "text": "TikTokinライブラリは、文字列の中でこれを見つけると、それらを正しく入れ替えます。"
  },
  {
    "start": 5012630,
    "end": 5025650,
    "text": "GPT-2インテック・トークンOpenAIパブリックPyのボキャブラリー、分割のパターンがある。"
  },
  {
    "start": 5025730,
    "end": 5035786,
    "text": "GPT-2では、テキスト終了トークンという特殊なトークンを1つ登録している。"
  },
  {
    "start": 5035808,
    "end": 5041250,
    "text": "ここでは、これまで説明したようにパターンが変わっただけでなく、このトークナイザーでは特別なトークンも変わっていることがわかる。"
  },
  {
    "start": 5041350,
    "end": 5050762,
    "text": "もちろん、GPD 2と同じようにテキストの終わりはあるが、ここではさらに3つ、失礼、4つのトークンが追加されている。"
  },
  {
    "start": 5050906,
    "end": 5051850,
    "text": "フィムとは？"
  },
  {
    "start": 5051930,
    "end": 5054286,
    "text": "フィムはフィル・イン・ミドルの略。"
  },
  {
    "start": 5054468,
    "end": 5062434,
    "text": "このアイデアについてもっと知りたいなら、この論文から来ている。"
  },
  {
    "start": 5062552,
    "end": 5066454,
    "text": "ということは、ここにもう1つ、セルプトークンがあることになる。"
  },
  {
    "start": 5066652,
    "end": 5068920,
    "text": "それもそのエンコーディングだ。"
  },
  {
    "start": 5069370,
    "end": 5072646,
    "text": "言語モデルをトレーニングするのは、基本的にはごく一般的なことだ。"
  },
  {
    "start": 5072828,
    "end": 5076658,
    "text": "もし必要なら、特別なトークンを追加することもできる。"
  },
  {
    "start": 5076834,
    "end": 5102250,
    "text": "というのも、基本的に整数を追加することになり、たとえば、語彙トークンの埋め込み行列は行を追加して拡張する必要があります。通常、この行は小さな乱数などで初期化されます。"
  },
  {
    "start": 5102410,
    "end": 5110738,
    "text": "それに加えて、トランスフォーマーの最終レイヤーに行き、一番最後にある分類器への投影が1つずつ拡張されていることを確認しなければならない。"
  },
  {
    "start": 5110824,
    "end": 5118574,
    "text": "基本的に、特別なトークンを追加する場合は、トークン化の変更と組み合わせてモデルの手術を行う必要があります。"
  },
  {
    "start": 5118702,
    "end": 5122998,
    "text": "これは、特にモデルを微調整したい場合に、人々が行う非常に一般的な操作である。"
  },
  {
    "start": 5123084,
    "end": 5127430,
    "text": "例えば、ベースモデルからチャットGPTのようなチャットモデルにする。"
  },
  {
    "start": 5128650,
    "end": 5133062,
    "text": "さて、この時点でトークナイザー用のGPTを構築するために必要なものはすべて揃ったはずだ。"
  },
  {
    "start": 5133206,
    "end": 5139130,
    "text": "さて、この講義を開発する過程で、私はそれを実行し、このリポジトリ「Minbpe」でコードを公開した。"
  },
  {
    "start": 5139790,
    "end": 5148720,
    "text": "MinBPのリポジトリは、おそらくかなり変わるだろう。"
  },
  {
    "start": 5149650,
    "end": 5154942,
    "text": "MimBPのリポジトリに加えて、私はこの練習の進行を公開しているので、それに従うことができる。"
  },
  {
    "start": 5155076,
    "end": 5166510,
    "text": "エクササイズMDをご覧いただくと、私が目の前のタスクを4つのステップに分割し、GPDの4つのトークナイザーへと構築していることがおわかりいただけるだろう。"
  },
  {
    "start": 5166590,
    "end": 5172022,
    "text": "というわけで、このステップを正確に踏んで、ここに示したガイダンスに少し従うのは自由だ。"
  },
  {
    "start": 5172156,
    "end": 5176898,
    "text": "行き詰まりを感じたら、いつでもこちらのMimbpeリポジトリを参照してほしい。"
  },
  {
    "start": 5177004,
    "end": 5181274,
    "text": "テストが役に立つか、MimbPリポジトリそのものが役に立つか。"
  },
  {
    "start": 5181392,
    "end": 5190090,
    "text": "私はこのコードをかなりクリーンでわかりやすいものに保つよう心がけているので、行き詰まったときはいつでも遠慮なく参照してほしい。"
  },
  {
    "start": 5191150,
    "end": 5196634,
    "text": "それに加えて、基本的には一度書いてしまえば、tictokenからこの動作を再現できるはずだ。"
  },
  {
    "start": 5196762,
    "end": 5206210,
    "text": "GPTの4つのトークナイザーを使えば、この文字列をエンコードしてトークンを得ることができる。"
  },
  {
    "start": 5206360,
    "end": 5212130,
    "text": "これに加えて、Ticktokenライブラリでは提供されていない独自の列車関数を実装することができるはずです。"
  },
  {
    "start": 5212200,
    "end": 5215686,
    "text": "これも推論コードだけだが、自分で列車を書くこともできるだろう。"
  },
  {
    "start": 5215788,
    "end": 5228450,
    "text": "membepも同様に、独自のトークンボキャブラリーを学習することができます。ここでは、membepの中のコードの一部を紹介します。"
  },
  {
    "start": 5228610,
    "end": 5232730,
    "text": "左はGPTの4つのマージ。"
  },
  {
    "start": 5233390,
    "end": 5236950,
    "text": "最初の256は生のバイトである。"
  },
  {
    "start": 5237030,
    "end": 5241334,
    "text": "GPT4がトレーニング中に行ったマージを視覚化したものだ。"
  },
  {
    "start": 5241472,
    "end": 5250494,
    "text": "GPT4が最初に行ったマージは、2つのスペースを1つのトークンにマージすることだった。"
  },
  {
    "start": 5250692,
    "end": 5254002,
    "text": "というわけで、これがGPT4トレーニング中に物事が統合された順番である。"
  },
  {
    "start": 5254136,
    "end": 5260510,
    "text": "これはmimbbeでトークナイザーを訓練することによって得られるマージ順序である。"
  },
  {
    "start": 5260590,
    "end": 5270262,
    "text": "この場合、私はテイラー・スウィフト（Taylor Swift）のウィキペディアのページで訓練した。"
  },
  {
    "start": 5270396,
    "end": 5271800,
    "text": "彼女はかなりクールだ。"
  },
  {
    "start": 5272250,
    "end": 5276098,
    "text": "私は何を言おうとしていたのだろう？"
  },
  {
    "start": 5276204,
    "end": 5281100,
    "text": "そう、だから例としてこの2つの語彙を比較することができる。"
  },
  {
    "start": 5282990,
    "end": 5289802,
    "text": "ここでは、GPT4がマージされてインになり、このトークン、2 5 9でもまったく同じことをしている。"
  },
  {
    "start": 5289936,
    "end": 5292442,
    "text": "ここでスペースtはスペースtとなる。"
  },
  {
    "start": 5292576,
    "end": 5295070,
    "text": "それは私たちにも少し後に起こったことだ。"
  },
  {
    "start": 5295140,
    "end": 5299326,
    "text": "ここでの違いは、私の理解ではトレーニングセットの違いに過ぎない。"
  },
  {
    "start": 5299428,
    "end": 5312594,
    "text": "例として、私は多くの空白を見たので、GPT 4はおそらくトレーニングセットに多くのパイソンコードを持っていたと予想します。"
  },
  {
    "start": 5312792,
    "end": 5317278,
    "text": "大雑把に言えば、同じアルゴリズムで動いているから同じに見える。"
  },
  {
    "start": 5317374,
    "end": 5322246,
    "text": "自分でトレーニングする場合、トレーニングの内容によっては似たようなものが得られるだろう。"
  },
  {
    "start": 5322348,
    "end": 5334474,
    "text": "それでは、TikTokenとOpenAIの文字列トークン化の方法から移って、llmsでトークン化を扱うのによく使われるライブラリ、sentence pieceについて説明します。"
  },
  {
    "start": 5334672,
    "end": 5343998,
    "text": "センテンス・ピースは、TikTokenとは異なり、学習と推論の両方を行うことができ、その両方で非常に効率的であるため、言語モデルで非常によく使用される。"
  },
  {
    "start": 5344164,
    "end": 5352914,
    "text": "ボキャブラリーをトレーニングするためのアルゴリズムをいくつかサポートしているが、そのうちのひとつが、これまで見てきたバイトペアエンコーディングアルゴリズムだ。"
  },
  {
    "start": 5353112,
    "end": 5358882,
    "text": "現在、センテンスピースはラマやミストラルシリーズをはじめ、多くのモデルに採用されている。"
  },
  {
    "start": 5359016,
    "end": 5361860,
    "text": "これはGitHubのGoogle Slash sentence pieceにある。"
  },
  {
    "start": 5362630,
    "end": 5374406,
    "text": "センテンス・ピースとの大きな違いは、説明するのが難しくて微妙なので例を見ていくが、ここでは操作の順序について異なる考えを持っているということだ。"
  },
  {
    "start": 5374508,
    "end": 5380210,
    "text": "チック・トークンの場合、まずコード・ポイントを文字列にする。"
  },
  {
    "start": 5380290,
    "end": 5383782,
    "text": "UTF8を使用してバイトにエンコードし、バイトをマージしている。"
  },
  {
    "start": 5383846,
    "end": 5387500,
    "text": "センテンスピースとしてはかなりシンプルだ。"
  },
  {
    "start": 5388110,
    "end": 5391466,
    "text": "それはコードポイントそのもののレベルで直接機能する。"
  },
  {
    "start": 5391648,
    "end": 5401680,
    "text": "トレーニングセットで利用可能なコードポイントを調べ、それらのコードポイントをマージし、BPEはコードポイントのレベルで実行される。"
  },
  {
    "start": 5402450,
    "end": 5408562,
    "text": "コードポイントを使い果たした場合は、あまり出てこないコードポイントもあるかもしれない。"
  },
  {
    "start": 5408616,
    "end": 5411886,
    "text": "レアリティはこの文字カバレッジのハイパーパラメータによって決定される。"
  },
  {
    "start": 5412078,
    "end": 5429558,
    "text": "そして、これらのコードポイントはUNCのような特別な未知のトークンにマッピングされるか、あるいはバイトフォールバックオプションをオンにしている場合は、これらのまれなコードポイントを受け取り、UTF 8を使ってエンコードし、そのエンコードの個々のバイトをトークンに変換する。"
  },
  {
    "start": 5429654,
    "end": 5433862,
    "text": "特別なバイト・トークンは、基本的にボキャブラリーに追加される。"
  },
  {
    "start": 5434006,
    "end": 5441340,
    "text": "コードポイントではBPEを使用し、レアなコードポイントではバイトにフォールバックする。"
  },
  {
    "start": 5442510,
    "end": 5444030,
    "text": "それが違いのようなものだ。"
  },
  {
    "start": 5444100,
    "end": 5451130,
    "text": "個人的にはTikTokの方が圧倒的にクリーンだが、トークン化のアプローチの仕方に微妙な、しかしかなり大きな違いがあるような気がする。"
  },
  {
    "start": 5451290,
    "end": 5457394,
    "text": "具体的な例を挙げて説明しよう。"
  },
  {
    "start": 5457592,
    "end": 5459540,
    "text": "具体的な例を挙げてみよう。"
  },
  {
    "start": 5459990,
    "end": 5462180,
    "text": "こうしてセンテンスピースを輸入することができる。"
  },
  {
    "start": 5462550,
    "end": 5468502,
    "text": "そしてここで、文の説明のようなものを取り出して、ちょっとしたおもちゃのようなデータセットを作ってみたんだ。"
  },
  {
    "start": 5468556,
    "end": 5469874,
    "text": "ファイルを持つのが本当に好きなんだ。"
  },
  {
    "start": 5469922,
    "end": 5473000,
    "text": "この内容でおもちゃのTXTファイルを作った。"
  },
  {
    "start": 5473930,
    "end": 5479378,
    "text": "さて、センテンスピースについてちょっとクレイジーなのは、オプションやコンフィギュレーションが山ほどあるということだ。"
  },
  {
    "start": 5479554,
    "end": 5486220,
    "text": "なぜそうなのかというと、センテンスピースは以前からあったものだと思うし、実に多様なものを扱おうとしているからだ。"
  },
  {
    "start": 5486590,
    "end": 5492686,
    "text": "歴史が長いだけに、歴史的なお荷物もかなり蓄積されていると思う。"
  },
  {
    "start": 5492868,
    "end": 5496506,
    "text": "特に、コンフィギュレーションに関する議論は山ほどある。"
  },
  {
    "start": 5496538,
    "end": 5497998,
    "text": "これがすべてでもない。"
  },
  {
    "start": 5498164,
    "end": 5510980,
    "text": "ここに行けば、すべてのトレーニングオプションを見ることができるし、トレーナーのスペックなどを表すために使われる生のプロトバフを見れば、かなり役に立つドキュメントもある。"
  },
  {
    "start": 5512310,
    "end": 5514834,
    "text": "これらの選択肢の多くは、私たちには関係のないものだ。"
  },
  {
    "start": 5514872,
    "end": 5518466,
    "text": "一例を挙げるとすれば、シュリンク要因だ。"
  },
  {
    "start": 5518658,
    "end": 5522162,
    "text": "この縮小係数は、バイトペア符号化アルゴリズムでは使用されない。"
  },
  {
    "start": 5522226,
    "end": 5525526,
    "text": "これは私たちには関係のない議論だ。"
  },
  {
    "start": 5525708,
    "end": 5527990,
    "text": "これは別のトレーニングアルゴリズムに適用される。"
  },
  {
    "start": 5530330,
    "end": 5541594,
    "text": "さて、ここで私が試みたのは、私が知る限り、非常に、非常に似ている、おそらく同じ、願わくばラマ2がトレーニングされた方法と同じような方法で、センテンスピースをセットアップしてみたということだ。"
  },
  {
    "start": 5541722,
    "end": 5559762,
    "text": "私がやった方法は、基本的に、メタがリリースしたトークナイザー・モデル・ファイルを、プロトバフのようなファイルを使って開き、すべてのオプションを検査するというものです。"
  },
  {
    "start": 5559816,
    "end": 5562638,
    "text": "関連しそうなオプションはすべてコピーしてみた。"
  },
  {
    "start": 5562814,
    "end": 5564398,
    "text": "ここで入力を設定する。"
  },
  {
    "start": 5564494,
    "end": 5566526,
    "text": "このファイルは生テキストだ。"
  },
  {
    "start": 5566638,
    "end": 5567854,
    "text": "これが出力になる。"
  },
  {
    "start": 5567902,
    "end": 5571670,
    "text": "フォトトーク、400のモデル、そしてボキャブラリーだ。"
  },
  {
    "start": 5572330,
    "end": 5576120,
    "text": "BPアルゴリズムを使用し、ボキャブサイズを400にしたい。"
  },
  {
    "start": 5576890,
    "end": 5587034,
    "text": "そして、基本的に前処理や正規化ルールと呼ばれるものについては、ここにたくさんの設定がある。"
  },
  {
    "start": 5587152,
    "end": 5592182,
    "text": "かつて正規化は、自然言語処理ではllms以前と言っていいほど、非常に普及していた。"
  },
  {
    "start": 5592246,
    "end": 5601962,
    "text": "機械翻訳やテキスト分類などでは、テキストを正規化して単純化し、すべて小文字にし、二重の空白をすべて削除する。"
  },
  {
    "start": 5602106,
    "end": 5604526,
    "text": "言語モデルにおいては、そのようなことは一切しない方がいい。"
  },
  {
    "start": 5604548,
    "end": 5606818,
    "text": "少なくとも、それがディープラーニングを専門とする私の好みだ。"
  },
  {
    "start": 5606904,
    "end": 5608370,
    "text": "データには触れないようにしたい。"
  },
  {
    "start": 5608440,
    "end": 5613300,
    "text": "生のデータはできるだけ生のまま残しておきたい。"
  },
  {
    "start": 5613990,
    "end": 5617240,
    "text": "できることなら、あなたは基本的にこの多くをオフにしようとしている。"
  },
  {
    "start": 5617610,
    "end": 5621426,
    "text": "センテンス・ピースのもうひとつの特徴は、センテンスという概念を持っていることだ。"
  },
  {
    "start": 5621618,
    "end": 5633526,
    "text": "文の部分というのは、確か初期の頃に開発されたもので、独立した文の束に対してトークナイザーをトレーニングするというものでした。"
  },
  {
    "start": 5633638,
    "end": 5639210,
    "text": "何センテンスでトレーニングするのか、センテンスの長さはどれくらいまでなのか、などなど。"
  },
  {
    "start": 5641710,
    "end": 5642886,
    "text": "シャッフルする文章。"
  },
  {
    "start": 5642998,
    "end": 5646346,
    "text": "そのため、文章は個々のトレーニング例のようなものだ。"
  },
  {
    "start": 5646458,
    "end": 5651310,
    "text": "llmsの文脈では、この区別はまるで無意味で奇妙なものだ。"
  },
  {
    "start": 5651730,
    "end": 5655090,
    "text": "文章は生データに触れてはいけないのと同じだ。"
  },
  {
    "start": 5655160,
    "end": 5663250,
    "text": "文章は確かに存在するが、生のデータセットには、何が文章で何が文章でないかというような、中間的なものがたくさんある。"
  },
  {
    "start": 5663990,
    "end": 5669174,
    "text": "だから、実際のセンテンスを定義するのは本当に難しいと思う。"
  },
  {
    "start": 5669292,
    "end": 5672774,
    "text": "言語によって概念が違うとか、そういうことかもしれない。"
  },
  {
    "start": 5672812,
    "end": 5674898,
    "text": "なぜそのコンセプトを導入したのか？"
  },
  {
    "start": 5674994,
    "end": 5676646,
    "text": "正直、意味がわからない。"
  },
  {
    "start": 5676668,
    "end": 5680470,
    "text": "私はファイルを巨大なバイトのストリームとして扱いたい。"
  },
  {
    "start": 5681210,
    "end": 5684230,
    "text": "レアな単語を扱うことが多い。"
  },
  {
    "start": 5684310,
    "end": 5686010,
    "text": "言葉というのはコード・ポイントのことだ。"
  },
  {
    "start": 5686080,
    "end": 5687834,
    "text": "この話はまた後でしよう。"
  },
  {
    "start": 5688032,
    "end": 5696542,
    "text": "この他にも、数字の分割、空白の分割、数字の扱い方など、多くのルールがある。"
  },
  {
    "start": 5696596,
    "end": 5698858,
    "text": "これはある種のマージ・ルールのようなものだ。"
  },
  {
    "start": 5698954,
    "end": 5705278,
    "text": "これは、TikTokで正規表現を使ってカテゴリーを分けるのと少し似ていると思う。"
  },
  {
    "start": 5705454,
    "end": 5715700,
    "text": "目を細めて文章を読めば、それに相当するものがあり、例えば、桁を分割したりすることもできる。"
  },
  {
    "start": 5716710,
    "end": 5719206,
    "text": "この他にもいくつかあるので、また後ほど。"
  },
  {
    "start": 5719228,
    "end": 5727610,
    "text": "UNCトークン、文頭トークン、文末トークン、パッドトークン。"
  },
  {
    "start": 5728510,
    "end": 5732010,
    "text": "私の理解では、UNCトークンは存在するはずだ。"
  },
  {
    "start": 5732350,
    "end": 5735100,
    "text": "それから、トレーニングができるように、いくつかのシステムを導入する。"
  },
  {
    "start": 5735550,
    "end": 5742074,
    "text": "trainを押すと、talk 400 modelとtalk 400 vocabというファイルが作成される。"
  },
  {
    "start": 5742202,
    "end": 5747086,
    "text": "モデルファイルをロードして、その語彙を検査することができる。"
  },
  {
    "start": 5747268,
    "end": 5752560,
    "text": "そこで、このテキストでボキャブラリーサイズ400のトレーニングを行った。"
  },
  {
    "start": 5752930,
    "end": 5757682,
    "text": "これが個々のピースであり、文のピースが作る個々のトークンである。"
  },
  {
    "start": 5757816,
    "end": 5762690,
    "text": "冒頭で、UNCトークンのIDがゼロであることがわかる。"
  },
  {
    "start": 5762840,
    "end": 5767286,
    "text": "そして、シークエンスの始まり、シークエンスの終わり1、2がある。"
  },
  {
    "start": 5767468,
    "end": 5771542,
    "text": "パッドIDはネガティブなものなので、使わないことにした。"
  },
  {
    "start": 5771676,
    "end": 5773560,
    "text": "ここにはパッドIDはない。"
  },
  {
    "start": 5774330,
    "end": 5777602,
    "text": "これは個々のバイト・トークンである。"
  },
  {
    "start": 5777746,
    "end": 5782010,
    "text": "ここで、llamaのバイトフォールバックがオンになっていることを確認した。"
  },
  {
    "start": 5782080,
    "end": 5783098,
    "text": "本当だ。"
  },
  {
    "start": 5783264,
    "end": 5789870,
    "text": "以下が256バイトのトークンで、これがそのIDである。"
  },
  {
    "start": 5792610,
    "end": 5801338,
    "text": "そして一番下には、バイト・トークンの後にマージがあり、これらはマージの親ノードである。"
  },
  {
    "start": 5801434,
    "end": 5804820,
    "text": "私たちが見ているのは子供たちではなく、両親とその偶像だ。"
  },
  {
    "start": 5805430,
    "end": 5812782,
    "text": "そしてマージの後、最終的に個々のトークンとそのIDが来る。"
  },
  {
    "start": 5812926,
    "end": 5819542,
    "text": "これが個々のトークンであり、個々のコートポイントトークンである。"
  },
  {
    "start": 5819676,
    "end": 5823730,
    "text": "これは、センテンス・ピースがボキャブラリーを表現する順序のようなものだ。"
  },
  {
    "start": 5823810,
    "end": 5830326,
    "text": "特殊トークンから始まり、バイト・トークン、マージ・トークン、そして個々のコードポイント・トークン。"
  },
  {
    "start": 5830518,
    "end": 5836300,
    "text": "これらの生のコードポイント・トークンはすべて、トレーニング・セットで遭遇したものである。"
  },
  {
    "start": 5836990,
    "end": 5843440,
    "text": "これらの個々のコードポイントは、ここで発生したコードポイント全体である。"
  },
  {
    "start": 5845250,
    "end": 5850762,
    "text": "そして、キャラクターのカバー率によって決定される、極めてレアなものだ。"
  },
  {
    "start": 5850826,
    "end": 5861410,
    "text": "もし、あるコードポイントが100万センテンスのうち1回しか出現しないとか、そういうことであれば、それは無視され、語彙に加えられることはないだろう。"
  },
  {
    "start": 5861910,
    "end": 5867560,
    "text": "ボキャブラリーがあれば、IDにエンコードしてリストを得ることができる。"
  },
  {
    "start": 5868250,
    "end": 5876134,
    "text": "そして、ここで私はまた、個々のトークンを、彼らが言うように、小さな断片にデコードしている。"
  },
  {
    "start": 5876332,
    "end": 5878360,
    "text": "ここで何が起こったかを見てみよう。"
  },
  {
    "start": 5878810,
    "end": 5880006,
    "text": "こんにちは、宇宙。"
  },
  {
    "start": 5880108,
    "end": 5881430,
    "text": "オニャンガセオ"
  },
  {
    "start": 5881950,
    "end": 5885370,
    "text": "これが戻ってきたトークンIDです。"
  },
  {
    "start": 5885520,
    "end": 5890620,
    "text": "ここに目を向けると、いくつかのことが思い浮かぶ。"
  },
  {
    "start": 5891150,
    "end": 5894042,
    "text": "第一に、これらのキャラクターを見てほしい。"
  },
  {
    "start": 5894106,
    "end": 5896910,
    "text": "もちろん、韓国語の文字はトレーニングセットには含まれていない。"
  },
  {
    "start": 5897060,
    "end": 5905618,
    "text": "センテンスピースはトレーニング中に見たことのないコードポイントに遭遇しており、それらのコードポイントにはトークンが関連付けられていない。"
  },
  {
    "start": 5905704,
    "end": 5909406,
    "text": "突然のことだが、これは未知のトークンである。"
  },
  {
    "start": 5909598,
    "end": 5915554,
    "text": "なぜなら、バイトフォールバックが真だからである。"
  },
  {
    "start": 5915682,
    "end": 5924390,
    "text": "そのため、これをUTF 8でエンコードし、トークンを使ってバイトを表現する。"
  },
  {
    "start": 5924890,
    "end": 5927420,
    "text": "それが、私たちがここで手に入れようとしているものだ。"
  },
  {
    "start": 5927790,
    "end": 5938058,
    "text": "これはUTF 8エンコーディングで、先にIDを持つ特別なトークンがあるため、3つシフトされている。"
  },
  {
    "start": 5938224,
    "end": 5939900,
    "text": "それがここで起きたことだ。"
  },
  {
    "start": 5940290,
    "end": 5949034,
    "text": "さて、もうひとつ、その前に、バイトフォールバックについて説明する前に、バイトフォールバックを削除しておこう。"
  },
  {
    "start": 5949162,
    "end": 5951486,
    "text": "これが嘘だとしたら、どうなるんだ？"
  },
  {
    "start": 5951588,
    "end": 5952830,
    "text": "鍛え直そう。"
  },
  {
    "start": 5953410,
    "end": 5956958,
    "text": "最初に起こったことは、すべてのバイト・トークンが消えてしまったということですね？"
  },
  {
    "start": 5957124,
    "end": 5966150,
    "text": "なぜなら、すべてのバイトでウーキャップ・サイズのスペースを占有していないからだ。"
  },
  {
    "start": 5966730,
    "end": 5971400,
    "text": "これをエンコードするとゼロになる。"
  },
  {
    "start": 5971770,
    "end": 5974914,
    "text": "この文字列全体は、突然バイトフォールバックがなくなった。"
  },
  {
    "start": 5974962,
    "end": 5978626,
    "text": "これは未知であり、未知はUNCである。"
  },
  {
    "start": 5978818,
    "end": 5983260,
    "text": "UNCトークンはトークン・ゼロなので、これはゼロである。"
  },
  {
    "start": 5983870,
    "end": 5987610,
    "text": "言語モデルに反映されることを念頭に置く必要がある。"
  },
  {
    "start": 5987680,
    "end": 5994810,
    "text": "希少であるがゆえに認識されないさまざまなものがUNCにマッピングされてしまうとき、言語モデルはどうすればいいのだろうか？"
  },
  {
    "start": 5994890,
    "end": 5996878,
    "text": "あなたが望むような物件ではない。"
  },
  {
    "start": 5996964,
    "end": 6008366,
    "text": "なぜなら、未知のコードポイントやレアなコードポイントを何らかの形でモデルに反映させたいからだ。"
  },
  {
    "start": 6008478,
    "end": 6010850,
    "text": "次にお見せしたいのは次のことだ。"
  },
  {
    "start": 6011510,
    "end": 6020674,
    "text": "個々のトークンをデコードするとき、スペースが太いアンダーラインになるのがわかるだろう。"
  },
  {
    "start": 6020802,
    "end": 6027014,
    "text": "ところで、なぜセンテンス・ビーが空白を太字のアンダースコアに切り替えるのか、私には100％わからない。"
  },
  {
    "start": 6027142,
    "end": 6028262,
    "text": "視覚化のためかもしれない。"
  },
  {
    "start": 6028326,
    "end": 6031898,
    "text": "なぜそうなるのかは100％わからないが、これを見てほしい。"
  },
  {
    "start": 6031984,
    "end": 6037580,
    "text": "なぜハローの前に余分なスペースがあるのか？"
  },
  {
    "start": 6039090,
    "end": 6040366,
    "text": "これはどこから来ているのか？"
  },
  {
    "start": 6040468,
    "end": 6042880,
    "text": "まあ、このオプションから来るんだけどね。"
  },
  {
    "start": 6045970,
    "end": 6047822,
    "text": "ダミーの接頭辞を付けるは真。"
  },
  {
    "start": 6047956,
    "end": 6057026,
    "text": "ドキュメントを見るときは、world in worldとhello worldをまったく同じように扱うために、テキストの先頭にダミーの空白を追加する。"
  },
  {
    "start": 6057128,
    "end": 6059250,
    "text": "これがやろうとしていることは次のようなことだ。"
  },
  {
    "start": 6060150,
    "end": 6069766,
    "text": "チック・トークナイザーの世界に戻ると、トークン単体ではスペース・ワールドとは異なるIDを持っている。"
  },
  {
    "start": 6069948,
    "end": 6074466,
    "text": "これは1917年だが、これは14年だ。"
  },
  {
    "start": 6074578,
    "end": 6076870,
    "text": "これらは言語モデルにとって2つの異なるトークンである。"
  },
  {
    "start": 6076940,
    "end": 6080998,
    "text": "言語モデルはデータから、それらが実は非常に似た概念のようなものであることを学ばなければならない。"
  },
  {
    "start": 6081094,
    "end": 6094000,
    "text": "TikTokの世界の言語モデルには、基本的に、文頭の単語と文中の単語は実際にはまったく違って見えるので、それらがほぼ同じであることを学習させなければならない。"
  },
  {
    "start": 6094370,
    "end": 6098318,
    "text": "このダミープレフィックスを追加することで、それに少しでも対抗しようとしているのだ。"
  },
  {
    "start": 6098404,
    "end": 6103982,
    "text": "その仕組みは、基本的にダミーの接頭辞を追加するというものだ。"
  },
  {
    "start": 6104046,
    "end": 6117510,
    "text": "前処理の一環として、文字列を取り込み、スペースを追加する。"
  },
  {
    "start": 6117580,
    "end": 6119400,
    "text": "2人とも宇宙の世界になる。"
  },
  {
    "start": 6119770,
    "end": 6123302,
    "text": "これは、オンになっているもう一つの前処理オプションである。"
  },
  {
    "start": 6123356,
    "end": 6126502,
    "text": "llama2もこのオプションを使っている。"
  },
  {
    "start": 6126636,
    "end": 6142022,
    "text": "ということで、文のプレビューのために言いたいことと、どう違うのか、たぶんここで私がやったことは、llamaの2人が訓練したトークナイザーの、基本的には生のプロトコル・バッファ表現を入れただけだと思います。"
  },
  {
    "start": 6142166,
    "end": 6144462,
    "text": "自由にステップを踏んでください。"
  },
  {
    "start": 6144516,
    "end": 6153630,
    "text": "もし、トークン化をmeta llamaの2つのトークン化と同じにしたいのであれば、私が上記で試みたように、これらの設定をコピーペーストすることになる。"
  },
  {
    "start": 6153970,
    "end": 6157662,
    "text": "ええ、このセクションはこれで終わりです。"
  },
  {
    "start": 6157726,
    "end": 6164098,
    "text": "センテンスピースに対する私の考えをまとめると、第一に、センテンスピースには歴史的なお荷物がたくさんあるということだ。"
  },
  {
    "start": 6164184,
    "end": 6172680,
    "text": "文章という概念やその最大長など、少し混乱しそうで、フットガンを含みそうな概念がたくさんある。"
  },
  {
    "start": 6173610,
    "end": 6180806,
    "text": "それ以外は、効率的で学習と推論の両方ができるため、業界でかなり一般的に使われている。"
  },
  {
    "start": 6180998,
    "end": 6188086,
    "text": "例えば、unctokenが存在しなければならないとか、バイトフォールバックの方法など、いくつかの癖があり、特にエレガントとは思えない。"
  },
  {
    "start": 6188198,
    "end": 6190534,
    "text": "残念なことに、あまり文書化されていないと言わざるを得ない。"
  },
  {
    "start": 6190582,
    "end": 6202398,
    "text": "私自身、この作業で多くの時間を要し、物事を視覚化し、ここで何が起こっているのかを理解しようとしました。"
  },
  {
    "start": 6202564,
    "end": 6207922,
    "text": "今すぐ自分のトークナイザーを訓練したいのであれば、とても素晴らしいレポが利用できる。"
  },
  {
    "start": 6208056,
    "end": 6209554,
    "text": "さて、ここで再びギアを入れ替えよう。"
  },
  {
    "start": 6209592,
    "end": 6217926,
    "text": "ボキャブラリーのサイズをどのように設定するべきか、また、ボキャブラリーのサイズにまつわる考慮事項にはどのようなものがあるのか。"
  },
  {
    "start": 6218108,
    "end": 6225346,
    "text": "そこで、前回のビデオでGPTをゼロから構築したときに開発したモデル・アーキテクチャに戻りたいと思う。"
  },
  {
    "start": 6225538,
    "end": 6230586,
    "text": "これは、前のビデオで作成したファイルで、トランスフォーマーのモデルを定義しました。"
  },
  {
    "start": 6230688,
    "end": 6234534,
    "text": "具体的にボカップのサイズと、このファイルのどこに表示されるかを見てみよう。"
  },
  {
    "start": 6234662,
    "end": 6236620,
    "text": "ここではボカップのサイズを定義する。"
  },
  {
    "start": 6237390,
    "end": 6240650,
    "text": "この時は65人とか、極めて少数だった。"
  },
  {
    "start": 6240720,
    "end": 6242806,
    "text": "これはもっと大きくなるだろう。"
  },
  {
    "start": 6242998,
    "end": 6246090,
    "text": "これらのレイヤーのほとんどで、ボキャブラリーの大きさがあまり出てこないことがわかるだろう。"
  },
  {
    "start": 6246170,
    "end": 6250606,
    "text": "それが出てくるのは、まさにこの2カ所だけだ。"
  },
  {
    "start": 6250788,
    "end": 6263102,
    "text": "言語モデルを定義するとき、トークン埋め込みテーブルがある。これは2次元の配列で、ボキャブラリーのサイズは基本的に行数と各ボキャブラリー要素の数である。"
  },
  {
    "start": 6263166,
    "end": 6267070,
    "text": "各トークンは、バックプロパゲーションを使って学習するベクトルを持っている。"
  },
  {
    "start": 6267150,
    "end": 6271138,
    "text": "そのベクトルは、トランスフォーマーのチャンネル数であるembedと同じ大きさである。"
  },
  {
    "start": 6271314,
    "end": 6276614,
    "text": "基本的に、ボキャブのサイズが大きくなるにつれて、この埋め込みテーブルも大きくなっていく。"
  },
  {
    "start": 6276652,
    "end": 6278150,
    "text": "これから行を追加していく。"
  },
  {
    "start": 6278570,
    "end": 6283942,
    "text": "それに加えて、トランスの末端にはLMHAD層というリニアな層がある。"
  },
  {
    "start": 6284086,
    "end": 6290486,
    "text": "このレイヤーは、次のトークンの確率となるロジットを生成するために、一番最後に使われていることがわかるだろう。"
  },
  {
    "start": 6290518,
    "end": 6300154,
    "text": "つまり、直感的に言えば、私たちはトランスフォーマーのどの時点においても、次に来る可能性のあるトークンひとつひとつの確率を計算しようとしているのだ。"
  },
  {
    "start": 6300282,
    "end": 6304074,
    "text": "トークンの数が増えれば増えるほど、より多くの確率を生み出す必要がある。"
  },
  {
    "start": 6304202,
    "end": 6312126,
    "text": "トークンひとつひとつが、トランスフォーマーの最終レイヤーのために、このリニアレイヤーで行わなければならない追加のドット積を導入することになる。"
  },
  {
    "start": 6312318,
    "end": 6315198,
    "text": "なぜボキャブラリーのサイズは無限ではないのか？"
  },
  {
    "start": 6315294,
    "end": 6316574,
    "text": "なぜ無限に成長できないのか？"
  },
  {
    "start": 6316622,
    "end": 6319880,
    "text": "第一に、トークン埋め込みテーブルが大きくなる。"
  },
  {
    "start": 6320730,
    "end": 6322854,
    "text": "線状の層が伸びていくんだ。"
  },
  {
    "start": 6322972,
    "end": 6328070,
    "text": "というのも、このLMヘッドレイヤーは計算コストが高くなるからだ。"
  },
  {
    "start": 6328570,
    "end": 6335530,
    "text": "その2、パラメーターが増えたことで、いくつかのパラメーターのトレーニング不足を心配することができる。"
  },
  {
    "start": 6336110,
    "end": 6347582,
    "text": "つまり、直感的には、語彙のサイズが非常に大きい場合、例えば100万個のトークンがあるとすると、これらのトークンのひとつひとつが、トレーニング・データではより稀にしか出てこないことになります。"
  },
  {
    "start": 6347716,
    "end": 6352682,
    "text": "そのため、個々のトークンの例を見ることは少なくなっていくだろう。"
  },
  {
    "start": 6352826,
    "end": 6361940,
    "text": "というのも、基本的にすべてのトークンに関連するベクターは、あまり頻繁に出てこないし、フォワード・バックワード・パスにも参加しないからだ。"
  },
  {
    "start": 6362310,
    "end": 6366930,
    "text": "それに加えて、ボキャブラリーが増えるにつれて、シークエンスはどんどん狭まっていく。"
  },
  {
    "start": 6367000,
    "end": 6367522,
    "text": "そうだね。"
  },
  {
    "start": 6367656,
    "end": 6371718,
    "text": "というのも、これからますます多くのテキストを読むことになるからだ。"
  },
  {
    "start": 6371804,
    "end": 6372726,
    "text": "それはいいね。"
  },
  {
    "start": 6372828,
    "end": 6387466,
    "text": "また、あまりにも大きな塊が1つのトークンに押し込められているため、モデルがテキストの文字数ごとに考える時間が少なくなっていることを心配しているのかもしれません。"
  },
  {
    "start": 6387488,
    "end": 6387818,
    "text": "そうだろう？"
  },
  {
    "start": 6387904,
    "end": 6395754,
    "text": "基本的に、1つのトークンに多くの情報を詰め込みすぎている。そして、トランスフォーマーのフォワードパスでは、その情報を実際に適切に処理するには不十分なのだ。"
  },
  {
    "start": 6395882,
    "end": 6399566,
    "text": "ボキャブラリーの大きさを設計する際には、このような点を考慮する必要があります。"
  },
  {
    "start": 6399668,
    "end": 6402058,
    "text": "前述したように、これはほとんど経験的なハイパーパラメータである。"
  },
  {
    "start": 6402154,
    "end": 6408962,
    "text": "今日の最新鋭のアーキテクチャでは、これは通常、1万台後半か10万台前後のようだ。"
  },
  {
    "start": 6409096,
    "end": 6416322,
    "text": "次に簡単にお話ししたいのは、事前学習済みのモデルを使って、語彙数を増やしたい場合はどうするかということです。"
  },
  {
    "start": 6416456,
    "end": 6418134,
    "text": "これは実際、かなり一般的に行われていることだ。"
  },
  {
    "start": 6418172,
    "end": 6431446,
    "text": "例えば、チャチpptのファインチューニングを行う場合、ユーザーとアシスタント間のメタデータや会話オブジェクトの構造を維持するために、ベースモデルの上にさらに多くの新しい特殊トークンが導入される。"
  },
  {
    "start": 6431558,
    "end": 6433670,
    "text": "それは多くの特別なトークンを必要とする。"
  },
  {
    "start": 6433750,
    "end": 6438682,
    "text": "また、ブラウザやその他のツールを使用する場合など、特別なトークンを追加することもできる。"
  },
  {
    "start": 6438816,
    "end": 6443526,
    "text": "だから、あらゆる種類の特別な機能のためにたくさんのトークンを追加したくなる。"
  },
  {
    "start": 6443718,
    "end": 6446734,
    "text": "トークンを追加したいのなら、それは全然可能でしょう？"
  },
  {
    "start": 6446852,
    "end": 6450890,
    "text": "私たちがしなければならないのは、この埋め込み部分のサイズを変更し、行を追加することです。"
  },
  {
    "start": 6450970,
    "end": 6458770,
    "text": "これらのパラメータをゼロから小さな乱数として初期化し、この線形内部でウェイトを拡張する必要がある。"
  },
  {
    "start": 6459110,
    "end": 6466334,
    "text": "この新しいトークンの確率を基本的に計算するために、関連するパラメータとのドット積も作り始めなければならない。"
  },
  {
    "start": 6466462,
    "end": 6469298,
    "text": "いずれも単なるリサイズ操作である。"
  },
  {
    "start": 6469394,
    "end": 6473090,
    "text": "非常に軽いモデル手術で、かなり簡単にできる。"
  },
  {
    "start": 6473170,
    "end": 6475798,
    "text": "基本的にはベースモデルを凍結するのが一般的だ。"
  },
  {
    "start": 6475884,
    "end": 6481690,
    "text": "新しいパラメーターを導入し、その新しいパラメーターをトレーニングして、新しいトークンをアーキテクチャに導入する。"
  },
  {
    "start": 6482350,
    "end": 6487338,
    "text": "だから、任意の部分を凍らせることもできるし、任意の部分をトレーニングすることもできる。"
  },
  {
    "start": 6487424,
    "end": 6491222,
    "text": "新しいトークンを導入したい場合は、基本的に小手術が必要だ。"
  },
  {
    "start": 6491286,
    "end": 6501962,
    "text": "最後に、ボキャブラリーに新しいトークンを導入するという点では、単に特別なトークンを追加したり、特別な新機能を追加したりするだけでなく、そのアプリケーションの設計空間全体が存在するということを述べておきたい。"
  },
  {
    "start": 6502106,
    "end": 6504014,
    "text": "デザインスペースの感覚を味わってほしい。"
  },
  {
    "start": 6504052,
    "end": 6506274,
    "text": "これだけで1本のビデオになる。"
  },
  {
    "start": 6506472,
    "end": 6511742,
    "text": "これは、ギストトークンと呼ばれるものを使ってプロンプトを圧縮する学習に関する論文である。"
  },
  {
    "start": 6511886,
    "end": 6517054,
    "text": "大まかなアイデアは、非常に長いプロンプトを必要とする設定で言語モデルを使用していると仮定します。"
  },
  {
    "start": 6517182,
    "end": 6526530,
    "text": "プロンプトが長いと、エンコードして、それを使って、その上に出席して......と、すべてが遅くなる。"
  },
  {
    "start": 6526690,
    "end": 6536006,
    "text": "その代わりに、この論文で彼らが行っているのは、新しいトークンを導入し、基本的にいくつかの新しいトークンを持つことを想像することだ。"
  },
  {
    "start": 6536038,
    "end": 6540502,
    "text": "それらを順番に並べ、蒸留によってモデルを訓練する。"
  },
  {
    "start": 6540646,
    "end": 6557522,
    "text": "モデル全体を凍結したまま、新しいトークンの表現、その埋め込みだけをトレーニングし、新しいトークンに対して最適化することで、言語モデルの動作が、あなたのために機能する非常に長いプロンプトを持つモデルと同じになるようにするのです。"
  },
  {
    "start": 6557576,
    "end": 6562878,
    "text": "つまり、非常に長いプロンプトを数個の新しいgistトークンに圧縮する圧縮技術なのだ。"
  },
  {
    "start": 6563054,
    "end": 6573282,
    "text": "これを訓練して、テスト時に古いプロンプトを破棄してトークンを入れ替えるだけで、非常に長いプロンプトの代わりをするようなもので、ほとんど同じパフォーマンスが得られる。"
  },
  {
    "start": 6573426,
    "end": 6586326,
    "text": "このテクニックは、パラメーターの効率的な微調整テクニックの1つで、モデルの大部分は基本的に固定されており、モデルの重みのトレーニングや、ローラなどの新しいパラメーターのトレーニングは行われない。"
  },
  {
    "start": 6586518,
    "end": 6590262,
    "text": "学習するパラメータは、トークンの埋め込みだけとなる。"
  },
  {
    "start": 6590406,
    "end": 6591610,
    "text": "これはほんの一例だ。"
  },
  {
    "start": 6591680,
    "end": 6593706,
    "text": "これはまたビデオ全体のようなものになるかもしれない。"
  },
  {
    "start": 6593808,
    "end": 6609662,
    "text": "将来的に探求する価値のある設計空間がここにあることを感じていただくために、次に簡単に触れておきたいのは、入力モダリティとしてテキストだけでなく、他の多くのモダリティを同時に処理できるトランスフォーマーを構築する方法について、最近多くの機運が高まっているということです。"
  },
  {
    "start": 6609726,
    "end": 6612686,
    "text": "画像であれ、ビデオであれ、オーディオであれ。"
  },
  {
    "start": 6612798,
    "end": 6618526,
    "text": "どのようにしてこれらのモダリティを送り込み、変圧器からこれらのモダリティを潜在的に予測するのか？"
  },
  {
    "start": 6618718,
    "end": 6620982,
    "text": "アーキテクチャーを根本的に変える必要があるのか？"
  },
  {
    "start": 6621036,
    "end": 6626290,
    "text": "私は、多くの人々がアーキテクチャを変えず、トランスフォーマーにこだわるという方向に収束し始めていると思う。"
  },
  {
    "start": 6626370,
    "end": 6635062,
    "text": "入力ドメインをトークン化し、それを単なるテキスト・トークンに見せかけて、他のことはすべて同じように行う。"
  },
  {
    "start": 6635206,
    "end": 6642510,
    "text": "例えば、初期の論文で、画像をどのように切り詰め、整数にすることができるかを描いたものがある。"
  },
  {
    "start": 6645090,
    "end": 6648606,
    "text": "これらは基本的に、例えば画像のトークンになる。"
  },
  {
    "start": 6648788,
    "end": 6653502,
    "text": "これらのトークンは、整数であることを強制するハードトークンにすることができる。"
  },
  {
    "start": 6653566,
    "end": 6660174,
    "text": "また、ソフトトークンとして、目立たないようにすることもできる。"
  },
  {
    "start": 6660222,
    "end": 6665250,
    "text": "オートエンコーダのように、ボトルネックになるような表現を強いることはない。"
  },
  {
    "start": 6665830,
    "end": 6674662,
    "text": "また、OpenAI Soraから発表されたこの論文は、多くの人々の度肝を抜き、何が可能かという点で、多くの人々にインスピレーションを与えたと思う。"
  },
  {
    "start": 6674796,
    "end": 6679494,
    "text": "ここにグラフィックがあり、lmsがどのようにテキスト・トークンを持つかについて簡単に話している。"
  },
  {
    "start": 6679542,
    "end": 6681158,
    "text": "そらにはビジュアルパッチがある。"
  },
  {
    "start": 6681254,
    "end": 6687318,
    "text": "彼らはまた、ビデオを独自のボキャブラリーを持つトークンに切り捨てる方法を考え出した。"
  },
  {
    "start": 6687414,
    "end": 6693514,
    "text": "その場合、例えば自己回帰モデルを使って離散トークンを処理することもできるし、拡散モデルを使ってソフトトークンを処理することもできる。"
  },
  {
    "start": 6693642,
    "end": 6700094,
    "text": "このビデオの範囲を超えている。"
  },
  {
    "start": 6700132,
    "end": 6701898,
    "text": "ただ、簡単に言っておきたいことがある。"
  },
  {
    "start": 6701994,
    "end": 6714274,
    "text": "さて、トークン化のアルゴリズムについてかなり深く掘り下げ、その仕組みについてだいぶ理解できたところで、このビデオの最初に戻って、これらの箇条書きのいくつかを見ていき、なぜそのようなことが起こるのかを実際に確認してみましょう。"
  },
  {
    "start": 6714472,
    "end": 6720150,
    "text": "まず第一に、なぜ私のLLMは単語のスペルがうまく書けないのですか？"
  },
  {
    "start": 6721450,
    "end": 6729558,
    "text": "基本的にこれは、見たように、文字がトークンに分割され、トークンの中にはかなり長いものもあるからだ。"
  },
  {
    "start": 6729724,
    "end": 6735222,
    "text": "例として、私はGPTの4つの語彙に行き、長いトークンの1つを見た。"
  },
  {
    "start": 6735286,
    "end": 6739270,
    "text": "ドット・デフォルトのスタイルは、単一の個別トークンであることが判明した。"
  },
  {
    "start": 6739350,
    "end": 6741510,
    "text": "トークン1つにしては文字数が多い。"
  },
  {
    "start": 6741670,
    "end": 6745626,
    "text": "私の疑念は、このトークンひとつにあまりにも多くのことが詰め込まれていることだ。"
  },
  {
    "start": 6745738,
    "end": 6753810,
    "text": "このモデルは、このトークンのスペルに関するタスクはあまり得意ではないはずだ、というのが私の疑念だった。"
  },
  {
    "start": 6754150,
    "end": 6759006,
    "text": "デフォルト・スタイルという言葉にはlという文字が何文字あるのか？"
  },
  {
    "start": 6759198,
    "end": 6766494,
    "text": "もちろん、私のプロンプトは意図的にそのようにしている。"
  },
  {
    "start": 6766542,
    "end": 6767998,
    "text": "これがモデルが見ているものだ。"
  },
  {
    "start": 6768094,
    "end": 6771778,
    "text": "私の疑念では、これはあまり得意ではないだろうし、実際得意ではない。"
  },
  {
    "start": 6771864,
    "end": 6774066,
    "text": "Lがいくつあるかはわからない。"
  },
  {
    "start": 6774088,
    "end": 6778460,
    "text": "3つあると思っているが、実際には4つある。"
  },
  {
    "start": 6779150,
    "end": 6781114,
    "text": "それは非常にうまくいかなかった。"
  },
  {
    "start": 6781232,
    "end": 6785334,
    "text": "別の種類のキャラクターレベルのタスクを見てみよう。"
  },
  {
    "start": 6785462,
    "end": 6796254,
    "text": "例えば、ここでGPT4に文字列のデフォルトスタイルを逆にするように頼んだら、コードインタプリタを使おうとしたので、それを止めた。"
  },
  {
    "start": 6796372,
    "end": 6798778,
    "text": "それが私を混乱させた。"
  },
  {
    "start": 6798874,
    "end": 6804430,
    "text": "は、右から左へ行くこの文字列を反転させる方法を実は知らない。"
  },
  {
    "start": 6804580,
    "end": 6806258,
    "text": "それは間違った結果をもたらした。"
  },
  {
    "start": 6806424,
    "end": 6812206,
    "text": "トークン化のせいではないかという仮説を立てながら、別のアプローチを試みた。"
  },
  {
    "start": 6812238,
    "end": 6817134,
    "text": "よし、まったく同じストリングを逆にしてみよう。"
  },
  {
    "start": 6817262,
    "end": 6822982,
    "text": "ステップ1では、空白で区切られたすべての文字を印刷し、ステップ2として、そのリストを反転させる。"
  },
  {
    "start": 6823116,
    "end": 6832682,
    "text": "もう一度ツールを使おうとしたが、止めたところ、まずすべての文字が表示され、それは実際に正しかった。"
  },
  {
    "start": 6832816,
    "end": 6839958,
    "text": "でも、最初に順番にリストアップしていくと、どうにかできるんだ。"
  },
  {
    "start": 6840054,
    "end": 6845258,
    "text": "このように分割されると、個々のキャラクターがすべて登場することになる。"
  },
  {
    "start": 6845354,
    "end": 6851120,
    "text": "これで、個々のトークンを見て反転させ、プリントアウトするのがより簡単になった。"
  },
  {
    "start": 6851670,
    "end": 6853860,
    "text": "それはちょっと面白いね。"
  },
  {
    "start": 6854390,
    "end": 6856034,
    "text": "さあ、続けよう。"
  },
  {
    "start": 6856232,
    "end": 6860206,
    "text": "なぜllmsは英語以外の言語が苦手なのか？"
  },
  {
    "start": 6860318,
    "end": 6875910,
    "text": "これについてはすでに簡単に説明しましたが、基本的には、言語モデルがモデル・パラメーターのトレーニング中に英語以外のデータを目にする機会が少ないだけでなく、トークナイザーが英語以外のデータで十分にトレーニングされていないことが原因です。"
  },
  {
    "start": 6876060,
    "end": 6878234,
    "text": "例えば、「こんにちは、お元気ですか？"
  },
  {
    "start": 6878272,
    "end": 6882262,
    "text": "トークンは5個で、その訳は15個である。"
  },
  {
    "start": 6882326,
    "end": 6884666,
    "text": "これは3度の爆発だ。"
  },
  {
    "start": 6884848,
    "end": 6890726,
    "text": "例えば、アニョンハセヨは韓国語で \"こんにちは \"だから、結局3トークンになる。"
  },
  {
    "start": 6890758,
    "end": 6893706,
    "text": "実は、その言葉にはちょっと驚いているんだ。"
  },
  {
    "start": 6893818,
    "end": 6899578,
    "text": "典型的な挨拶は、\"こんにちは \"のようなもので、3トークンになってしまうが、私たちの \"こんにちは \"は1トークンだ。"
  },
  {
    "start": 6899674,
    "end": 6902218,
    "text": "だから、基本的にすべてがより肥大化し、拡散している。"
  },
  {
    "start": 6902234,
    "end": 6906930,
    "text": "これが、このモデルが他の言語でうまく機能しない理由のひとつだと思う。"
  },
  {
    "start": 6908070,
    "end": 6915720,
    "text": "話を戻すが、なぜLMは数字のトークン化に関係する単純計算が苦手なのだろうか？"
  },
  {
    "start": 6916410,
    "end": 6925442,
    "text": "例えば足し算は、文字レベルのアルゴリズムがあることに気づくだろう。"
  },
  {
    "start": 6925586,
    "end": 6929702,
    "text": "例えば、ここではまず1を足し、次に10、そして100を足す。"
  },
  {
    "start": 6929766,
    "end": 6940570,
    "text": "これらの数字の特定の部分を参照しなければならないが、これらの数字は、トークン化の過程でマージされたかされなかったかに基づいて、完全に任意に表現される。"
  },
  {
    "start": 6940720,
    "end": 6943518,
    "text": "このことについては、ブログの記事全体がとてもいいと思う。"
  },
  {
    "start": 6943604,
    "end": 6945338,
    "text": "整数のトークン化は正気の沙汰ではない。"
  },
  {
    "start": 6945434,
    "end": 6964638,
    "text": "この人物は基本的に、gpt-2だと思うが、数字のトークン化を系統的に調査しており、例えば4桁の数字について、1つのトークンなのか、2つのトークンなのか、つまりone threeなのかtwo twoなのか、あるいはthree oneの組み合わせなのかを調べることができることに気づいた。"
  },
  {
    "start": 6964734,
    "end": 6967570,
    "text": "つまり、異なる数字はすべて異なる組み合わせということになる。"
  },
  {
    "start": 6967650,
    "end": 6970294,
    "text": "これは完全に恣意的なものだと想像できるだろう。"
  },
  {
    "start": 6970412,
    "end": 6981902,
    "text": "このモデルでは、残念ながら、4桁すべてのトークンを見ることもあれば、3桁、2桁、1桁のトークンを見ることもある。"
  },
  {
    "start": 6981986,
    "end": 6985850,
    "text": "言語モデルにとって、これは間違いなく逆風だ。"
  },
  {
    "start": 6985920,
    "end": 6990758,
    "text": "でも、理想的とは言えない。"
  },
  {
    "start": 6990854,
    "end": 7003178,
    "text": "そのため、例えば、ラマ2アルゴリズムとI use sentence pieceを訓練する際に、ラマ2の例としてすべての桁を分割するようにしていることがわかった。"
  },
  {
    "start": 7003284,
    "end": 7007090,
    "text": "これは、単純計算のようなパフォーマンスを向上させるためでもある。"
  },
  {
    "start": 7007830,
    "end": 7011838,
    "text": "最後に、なぜGPT-2はPythonではあまり良くないのですか？"
  },
  {
    "start": 7011934,
    "end": 7017058,
    "text": "繰り返しになるが、これは部分的にはモデリングの問題であり、アーキテクチャーやデータセット、モデルの強さの問題である。"
  },
  {
    "start": 7017144,
    "end": 7029106,
    "text": "というのも、Pythonの単純な例で見たように、Pythonのスペースを扱うためのトークナイザーのエンコード効率は最悪で、スペースひとつひとつが個別のトークンだからだ。"
  },
  {
    "start": 7029218,
    "end": 7033082,
    "text": "これにより、モデルが横断的に参加できるコンテキストの長さが劇的に短くなる。"
  },
  {
    "start": 7033216,
    "end": 7039178,
    "text": "これはGPT-2のトークン化のバグのようなもので、後にGPT 4で修正されました。"
  },
  {
    "start": 7039344,
    "end": 7040998,
    "text": "さて、ここでもう1つ楽しい話をしよう。"
  },
  {
    "start": 7041104,
    "end": 7044766,
    "text": "私のLLMは、文字列の末尾を見ると突然停止する。"
  },
  {
    "start": 7044948,
    "end": 7047882,
    "text": "これは非常に奇妙な行動だ。"
  },
  {
    "start": 7048026,
    "end": 7053626,
    "text": "文字列の末尾に文字列をプリントしてください。"
  },
  {
    "start": 7053818,
    "end": 7056322,
    "text": "文末を教えろ、と言っているんだ。"
  },
  {
    "start": 7056376,
    "end": 7057890,
    "text": "問題があるようだ。"
  },
  {
    "start": 7057960,
    "end": 7065618,
    "text": "テキストの終わりが表示されないので、テキストの終わりが文字列であることを示すと、ここに文字列が表示される。"
  },
  {
    "start": 7065704,
    "end": 7069266,
    "text": "スペシャル・トークンの扱いに関して、明らかに何かが壊れている。"
  },
  {
    "start": 7069378,
    "end": 7087958,
    "text": "OpenAIがフードの下で何をしているのか、トークンを処理する特別なロジックを使わずに、テキストを個々の断片としてではなく、実際のトークンとしてパースしている可能性があるのか、実際のところはわかりません。"
  },
  {
    "start": 7088134,
    "end": 7098986,
    "text": "ということは、誰かがドット・エンコードを呼び出すときに、許可された特殊文字を渡して、ユーザー・プロンプトの特殊文字としてテキストの終わりを許可しているのかもしれない。"
  },
  {
    "start": 7099098,
    "end": 7103426,
    "text": "ユーザー・プロンプトは、もちろん、アタッカーがコントロールするテキストのようなものだ。"
  },
  {
    "start": 7103528,
    "end": 7112546,
    "text": "このような入力を解析したり、特別なトークンを使用したりしないことを望むだろうが、ここで何かが間違っているのは間違いないようだ。"
  },
  {
    "start": 7112648,
    "end": 7117810,
    "text": "そのため、これらの特別なトークンに関する知識は、潜在的には攻撃対象となる。"
  },
  {
    "start": 7117970,
    "end": 7125926,
    "text": "llmsを混乱させたいのであれば、特別なトークンを与えてみて、偶然に何かを壊していないかどうか見てみるといい。"
  },
  {
    "start": 7126108,
    "end": 7131802,
    "text": "さて、次は本当に楽しい問題で、末尾の空白の問題だ。"
  },
  {
    "start": 7131936,
    "end": 7138118,
    "text": "プレーグラウンドに来て、GPT3.5ターボのインストラクターになる。"
  },
  {
    "start": 7138214,
    "end": 7140682,
    "text": "これはチャットモデルではなく、コンプリートモデルだ。"
  },
  {
    "start": 7140736,
    "end": 7144590,
    "text": "よりベースモデルに近いと考えればいい。"
  },
  {
    "start": 7144660,
    "end": 7148110,
    "text": "完了すると、トークン・シーケンスを継続する。"
  },
  {
    "start": 7148450,
    "end": 7155342,
    "text": "これはアイスクリーム屋のキャッチフレーズで、私たちはこのシークエンスを続けたい。"
  },
  {
    "start": 7155486,
    "end": 7156994,
    "text": "オーケー、問題ない。"
  },
  {
    "start": 7157192,
    "end": 7162834,
    "text": "でも、ここでsubmitを押す代わりに、こうするんだ。"
  },
  {
    "start": 7162872,
    "end": 7164980,
    "text": "アイスクリームショップ「スペース」のキャッチフレーズだ。"
  },
  {
    "start": 7165670,
    "end": 7167126,
    "text": "ここにスペースがある。"
  },
  {
    "start": 7167228,
    "end": 7170962,
    "text": "送信をクリックする前に警告が出る。"
  },
  {
    "start": 7171106,
    "end": 7176930,
    "text": "APIがテキストをトークンに分割するため、パフォーマンスが低下します。"
  },
  {
    "start": 7177090,
    "end": 7178166,
    "text": "何が起こっているんだ？"
  },
  {
    "start": 7178188,
    "end": 7182380,
    "text": "それでもここで完成を見たが、何が起きているのか見てみよう。"
  },
  {
    "start": 7183630,
    "end": 7190170,
    "text": "これはアイスクリーム屋のキャッチフレーズだが、実際のトレーニングデータではどうなるだろうか？"
  },
  {
    "start": 7190240,
    "end": 7196126,
    "text": "例えば、インターネット上のどこかの訓練用文書に完成形があり、LLMがこのデータで訓練を行ったとしよう。"
  },
  {
    "start": 7196228,
    "end": 7201194,
    "text": "もしかしたら、「ああ、そうだ、これがキャッチフレーズなんだ、ひどいキャッチフレーズだ。"
  },
  {
    "start": 7201242,
    "end": 7210974,
    "text": "oを作成するとき、GPTではこれらのトークンの前に必ずスペースが入ることに注意してください。"
  },
  {
    "start": 7211102,
    "end": 7213950,
    "text": "それはOトークンではなく、スペースOトークンだ。"
  },
  {
    "start": 7214030,
    "end": 7220520,
    "text": "スペースは \"O \"の一部であり、これらを合わせてトークン8840、つまりスペース \"O \"となる。"
  },
  {
    "start": 7220970,
    "end": 7230758,
    "text": "ここで起こっているのは、このようにして次のトークンを完成させると、スペースOのトークンをサンプリングできるということだ。"
  },
  {
    "start": 7230934,
    "end": 7243266,
    "text": "この文字列をエンコードするときにやっていることは、基本的には、アイスクリーム・ショップのキャッチフレーズで、最後のスペースがトークン220になる。"
  },
  {
    "start": 7243268,
    "end": 7254770,
    "text": "というわけで、トークン220を追加しました。このトークンは、そうでなければキャッチフレーズの一部となります。"
  },
  {
    "start": 7255110,
    "end": 7261310,
    "text": "なぜなら、このスペースは次のトークンの一部だからだ。"
  },
  {
    "start": 7261390,
    "end": 7272610,
    "text": "このようにここに置くと、モデルは実際のスペースのデータをほとんど見たことがない。"
  },
  {
    "start": 7272690,
    "end": 7285322,
    "text": "問題なのは、最初のトークンを作り始めたのに、それが分裂してしまい、流通が途絶えてしまったことだ。"
  },
  {
    "start": 7285456,
    "end": 7287526,
    "text": "だから警告を受けるんだ。"
  },
  {
    "start": 7287638,
    "end": 7295162,
    "text": "ここでの根本的な問題は、もちろんLLMがこれらのトークンの上にあり、これらのトークンがテキストチャンクであるということだ。"
  },
  {
    "start": 7295226,
    "end": 7298062,
    "text": "彼らはあなたや私が考えるようなキャラクターではない。"
  },
  {
    "start": 7298196,
    "end": 7300782,
    "text": "これらはLLMが見ている原子のようなものだ。"
  },
  {
    "start": 7300836,
    "end": 7303010,
    "text": "そこから変なものがたくさん出てくる。"
  },
  {
    "start": 7303080,
    "end": 7306542,
    "text": "デフォルトのセル・スタイルに戻りましょう。"
  },
  {
    "start": 7306686,
    "end": 7314740,
    "text": "このモデルは、トレーニングセットの中で、\"le \"が入っていないデフォルトのセルを見たことがないに違いない。"
  },
  {
    "start": 7315110,
    "end": 7320520,
    "text": "これはある種の機能なので、常にひとつのグループとして見られている。"
  },
  {
    "start": 7321610,
    "end": 7323078,
    "text": "これが何の一部なのか、実は知らないんだ。"
  },
  {
    "start": 7323084,
    "end": 7324082,
    "text": "これはある種のAPIだ。"
  },
  {
    "start": 7324146,
    "end": 7332102,
    "text": "このようなトークンの組み合わせは、トレーニング・データでは見たことがないはずだ。"
  },
  {
    "start": 7332166,
    "end": 7339718,
    "text": "これをコピーしてここに貼り付け、完成させようとしたら、すぐに大きなエラーが出た。"
  },
  {
    "start": 7339814,
    "end": 7343914,
    "text": "そのモデルは、あなたが解決しようとしているストップシーケンスで始まる完了を予測し、何も出力しないと書いてあった。"
  },
  {
    "start": 7343962,
    "end": 7346122,
    "text": "プロンプト・シークエンスやストップ・シークエンスの調整を検討する。"
  },
  {
    "start": 7346266,
    "end": 7354062,
    "text": "submitをクリックすると、すぐにモデルがテキスト終了トークンのようなものを出した。"
  },
  {
    "start": 7354196,
    "end": 7358414,
    "text": "基本的に停止シーケンスが即座に予測されるため、完成度は皆無だった。"
  },
  {
    "start": 7358542,
    "end": 7367646,
    "text": "というわけで、また警告が出たわけです。データ分布から外れていて、モデルがまったく恣意的なことを予測しているからです。"
  },
  {
    "start": 7367688,
    "end": 7368626,
    "text": "本当に混乱しているんだ。"
  },
  {
    "start": 7368658,
    "end": 7371026,
    "text": "基本的に、これは脳にダメージを与えている。"
  },
  {
    "start": 7371058,
    "end": 7372054,
    "text": "こんなことは初めてだ。"
  },
  {
    "start": 7372092,
    "end": 7375190,
    "text": "それはショックで、テキストの終わりか何かを予測している。"
  },
  {
    "start": 7375340,
    "end": 7378666,
    "text": "ここでもう一度試してみたが、この場合は完成した。"
  },
  {
    "start": 7378768,
    "end": 7382678,
    "text": "その場合、このリクエストは何らかの理由で当社の利用ポリシーに違反する可能性があります。"
  },
  {
    "start": 7382774,
    "end": 7384010,
    "text": "これにはフラグが立った。"
  },
  {
    "start": 7385470,
    "end": 7387114,
    "text": "基本的に何かがうまくいかない。"
  },
  {
    "start": 7387152,
    "end": 7395166,
    "text": "ジャンクのようなものもあります。モデルがこのようなことに非常に不満で、トレーニングセットで発生したことがないため、それを完了させる方法を知らないので、ジャンクを感じることができます。"
  },
  {
    "start": 7395268,
    "end": 7399390,
    "text": "トレーニングセットでは常にこのように表示され、1つのトークンになる。"
  },
  {
    "start": 7399890,
    "end": 7411026,
    "text": "トークンが次のトークンの最初の文字を完成させるようなものであったり、長いトークンで一部の文字が欠けていたりする。"
  },
  {
    "start": 7411208,
    "end": 7416598,
    "text": "これらはすべて、部分的なトークンに関する問題のようなものだ。"
  },
  {
    "start": 7416764,
    "end": 7431270,
    "text": "実際にtuktokenのリポジトリを調べ、錆のコードに行き、unstableを検索すれば、unstableなネイティブ・トークンのエンコードや、多くの特殊なケースの処理を見ることができる。"
  },
  {
    "start": 7431430,
    "end": 7442154,
    "text": "不安定なトークンに関するこのようなことはどこにも文書化されていないが、不安定なトークンを扱うコードは山ほどあり、不安定なトークンはまさに私がここで説明しているようなものだ。"
  },
  {
    "start": 7442272,
    "end": 7446314,
    "text": "補完APIに求めるものは、もっともっと派手なものだ。"
  },
  {
    "start": 7446362,
    "end": 7452622,
    "text": "例えば、デフォルトのセル・スターを入れる場合、次のトークン・シーケンスを要求しているとしても、実際には次のトークンを追加しようとしているわけではない。"
  },
  {
    "start": 7452686,
    "end": 7453010,
    "text": "その通りだ。"
  },
  {
    "start": 7453080,
    "end": 7478438,
    "text": "このリストの後、トークンをたくさん検討しようとしています。トークンを再認識した場合、高い確率で再認識されるであろう文字を検索しようとしているのだと思います。"
  },
  {
    "start": 7478604,
    "end": 7482730,
    "text": "これを説明するのは非常に難しい。"
  },
  {
    "start": 7482800,
    "end": 7487702,
    "text": "結局は非常に厄介で毛むくじゃらの種類のトピックで、根本的にはトークン化に由来する。"
  },
  {
    "start": 7487766,
    "end": 7492798,
    "text": "将来的には、不安定なトークンについて話すビデオも作れるかもしれない。"
  },
  {
    "start": 7492964,
    "end": 7493294,
    "text": "いいかい？"
  },
  {
    "start": 7493332,
    "end": 7495166,
    "text": "本当に最高のものを最後まで取っておくよ。"
  },
  {
    "start": 7495268,
    "end": 7498750,
    "text": "私の一番のお気に入りはソリッドゴールドのマジックシャープ。"
  },
  {
    "start": 7501010,
    "end": 7509710,
    "text": "これはこのブログの記事、ソリッドゴールド・マジックハープから来ている。"
  },
  {
    "start": 7509870,
    "end": 7513426,
    "text": "基本的には、このブログ記事の全文を読むことをお勧めする。"
  },
  {
    "start": 7513528,
    "end": 7525190,
    "text": "基本的に、この人がやっていたのは、トークンの埋め込みを安定させ、その埋め込み表現に基づいてトークンをクラスタリングすることだ。"
  },
  {
    "start": 7525610,
    "end": 7530310,
    "text": "この人は、本当に奇妙に見えるトークンの集まりがあることに気づいた。"
  },
  {
    "start": 7530390,
    "end": 7535610,
    "text": "ロット・イーストリームの名声、ソリッド・ゴールド・マジカルプのシグネット・メッセージにクラスターがある。"
  },
  {
    "start": 7535680,
    "end": 7541254,
    "text": "このエンベッディングクラスターでは、基本的に本当に奇妙なトークンだ。"
  },
  {
    "start": 7541382,
    "end": 7543726,
    "text": "では、このトークンはどこから来たのか？"
  },
  {
    "start": 7543748,
    "end": 7545178,
    "text": "ソリッド・ゴールド・マジック・カープって何？"
  },
  {
    "start": 7545194,
    "end": 7546238,
    "text": "意味がない。"
  },
  {
    "start": 7546404,
    "end": 7562638,
    "text": "というのも、これらのトークンについてモデルに尋ねると、筋書きがより濃くなることに気づいたからだ。"
  },
  {
    "start": 7562814,
    "end": 7567298,
    "text": "そうすると、基本的に完全に壊れたLLMの振る舞いがいろいろ出てくる。"
  },
  {
    "start": 7567394,
    "end": 7573590,
    "text": "申し訳ないが聞こえない」と回避されるか、「幻覚が見える」と言われるかのどちらかだ。"
  },
  {
    "start": 7574170,
    "end": 7576130,
    "text": "侮辱のように仕返しすることもできる。"
  },
  {
    "start": 7576210,
    "end": 7582410,
    "text": "Streamerのボットについて尋ねると、そのモデルはただあなたを罵倒するだけだ。"
  },
  {
    "start": 7582990,
    "end": 7585206,
    "text": "あるいは、奇妙なユーモアを思いつく。"
  },
  {
    "start": 7585318,
    "end": 7592006,
    "text": "ロート製薬やソルゴー・マジックアプのような非常にシンプルなストリングスについて質問することは、実際にモデルを壊している。"
  },
  {
    "start": 7592118,
    "end": 7593422,
    "text": "一体何が起こっているんだ？"
  },
  {
    "start": 7593476,
    "end": 7596350,
    "text": "ここにはさまざまな行動が記録されている。"
  },
  {
    "start": 7596770,
    "end": 7600938,
    "text": "販売されているゴールドマジックアープだけでなく、そのような動作をするトークンはたくさんある。"
  },
  {
    "start": 7601114,
    "end": 7603646,
    "text": "だから、基本的にはトリガーとなる言葉がたくさんある。"
  },
  {
    "start": 7603748,
    "end": 7619350,
    "text": "これらのトリガー・ワードについてモデルに尋ねたり、プロンプトに盛り込んだりすると、モデルはおかしくなり、典型的な安全ガイドラインに違反するようなものや、悪態をついているようなモデルのアライメントなど、実に奇妙な行動をとるようになる。"
  },
  {
    "start": 7619500,
    "end": 7621030,
    "text": "ここで何が起こっているのか？"
  },
  {
    "start": 7621100,
    "end": 7622918,
    "text": "どうしてそんなことが言えるのか？"
  },
  {
    "start": 7623084,
    "end": 7625810,
    "text": "まあ、これもトークン化ということになる。"
  },
  {
    "start": 7625970,
    "end": 7631366,
    "text": "ここで起こっているのは、ゴールドマジックロープがRedditのユーザーであるということだ。"
  },
  {
    "start": 7631478,
    "end": 7634330,
    "text": "ソルゴールドマジックカープがある。"
  },
  {
    "start": 7634910,
    "end": 7649342,
    "text": "おそらくここで起こったことは、まだ明確に調査されたわけではないが、トークン化のデータセットが実際の言語モデルのトレーニングデータセットと大きく異なっていたことだと思われる。"
  },
  {
    "start": 7649476,
    "end": 7657038,
    "text": "トークン化されたデータセットの中に、SaltGold Magikarpというユーザーが本文中で言及されている可能性のあるRedditのデータが大量にあった。"
  },
  {
    "start": 7657204,
    "end": 7662978,
    "text": "ソルトゴールド・マギカープは、よく投稿するタイプの人間だったからだ。"
  },
  {
    "start": 7663144,
    "end": 7666690,
    "text": "これは、トークン化データセットの中で何度も出現する文字列である。"
  },
  {
    "start": 7666840,
    "end": 7669910,
    "text": "トークン化のデータセットに何度も出てくるからだ。"
  },
  {
    "start": 7669980,
    "end": 7676226,
    "text": "これらのトークンは最終的に、ゴールド・マジックコープを販売した1人のRedditユーザーのために、1つのトークンに統合されることになる。"
  },
  {
    "start": 7676258,
    "end": 7683830,
    "text": "GPT-2の5万トークンだったかな、そのRedditユーザー専用のトークンを持っている。"
  },
  {
    "start": 7683990,
    "end": 7688374,
    "text": "その場合、トークン化データセットにはこれらの文字列が含まれることになる。"
  },
  {
    "start": 7688502,
    "end": 7695680,
    "text": "その後、モデルを訓練するとき、つまり言語モデル自体を訓練するとき、Redditからのこのデータは存在しない。"
  },
  {
    "start": 7696130,
    "end": 7702202,
    "text": "そのため、言語モデルのトレーニングセット全体において、ゴールドマジッャープが販売されることはない。"
  },
  {
    "start": 7702266,
    "end": 7706820,
    "text": "そのトークンは、後で実際の言語モデルのトレーニングセットに現れることはない。"
  },
  {
    "start": 7707350,
    "end": 7709646,
    "text": "このトークンは決してアクティブにならない。"
  },
  {
    "start": 7709758,
    "end": 7712430,
    "text": "最適化の最初にランダムに初期化される。"
  },
  {
    "start": 7712590,
    "end": 7716734,
    "text": "そして、前方から後方へのパスとモデルの更新が行われるが、このトークンは更新されることはない。"
  },
  {
    "start": 7716782,
    "end": 7723538,
    "text": "エンベッディングテーブルでは、その行ベクトルはサンプリングされることもなく、使われることもないので、トレーニングされることもなく、完全に未トレーニングなのだ。"
  },
  {
    "start": 7723634,
    "end": 7728966,
    "text": "c言語などで書かれた典型的なバイナリプログラムにおける未割り当てメモリのようなものだ。"
  },
  {
    "start": 7729068,
    "end": 7730562,
    "text": "それは未割り当てのメモリである。"
  },
  {
    "start": 7730626,
    "end": 7740534,
    "text": "テスト時にこのトークンを呼び出すと、基本的に、まったく訓練されていないエンベッディング・テーブルの行を抜き出すことになり、それがトランスフォーマーに入力され、未定義の動作を生み出すことになる。"
  },
  {
    "start": 7740662,
    "end": 7745338,
    "text": "このように、まったく未定義の、これまでにないトレーニング動作が見られるのだ。"
  },
  {
    "start": 7745514,
    "end": 7755550,
    "text": "つまり、このような奇妙なトークンのようなものが、このような振る舞いを引き起こすのだ。"
  },
  {
    "start": 7755890,
    "end": 7756254,
    "text": "オーケー。"
  },
  {
    "start": 7756292,
    "end": 7771878,
    "text": "最後に、多くの人が気づいていると思いますが、GPTトークナイザーや他のLLAMのトークナイザーでは、異なる種類のフォーマットや異なる表現、異なる言語などが、より効率的であったり、より効率的でなかったりする可能性があるということを、簡単に述べておきたいと思います。"
  },
  {
    "start": 7771964,
    "end": 7778230,
    "text": "例えば、JSonはトークンの密度が高く、YAmLはトークンの効率が高い。"
  },
  {
    "start": 7778810,
    "end": 7787386,
    "text": "例えば、JSonではこれらは同じで、YAMLではJSONは116でYAMLは99です。"
  },
  {
    "start": 7787488,
    "end": 7789142,
    "text": "かなり改善されている。"
  },
  {
    "start": 7789286,
    "end": 7802606,
    "text": "トークン・エコノミーでは、トークン単位でさまざまな支払いをしており、コンテキストの長さで支払いをしている。"
  },
  {
    "start": 7802788,
    "end": 7805130,
    "text": "Jsonsよりもyamlsを使う方がいい。"
  },
  {
    "start": 7805210,
    "end": 7820326,
    "text": "一般的に、トークン化の密度は、常に気にかけ、心配しなければならないものです。効率的なエンコード方式を見つけ、チック・トークナイザーに多くの時間を費やし、さまざまなフォーマットや設定などによるトークン効率を測定しようとします。"
  },
  {
    "start": 7820428,
    "end": 7824066,
    "text": "さて、これで私のトークン化に関するかなり長いビデオは終わりです。"
  },
  {
    "start": 7824258,
    "end": 7828018,
    "text": "乾燥しているのはわかるし、イライラするのもわかる。"
  },
  {
    "start": 7828114,
    "end": 7830518,
    "text": "個人的には、この舞台は本当に嫌いだ。"
  },
  {
    "start": 7830694,
    "end": 7833594,
    "text": "この時点で私が言わなければならないことは、それを払い除けることはないということだ。"
  },
  {
    "start": 7833712,
    "end": 7842842,
    "text": "言語モデルに未割り当てのメモリーを差し込むのを見たように、ここには多くのフットガン、鋭いエッジ、セキュリティ問題、AIの安全性の問題がある。"
  },
  {
    "start": 7842986,
    "end": 7846110,
    "text": "この段階を理解する価値はある。"
  },
  {
    "start": 7847250,
    "end": 7851166,
    "text": "とはいえ、それを取り除くことができた者には永遠の栄光がもたらされることは間違いない。"
  },
  {
    "start": 7851268,
    "end": 7857730,
    "text": "私は、それを試みた可能性のある論文をひとつお見せした。"
  },
  {
    "start": 7857880,
    "end": 7872322,
    "text": "GPT、4つのトークン、そして語彙をアプリケーションで再利用できるのであれば、それは検討すべきことです。"
  },
  {
    "start": 7872466,
    "end": 7876630,
    "text": "tictokenやOpenAIが使っているバイトレベルのBPEもとても気に入っている。"
  },
  {
    "start": 7877290,
    "end": 7885580,
    "text": "もし、何らかの理由で自分の語彙力をゼロから鍛えたいのであれば、BPE with sentence pieceを使うだろう。"
  },
  {
    "start": 7887070,
    "end": 7889370,
    "text": "前述したように、私はセンテンスピースの大ファンではない。"
  },
  {
    "start": 7889440,
    "end": 7896094,
    "text": "私はそのバイトフォールバックが好きではないし、ユニコードのコードポイントでBPEをしているのも好きではない。"
  },
  {
    "start": 7896212,
    "end": 7907778,
    "text": "また、100万通りもの設定項目があり、フットカンがたくさんあるため、誤操作しやすく、理解しきれないパラメーターのせいで文章をトリミングしてしまったりすることもあると思います。"
  },
  {
    "start": 7907944,
    "end": 7920200,
    "text": "設定には細心の注意を払い、メタがやったことを正確にコピーペーストしてみるか、基本的にはすべてのハイパーパラメータを見ることに多くの時間を費やし、文のコードを調べて、これが正しいことを確認する。"
  },
  {
    "start": 7921210,
    "end": 7926920,
    "text": "たとえすべての設定が正しかったとしても、ここで起きていることに比べれば、アルゴリズムは劣っていると思う。"
  },
  {
    "start": 7927290,
    "end": 7935500,
    "text": "本当に語彙力を鍛える必要があるのなら、可能な限り効率的になるのを待つのがベストなのかもしれない。"
  },
  {
    "start": 7935870,
    "end": 7939098,
    "text": "というのは、多分、私が取り組みたいと思っていることだ。"
  },
  {
    "start": 7939264,
    "end": 7942174,
    "text": "いずれ、基本的なトレーニングができるようになるだろう。"
  },
  {
    "start": 7942372,
    "end": 7945630,
    "text": "私たちが求めているのは、ティック・トークンとトレーニング・コードなのだ。"
  },
  {
    "start": 7945780,
    "end": 7954350,
    "text": "それは現在存在しない理想的なもので、MImBPはその実装に取り組んでいる。"
  },
  {
    "start": 7954690,
    "end": 7957806,
    "text": "これがトークン化について私が言わなければならないことだ。"
  },
  {
    "start": 7957918,
    "end": 7964950,
    "text": "今後、さらにドライでさらに詳細な上級者向けのビデオが出るかもしれないが、今はこの辺で。"
  },
  {
    "start": 7965020,
    "end": 7967714,
    "text": "参考になっただろうか。"
  },
  {
    "start": 7967842,
    "end": 7968630,
    "text": "さようなら。"
  },
  {
    "start": 7973970,
    "end": 7984480,
    "text": "彼らはこのコンテキストサイズをGPT 1の512から次のGPT 4の2の1024に増やした。"
  },
  {
    "start": 7986230,
    "end": 7992370,
    "text": "さて、次にOpenAIのGPT-2エンコーダーPYのコードを簡単に説明したいと思います。"
  },
  {
    "start": 7996710,
    "end": 7998450,
    "text": "ごめん、くしゃみが出そうだ。"
  },
  {
    "start": 7998950,
    "end": 8005900,
    "text": "ということは、ここで起きているのは偽レイヤーということになる。"
  },
  {
    "start": 8006990,
    "end": 8008200,
    "text": "ここで起きているのは"
  }
]