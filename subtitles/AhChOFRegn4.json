[
  {
    "start": 5290,
    "end": 13390,
    "text": "トランスフォーマー・モデルをインスタンス化する方法 このビデオでは、トランスフォーマー・ライブラリーからモデルを作成して使用する方法を見ていきます。"
  },
  {
    "start": 14050,
    "end": 21790,
    "text": "前に見たように、自動モデルクラスは、インギングフェーズハブ上の任意のチェックポイントから事前学習されたモデルをインスタンス化することができます。"
  },
  {
    "start": 22130,
    "end": 29800,
    "text": "ライブラリから適切なモデルクラスを選んで適切なアーキテクチャをインスタンス化し、事前に学習したモデルの重みを内部にロードする。"
  },
  {
    "start": 30490,
    "end": 49366,
    "text": "見てわかるように、鳥のチェックポイントが与えられると、誕生モデルができあがる。GPT-2や舞台裏の一部でも同様だが、このAPIはハブ上のチェックポイントの名前を受け取ることができ、その場合、設定ファイルとモデルの重みファイルをダウンロードしてキャッシュする。"
  },
  {
    "start": 49558,
    "end": 56670,
    "text": "また、有効な設定ファイルとモデル廃棄ファイルを含むローカルフォルダへのパスを指定することもできます。"
  },
  {
    "start": 57330,
    "end": 65460,
    "text": "再トレーニングされたモデルをインスタンス化するために、オートモデル API はまずコンフィギュレーションファイルを開き、使用すべきコンフィギュレーションクラスを調べます。"
  },
  {
    "start": 66070,
    "end": 71666,
    "text": "コンフィギュレーション・クラスは、モデルのタイプ、バート、GPT-2、パートによって異なります。"
  },
  {
    "start": 71768,
    "end": 72690,
    "text": "例えば、こうだ。"
  },
  {
    "start": 73510,
    "end": 81240,
    "text": "適切なコンフィギュレーション・クラスがあれば、そのコンフィギュレーションをインスタンス化することができる。"
  },
  {
    "start": 81690,
    "end": 89800,
    "text": "また、このコンフィギュレーションクラスを使って適切なモデルクラスを見つけ、それをロードされたコンフィギュレーションと組み合わせてモデルをロードします。"
  },
  {
    "start": 90830,
    "end": 96170,
    "text": "このモデルはランダムな重みで初期化されただけなので、まだ事前学習されたモデルではない。"
  },
  {
    "start": 96510,
    "end": 101020,
    "text": "最後のステップは、このモデルの中にモデルファイルから重みをロードすることである。"
  },
  {
    "start": 102130,
    "end": 110320,
    "text": "任意のチェックポイントや設定ファイルを含むフォルダからモデルの設定を簡単にロードするには、auto configクラスを使用します。"
  },
  {
    "start": 111010,
    "end": 115810,
    "text": "自動モデルクラスのように、ライブラリから適切なコンフィギュレーションクラスを選びます。"
  },
  {
    "start": 116790,
    "end": 124770,
    "text": "チェックポイントに対応する特定のクラスを使うこともできるが、異なるモデル・アーキテクチャを試したい場合は、その都度コードを変更する必要がある。"
  },
  {
    "start": 125750,
    "end": 133270,
    "text": "先に述べたように、モデルのコンフィギュレーションは、モデル・アーキテクチャを作成するために必要なすべての情報を含む青写真です。"
  },
  {
    "start": 133610,
    "end": 146780,
    "text": "例えば、BeRtに基づくケース・チェックポイントに関連するバート・モデルは12層、隠れサイズは768、語彙サイズは28 996である。"
  },
  {
    "start": 147790,
    "end": 154670,
    "text": "設定ができたら、チェックポイントと同じアーキテクチャで、ランダムに初期化されるモデルを作ることができる。"
  },
  {
    "start": 155010,
    "end": 156634,
    "text": "そうすれば、ゼロからトレーニングすることができる。"
  },
  {
    "start": 156682,
    "end": 163706,
    "text": "他のPytorchモデルと同じように、キーワード引数を使用して、コンフィギュレーションの任意の部分を変更することもできます。"
  },
  {
    "start": 163898,
    "end": 170560,
    "text": "2つ目のコードの断片は、ランダムに初期化された出生モデルを、12層ではなく10層でインスタンス化している。"
  },
  {
    "start": 171650,
    "end": 175030,
    "text": "一度トレーニングまたは微調整したモデルを保存するのはとても簡単だ。"
  },
  {
    "start": 175220,
    "end": 177750,
    "text": "安全なプレトレーニングの方法を使うだけだ。"
  },
  {
    "start": 178410,
    "end": 184710,
    "text": "ここで、モデルは現在の作業ディレクトリ内のMybelt modelというフォルダに保存されます。"
  },
  {
    "start": 185210,
    "end": 188790,
    "text": "このようなモデルは、事前訓練されたモデルから再ロードすることができる。"
  },
  {
    "start": 189210,
    "end": 193140,
    "text": "このモデルを簡単にハブにアップロードする方法については、プッシュ・トゥ・アプリのビデオをご覧ください。"
  }
]