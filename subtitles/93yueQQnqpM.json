[
  {
    "start": 170,
    "end": 6154,
    "text": "さて、このビデオでは、リャマ2700億のモデルを使って、検索QAのフォローアップを行います。"
  },
  {
    "start": 6282,
    "end": 10814,
    "text": "これは私がここで使っている基本的なチャットモデルだ。"
  },
  {
    "start": 10932,
    "end": 14654,
    "text": "ここではインストラクター用に微調整されている。"
  },
  {
    "start": 14692,
    "end": 20782,
    "text": "ここでは、前回のビデオでセットアップ方法を紹介したtogether computeを使っているのがわかるだろう。"
  },
  {
    "start": 20836,
    "end": 29942,
    "text": "APIを使おうが、ボイドGPUを使って自分でロードしようが、モデルの使い方に違いはないだろう？"
  },
  {
    "start": 29996,
    "end": 34470,
    "text": "私が設定した方法は、ここではtogether APIを使っている。"
  },
  {
    "start": 34970,
    "end": 41322,
    "text": "ここでラングチェーンLLMを作らなければならないが、基本的にはそこで設定しただけのことだ。"
  },
  {
    "start": 41456,
    "end": 47082,
    "text": "それでは、いろいろな書類が入ったzipファイルを持ってきます。"
  },
  {
    "start": 47136,
    "end": 58190,
    "text": "フラッシュ・アテンション・ペーパー、ラマ・ツー・ペーパー・ツール・フォーマー、リアクト・ペーパー、そしておそらく他の2、3のペーパー、さらにその中にllmsのペーパーを補強する。"
  },
  {
    "start": 58260,
    "end": 61390,
    "text": "ここでのアイデアは、基本的にこれらを持ち込むということだ。"
  },
  {
    "start": 61460,
    "end": 65738,
    "text": "複数のpdfファイルを用意して、Chromadbに貼り付けます。"
  },
  {
    "start": 65834,
    "end": 67698,
    "text": "クラウドに置く必要はない。"
  },
  {
    "start": 67784,
    "end": 70194,
    "text": "そんなことをする必要はない。"
  },
  {
    "start": 70312,
    "end": 72994,
    "text": "我々が使っているのは完全にオープンソースだ。"
  },
  {
    "start": 73032,
    "end": 74674,
    "text": "我々が使うことを許されたモデルだ。"
  },
  {
    "start": 74712,
    "end": 76542,
    "text": "地元ではクロマを使っている。"
  },
  {
    "start": 76686,
    "end": 87862,
    "text": "クラウドからpingを送信しているという点で、厳密にはローカルモデルではないのでしょうが、十分なGPUがあればローカルで実行することもできます。"
  },
  {
    "start": 87996,
    "end": 91474,
    "text": "それなら、基本的にはラングチェーンに持ち込むだけだ。"
  },
  {
    "start": 91522,
    "end": 92722,
    "text": "クロマを持ち込んでいる。"
  },
  {
    "start": 92786,
    "end": 95066,
    "text": "私はこのためにディレクトリローダーを持ち込んでいる。"
  },
  {
    "start": 95168,
    "end": 98518,
    "text": "ここで使っているエンベッディングは、インストラクターのエンベッディングだ。"
  },
  {
    "start": 98614,
    "end": 104494,
    "text": "最近、これらのエンベッディングよりも良いスコアを獲得していると思われる新しいエンベッディングがいくつか登場した。"
  },
  {
    "start": 104532,
    "end": 106766,
    "text": "そのいくつかをビデオで紹介するつもりだ。"
  },
  {
    "start": 106868,
    "end": 113386,
    "text": "インストラクターのエンベッディングは、本当にしっかりとしたエンベッディングを目指すには本当に良い方法だと今でも思っている。"
  },
  {
    "start": 113418,
    "end": 114766,
    "text": "これを見ればわかるだろう。"
  },
  {
    "start": 114868,
    "end": 120290,
    "text": "質問と適切な文脈を見つけることに関しても、本当にうまく機能しているようだ。"
  },
  {
    "start": 120440,
    "end": 124014,
    "text": "あとは基本的にPDfファイルをインジェストするだけだ。"
  },
  {
    "start": 124062,
    "end": 125266,
    "text": "それがここでやっていることだ。"
  },
  {
    "start": 125368,
    "end": 128566,
    "text": "私たちは基本的にPYPDFローダーを使っています。"
  },
  {
    "start": 128668,
    "end": 132034,
    "text": "ページ、エトセトラ、チャンクに分けよう。"
  },
  {
    "start": 132082,
    "end": 134822,
    "text": "それなら、私たちは今、この書類を持っている。"
  },
  {
    "start": 134956,
    "end": 138066,
    "text": "さて、ドキュメントをキャラクタ・スプリッタで分割する。"
  },
  {
    "start": 138098,
    "end": 141286,
    "text": "私はキャラクター・スプリッターのファンではない。"
  },
  {
    "start": 141308,
    "end": 146282,
    "text": "テキストを分割するには、このような方法よりももっと知的な方法がたくさんあると思う。"
  },
  {
    "start": 146416,
    "end": 149082,
    "text": "しかし、おかしなことに、それはまだほとんど機能している。"
  },
  {
    "start": 149216,
    "end": 161280,
    "text": "2つのテキストの塊の間に1つのアイデアがある場合、それを1つの塊にまとめることができるように、塊の重なりを持たせるのが好きなんだ。"
  },
  {
    "start": 161810,
    "end": 165742,
    "text": "ということは、基本的には、このような抱き合う顔のインストラクターの埋め込みを設定するだけだ。"
  },
  {
    "start": 165806,
    "end": 170354,
    "text": "これは香港大学のNLPチームによるエンベッディングである。"
  },
  {
    "start": 170472,
    "end": 172498,
    "text": "ここではインストラクターのXLを使っている。"
  },
  {
    "start": 172584,
    "end": 180454,
    "text": "実際にこれを実行すると、T fourのGPUでは少し遅くなる。"
  },
  {
    "start": 180652,
    "end": 194390,
    "text": "エンベッディング、エンベッディング・メイカー、またはエンベッディング・モデルがロードされたら、基本的には、これらのインストラクターのエンベッディングを使って、クロマDBを作成します。"
  },
  {
    "start": 194470,
    "end": 201878,
    "text": "というのも、200の文書を調べなければならず、現在は1000文字の塊に分割されているからだ。"
  },
  {
    "start": 201974,
    "end": 205742,
    "text": "そのひとつひとつをこの中に埋め込んでいく。"
  },
  {
    "start": 205876,
    "end": 214350,
    "text": "これを実行すると、このドライブにデータベースがあるのがわかるだろう。"
  },
  {
    "start": 214500,
    "end": 217326,
    "text": "次はレトリーバーを作る。"
  },
  {
    "start": 217438,
    "end": 220770,
    "text": "ここではベクターストア・レトリーバーを使うだけだ。"
  },
  {
    "start": 220920,
    "end": 229640,
    "text": "検索引数をk=5とする。つまり、ここから5つのコンテキストを返すということだ。"
  },
  {
    "start": 230090,
    "end": 237862,
    "text": "どのPDfファイルから来たかを示す小さな引用文も、この中に入れよう。"
  },
  {
    "start": 237916,
    "end": 240234,
    "text": "それは多くの人が知りたいことだと思う。"
  },
  {
    "start": 240352,
    "end": 245306,
    "text": "さて、この答えは、そこにある情報の塊のどこから出てきたのだろう。"
  },
  {
    "start": 245408,
    "end": 255322,
    "text": "だから、具体的に引用とかそういうものを表示するわけではないのですが、ここでは、ああ、これはこのPDFから、あるいはこれらのPDFの組み合わせから来たものだ、ということを表示しています。"
  },
  {
    "start": 255386,
    "end": 261166,
    "text": "よし、じゃあ基本的に、このラマ2億7000万チャットモデルをインスタンス化しよう。"
  },
  {
    "start": 261348,
    "end": 264558,
    "text": "温度は0.1に設定してある。"
  },
  {
    "start": 264644,
    "end": 268130,
    "text": "ここでは最大トークンを1024に設定している。"
  },
  {
    "start": 268280,
    "end": 271874,
    "text": "あとは基本的に、ここで検索チェーンを組み立てるだけだ。"
  },
  {
    "start": 271912,
    "end": 274930,
    "text": "LLMでは基本的に合格です。"
  },
  {
    "start": 275350,
    "end": 277266,
    "text": "何でも詰め込むんだ。"
  },
  {
    "start": 277368,
    "end": 283334,
    "text": "llama2の4096コンテクスト・ウィンドウに収まるなら、何の問題もないはずだ。"
  },
  {
    "start": 283372,
    "end": 284822,
    "text": "私たちはただ物を使うだけだ。"
  },
  {
    "start": 284956,
    "end": 296860,
    "text": "レトリーバーはクロマDBで、基本的には、そこで得た実際の答えから5つのコンテキストを返す。"
  },
  {
    "start": 297230,
    "end": 302362,
    "text": "それから、ちょっとしたヘルパー関数をいくつか用意した。"
  },
  {
    "start": 302416,
    "end": 305134,
    "text": "ここではソース・ドキュメントをtrueと等しい値で返している。"
  },
  {
    "start": 305252,
    "end": 307790,
    "text": "僕たちは基本的にそれを使ってトレーニングしているんだ。"
  },
  {
    "start": 307860,
    "end": 310350,
    "text": "PDFはどこから来たんですか？"
  },
  {
    "start": 310420,
    "end": 314330,
    "text": "それが済んだら、実際にこれを簡単にやってみよう。"
  },
  {
    "start": 314410,
    "end": 316466,
    "text": "見てわかるように、私はただ尋ねることから始めるんだ。"
  },
  {
    "start": 316488,
    "end": 318050,
    "text": "さて、フラッシュ・アテンションとは何だろう？"
  },
  {
    "start": 318630,
    "end": 324530,
    "text": "これは実際にはフラッシュ・アテンション1の論文であり、ここにある新しいフラッシュ・アテンション2の論文ではない。"
  },
  {
    "start": 324680,
    "end": 325714,
    "text": "そうだろう。"
  },
  {
    "start": 325752,
    "end": 326882,
    "text": "よし、これでいい。"
  },
  {
    "start": 326936,
    "end": 327902,
    "text": "フラッシュ・アテンションとは何か？"
  },
  {
    "start": 327966,
    "end": 332834,
    "text": "フラッシュアテンションは、より少ないメモリアクセスで正確なアテンションを計算する新しいアテンションアルゴリズムである。"
  },
  {
    "start": 332882,
    "end": 335590,
    "text": "それは私たちにたくさんの情報を与えてくれる。"
  },
  {
    "start": 335660,
    "end": 337202,
    "text": "情報源も教えてくれる。"
  },
  {
    "start": 337266,
    "end": 345398,
    "text": "この例では、kを5とし、5人すべてのコンタクトがフラッシュ・アテンションPDFから戻ってきた。"
  },
  {
    "start": 345494,
    "end": 357242,
    "text": "これは、私たちの埋め込みが、この質問を取り込み、埋め込み、そして私たちの彩度データベースで最も類似した例を探すことから、非常にうまく調べていることを示しています。"
  },
  {
    "start": 357306,
    "end": 359502,
    "text": "そこで、私たちはこうしているのを見ることができる。"
  },
  {
    "start": 359556,
    "end": 363774,
    "text": "ここで、フラッシュ・アテンションの意味するところについての質問がある。"
  },
  {
    "start": 363812,
    "end": 365902,
    "text": "基本的にはそれも可能だ。"
  },
  {
    "start": 366036,
    "end": 368500,
    "text": "ラマ2について尋ねたらどうだろう？"
  },
  {
    "start": 369030,
    "end": 370786,
    "text": "と聞いている。"
  },
  {
    "start": 370808,
    "end": 372686,
    "text": "では、llamaのコンテキストウィンドウは何ですか？"
  },
  {
    "start": 372718,
    "end": 373314,
    "text": "それが戻ってくる。"
  },
  {
    "start": 373352,
    "end": 377650,
    "text": "llama 2のコンテキスト・ウィンドウは4096トークンで、これは正しい。"
  },
  {
    "start": 377800,
    "end": 386230,
    "text": "さて、面白いことに、何度もこのような奇妙なことが起こり、丁寧な答えが返ってくる。"
  },
  {
    "start": 386300,
    "end": 386742,
    "text": "この中に"
  },
  {
    "start": 386796,
    "end": 390614,
    "text": "繰り返しになるが、我々のエンベッディングは、これとこれに対してうまく機能していることがわかる。"
  },
  {
    "start": 390732,
    "end": 392474,
    "text": "その中で私たちはラマ2について尋ねてきた。"
  },
  {
    "start": 392512,
    "end": 393690,
    "text": "ラマ2匹が戻ってきた。"
  },
  {
    "start": 393760,
    "end": 395658,
    "text": "ラマ2は何枚のトークンで調教されたのですか？"
  },
  {
    "start": 395744,
    "end": 398570,
    "text": "Llama 2は2兆トークンのデータで学習された。"
  },
  {
    "start": 398720,
    "end": 401114,
    "text": "知らないことを聞くのはどうだろう？"
  },
  {
    "start": 401152,
    "end": 402874,
    "text": "ラマ3はいつ来るの？"
  },
  {
    "start": 403072,
    "end": 407294,
    "text": "このモデルはかなり優れていると思う。"
  },
  {
    "start": 407332,
    "end": 407854,
    "text": "分からないよ。"
  },
  {
    "start": 407892,
    "end": 410586,
    "text": "この論文では、ラマ2世とその分散についてのみ論じている。"
  },
  {
    "start": 410618,
    "end": 412606,
    "text": "ラマ3については言及されていない。"
  },
  {
    "start": 412788,
    "end": 420962,
    "text": "これは、この種のクエリに最も近いものを返したが、どれも実際にラマ3世に言及したものではないだろう。"
  },
  {
    "start": 421096,
    "end": 429862,
    "text": "言語モデルは、基本的にその言語モデルを自分で調べて、よし、この中にラマ3世に関する記述があるかどうかを探さなければならなかった。"
  },
  {
    "start": 429916,
    "end": 432182,
    "text": "関連情報はここにあるのか？"
  },
  {
    "start": 432316,
    "end": 444090,
    "text": "オープンソースの小規模なモデルや、あまり訓練されていないモデルの場合、この種のものでは、ここにあるものを使って答えを作り上げてしまうという危険性がある。"
  },
  {
    "start": 444160,
    "end": 446634,
    "text": "ここでそうならなかったのは非常に良いことだ。"
  },
  {
    "start": 446752,
    "end": 449386,
    "text": "さらに混乱させようとしたらどうだろう？"
  },
  {
    "start": 449568,
    "end": 453266,
    "text": "よし、メタコールの新モデルは何だ？"
  },
  {
    "start": 453318,
    "end": 459166,
    "text": "さて、これはひっかけ問題で、リャマ2という可能性もあるからだ。"
  },
  {
    "start": 459268,
    "end": 463774,
    "text": "というのも、ツールフォームもメタアプリから生まれたものだからだ。"
  },
  {
    "start": 463812,
    "end": 471390,
    "text": "これらの書類には、基本的にそれを使って決定する日付はないと思う。"
  },
  {
    "start": 471470,
    "end": 480120,
    "text": "このソースを見ると、llamaから2つ、オーギュメントllmsから2つ、そしてllamaの2つからもう1つ得ていることがわかる。"
  },
  {
    "start": 480970,
    "end": 485826,
    "text": "増強LLMの調査は、基本的に多くの異なるモデルについて言及している。"
  },
  {
    "start": 485858,
    "end": 488934,
    "text": "だから、そこからいくつかのものが出てくるのだと思う。"
  },
  {
    "start": 489052,
    "end": 491642,
    "text": "この場合は、そうなる。"
  },
  {
    "start": 491696,
    "end": 492010,
    "text": "オーケー。"
  },
  {
    "start": 492080,
    "end": 506126,
    "text": "新しいモデルは「ラマ2」と呼ばれているが、「ラマ2」のチャットモデルでRLHFに使われた「安全報酬モデル」や「有用性報酬モデル」についても語られている。"
  },
  {
    "start": 506228,
    "end": 509646,
    "text": "繰り返しになるが、このような返しがあるのは興味深いことだ。"
  },
  {
    "start": 509828,
    "end": 516930,
    "text": "ちょっと不公平な質問だけど、自分のシステムを試してみて、こういうことにどう反応するかを確認するのはいいことだ。"
  },
  {
    "start": 517080,
    "end": 519086,
    "text": "次は、toolformaとは何か？"
  },
  {
    "start": 519278,
    "end": 522626,
    "text": "すぐにToolformaが何であるかを理解することができる。"
  },
  {
    "start": 522808,
    "end": 525934,
    "text": "基本的には、この定義でOKだ。"
  },
  {
    "start": 525982,
    "end": 528598,
    "text": "ツールフォーマーとは何なのか？"
  },
  {
    "start": 528684,
    "end": 535458,
    "text": "検索エンジン、計算機、APIコールによる翻訳システムなど。"
  },
  {
    "start": 535634,
    "end": 538682,
    "text": "それぞれのツールについて、いくつの例を示す必要があるのか？"
  },
  {
    "start": 538816,
    "end": 550874,
    "text": "というのも、ツール・フォーマットや、ツール・フォーマットを扱ったサーベイ・ペーパー、その他のペーパーでも言及されていることだからだ。"
  },
  {
    "start": 550912,
    "end": 552538,
    "text": "それについては、いくつか見えていることがある。"
  },
  {
    "start": 552624,
    "end": 553594,
    "text": "リアクトは？"
  },
  {
    "start": 553642,
    "end": 555774,
    "text": "そこに反応紙がある。"
  },
  {
    "start": 555972,
    "end": 563658,
    "text": "案の定、reactは一般的なタスク解決のための言語モデルにおいて、推論と行動を組み合わせたプロンプトベースの新しいパラダイムである。"
  },
  {
    "start": 563754,
    "end": 567746,
    "text": "これは、これを見る上でとてもいい定義だ。"
  },
  {
    "start": 567848,
    "end": 570162,
    "text": "全体として、このモデルはかなり良くできていると思う。"
  },
  {
    "start": 570216,
    "end": 577670,
    "text": "さて、もしこれを実戦で使うとしたら、おそらく箱から出してすぐにこのモデルを使うことはないだろう。"
  },
  {
    "start": 577740,
    "end": 587862,
    "text": "特定のタイプの検索問題や、あなたが目指していることに適した微調整されたバージョンを選ぶことになる。"
  },
  {
    "start": 587916,
    "end": 591830,
    "text": "テストしてみて、これは可能性がある。"
  },
  {
    "start": 591990,
    "end": 600054,
    "text": "それなら、ラグやリトリーブの世代交代タスクに重点を置いたファインチューンはないだろうか？"
  },
  {
    "start": 600182,
    "end": 602570,
    "text": "あるいは自分で作ることも考えている。"
  },
  {
    "start": 602720,
    "end": 605726,
    "text": "おそらくそれは、将来このビデオで見ることができるだろう。"
  },
  {
    "start": 605828,
    "end": 613118,
    "text": "最後に、今回はAPIを使っているので、基本的にAPIを停止するだけで、この件で何か請求されることはない。"
  },
  {
    "start": 613284,
    "end": 620754,
    "text": "もし4つのGPUを使っているのなら、ローカルで動かしているのでない限り、そのために大騒ぎにならないよう、GPUを停止させることも必要だろう。"
  },
  {
    "start": 620792,
    "end": 631730,
    "text": "いずれにせよ、これはリトリーバルQAとラグやリトリーバル拡張世代を行うためのラマ2の強さを示すものだ。"
  },
  {
    "start": 631890,
    "end": 634326,
    "text": "これでできることは間違いなくある。"
  },
  {
    "start": 634428,
    "end": 640022,
    "text": "しかし、最良の結果を得るためには、特定のタスクに合わせて微調整する必要がある。"
  },
  {
    "start": 640156,
    "end": 643974,
    "text": "何か質問があれば、下のコメント欄に書いてください。"
  },
  {
    "start": 644092,
    "end": 647266,
    "text": "このビデオがお役に立ちましたら、「いいね！」と「購読」をクリックしてください。"
  },
  {
    "start": 647378,
    "end": 648982,
    "text": "次のビデオで話そう。"
  },
  {
    "start": 649116,
    "end": 650222,
    "text": "とりあえず、さようなら。"
  }
]