[
  {
    "start": 250,
    "end": 510,
    "text": "オーケー。"
  },
  {
    "start": 580,
    "end": 9162,
    "text": "前回のビデオでは、ChromidBを使ってマルチドキュメント検索にインストラクターのエンベッディングを追加してみた。"
  },
  {
    "start": 9306,
    "end": 17450,
    "text": "つまり、埋め込みにローカルモデルを使うようになったわけだが、OpenAIを使うことに変わりはない。"
  },
  {
    "start": 17530,
    "end": 20282,
    "text": "このビデオでは、OpenAIの排除について見ていく。"
  },
  {
    "start": 20346,
    "end": 20810,
    "text": "まったくだ。"
  },
  {
    "start": 20890,
    "end": 26326,
    "text": "もしまだビデオを見ていないなら、私がビデオをアップしているチャンネルはこちらだ。"
  },
  {
    "start": 26428,
    "end": 28594,
    "text": "定期的にご覧になりたい方は、ぜひご購読ください。"
  },
  {
    "start": 28642,
    "end": 32600,
    "text": "私たちはラングチェーンのビデオや大規模な言語モデルのビデオをたくさん作ってきました。"
  },
  {
    "start": 32970,
    "end": 52438,
    "text": "まず最初に、4つの異なるモデルを紹介し、これらを使用した結果をお見せします。また、検索QAを行うためにラングチェーンのようなものを使用して、これらのモデルを使用した場合の利点と欠点について少しお話します。"
  },
  {
    "start": 52614,
    "end": 56494,
    "text": "最初に選んだのは、網羅的にリストアップしたわけではない。"
  },
  {
    "start": 56532,
    "end": 59118,
    "text": "また別のビデオを撮るかもしれない。"
  },
  {
    "start": 59204,
    "end": 63102,
    "text": "最初に紹介するのはFLAM T 5 XLモデルだ。"
  },
  {
    "start": 63236,
    "end": 65326,
    "text": "これは30億のパラメーターを持つモデルだ。"
  },
  {
    "start": 65428,
    "end": 67018,
    "text": "シーク・トゥ・シークのモデルだ。"
  },
  {
    "start": 67124,
    "end": 69534,
    "text": "T5モデルのひとつだ。"
  },
  {
    "start": 69662,
    "end": 73138,
    "text": "ここには基本的にエンコーダーとデコーダーがある。"
  },
  {
    "start": 73224,
    "end": 78282,
    "text": "これを持ち込むときは、言語モデリングを求めるために持ち込むことを確認する必要がある。"
  },
  {
    "start": 78366,
    "end": 86070,
    "text": "もうひとつ、このパイプラインを設定する際には、シーク・トゥ・シーク・モデルでテキストからチェックへの生成を行う必要がある。"
  },
  {
    "start": 86220,
    "end": 89240,
    "text": "まず最初に、私はこのモデルを持ち込んだだけだ。"
  },
  {
    "start": 89770,
    "end": 94042,
    "text": "私たちが調べた書類、PDFファイルを持ってくる。"
  },
  {
    "start": 94096,
    "end": 105070,
    "text": "まず最初に、モデルを持ってきて、そのモデルのパイプラインを設定し、そのモデルを使うためにラングチェインの抱きつき顔のパイプラインを設定し、それがうまくいくかどうかをテストしているところです。"
  },
  {
    "start": 105140,
    "end": 105422,
    "text": "そうだね。"
  },
  {
    "start": 105476,
    "end": 112910,
    "text": "これは、ここで定義したローカルLLMを使い、生のプロンプトを実行し、そこから何が出てくるかを見ているだけだ。"
  },
  {
    "start": 113060,
    "end": 117842,
    "text": "あとは、前のビデオでやったことと比べれば、非常にシンプルになる。"
  },
  {
    "start": 117896,
    "end": 122654,
    "text": "私たちは基本的に、すべてを持ち込み、書類をPDFで持ち込むだけです。"
  },
  {
    "start": 122702,
    "end": 127154,
    "text": "このビデオを初めてご覧になる方は、もう1つのビデオをもう一度ご覧ください。"
  },
  {
    "start": 127192,
    "end": 129782,
    "text": "そういう部分について、私がたくさん話しているのを見るだろう。"
  },
  {
    "start": 129916,
    "end": 140402,
    "text": "そしてインストラクターのエンベッディングをセットアップし、GPU上で2つのモデルを実行する。"
  },
  {
    "start": 140466,
    "end": 152406,
    "text": "エンベッディング・モデルはそれほど大きくないが、言語モデルはかなり大きくなる。"
  },
  {
    "start": 152518,
    "end": 155934,
    "text": "これらのデータベースはここで作成する。"
  },
  {
    "start": 155972,
    "end": 159118,
    "text": "わざわざデータベースを削除して復活させたりはしていない。"
  },
  {
    "start": 159204,
    "end": 162080,
    "text": "もし興味があれば、過去のビデオを見てほしい。"
  },
  {
    "start": 162470,
    "end": 173774,
    "text": "私たちはレトリーバーをセットし、OpenAIのようなものとは対照的に、地元のLLMとどう付き合うかを考え始める必要がある。"
  },
  {
    "start": 173902,
    "end": 176974,
    "text": "OpenAIでは4000トークンを持っている。"
  },
  {
    "start": 177102,
    "end": 181522,
    "text": "基本的なフランモデルの場合、トークンは512個しかない。"
  },
  {
    "start": 181666,
    "end": 190790,
    "text": "もっと長く設定しようとしても、512までしか設定できないんだ。"
  },
  {
    "start": 190870,
    "end": 194518,
    "text": "さて、基本的にデータベースをセットアップしたら、レトリーバーが必要だ。"
  },
  {
    "start": 194614,
    "end": 197978,
    "text": "となると、この部分が重要になってくる。"
  },
  {
    "start": 198064,
    "end": 203514,
    "text": "トークンの数は、いくつのコンテキストを渡すか？"
  },
  {
    "start": 203632,
    "end": 214878,
    "text": "この場合は3枚にするけど、うまくいけば、この例では収まりきらない、トークンの数が多くなりすぎる、ということがあるかもしれない。"
  },
  {
    "start": 214964,
    "end": 220466,
    "text": "それなら、2人で行くのか、それともどういう戦略で行くのかを考えたい。"
  },
  {
    "start": 220568,
    "end": 226114,
    "text": "OpenAIでは4000トークンを使ってプレーすることができたので、このようなことは考える必要がなかった。"
  },
  {
    "start": 226312,
    "end": 235890,
    "text": "次に、チェインを作り、チェインをセットアップし、小さなラッパー関数を用意し、これを試してみる。"
  },
  {
    "start": 236050,
    "end": 241222,
    "text": "答えは返ってくるが、あまり饒舌な答えは返ってこない。"
  },
  {
    "start": 241286,
    "end": 243254,
    "text": "あまり詳しい答えは得られていない。"
  },
  {
    "start": 243302,
    "end": 246586,
    "text": "ここで最も有益な答えが返ってくるとは限らない。"
  },
  {
    "start": 246688,
    "end": 249180,
    "text": "アイオワレとは何か？"
  },
  {
    "start": 249550,
    "end": 255310,
    "text": "そのうちのいくつかは、その答えがもたらすものがそれほど大きくないというところまで来ている。"
  },
  {
    "start": 255460,
    "end": 258058,
    "text": "前者のツールとは何だろう？"
  },
  {
    "start": 258074,
    "end": 266506,
    "text": "どのAPIを呼び出すか、いつ呼び出すか、どのような引数を渡すか、そしてその結果を将来のトークン予測にどのように組み込むのが最適かを決定するために学習されたモデル。"
  },
  {
    "start": 266538,
    "end": 268202,
    "text": "それは技術的には正しい。"
  },
  {
    "start": 268276,
    "end": 273122,
    "text": "どのようなツールが使用できるのか、他のツールのように多くの情報は得られない。"
  },
  {
    "start": 273176,
    "end": 277080,
    "text": "ここでもまた、非常に簡潔で要点をついた答えになる。"
  },
  {
    "start": 277450,
    "end": 282006,
    "text": "極めて簡潔な答えがここにある。"
  },
  {
    "start": 282188,
    "end": 289574,
    "text": "そのうちの何人かは、時折、答えが返ってくるのを見るだろう。"
  },
  {
    "start": 289772,
    "end": 297526,
    "text": "文脈が512を超えるとエラーになる。"
  },
  {
    "start": 297558,
    "end": 297754,
    "text": "そうだね。"
  },
  {
    "start": 297792,
    "end": 298634,
    "text": "そう言ってくれるだろう。"
  },
  {
    "start": 298672,
    "end": 302390,
    "text": "ああ、そうか、これをパスするためにトークンを持ちすぎたんだね。"
  },
  {
    "start": 302560,
    "end": 304814,
    "text": "これがフランTの5番だね？"
  },
  {
    "start": 304852,
    "end": 307722,
    "text": "それが30億の基本的なものだ。"
  },
  {
    "start": 307786,
    "end": 315538,
    "text": "今、GPT for allを作ったのと同じ人たちによって作られた、基本的にこれを微調整した別のモデルがある。"
  },
  {
    "start": 315704,
    "end": 318050,
    "text": "これがファストチャットTファイブだ。"
  },
  {
    "start": 318200,
    "end": 322740,
    "text": "これは基本的に同じように使うことができ、これを持ち込むことができる。"
  },
  {
    "start": 323110,
    "end": 328002,
    "text": "このコードは基本的にすべて同じであることがわかるだろう。"
  },
  {
    "start": 328136,
    "end": 336038,
    "text": "ある意味、ここでの答えの方が少し良いことがわかるだろうが、ここでは奇妙な水増しトークンが始まっている。"
  },
  {
    "start": 336204,
    "end": 338930,
    "text": "スペーシングに問題があるのは確かだ。"
  },
  {
    "start": 339010,
    "end": 345322,
    "text": "ここでも奇妙なダブルスペースが発生している。"
  },
  {
    "start": 345456,
    "end": 345850,
    "text": "間違いない。"
  },
  {
    "start": 345920,
    "end": 347494,
    "text": "ああ、実に奇妙な間隔だ。"
  },
  {
    "start": 347542,
    "end": 349146,
    "text": "それがなぜなのかは分からない。"
  },
  {
    "start": 349248,
    "end": 353006,
    "text": "何が原因なのかを調べて、変えることはできるだろう。"
  },
  {
    "start": 353108,
    "end": 364014,
    "text": "ただ、このモデルは微調整ができるため、検索されたコンテキストから情報を抽出する能力が少し優れているのだろうということを示しただけだ。"
  },
  {
    "start": 364132,
    "end": 367778,
    "text": "次に見るのは、本当に大きくなった場合だ。"
  },
  {
    "start": 367864,
    "end": 372130,
    "text": "安定したビキューナモデルのようなものに移行するのであれば。"
  },
  {
    "start": 372280,
    "end": 383442,
    "text": "これは130億のモデルだが、ここにこれを持ち込むことで、我々はこれとは異なる問題に直面することになる。"
  },
  {
    "start": 383506,
    "end": 387318,
    "text": "これで2048年のコンテクストまで行ける。"
  },
  {
    "start": 387404,
    "end": 391082,
    "text": "これで4000まで行けると思う。"
  },
  {
    "start": 391216,
    "end": 401130,
    "text": "生チェックを行うだけで、プロンプトが特定の方法で処理されるように作られていることがわかるだろう。"
  },
  {
    "start": 401280,
    "end": 406206,
    "text": "ということは、僕らがこれをやるとき、ちょっとした悲しみのようなものを引き起こすことになる。"
  },
  {
    "start": 406308,
    "end": 414334,
    "text": "ハッシュ、ハッシュ、ハッシュ、ヒューマン......このようなプロンプトが表示される。"
  },
  {
    "start": 414452,
    "end": 419522,
    "text": "正しい情報を与えてくれてはいるが、いい形で与えてくれていないのかもしれない。"
  },
  {
    "start": 419656,
    "end": 423300,
    "text": "このアウトプットを見てみよう。"
  },
  {
    "start": 423750,
    "end": 428482,
    "text": "ここでは良いアウトプットが得られることがわかるだろう。"
  },
  {
    "start": 428546,
    "end": 429154,
    "text": "フラッシュ注意。"
  },
  {
    "start": 429202,
    "end": 432818,
    "text": "フラッシュ・アテンションは、正確なアテンションを計算する新しいアテンション・アルゴリズムである。"
  },
  {
    "start": 432914,
    "end": 436742,
    "text": "まずまずのアウトプットを得ているのは確かだ。"
  },
  {
    "start": 436796,
    "end": 442138,
    "text": "それは確かに、ここでの文脈に注意を払っている。"
  },
  {
    "start": 442224,
    "end": 447210,
    "text": "そして、フラッシュ・アテンションはどのように機能するのか？"
  },
  {
    "start": 447280,
    "end": 448406,
    "text": "すると、こう答えた。"
  },
  {
    "start": 448438,
    "end": 457070,
    "text": "さて、これらはあなたにとっても、あなたが考えていた質問であるという点で、本当に役に立つかもしれない。"
  },
  {
    "start": 457220,
    "end": 459614,
    "text": "できるときもあれば、できないときもある。"
  },
  {
    "start": 459652,
    "end": 463726,
    "text": "本当に素晴らしい答えがここにある。"
  },
  {
    "start": 463828,
    "end": 465314,
    "text": "今回もかなりいい答えが出た。"
  },
  {
    "start": 465352,
    "end": 469058,
    "text": "そして、そのための自問自答に入る。"
  },
  {
    "start": 469144,
    "end": 471134,
    "text": "ここでもツール形式だ。"
  },
  {
    "start": 471182,
    "end": 473406,
    "text": "今回も質の高い答えが返ってきた。"
  },
  {
    "start": 473438,
    "end": 483334,
    "text": "私たちが抱えている課題は、基本的に赤い小切手を何枚か発行して、その小切手を取り除くことだ。"
  },
  {
    "start": 483372,
    "end": 486802,
    "text": "プロンプトの前処理と後処理。"
  },
  {
    "start": 486946,
    "end": 491590,
    "text": "そうなると、課題のひとつは、プロンプトをこう書き換えればいいのか、ということになる。"
  },
  {
    "start": 491660,
    "end": 495738,
    "text": "ここでは、私がプロンプトを見て、それを実際に書き直してみるところを見ていただきたい。"
  },
  {
    "start": 495824,
    "end": 503674,
    "text": "これをこのような形式に書き換えることで、実行し、うまくいくかどうかを確認することができる。"
  },
  {
    "start": 503712,
    "end": 505594,
    "text": "実際、多くの場合はそうではない。"
  },
  {
    "start": 505642,
    "end": 511038,
    "text": "あまりいじりすぎると、「私は十分な情報を得た上で答えを出すだけの十分な文脈を持ち合わせていない。"
  },
  {
    "start": 511204,
    "end": 513118,
    "text": "詳しい情報をお願いします。"
  },
  {
    "start": 513204,
    "end": 523810,
    "text": "さて、この場合、興味深いのは、おそらくこの特定の質問については、この論文が最適であっただろう。"
  },
  {
    "start": 523880,
    "end": 533190,
    "text": "でも、これらのモデルが、他のモデルが、ここから何かを得ているように見えるのは、ちょっと興味深い。"
  },
  {
    "start": 533340,
    "end": 535734,
    "text": "私たちが問題を抱えているのは、回収の部分ではないと思う。"
  },
  {
    "start": 535772,
    "end": 537030,
    "text": "もっとこうだ。"
  },
  {
    "start": 537180,
    "end": 545962,
    "text": "このプロンプトは、あなたが選んだ特定のモデルに適したプロンプトになるまで、書き換えて遊んでみてほしい。"
  },
  {
    "start": 546096,
    "end": 549658,
    "text": "小さいのもいくつか見たし、大きいのも見た。"
  },
  {
    "start": 549824,
    "end": 553514,
    "text": "最後の1本は、スイートスポットだと思う。"
  },
  {
    "start": 553632,
    "end": 555866,
    "text": "これがウィザードLMだ。"
  },
  {
    "start": 556058,
    "end": 559562,
    "text": "ここにあるのは、普通にこれを持ち込んでいるところだ。"
  },
  {
    "start": 559626,
    "end": 560910,
    "text": "これはラマモデルだ。"
  },
  {
    "start": 560980,
    "end": 566180,
    "text": "llamaのトークナイザーもあるし、llamaの因果言語モデリングもある。"
  },
  {
    "start": 567350,
    "end": 571982,
    "text": "トークンを1024個に設定しました。"
  },
  {
    "start": 572126,
    "end": 574658,
    "text": "この場合は、生でテストしてみよう。"
  },
  {
    "start": 574744,
    "end": 577314,
    "text": "そう、ここでのアウトプットはかなりいい。"
  },
  {
    "start": 577432,
    "end": 578726,
    "text": "これはいい感じだ。"
  },
  {
    "start": 578828,
    "end": 583958,
    "text": "あとは通常通りにセットアップして、一通りやってから質問を始める。"
  },
  {
    "start": 584044,
    "end": 587986,
    "text": "両者の良いとこ取りをしたようなものだ。"
  },
  {
    "start": 588018,
    "end": 596762,
    "text": "非常に丁寧な回答を得ているが、安定したlmプロンプトにあったような邪魔になるような問題はここでは起きていない。"
  },
  {
    "start": 596816,
    "end": 598614,
    "text": "フラッシュ・アテンションとは何か？"
  },
  {
    "start": 598662,
    "end": 605286,
    "text": "フラッシュ・アテンションは、著者らによって提案された新しいアテンション・アルゴリズムで、メモリの読み書きの回数を減らし、非常に首尾一貫した答えを書き込む。"
  },
  {
    "start": 605318,
    "end": 607166,
    "text": "ここで、I o awareとは何を意味するのか？"
  },
  {
    "start": 607268,
    "end": 608014,
    "text": "それが通る。"
  },
  {
    "start": 608052,
    "end": 620162,
    "text": "というのも、この質問があったのは、OpenAIのバージョンで、IOを意識していることが答えに書かれていたからなんです。"
  },
  {
    "start": 620216,
    "end": 622178,
    "text": "ある意味、これで説明がつく。"
  },
  {
    "start": 622264,
    "end": 626446,
    "text": "IOを意識して使うのではなく、ここに分解している。"
  },
  {
    "start": 626638,
    "end": 630006,
    "text": "道具の形に注目すれば、道具の形とは何か？"
  },
  {
    "start": 630108,
    "end": 638758,
    "text": "Toolformは、検索エンジン、計算機、翻訳システムなどの外部ツールを簡単なAPIコールで使用するように訓練された言語モデルである。"
  },
  {
    "start": 638854,
    "end": 643898,
    "text": "これは間違いなく、何が狙いなのか的を得ている。"
  },
  {
    "start": 643984,
    "end": 650874,
    "text": "ここには前者のツールの筆頭著者のような人物もいるし、論文への言及もある。"
  },
  {
    "start": 650992,
    "end": 653010,
    "text": "toolformaで使えるツールは？"
  },
  {
    "start": 653030,
    "end": 656526,
    "text": "ツールフォーマーはどんな外部ツールでも使用できる。"
  },
  {
    "start": 656708,
    "end": 664354,
    "text": "他のものは基本的に計算機や検索エンジンと言っただけで、ここではもっと簡潔に言っていたのを覚えている。"
  },
  {
    "start": 664392,
    "end": 673166,
    "text": "GoogleやBingのような検索エンジン、Wolfram、Alphra、mathwayのような計算機、翻訳システムなどがその例だ。"
  },
  {
    "start": 673198,
    "end": 675698,
    "text": "技術的には間違っていなかった。"
  },
  {
    "start": 675864,
    "end": 681366,
    "text": "ただ、これでより多くの情報を得ることができた。"
  },
  {
    "start": 681468,
    "end": 685270,
    "text": "では、それぞれのツールについて、いくつ例を挙げればいいのだろう？"
  },
  {
    "start": 685340,
    "end": 694406,
    "text": "llmsに最適な検索補強についていくつか質問してみると、より思慮深い答えが返ってくるのがわかる。"
  },
  {
    "start": 694598,
    "end": 700362,
    "text": "その結果、良い答えが返ってくる。"
  },
  {
    "start": 700496,
    "end": 708222,
    "text": "ここで、1024にしか設定しなかったので、248に設定できた例を挙げよう。"
  },
  {
    "start": 708276,
    "end": 723714,
    "text": "もちろん、実行にはもう少し時間がかかるだろうが、意図的に1024だけに設定したので、この場合、長いコンテクストを取得すると、この1322がここで取得した最大長よりも長いことがわかる。"
  },
  {
    "start": 723832,
    "end": 735734,
    "text": "このようなエラーが表示される場合は、基本的に、新規トークンの最大数を増やす必要があります。"
  },
  {
    "start": 735852,
    "end": 738262,
    "text": "この中ではウィザードのLMだ。"
  },
  {
    "start": 738316,
    "end": 740434,
    "text": "おそらく、この4人の中では一番だと思う。"
  },
  {
    "start": 740492,
    "end": 742330,
    "text": "これは最高だ。"
  },
  {
    "start": 742480,
    "end": 748790,
    "text": "テストの余地はたくさんあると思うので、ぜひラミニのモデルをテストしてきてほしい。"
  },
  {
    "start": 748870,
    "end": 752086,
    "text": "彼らはとても小さいが、かなりよく訓練されている。"
  },
  {
    "start": 752198,
    "end": 754406,
    "text": "その中には、このようなことに適しているものもあるだろう。"
  },
  {
    "start": 754528,
    "end": 755614,
    "text": "実際にお見せするかもしれません。"
  },
  {
    "start": 755652,
    "end": 763538,
    "text": "また、いろいろなモデルを投入して、何がベストなのか検討したい。"
  },
  {
    "start": 763624,
    "end": 776450,
    "text": "高品質の結果をもたらす大規模な言語モデルと、それを実行するために大きなGPUを必要とするモデルとの間で、常にトレードオフのバランスを取っているのだ。"
  },
  {
    "start": 776520,
    "end": 779990,
    "text": "このためにトークンを生成する時間が必要だ。"
  },
  {
    "start": 780060,
    "end": 793594,
    "text": "大きなモデルを持っている場合、小さなモデルを持っている場合、かなり時間がかかることがある。"
  },
  {
    "start": 793712,
    "end": 801338,
    "text": "私たちは、このために作られたモデルについて、今後いくつかのことを検討していくつもりだ。"
  },
  {
    "start": 801424,
    "end": 809280,
    "text": "今後、このような特別な作業をするために微調整したモデルをお見せするかもしれない。"
  },
  {
    "start": 809650,
    "end": 811726,
    "text": "とにかく、今はここまでだ。"
  },
  {
    "start": 811828,
    "end": 816350,
    "text": "いつものように、この件に関して質問があれば、下のコメント欄に書き込んでください。"
  },
  {
    "start": 816500,
    "end": 818810,
    "text": "この記事がお役に立ちましたら、「いいね！」と「購読」をクリックしてください。"
  },
  {
    "start": 818890,
    "end": 832726,
    "text": "LangchainでカスタムLMSを使うことで、何がうまくいくのか、何がうまくいかないのか、あなたの特定のプロジェクトにどのようにモデルを選ぶことができるのか、もっといろいろなことをやっていくつもりだ。"
  },
  {
    "start": 832908,
    "end": 833350,
    "text": "とにかくだ。"
  },
  {
    "start": 833420,
    "end": 835366,
    "text": "次のビデオでお話ししましょう。"
  },
  {
    "start": 835468,
    "end": 836100,
    "text": "とりあえず、さようなら。"
  }
]