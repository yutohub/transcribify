[
  {
    "start": 26690,
    "end": 30626,
    "text": "では、次のスピーカーはルートヴィヒ・シュミットです。"
  },
  {
    "start": 30658,
    "end": 35186,
    "text": "ワシントン大学でコンピューターサイエンスの助教授を務めている。"
  },
  {
    "start": 35378,
    "end": 42038,
    "text": "彼はディストリビューション・シフト、そして最近ではデータ品質について多くのインパクトのある仕事をしている。"
  },
  {
    "start": 42204,
    "end": 47990,
    "text": "というわけで、今日は一般化に関するデータ中心の視点について話してくれるだろう。"
  },
  {
    "start": 48490,
    "end": 49334,
    "text": "クールだ。"
  },
  {
    "start": 49532,
    "end": 50014,
    "text": "博士"
  },
  {
    "start": 50092,
    "end": 51966,
    "text": "みんな、最後まで残ってくれて本当にありがとう。"
  },
  {
    "start": 51988,
    "end": 57022,
    "text": "今日は金曜日で、ワークショップの最後の長い話になるのは分かっている。"
  },
  {
    "start": 57156,
    "end": 62138,
    "text": "基本的に1カ月前、ウメッシュが僕にメールを送ってきたんだ。"
  },
  {
    "start": 62154,
    "end": 63102,
    "text": "そうだ、間違いない。"
  },
  {
    "start": 63156,
    "end": 64186,
    "text": "すべてが超エキサイティングだ。"
  },
  {
    "start": 64218,
    "end": 70446,
    "text": "このワークショップの目的は、現在進行中のトランスフォーマーと大規模言語モデルにおける革命を、広い視野から理解しようとすることである。"
  },
  {
    "start": 70478,
    "end": 73634,
    "text": "それで、革命の部分には絶対に賛成だと思ったんだ。"
  },
  {
    "start": 73672,
    "end": 80486,
    "text": "過去10年間、AIには大きな進歩があり、昨年末にはチャットGPTが登場した。"
  },
  {
    "start": 80588,
    "end": 82374,
    "text": "エキサイティングなことがたくさん起こっている。"
  },
  {
    "start": 82492,
    "end": 90226,
    "text": "ワークショップの説明で、私があまり確信が持てなかった点、トランスフォーマーと大規模言語モデルの革命が進行していることだ。"
  },
  {
    "start": 90258,
    "end": 94042,
    "text": "これは基本的に、私がしばらく考えていた疑問に触れたものだ。"
  },
  {
    "start": 94176,
    "end": 96794,
    "text": "トランスフォーマーは本当に必要なのか？"
  },
  {
    "start": 96832,
    "end": 103030,
    "text": "その通り、彼の説明にあるように、これは通常、トランスフォーマーアーキテクチャーのおかげで革命とみなされる。"
  },
  {
    "start": 103110,
    "end": 108062,
    "text": "私たちが解明した部分は、大規模な言語モデルは私たちが追跡すべき本当に重要なものだということです。"
  },
  {
    "start": 108196,
    "end": 112366,
    "text": "トランスフォーマーが本当に中心的な存在なのか、と言われるかもしれない。"
  },
  {
    "start": 112468,
    "end": 113854,
    "text": "答えは明らかにイエスだ。"
  },
  {
    "start": 113892,
    "end": 118066,
    "text": "これは少し馬鹿げた質問かもしれない。なぜなら、AI機械学習では今、文字通りいたるところで機械学習が行われているからだ。"
  },
  {
    "start": 118088,
    "end": 129746,
    "text": "自然言語処理、コンピューター・ビジョン、音声処理など、過去5年間を振り返ってみると、基本的にどの分野も既存のアーキテクチャーを壊してトランスフォーマーを導入している。"
  },
  {
    "start": 129778,
    "end": 131960,
    "text": "非常に強力なツールであることは明らかだ。"
  },
  {
    "start": 132410,
    "end": 139058,
    "text": "最もポピュラーな言語モデルであるGPTは、その名前からして実はトランスフォーマーである。"
  },
  {
    "start": 139154,
    "end": 141426,
    "text": "変圧器の紙はとてもインパクトがある。"
  },
  {
    "start": 141538,
    "end": 143446,
    "text": "彼らは86,000件の引用をチェックした。"
  },
  {
    "start": 143558,
    "end": 146598,
    "text": "また、このアーキテクチャーには非常に明確な利点がある。"
  },
  {
    "start": 146694,
    "end": 148790,
    "text": "効率的なハードウェアの実装がある。"
  },
  {
    "start": 148950,
    "end": 154990,
    "text": "テキスト画像、ロボット工学のようなスピーチアクションなど、多くの異なるモダリティを統一することができる。"
  },
  {
    "start": 155060,
    "end": 156990,
    "text": "長いシーケンスを扱えるようにする。"
  },
  {
    "start": 157650,
    "end": 160558,
    "text": "私は今でもこの質問は重要だと思う。"
  },
  {
    "start": 160644,
    "end": 166010,
    "text": "もし私たちがトランスフォーマーを発見していなかったら、言語モデリングにおける革命はすべて起こっていただろうか？"
  },
  {
    "start": 166090,
    "end": 171650,
    "text": "もしそうだとしたら、AIの進歩はどうなるのか？"
  },
  {
    "start": 172070,
    "end": 183794,
    "text": "この問いに答えるために、あるいは少なくともこの問いに答えようと試みるために、今日は大規模言語モデルに関するワークショップの5日目なので、スケーリング研究の観点から見てみよう。"
  },
  {
    "start": 183842,
    "end": 188450,
    "text": "幸いなことに、今や誰もがスケーリングの法則やスケーリングのトレンドについて耳にしたことがあるだろう。"
  },
  {
    "start": 188530,
    "end": 190346,
    "text": "ヤサマンはこの件についてとても素晴らしい話をしてくれた。"
  },
  {
    "start": 190368,
    "end": 193610,
    "text": "昨日は彼女のスライドをYouTubeから盗むことにしたんだ。"
  },
  {
    "start": 194750,
    "end": 209450,
    "text": "現在ディープラーニングで議論されているように、スケーリング・トレンド・スケーリング・ロスのハイレベルな考え方は、機械学習モデルの性能を、学習データ、モデル・サイズ、計算量などの基本的な量の関数として予測したいというものだ。"
  },
  {
    "start": 209530,
    "end": 218530,
    "text": "過去数年にわたる広範な実証的研究によって判明したことは、言語モデルの場合、これは非常にうまくいくということだ。"
  },
  {
    "start": 218600,
    "end": 225090,
    "text": "このようなプロットを使って、計算データ、セットサイズ、パラメーターの関数としてテスト損失を予測することができる。"
  },
  {
    "start": 225240,
    "end": 232950,
    "text": "というのも、その名の通り、言語モデルは大きいからだ。"
  },
  {
    "start": 233020,
    "end": 234998,
    "text": "彼らはトレーニングに多くのリソースを要する。"
  },
  {
    "start": 235084,
    "end": 239410,
    "text": "というのも、非常に大きなスケールのためにハイパーパラメーターを大量に検索することはできないからだ。"
  },
  {
    "start": 239490,
    "end": 244822,
    "text": "より小さなモデルで多くの探索をする必要があり、その結果、欠けている部分がある。"
  },
  {
    "start": 244966,
    "end": 246726,
    "text": "よし、私は小さな実験をすべて行う。"
  },
  {
    "start": 246758,
    "end": 247434,
    "text": "これは何を意味するのか？"
  },
  {
    "start": 247472,
    "end": 253426,
    "text": "もし私が同様のモデルを大規模に訓練していたとしたら、まさにスケーリング法則によって推定することができる。"
  },
  {
    "start": 253558,
    "end": 254718,
    "text": "オーケー、クールだ。"
  },
  {
    "start": 254884,
    "end": 265758,
    "text": "ディープラーニングのスケーリング法則に関する最初の、言ってみれば現代的な論文のひとつ、2017年末のこの論文を見てみよう。"
  },
  {
    "start": 265854,
    "end": 268270,
    "text": "ディープラーニングのスケーリングは経験的に予測可能である。"
  },
  {
    "start": 268350,
    "end": 270194,
    "text": "これは本当に素晴らしい論文だ。"
  },
  {
    "start": 270312,
    "end": 274066,
    "text": "これはGPTが登場する前のことで、非常に前向きなものだった。"
  },
  {
    "start": 274168,
    "end": 279654,
    "text": "抄録を読むと、基本的に5、6年後の今、超話題になっているキーワードがすべて書かれている。"
  },
  {
    "start": 279772,
    "end": 288950,
    "text": "彼らはすでに、ディープラーニングはモデルアーキテクチャの探索、大規模なトレーニングデータセットの作成、計算のスケーリングという好循環のレシピに従って、インパクトのある進歩を生み出すと述べている。"
  },
  {
    "start": 289290,
    "end": 295766,
    "text": "トレーニングセットのサイズ、計算規模、モデルの精度の関係をより深く理解したい、と彼らは言う。"
  },
  {
    "start": 295878,
    "end": 299094,
    "text": "彼らはすでに、抽象的な言い方だが、パワーロスに適合すると言っている。"
  },
  {
    "start": 299142,
    "end": 300474,
    "text": "パワーロスはうまく機能している。"
  },
  {
    "start": 300512,
    "end": 304640,
    "text": "その結果生じるスケーリング傾向の傾きなどについて話している。"
  },
  {
    "start": 305810,
    "end": 308686,
    "text": "ああ、この論文を読むことを本当に勧めるよ。"
  },
  {
    "start": 308868,
    "end": 310046,
    "text": "とても前向きだ。"
  },
  {
    "start": 310148,
    "end": 311246,
    "text": "いいところもたくさんある。"
  },
  {
    "start": 311268,
    "end": 321474,
    "text": "彼らが行った素晴らしい実験のひとつは、明らかに言語モデリングで、特に単語レベルの言語モデリングだった。"
  },
  {
    "start": 321512,
    "end": 323518,
    "text": "標準的な実験だ。"
  },
  {
    "start": 323694,
    "end": 328594,
    "text": "様々なサイズのモデルを、異なるハイパーパラメータで、様々な訓練セットサイズで訓練する。"
  },
  {
    "start": 328632,
    "end": 332166,
    "text": "ここでは、トレーニングセットのサイズの関数として、スタンドスケーリングとして入力したい。"
  },
  {
    "start": 332268,
    "end": 334342,
    "text": "とすれば、次のようなべき乗則が成り立つ。"
  },
  {
    "start": 334396,
    "end": 342774,
    "text": "誤差は、当惑度のように、訓練セットのサイズの関数として、mはアルファ、あるパラメータになる。"
  },
  {
    "start": 342822,
    "end": 348406,
    "text": "私たちは、スケーリング係数であるbのm乗に多少の誤差を加えて推定するつもりだ。"
  },
  {
    "start": 348438,
    "end": 349466,
    "text": "ガンマ、わからない。"
  },
  {
    "start": 349488,
    "end": 350326,
    "text": "アルファ、ベータ、ガンマ。"
  },
  {
    "start": 350358,
    "end": 351686,
    "text": "これをデータから推定する。"
  },
  {
    "start": 351808,
    "end": 353360,
    "text": "では、彼らは何を見つけたのか？"
  },
  {
    "start": 354050,
    "end": 355982,
    "text": "基本的に、彼らはこんなプロットを見つけた。"
  },
  {
    "start": 356036,
    "end": 359134,
    "text": "つまり、X軸はトレーニングセットのサイズである。"
  },
  {
    "start": 359252,
    "end": 362942,
    "text": "これは10億語データセットと呼ばれるデータセットのサブセットである。"
  },
  {
    "start": 362996,
    "end": 366898,
    "text": "だから、2017年の基準で言えば、10億語は多かった。"
  },
  {
    "start": 366984,
    "end": 372286,
    "text": "さて、これは小さなY軸が検証の損失とみなされる。"
  },
  {
    "start": 372398,
    "end": 378574,
    "text": "そして3つの異なるネットワーク、2種類のLMS（長期短期記憶ネットワーク）に対してこれを行った。"
  },
  {
    "start": 378622,
    "end": 380786,
    "text": "リカレント・ニューラル・ネットワークの一種。"
  },
  {
    "start": 380898,
    "end": 382610,
    "text": "これは古典的な建築だ。"
  },
  {
    "start": 382690,
    "end": 389890,
    "text": "そして、2017年当時最先端のリカレント・アーキテクチャであるRHN（リカレント・ハイウェイ・ネットワーク）のようなものだ。"
  },
  {
    "start": 389970,
    "end": 395162,
    "text": "全体的に定義されたものであれば、スケーリングは十分に予測できる。"
  },
  {
    "start": 395216,
    "end": 397594,
    "text": "と言うことができる。"
  },
  {
    "start": 397632,
    "end": 402538,
    "text": "だから、もっとデータ的なことを言えば、これはちょっと合わないかもしれない。"
  },
  {
    "start": 402624,
    "end": 408746,
    "text": "論文によれば、ハイパーパラメーターの選択肢を増やすにつれて、予測に近づいていったという。"
  },
  {
    "start": 408778,
    "end": 414250,
    "text": "当時すでに問題になっていたのは、このような大規模な実験を行うには膨大な計算量が必要だということだった。"
  },
  {
    "start": 414330,
    "end": 420542,
    "text": "彼らの仮説では、もしもっと計算能力があれば、より大きなモデルでも非常に近い線になったはずだ。"
  },
  {
    "start": 420606,
    "end": 427394,
    "text": "これは基本的に、数年後に『トランスフォーマー』で人々が理解したのと質的にまったく同じ図式だ。"
  },
  {
    "start": 427442,
    "end": 427606,
    "text": "そうだろう？"
  },
  {
    "start": 427628,
    "end": 436680,
    "text": "このカプランとアルペーパーは、トランスでも同じ実験を行い、予測可能なスケーリングを発見しました。"
  },
  {
    "start": 438830,
    "end": 441420,
    "text": "これらの論文の指数はすべてまったく異なっている。"
  },
  {
    "start": 442670,
    "end": 445002,
    "text": "指数はトレーニングセットに依存するだけかもしれない。"
  },
  {
    "start": 445056,
    "end": 445660,
    "text": "そうだね。"
  },
  {
    "start": 448350,
    "end": 450186,
    "text": "トレーニングセットは、ここでは全く同じではない。"
  },
  {
    "start": 450208,
    "end": 452074,
    "text": "ああ、いや、指数が違うんだ。"
  },
  {
    "start": 452272,
    "end": 454094,
    "text": "私たちはもっと直接的なところに行く。"
  },
  {
    "start": 454212,
    "end": 454814,
    "text": "いい質問だね。"
  },
  {
    "start": 454852,
    "end": 457818,
    "text": "この2つのより直接的な比較については、また後ほど。"
  },
  {
    "start": 457914,
    "end": 464260,
    "text": "今のところ、定性的には、パワーロスの予測可能なスケーリングはすでに存在していると思う。"
  },
  {
    "start": 465990,
    "end": 468670,
    "text": "異なるネットワーク・アーキテクチャでも、スケーリングの傾向は似ている。"
  },
  {
    "start": 468750,
    "end": 470338,
    "text": "彼らはすでに自分たちで考えていた。"
  },
  {
    "start": 470424,
    "end": 481270,
    "text": "リカレントハイウェイネットワークは、リカレンス構造はLstmsとは大きく異なるが、学習曲線はほぼ同じだという。"
  },
  {
    "start": 481850,
    "end": 482358,
    "text": "オーケー。"
  },
  {
    "start": 482444,
    "end": 484614,
    "text": "論文にはもっとクールな実験が載っていた。"
  },
  {
    "start": 484732,
    "end": 488354,
    "text": "次に彼らが行ったのは、キャラクターレベルの言語モデリングだった。"
  },
  {
    "start": 488402,
    "end": 491466,
    "text": "専門家を予測する代わりに、今はキャラクターレベルでこれを行う。"
  },
  {
    "start": 491568,
    "end": 492762,
    "text": "それ以外は同じ話だ。"
  },
  {
    "start": 492816,
    "end": 497178,
    "text": "同じ形のフィドルべき乗則で、様々な訓練セットのサイズでモデルを訓練する。"
  },
  {
    "start": 497344,
    "end": 499814,
    "text": "そうすると、彼らが得た結果は次のようになる。"
  },
  {
    "start": 499862,
    "end": 502426,
    "text": "ここでは今、一つの建築しか見ていない。"
  },
  {
    "start": 502528,
    "end": 505902,
    "text": "先ほどからRHNリカレントハイウェイネットワーク。"
  },
  {
    "start": 505956,
    "end": 508026,
    "text": "SGDとアダムを比較する。"
  },
  {
    "start": 508058,
    "end": 509898,
    "text": "2つの異なる最適化アルゴリズム。"
  },
  {
    "start": 509994,
    "end": 514480,
    "text": "その結果、やはりアダムは良くなった。"
  },
  {
    "start": 515330,
    "end": 517346,
    "text": "トレーニングデータが少なくて済む。"
  },
  {
    "start": 517448,
    "end": 520446,
    "text": "全体として、これらはかなり類似したスケーリング傾向である。"
  },
  {
    "start": 520478,
    "end": 525582,
    "text": "モデルをもう少し大きくしても構わないのであれば、SGD対アダムにこだわる必要はない。"
  },
  {
    "start": 525646,
    "end": 528050,
    "text": "これらは基本的に単なる計算修飾子である。"
  },
  {
    "start": 528550,
    "end": 528962,
    "text": "クールだ。"
  },
  {
    "start": 529016,
    "end": 533878,
    "text": "なるほど、これが2017年にすでに判明していたことで、2020年の論文ではこうなっている。"
  },
  {
    "start": 533964,
    "end": 536022,
    "text": "さて、グレッグの質問に移ろう。"
  },
  {
    "start": 536156,
    "end": 542374,
    "text": "トランスフォーマー対LSTMの実験もあった。"
  },
  {
    "start": 542502,
    "end": 546026,
    "text": "具体的には、今ここにあるこの筋書きだ。"
  },
  {
    "start": 546048,
    "end": 550474,
    "text": "素晴らしいのは、これがデータなど、モデルのX軸パラメータを制御していることだ。"
  },
  {
    "start": 550592,
    "end": 553606,
    "text": "Y軸、テストロス、青線トランス。"
  },
  {
    "start": 553638,
    "end": 556378,
    "text": "となると、lstmsには3つの行があることになる。"
  },
  {
    "start": 556474,
    "end": 561006,
    "text": "これを見ると、もう少し雑然とした感じだと思うかもしれない。"
  },
  {
    "start": 561028,
    "end": 564714,
    "text": "Lstmsについては、1層、2層、4層でスケーリングの傾向が異なる。"
  },
  {
    "start": 564762,
    "end": 571966,
    "text": "変圧器のラインはもっときれいに見えるが、論文の別の部分を見てみると、変圧器についてもまったく同じ現象が見られる。"
  },
  {
    "start": 571998,
    "end": 574286,
    "text": "これは現在、トランスのスケーリング傾向のみである。"
  },
  {
    "start": 574398,
    "end": 576318,
    "text": "1236層。"
  },
  {
    "start": 576414,
    "end": 578978,
    "text": "一層のスケーリングの傾向は全く異なる。"
  },
  {
    "start": 579064,
    "end": 581206,
    "text": "2つあれば、あと少しだ。"
  },
  {
    "start": 581388,
    "end": 584454,
    "text": "何層にもなると、ここで限界に達するということだ。"
  },
  {
    "start": 584492,
    "end": 587990,
    "text": "ちょっと面倒なことになったからと言って、そうすべきではないと思う。"
  },
  {
    "start": 588730,
    "end": 590838,
    "text": "これはトランスフォーマーでも起こることだ。"
  },
  {
    "start": 590934,
    "end": 592298,
    "text": "そこでも同じ現象が起きている。"
  },
  {
    "start": 592384,
    "end": 602422,
    "text": "全体として、これをどう見るかにもよるが、これを大きくすれば、おそらくこの傾向が続くというのは、かなり公平な見方だと思う。"
  },
  {
    "start": 602486,
    "end": 604638,
    "text": "彼らがこれをしなかったのは、これが彼らの主眼ではなかったからだ。"
  },
  {
    "start": 604724,
    "end": 612298,
    "text": "これは総合的に見て、Lstmsをさらに押し進めれば、トランスフォーマーと同じ場所にたどり着けるという、かなり良い証拠でもあると思う。"
  },
  {
    "start": 612394,
    "end": 614750,
    "text": "より多くのデータと計算が必要になるかもしれない。"
  },
  {
    "start": 615170,
    "end": 620878,
    "text": "この論文では、公平を期すために、lstmsの弱点も明確に指摘している。"
  },
  {
    "start": 621054,
    "end": 623074,
    "text": "これは以前から議論されていたことだ。"
  },
  {
    "start": 623112,
    "end": 626190,
    "text": "彼らは実験によって、このことを非常にうまく正確に表現している。"
  },
  {
    "start": 626350,
    "end": 631842,
    "text": "lstmsは特に、コンテキストの後半にあるトークンに対して悪化する。"
  },
  {
    "start": 631906,
    "end": 633062,
    "text": "これは陰謀だ。"
  },
  {
    "start": 633116,
    "end": 639110,
    "text": "X軸はコンテキストのインデックス、Y軸はトークンごとのテスト損失。"
  },
  {
    "start": 639270,
    "end": 643898,
    "text": "ここでは3組の実験をしているようなものだ。"
  },
  {
    "start": 643984,
    "end": 645510,
    "text": "赤い線はlstms。"
  },
  {
    "start": 645590,
    "end": 647222,
    "text": "青い線はトランスフォーマー。"
  },
  {
    "start": 647286,
    "end": 649270,
    "text": "対のパラメータ数。"
  },
  {
    "start": 649430,
    "end": 658906,
    "text": "この赤い線は、LSTMの方がトランスフォーマーよりもテストロスの低減という点で早くプラトーに達することを示しています。"
  },
  {
    "start": 658938,
    "end": 667810,
    "text": "transformとlstmの性能差の理由はおそらく、lstmが長いシーケンスで最も悪い結果を出すからだろう。"
  },
  {
    "start": 669110,
    "end": 669522,
    "text": "クールだ。"
  },
  {
    "start": 669576,
    "end": 672610,
    "text": "では、もう1つアーキテクチャの比較をしてみよう。"
  },
  {
    "start": 672950,
    "end": 675478,
    "text": "これもカプランとALPの論文から。"
  },
  {
    "start": 675564,
    "end": 678850,
    "text": "ここでは、2つの異なるトランスのバリエーション、リカレントに注目している。"
  },
  {
    "start": 679010,
    "end": 683670,
    "text": "あるいは、ユニバーサル・トランスとスタンダード・トランスと呼ばれることもある。"
  },
  {
    "start": 684010,
    "end": 685430,
    "text": "また同じ話だ。"
  },
  {
    "start": 685500,
    "end": 695542,
    "text": "リカレント・トランスフォーマーのさまざまなバリエーションを見ることができ、破線の標準的なトランスフォーマーと比較して、オフセットのようなものが少し異なることがわかります。"
  },
  {
    "start": 695606,
    "end": 698490,
    "text": "全体的に見て、両者は非常によく似たスケーリングを持っているように見える。"
  },
  {
    "start": 699630,
    "end": 700800,
    "text": "よし、いいぞ。"
  },
  {
    "start": 703330,
    "end": 709470,
    "text": "基本的には、異なるレイヤーとは対照的に、同じパラメーターを何度も使う。"
  },
  {
    "start": 712610,
    "end": 715866,
    "text": "オーケー、それで君はこう言うかもしれないが、これはいいことだよ、ルートヴィヒ。"
  },
  {
    "start": 715898,
    "end": 717614,
    "text": "いくつかの論文を見ている。"
  },
  {
    "start": 717742,
    "end": 721118,
    "text": "言語モデルの議論から学んだことがひとつある。"
  },
  {
    "start": 721214,
    "end": 722962,
    "text": "文章だけを見ていてはいけない。"
  },
  {
    "start": 723016,
    "end": 727570,
    "text": "テキストだけを見ていると、確率論的なオウム返しになってしまうからだ。"
  },
  {
    "start": 728310,
    "end": 730946,
    "text": "もっと地に足のついた意見を聞こうと思ったんだ。"
  },
  {
    "start": 731058,
    "end": 733362,
    "text": "専門家に聞いてみようと思ったんだ。"
  },
  {
    "start": 733426,
    "end": 737078,
    "text": "GPTやトランスフォーマーを実際にトレーニングした人や、発明した人もいる。"
  },
  {
    "start": 737164,
    "end": 738646,
    "text": "彼らの意見は？"
  },
  {
    "start": 738748,
    "end": 741862,
    "text": "最初に尋ねたのはアレック・ラトフォードだ。"
  },
  {
    "start": 741926,
    "end": 743782,
    "text": "彼はOpenAIのリサーチ・サイエンティストだ。"
  },
  {
    "start": 743846,
    "end": 747654,
    "text": "実際にGPTを発明したのは彼だ。"
  },
  {
    "start": 747782,
    "end": 749594,
    "text": "だから、彼なら知っているだろうと思ったんだ。"
  },
  {
    "start": 749632,
    "end": 751370,
    "text": "まあ、少なくとも彼はこの件に関して興味深い見解を持っている。"
  },
  {
    "start": 751440,
    "end": 755338,
    "text": "彼は基本的に、トランスフォーマーがなくても同じLMの軌跡をたどるだろう、と言った。"
  },
  {
    "start": 755434,
    "end": 758586,
    "text": "rnNにもう少し手を加える必要があるかもしれない。"
  },
  {
    "start": 758698,
    "end": 762474,
    "text": "全体的に、研究コミュニティはアーキテクチャを微調整するのが得意だ。"
  },
  {
    "start": 762602,
    "end": 764410,
    "text": "おそらく支障はないだろう。"
  },
  {
    "start": 764570,
    "end": 768366,
    "text": "次に尋ねたのは、実はこのワークショップに参加しているルカシュ・カイザーだ。"
  },
  {
    "start": 768478,
    "end": 770610,
    "text": "彼は今日ここにいないと思う。"
  },
  {
    "start": 770680,
    "end": 772434,
    "text": "彼はいくつかここにいた。"
  },
  {
    "start": 772552,
    "end": 775266,
    "text": "黄色いTシャツに身を包んだ、超イケメンの彼を見たことがあるかもしれない。"
  },
  {
    "start": 775368,
    "end": 778982,
    "text": "彼はグーグル時代に変圧器を発明した。"
  },
  {
    "start": 779036,
    "end": 782802,
    "text": "今、彼はOpenAIと言ったが、もう少し両義的だった。"
  },
  {
    "start": 782946,
    "end": 790114,
    "text": "しかし、変圧器と現在のネットワークを一致させるためには、100倍以上の計算が必要になるかもしれない。"
  },
  {
    "start": 790162,
    "end": 790902,
    "text": "それはフェアだと思う。"
  },
  {
    "start": 790956,
    "end": 794230,
    "text": "これまで見てきたプロットからすると、より多くの計算能力が必要なのかもしれない。"
  },
  {
    "start": 794390,
    "end": 798842,
    "text": "ロカシュは興味深いことに、イリヤとこの件について時々行ったり来たりしていると話してくれた。"
  },
  {
    "start": 798896,
    "end": 803322,
    "text": "ロカシュによると、イリヤも「そうだ、これでいい」という陣営だという。"
  },
  {
    "start": 803376,
    "end": 806702,
    "text": "リカレント・ニューラル・ネットワークでは、非常に大きな隠れ状態が必要になるかもしれない。"
  },
  {
    "start": 806836,
    "end": 808506,
    "text": "そう、エンジニアリングの挑戦だ。"
  },
  {
    "start": 808538,
    "end": 814830,
    "text": "トランスフォーマーの方が計算効率が高いのは確かだが、根本的に違うところはない。"
  },
  {
    "start": 814900,
    "end": 820210,
    "text": "カーゴネットワークを使うだけでも、同じ高品質の言語モデルが得られるだろう。"
  },
  {
    "start": 821750,
    "end": 833874,
    "text": "余談だが、言語モデリングについてもっと学びたいなら、2020年にピーター・ビードルの教師なし深層学習クラスでゲスト講義を行ったアレックスの講義が素晴らしいと思う。"
  },
  {
    "start": 833922,
    "end": 841660,
    "text": "アレックスはGPT-2に至るまでの言語モデリングのような歴史を説明する素晴らしい仕事をしていると思う。"
  },
  {
    "start": 842030,
    "end": 850380,
    "text": "NLPの視点はここまでにして、次はコンピューター・ビジョンを見てみよう。"
  },
  {
    "start": 850850,
    "end": 858378,
    "text": "実際、コンピュータ・ビジョンに最適なアーキテクチャは何かというのは、非常に単純な問題だ。"
  },
  {
    "start": 858554,
    "end": 865486,
    "text": "畳み込みニューラルネットワークである。"
  },
  {
    "start": 865518,
    "end": 865714,
    "text": "そうだね。"
  },
  {
    "start": 865752,
    "end": 878194,
    "text": "この最近のディープラーニングの波は、トロントのグループによるアレックス・ネットのブレークスルーから始まった。"
  },
  {
    "start": 878312,
    "end": 883720,
    "text": "それから8年間、コンピューター・ビジョンのプラグに携わった。"
  },
  {
    "start": 884730,
    "end": 892534,
    "text": "2020年、人々は初めて、まったく異なるアーキテクチャーで同様の優れたパフォーマンスを手に入れた。"
  },
  {
    "start": 892582,
    "end": 906414,
    "text": "特に、チューリッヒにあるグーグル・ブレインのグループが発表したビジョン・トランスフォーマーに関する論文はとても素晴らしい。"
  },
  {
    "start": 906452,
    "end": 911466,
    "text": "彼らがやったのは、入力画像をパッチに分割することだ。"
  },
  {
    "start": 911578,
    "end": 914170,
    "text": "これらのパッチがトランスフォーマーのトークンとなる。"
  },
  {
    "start": 914250,
    "end": 921246,
    "text": "リニア・プロジェクション・レイヤーが1つあるだけで、あとはすべて、LPがトランスを棚から取り出せるのと同じです。"
  },
  {
    "start": 921358,
    "end": 926110,
    "text": "いくつかのベンチマークでは、当時の最先端技術であった自信よりも優れたパフォーマンスを示したほどだ。"
  },
  {
    "start": 926280,
    "end": 927954,
    "text": "その後、このようなやり取りがあった。"
  },
  {
    "start": 928082,
    "end": 931846,
    "text": "自信が最高なのか、ビジョントランスフォーマーが最高なのか？"
  },
  {
    "start": 931948,
    "end": 947340,
    "text": "興味深いことに、その直後にグーグル・フレーム・チューリッヒの同じグループが、これまた全く異なるアーキテクチャを発表した。"
  },
  {
    "start": 947710,
    "end": 951866,
    "text": "これのクールなところは、全MLPアーキテクチャと呼ばれていることだ。"
  },
  {
    "start": 951978,
    "end": 954606,
    "text": "基本的には、できるだけシンプルなものを求めていた。"
  },
  {
    "start": 954708,
    "end": 962182,
    "text": "もう自己注視も、畳み込みもない。"
  },
  {
    "start": 962346,
    "end": 964830,
    "text": "具体的には、レイヤーの正規化だ。"
  },
  {
    "start": 964910,
    "end": 972542,
    "text": "標準的な正規化レイヤー、次にmlps、そして非常に強力な転置演算と非線形性。"
  },
  {
    "start": 972606,
    "end": 974530,
    "text": "これがすべての構成要素だ。"
  },
  {
    "start": 975030,
    "end": 977794,
    "text": "面白いのは、ではイギリスでは何がベストなのかということだ。"
  },
  {
    "start": 977832,
    "end": 978034,
    "text": "ここで？"
  },
  {
    "start": 978072,
    "end": 979126,
    "text": "これがベストだと主張している。"
  },
  {
    "start": 979148,
    "end": 980422,
    "text": "もうひとつは、ベストだと主張することだ。"
  },
  {
    "start": 980476,
    "end": 994786,
    "text": "同じ論文で、MLPミキサー・ファミリーの異なるネットワーク、トランスフォーマーとレゾナンスを見て、トレーニングの計算量をコントロールしながら比較するという、非常に興味深い実験、あるいは非常に慎重な実験を行っている。"
  },
  {
    "start": 994818,
    "end": 995514,
    "text": "ここで、グーグル。"
  },
  {
    "start": 995552,
    "end": 1001900,
    "text": "Tpusで走らせ、コンピュートをコントロールすれば、実はどれも同じようなものだ。"
  },
  {
    "start": 1003170,
    "end": 1012320,
    "text": "そして、レゾネートが少し古くなったかもしれないが、それも事実ではないことがわかった。"
  },
  {
    "start": 1013570,
    "end": 1020162,
    "text": "その直後、フェアとバークレーから2020年代のコンフネットを提案する非常に素晴らしい論文が発表された。"
  },
  {
    "start": 1020216,
    "end": 1032706,
    "text": "基本的に、彼らは過去5年間のコンフネットの改良を検討し、何が本当に効果的かを見極め、これをコンフネクストと呼ばれる新しいアーキテクチャにまとめ、そしてVitセグメントを凌駕した。"
  },
  {
    "start": 1032898,
    "end": 1038102,
    "text": "全体として、コンピュータ・ビジョンの高いレベルでは、トランスフォーマーは特別なものではないという話だ。"
  },
  {
    "start": 1038156,
    "end": 1042906,
    "text": "高いレベルで異なるアーキテクチャーを行ったり来たりしている。"
  },
  {
    "start": 1043008,
    "end": 1046198,
    "text": "MLPのミキサーとか、ヴィップスとか、コンフィネットとか。"
  },
  {
    "start": 1046294,
    "end": 1050362,
    "text": "本当に重要なのは、トレードのレシピの詳細をすべて正しく理解することだ。"
  },
  {
    "start": 1050416,
    "end": 1056350,
    "text": "レシピを得るために十分な実験をすれば、そのすべてを最先端のものにすることができる。"
  },
  {
    "start": 1057730,
    "end": 1060462,
    "text": "では、質問に戻ろう。"
  },
  {
    "start": 1060516,
    "end": 1063050,
    "text": "トランスフォーマーがなかったら、LLM革命は起こっただろうか？"
  },
  {
    "start": 1063130,
    "end": 1065070,
    "text": "だから、はっきりしたことはわからない。"
  },
  {
    "start": 1065140,
    "end": 1070398,
    "text": "GPT3.5やGPT4レベルのLSTMを訓練した人はいない。"
  },
  {
    "start": 1070494,
    "end": 1074926,
    "text": "近い将来、誰もそうなるとは思わない。"
  },
  {
    "start": 1074958,
    "end": 1079186,
    "text": "Lstmsにこの計算をすべて投入するのは、ちょっと馬鹿げている。"
  },
  {
    "start": 1079298,
    "end": 1086390,
    "text": "全体として、リカレント・ニューラル・ネットワークでも本当に質の高いllmsが得られるというスケーリング・エビデンスはかなり高いと思う。"
  },
  {
    "start": 1086810,
    "end": 1087174,
    "text": "いいね。"
  },
  {
    "start": 1087212,
    "end": 1087426,
    "text": "オーケー。"
  },
  {
    "start": 1087468,
    "end": 1094570,
    "text": "私たちは、これは思考実験に過ぎないかもしれないが、AIの進歩について何かを物語っている、と述べた。"
  },
  {
    "start": 1094910,
    "end": 1095660,
    "text": "そうだ。"
  },
  {
    "start": 1098670,
    "end": 1099274,
    "text": "ああ、そうだ。"
  },
  {
    "start": 1099312,
    "end": 1101034,
    "text": "人々は自信をもって言葉を使ってきた。"
  },
  {
    "start": 1101082,
    "end": 1104240,
    "text": "また、私はそこまでの専門家ではない。"
  },
  {
    "start": 1106770,
    "end": 1107230,
    "text": "オーケー。"
  },
  {
    "start": 1107300,
    "end": 1127590,
    "text": "6、7年前の論文で、現在のニューラルネットワークとは対照的に、センチメント分類や信頼性のあるテキスト分類など、当時の多くの自然言語処理タスクで実に優れたパフォーマンスが得られることを示したものがある。"
  },
  {
    "start": 1130570,
    "end": 1134066,
    "text": "スタンフォード大学のクリス・リース・グループの研究がある。"
  },
  {
    "start": 1134178,
    "end": 1142038,
    "text": "ハイエナ・アーキテクチャーと呼ばれるもので、畳み込むような性質を持つ。"
  },
  {
    "start": 1142134,
    "end": 1148390,
    "text": "大陸のNLPで何ができるのか、それは素晴らしい問題だと思います。"
  },
  {
    "start": 1148550,
    "end": 1150750,
    "text": "意外と遠くまで行けると思う。"
  },
  {
    "start": 1150820,
    "end": 1155326,
    "text": "正確な実験内容や数字はわからないが、素晴らしい質問だ。"
  },
  {
    "start": 1155508,
    "end": 1158190,
    "text": "ああ、次のバージョンのファイルにそう書くべきだね。"
  },
  {
    "start": 1158850,
    "end": 1159630,
    "text": "他に質問は？"
  },
  {
    "start": 1159700,
    "end": 1160222,
    "text": "そうだね。"
  },
  {
    "start": 1160356,
    "end": 1165794,
    "text": "また、あなたが示していた指標のほとんどは、法律の改善のようなものだったのですか？"
  },
  {
    "start": 1165912,
    "end": 1173940,
    "text": "というのも、実際に他のベンチマークの値でテストしたいのであれば、このモデルである可能性があるからだ。"
  },
  {
    "start": 1175350,
    "end": 1176850,
    "text": "ああ、素晴らしい質問だね。"
  },
  {
    "start": 1176920,
    "end": 1181794,
    "text": "基本的に、問題は、これらのスケーリング・プロットが常に当惑の目を向けているということだ。"
  },
  {
    "start": 1181842,
    "end": 1182982,
    "text": "オーケー、これは違う。"
  },
  {
    "start": 1183036,
    "end": 1184418,
    "text": "イマグネットにはパープレックスがない。"
  },
  {
    "start": 1184514,
    "end": 1186098,
    "text": "コンピュータービジョンには当惑はない。"
  },
  {
    "start": 1186194,
    "end": 1187794,
    "text": "これらのスケーリング則プロットはすべて"
  },
  {
    "start": 1187842,
    "end": 1190898,
    "text": "Y軸は常に言語モデリングの当惑度である。"
  },
  {
    "start": 1191074,
    "end": 1193482,
    "text": "ああ、これは本当にいい指摘だ。"
  },
  {
    "start": 1193616,
    "end": 1196646,
    "text": "同じような戸惑いを耳にするはずだ。"
  },
  {
    "start": 1196758,
    "end": 1200598,
    "text": "また、下流のタスク精度にも注目すべきだ。"
  },
  {
    "start": 1200694,
    "end": 1206618,
    "text": "実は、私はこのことをよく指摘する大推薦者の一人なのだが、人々は当惑の表情を見せるだけだ。"
  },
  {
    "start": 1206714,
    "end": 1213098,
    "text": "とはいえ、全体的に見れば、この言語モデリングは基本的に複雑さを改善しようという教会だ。"
  },
  {
    "start": 1213194,
    "end": 1214354,
    "text": "ある意味、当惑。"
  },
  {
    "start": 1214392,
    "end": 1220690,
    "text": "言語モデルにおける当惑を改善すれば、より優れたモデルが得られるという信念があるようにね。"
  },
  {
    "start": 1222070,
    "end": 1224638,
    "text": "あなたは、基本的にLstmsではこれが崩れる可能性があると考えている。"
  },
  {
    "start": 1224734,
    "end": 1225006,
    "text": "そうだね。"
  },
  {
    "start": 1225048,
    "end": 1227510,
    "text": "本当の建築ではないかもしれない。"
  },
  {
    "start": 1228570,
    "end": 1228982,
    "text": "いや。"
  },
  {
    "start": 1229036,
    "end": 1229766,
    "text": "いい質問だね。"
  },
  {
    "start": 1229868,
    "end": 1236802,
    "text": "ビジョンの例で考えなければならないことがある。"
  },
  {
    "start": 1236866,
    "end": 1239058,
    "text": "それは当惑に等しかったのだろうか？"
  },
  {
    "start": 1239154,
    "end": 1239574,
    "text": "ああ、そうだ。"
  },
  {
    "start": 1239612,
    "end": 1252298,
    "text": "視覚の分野では、一般的に、イメージネットの精度を、モデルがどれだけ優れているかを広く代表する指標のようなものとして見るのが普通です。"
  },
  {
    "start": 1252384,
    "end": 1256350,
    "text": "公正を期すために、それは他の多くの仕事と非常によく相関している。"
  },
  {
    "start": 1258450,
    "end": 1261626,
    "text": "ジャスミンと同じ心配をしているかもしれないが、それはもっともな心配だ。"
  },
  {
    "start": 1261738,
    "end": 1263006,
    "text": "他の仕事についてはどうですか？"
  },
  {
    "start": 1263038,
    "end": 1272420,
    "text": "もし1つしか見ていないとしたら、コンフネットやトランスの性能を過小評価しているのかもしれない。"
  },
  {
    "start": 1275510,
    "end": 1278322,
    "text": "私たちは別の方法でそこにたどり着いたんだ。"
  },
  {
    "start": 1278376,
    "end": 1294486,
    "text": "つまり、先ほど長距離への注意が重要だと指摘されましたが、現在、トランスフォーマーを再びリニアにしようとする動きがかなり活発になっています。"
  },
  {
    "start": 1294678,
    "end": 1295654,
    "text": "注意力散漫。"
  },
  {
    "start": 1295702,
    "end": 1301420,
    "text": "しかし、それがどのようなものであったか、おわかりになりますか？"
  },
  {
    "start": 1301810,
    "end": 1312682,
    "text": "どのようにして長距離を確保するのか、どのようにしてLSTMのようなものから長距離の注意を引くのか、あるいは逆に、どのようにしてトランスフォーマーを確保するのか。"
  },
  {
    "start": 1312746,
    "end": 1313662,
    "text": "ああ、いい質問だね。"
  },
  {
    "start": 1313716,
    "end": 1320702,
    "text": "つまり、トランスフォーマーの方がLstmsよりも長距離の依存関係が強いということだ。"
  },
  {
    "start": 1320846,
    "end": 1326214,
    "text": "LSTMには明確なカットオフのようなものはなく、ある一定以上のLSTMを得ることはできないと思う。"
  },
  {
    "start": 1326252,
    "end": 1331190,
    "text": "LSTMを十分に大きくすれば、より長い依存関係を得ることができる。"
  },
  {
    "start": 1332730,
    "end": 1336370,
    "text": "ここで重要なのは、計算効率の面を理解することだ。"
  },
  {
    "start": 1336450,
    "end": 1342038,
    "text": "計算効率の問題であることは間違いないが、lstmsを十分に大きくすれば、質的な違いはない。"
  },
  {
    "start": 1342124,
    "end": 1350170,
    "text": "LSTMを十分に大きくすれば、より長い文脈を得ることができる。"
  },
  {
    "start": 1352130,
    "end": 1353502,
    "text": "これを見てほしい。"
  },
  {
    "start": 1353556,
    "end": 1355760,
    "text": "あなたはあまり納得していないようだが、それはもっともだ。"
  },
  {
    "start": 1358130,
    "end": 1361950,
    "text": "トランスフォーマーが釘を打つ領域はたくさんある。"
  },
  {
    "start": 1362290,
    "end": 1364254,
    "text": "うん、いや、素晴らしい建築だよ。"
  },
  {
    "start": 1364382,
    "end": 1367170,
    "text": "ああ、いや、まったくだ。"
  },
  {
    "start": 1369350,
    "end": 1370994,
    "text": "特定のドメインを念頭に置いていますか？"
  },
  {
    "start": 1371032,
    "end": 1372462,
    "text": "まあ、実際、私はゲノムのことを考えている。"
  },
  {
    "start": 1372526,
    "end": 1398546,
    "text": "遺伝子制御のモデルを見ていると、ディープマインドが生まれ、他のディープマインドと一緒になった。"
  },
  {
    "start": 1398588,
    "end": 1400362,
    "text": "トレーニングがしやすいということなのだろうか？"
  },
  {
    "start": 1400416,
    "end": 1400970,
    "text": "電車？"
  },
  {
    "start": 1401120,
    "end": 1401530,
    "text": "そうだ。"
  },
  {
    "start": 1401600,
    "end": 1405094,
    "text": "いや、これはトランスフォーマーの大きな利点のひとつだと思う。"
  },
  {
    "start": 1405142,
    "end": 1408746,
    "text": "はっきり言っておくが、私はトランスを捨てるべきだと主張したいのではない。"
  },
  {
    "start": 1408778,
    "end": 1410906,
    "text": "申し上げたように、これは非常に優れた発明だ。"
  },
  {
    "start": 1411018,
    "end": 1413102,
    "text": "それでも、考えるのは面白いと思う。"
  },
  {
    "start": 1413156,
    "end": 1415034,
    "text": "実際、LLMの中核をなすものなのでしょうか？"
  },
  {
    "start": 1415082,
    "end": 1419940,
    "text": "具体的に言えば、他の領域では、トランスフォーマーが核となる部分であることが多いかもしれない。"
  },
  {
    "start": 1420870,
    "end": 1428840,
    "text": "今のところ、私が興味があるのは、これらのクールなチャット特性などをすべて備えた高品質の言語モデルを作るための核となる部分なのか、ということだ。"
  },
  {
    "start": 1429610,
    "end": 1430454,
    "text": "おそらくね。"
  },
  {
    "start": 1430652,
    "end": 1436706,
    "text": "バイオシークエンスは、言語の外側に最も近い、間違いなく最も近い例だと思う。"
  },
  {
    "start": 1436898,
    "end": 1448842,
    "text": "基本的には、このプロットはその答えにはなっていない。"
  },
  {
    "start": 1448896,
    "end": 1457422,
    "text": "私たちが言ったように、400kのパラメーターラインがプラトーになるのは、これがトランスの性能が良い証拠であるようなものだ。"
  },
  {
    "start": 1457556,
    "end": 1466190,
    "text": "LSTMのサイズを大きくすると、200万行、2億行となる。"
  },
  {
    "start": 1466340,
    "end": 1469234,
    "text": "さて、問題は、このプロットも、このプロットより早いのだろうか？"
  },
  {
    "start": 1469272,
    "end": 1471650,
    "text": "プロットから読み取るのは少し難しいと思う。"
  },
  {
    "start": 1472470,
    "end": 1475640,
    "text": "もっと実験が必要だと思う。"
  },
  {
    "start": 1477530,
    "end": 1494086,
    "text": "VLSIやスペキュレーション、パイプラインなど、細部に至るまで細心の注意が払われているんだ。"
  },
  {
    "start": 1494198,
    "end": 1499578,
    "text": "そして、2つ目と3つ目の要素はすべて、この方法で勝つということが明らかになった。"
  },
  {
    "start": 1499744,
    "end": 1504510,
    "text": "数年待てば、ムーアの法則に押し流されるだけだろ？"
  },
  {
    "start": 1504660,
    "end": 1508320,
    "text": "だから、ここも似たようなものなのかなと思っている。"
  },
  {
    "start": 1508850,
    "end": 1515138,
    "text": "もしあなたの目標が他のAI企業に勝つことなら、これらすべてが必要だ。"
  },
  {
    "start": 1515224,
    "end": 1515570,
    "text": "そうだろう？"
  },
  {
    "start": 1515640,
    "end": 1518494,
    "text": "長期的な進歩に興味があるのなら、そうではない。"
  },
  {
    "start": 1518542,
    "end": 1520820,
    "text": "私はまさにこのように考えている。"
  },
  {
    "start": 1529850,
    "end": 1530806,
    "text": "オーケー、クールだ。"
  },
  {
    "start": 1530908,
    "end": 1533014,
    "text": "では続けよう。"
  },
  {
    "start": 1533132,
    "end": 1534520,
    "text": "本当にいい質問ばかりだ。"
  },
  {
    "start": 1537290,
    "end": 1538854,
    "text": "さて、これはどういう意味だろう？"
  },
  {
    "start": 1538892,
    "end": 1546214,
    "text": "今スコットが言ったように、アルゴリズムの改良を考えるひとつの方法は、基本的にコンピュートやデータの乗数として考えることだ。"
  },
  {
    "start": 1546262,
    "end": 1549082,
    "text": "私たちが変圧器を発明したときのように、これは超重要なことなんだ。"
  },
  {
    "start": 1549136,
    "end": 1551338,
    "text": "私たちがアダムを発明するとき、これは超重要なことだ。"
  },
  {
    "start": 1551504,
    "end": 1553402,
    "text": "これは質的な変化ではない。"
  },
  {
    "start": 1553456,
    "end": 1557950,
    "text": "これによってスケールを大きくすることができる。"
  },
  {
    "start": 1558290,
    "end": 1567010,
    "text": "これらの異なるアーキテクチャを比較した場合、これはコンピューター・ビジョンで見られることだが、トレーニング・レシピは高レベルのアーキテクチャと同じくらい重要なのだ。"
  },
  {
    "start": 1567990,
    "end": 1575166,
    "text": "言語について、言語モデルについて、何を理解したいのかにもよるが、この理解の一部はアーキテクチャでなければならないかもしれない。"
  },
  {
    "start": 1575198,
    "end": 1584434,
    "text": "オースティン、ひとつのアプローチとして、言語モデルがなぜある種の創発現象を起こすのかを理解するために、変換器の特性を具体的に研究してみるというのはどうだろう。"
  },
  {
    "start": 1584562,
    "end": 1594214,
    "text": "もしLSTMを非常に大規模なものにすると、同じ現象が現れるのであれば、この現象はアーキテクチャよりも普遍的なものなのかもしれない。"
  },
  {
    "start": 1594342,
    "end": 1600950,
    "text": "だから、現実的に言えば、最終的には、llmsを向上させたいのであれば、みんなNvidiaで働けばいいのではないかというのが、ひとつの興味深い疑問だと思う。"
  },
  {
    "start": 1601030,
    "end": 1613238,
    "text": "というのも、規模がすべてだとすれば、現実的に言えば、過去10年間のAIの進歩の多くは、Nvidiaがより優れたAIアクセラレーターを作るというアジェンダを非常にうまく実行できるかどうかにかかっているからだ。"
  },
  {
    "start": 1613354,
    "end": 1614914,
    "text": "彼らを助けるべきだ"
  },
  {
    "start": 1614952,
    "end": 1617140,
    "text": "これがNvidiaの採用情報ページだ。"
  },
  {
    "start": 1618550,
    "end": 1620450,
    "text": "私はNvidiaのスポンサーではない。"
  },
  {
    "start": 1622950,
    "end": 1623362,
    "text": "クールだ。"
  },
  {
    "start": 1623416,
    "end": 1627922,
    "text": "さて、あるレベルでは、これはトークの前半のようなもので、ここで終わってもいい。"
  },
  {
    "start": 1627976,
    "end": 1631554,
    "text": "ワークショップでは、他の講演者たちもすでにこのようなことを言っている。"
  },
  {
    "start": 1631602,
    "end": 1633030,
    "text": "AIは計算とデータだ。"
  },
  {
    "start": 1633100,
    "end": 1637320,
    "text": "コンピュートとデータを非常に大きくすれば、すべてがうまくいく。"
  },
  {
    "start": 1639310,
    "end": 1657694,
    "text": "私がいつも少し物足りないと思うのは、コミュニティとして、アルゴリズム面、計算面、より良い最適化手順、より良いモデル・アーキテクチャ、より良い損失関数などをどうするかを考えることに多くの時間を費やしてきたことだ。"
  },
  {
    "start": 1657732,
    "end": 1660222,
    "text": "これは文字通り、今では毎年何千もの論文になっている。"
  },
  {
    "start": 1660276,
    "end": 1661146,
    "text": "これは素晴らしい。"
  },
  {
    "start": 1661178,
    "end": 1662334,
    "text": "私たちはこのことについて多くのことを学んでいる。"
  },
  {
    "start": 1662372,
    "end": 1665510,
    "text": "何度も言っているように、トランスフォーマーはブレークスルーの終着点にある。"
  },
  {
    "start": 1665530,
    "end": 1670370,
    "text": "スケールがすべてなら、スケーリングで10倍や100倍を出すのは素晴らしいことだ。"
  },
  {
    "start": 1670710,
    "end": 1679298,
    "text": "というのも、複数の登壇者が、トレーニングデータがすべての話のカギを握っていることに同意しているからだ。"
  },
  {
    "start": 1679384,
    "end": 1680806,
    "text": "私たちはそのことについてあまり話しません。"
  },
  {
    "start": 1680828,
    "end": 1684614,
    "text": "私たちはこれを与えられたものとして受け止め、あとは他のことを話すだけだ。"
  },
  {
    "start": 1684652,
    "end": 1689160,
    "text": "実は、あと何分あるのかわからないのですが、この話の後半でやりたいことがあります。"
  },
  {
    "start": 1691450,
    "end": 1691910,
    "text": "40."
  },
  {
    "start": 1691980,
    "end": 1692742,
    "text": "よし、いいぞ。"
  },
  {
    "start": 1692876,
    "end": 1699098,
    "text": "講演の後半では、トレーニングデータをより深く理解するための私の研究についてもう少しお話ししたいと思います。"
  },
  {
    "start": 1699184,
    "end": 1703210,
    "text": "その前に、少し背景を説明しておこう。"
  },
  {
    "start": 1703290,
    "end": 1707662,
    "text": "なぜそう思うかというと、私たちはトレーニングデータの研究をあまりしていないからだ。"
  },
  {
    "start": 1707716,
    "end": 1715338,
    "text": "機械学習における現在のベンチマークパラダイムの本質的な限界は、データが常に固定されていることだと思う。"
  },
  {
    "start": 1715354,
    "end": 1718190,
    "text": "まず最初に、ベンチマークのパラダイムとはどういう意味なのか？"
  },
  {
    "start": 1718270,
    "end": 1722238,
    "text": "機械学習の経験的研究の多くは、基本的にこのレシピに従っている。"
  },
  {
    "start": 1722334,
    "end": 1732306,
    "text": "重要だと思われるデータセットについてはある程度合意している。そして、その重要なデータセットについて、正確さの数値を押し上げたり、当惑度の数値を押し下げたりすることが目標だ。"
  },
  {
    "start": 1732338,
    "end": 1733270,
    "text": "これはその一例だ。"
  },
  {
    "start": 1733340,
    "end": 1743674,
    "text": "コンピュータビジョンのMs Coco物体検出データセットは2015年に発表され、その後、コミュニティは時間をかけて着実にこれらのモデルを一歩一歩改良してきた。"
  },
  {
    "start": 1743712,
    "end": 1749222,
    "text": "その結果、MS Cuckooのような貧弱な固定物体検出トレーニングセットについて多くのことがわかった。"
  },
  {
    "start": 1749286,
    "end": 1750986,
    "text": "どうやって数字を押し上げるのか？"
  },
  {
    "start": 1751168,
    "end": 1753354,
    "text": "非常に慎重なアブレーション実験がたくさん行われた。"
  },
  {
    "start": 1753402,
    "end": 1754240,
    "text": "素晴らしいよ。"
  },
  {
    "start": 1754690,
    "end": 1766202,
    "text": "このフレームワークでは、実際に別のトレーニングセットを試したり、自分のモデルをより良く汎化させるためには、MSココのトレーニングセットのどこを変えればいいのかという質問をしたりする人はいない。"
  },
  {
    "start": 1766276,
    "end": 1769090,
    "text": "トレーニングデータを使った実験をしている論文がほとんどないこと。"
  },
  {
    "start": 1769160,
    "end": 1770866,
    "text": "こんなことを言うのは私だけではない。"
  },
  {
    "start": 1770968,
    "end": 1777426,
    "text": "2021年のコンピュータ・ヒューマン・インタラクション会議（KaI）で、この件に関する非常に素晴らしい論文が発表された。"
  },
  {
    "start": 1777608,
    "end": 1780182,
    "text": "みんな、データの仕事ではなく、モデルの仕事をしたがる。"
  },
  {
    "start": 1780236,
    "end": 1786582,
    "text": "データ・カスケードとハイステークス・AI』では、人々が常にデータではなくモデルで仕事をしたがる理由のいくつかを語っている。"
  },
  {
    "start": 1786636,
    "end": 1788680,
    "text": "では、その効果は？"
  },
  {
    "start": 1789050,
    "end": 1803162,
    "text": "アンドリュー・エンは2021年にデータ中心AIリソースハブを開始し、2021年には、Neuropsはデータセットとベンチマークに関する作業を促進し、特に強調するために、データセットとベンチマークのトラックも追加した。"
  },
  {
    "start": 1803226,
    "end": 1804858,
    "text": "これは急速に拡大している。"
  },
  {
    "start": 1804954,
    "end": 1810750,
    "text": "2023年の今年は、すでに1000件を超えるデータセットとベンチマークの提出があった。"
  },
  {
    "start": 1812370,
    "end": 1812782,
    "text": "クールだ。"
  },
  {
    "start": 1812836,
    "end": 1822114,
    "text": "講演の後半でお話ししたいのは、これらのデータセットが本当に重要だと私が考える理由を、もう少し強調して説明することです。"
  },
  {
    "start": 1822232,
    "end": 1828946,
    "text": "具体的には、コンピューター・ビジョンで学んだことに基づいて、信頼性の高い汎化を行うための鍵となるのだろうか？"
  },
  {
    "start": 1829138,
    "end": 1836120,
    "text": "講演の後半では、より良い事前学習データを構築するために、オープンソースAIで現在何が行われているのか？"
  },
  {
    "start": 1837290,
    "end": 1838374,
    "text": "オーケー、クールだ。"
  },
  {
    "start": 1838492,
    "end": 1839574,
    "text": "最初の部分から始めよう。"
  },
  {
    "start": 1839612,
    "end": 1844246,
    "text": "もう時間がないので、手短に動機を説明する。"
  },
  {
    "start": 1844438,
    "end": 1848442,
    "text": "機械学習の研究は素晴らしいもので、現実の世界に導入すれば多くの進歩がある。"
  },
  {
    "start": 1848496,
    "end": 1851942,
    "text": "他のスピーカーも言っているように、このワークショップはかなり難しい。"
  },
  {
    "start": 1852006,
    "end": 1857934,
    "text": "課題のひとつは、このようなモデルはしばしば、あるべきほど信頼性の高い一般化ができないということだ。"
  },
  {
    "start": 1858052,
    "end": 1860894,
    "text": "これはチャットボットの時代にも当てはまる。"
  },
  {
    "start": 1860932,
    "end": 1866214,
    "text": "最近のインタビューでエリア・ソッツケワは、彼の主な仮説をこう語っている。"
  },
  {
    "start": 1866362,
    "end": 1880840,
    "text": "2030年までに、LLMが多くの経済的価値を獲得できていないのであれば、彼の仮説では、主なボトルネックは信頼性である。"
  },
  {
    "start": 1883050,
    "end": 1883654,
    "text": "オーケー。"
  },
  {
    "start": 1883772,
    "end": 1887894,
    "text": "このことをよりよく理解するために、私のお気に入りのデータを見てみよう。"
  },
  {
    "start": 1887932,
    "end": 1896170,
    "text": "スペル・イメージネットは、ディープラーニングがコンピューター・ビジョンにおいて本当に重要であることを明らかにし、非常にインパクトのある進歩を遂げた。"
  },
  {
    "start": 1896510,
    "end": 1897434,
    "text": "誰もが見たことがあるだろう。"
  },
  {
    "start": 1897472,
    "end": 1902502,
    "text": "これは大規模な画像分類データセットで、100万枚のトレーニング画像、1000の異なるクラスがある。"
  },
  {
    "start": 1902646,
    "end": 1907680,
    "text": "ここでストックのイメージを持ち、それをゴールデンレトリバーなどに分類する必要がある。"
  },
  {
    "start": 1909490,
    "end": 1913390,
    "text": "10年以上にわたって多くの人々がこの問題に取り組んできた。"
  },
  {
    "start": 1913540,
    "end": 1921182,
    "text": ""
  },
  {
    "start": 1921246,
    "end": 1928486,
    "text": "他のデータ分布に適用したとしても、人間ほど信頼できるものではなかった。"
  },
  {
    "start": 1928588,
    "end": 1936898,
    "text": "その後、さまざまな研究者が、分布外のテストセットを作成することによって、これをより正確なものにした。"
  },
  {
    "start": 1936914,
    "end": 1942806,
    "text": "つまり、imagenetと同じクラス構造を持つテストセットがあるということだ。"
  },
  {
    "start": 1942838,
    "end": 1949334,
    "text": "イメージネットモデルを使えば、微調整なしで、新しいテストセットで直接評価することができます。"
  },
  {
    "start": 1949462,
    "end": 1952326,
    "text": "このテスト分布は何かが違う。"
  },
  {
    "start": 1952438,
    "end": 1958958,
    "text": "もしかしたら、クラウドソーシングの作業工程が変わったかもしれないし、スケッチなど別の方法を使うかもしれない。"
  },
  {
    "start": 1959124,
    "end": 1962298,
    "text": "バークレー校は、この研究の方向性で非常によく知られている。"
  },
  {
    "start": 1962394,
    "end": 1965120,
    "text": "Imagenet v twoとimagenetはどちらもここから。"
  },
  {
    "start": 1966390,
    "end": 1976434,
    "text": "分布の変化で何が起こっているのかを理解する一つの方法は、多くの異なるモデルをプロットすることだ。"
  },
  {
    "start": 1976472,
    "end": 1978466,
    "text": "このような散布図を見てみよう。"
  },
  {
    "start": 1978578,
    "end": 1980290,
    "text": "X軸イメージネット。"
  },
  {
    "start": 1980450,
    "end": 1981334,
    "text": "これが第1巻だ。"
  },
  {
    "start": 1981372,
    "end": 1983826,
    "text": "私はY軸でトレーニングを行った。"
  },
  {
    "start": 1983858,
    "end": 1985462,
    "text": "まずはimagenet b twoから。"
  },
  {
    "start": 1985596,
    "end": 1988678,
    "text": "そこで、80種類のモデルをプロットしてみた。"
  },
  {
    "start": 1988764,
    "end": 1997446,
    "text": "これらの青い点はすべて、VgG Resnet上の代表的なAlexnetから評価した1つの標準イメージネットモデルである。"
  },
  {
    "start": 1997478,
    "end": 2002380,
    "text": "これら広く使われているアーキテクチャはすべて、当時の最先端技術に対応したものであり、b7で効率的なものであった。"
  },
  {
    "start": 2002750,
    "end": 2007594,
    "text": "ここで興味深いのは、これらのモデルはすべて線形トレンドに忠実に従っているということだ。"
  },
  {
    "start": 2007642,
    "end": 2009022,
    "text": "それはなぜか？"
  },
  {
    "start": 2009076,
    "end": 2014294,
    "text": "今のところは、これをベースラインとすることにしよう。"
  },
  {
    "start": 2014362,
    "end": 2017102,
    "text": "誰かが新しいニューラルネットワークを発明したとしよう。"
  },
  {
    "start": 2017166,
    "end": 2021854,
    "text": "UmishはUmishnet、umagenetはimagenetで71％を獲得している。"
  },
  {
    "start": 2021982,
    "end": 2028206,
    "text": "そこで、umagenetが既存のネットワークよりも信頼性が高いかどうかを理解したい。"
  },
  {
    "start": 2028238,
    "end": 2029990,
    "text": "ということは、71％ということになる。"
  },
  {
    "start": 2030060,
    "end": 2031266,
    "text": "ベースラインに戻ろう。"
  },
  {
    "start": 2031378,
    "end": 2034306,
    "text": "V2のエミッションでは、58％の精度が期待できる。"
  },
  {
    "start": 2034338,
    "end": 2037734,
    "text": "もしウマジネットが58％の精度を出せば、オーケー、上出来だ。"
  },
  {
    "start": 2037772,
    "end": 2040498,
    "text": "基本的には、先行研究から予想されることだ。"
  },
  {
    "start": 2040604,
    "end": 2045100,
    "text": "私たちが本当に求めているのは、この赤い線の上にあるネットワークだ。"
  },
  {
    "start": 2045550,
    "end": 2046966,
    "text": "なぜこれがゴールなのか？"
  },
  {
    "start": 2047078,
    "end": 2049210,
    "text": "人間にはこの性質があることを知っているからだ。"
  },
  {
    "start": 2049280,
    "end": 2054330,
    "text": "我々は、専門家でないラベラーがイメージネットを分類し、V2で想像するという実験を行った。"
  },
  {
    "start": 2054400,
    "end": 2056734,
    "text": "となると、人間はまさにこの破線の上にいることになる。"
  },
  {
    "start": 2056772,
    "end": 2057614,
    "text": "Yはxに等しい。"
  },
  {
    "start": 2057652,
    "end": 2059966,
    "text": "人間には、この分布の変化による影響はない。"
  },
  {
    "start": 2059988,
    "end": 2061198,
    "text": "V2を想像してみてほしい。"
  },
  {
    "start": 2061284,
    "end": 2063178,
    "text": "イメージネットの列車ネットワークは"
  },
  {
    "start": 2063284,
    "end": 2068260,
    "text": "この赤い線より上になるように、私たちが効果的なロバストネスと呼ぶものを達成したモデルはあるのだろうか？"
  },
  {
    "start": 2069030,
    "end": 2070820,
    "text": "人間は訓練されているのか？"
  },
  {
    "start": 2072390,
    "end": 2073362,
    "text": "いい質問だ。"
  },
  {
    "start": 2073416,
    "end": 2073922,
    "text": "そうだね。"
  },
  {
    "start": 2074056,
    "end": 2075842,
    "text": "なぜ人間はラインより少し下なのか？"
  },
  {
    "start": 2075976,
    "end": 2077538,
    "text": "いや、人間がかかっているんだ。"
  },
  {
    "start": 2077704,
    "end": 2078034,
    "text": "オーケー。"
  },
  {
    "start": 2078072,
    "end": 2082278,
    "text": "いや、中には1％ポイントとか、本当に少し下回るものもある。"
  },
  {
    "start": 2082364,
    "end": 2085254,
    "text": "これは、V2のイメージがあるからだ。"
  },
  {
    "start": 2085292,
    "end": 2086578,
    "text": "もう少し騒々しい。"
  },
  {
    "start": 2086674,
    "end": 2087746,
    "text": "もう少し騒々しい。"
  },
  {
    "start": 2087778,
    "end": 2091370,
    "text": "この実験を行ったのは、どれだけノイズが多いかを知りたかったからだ。"
  },
  {
    "start": 2092830,
    "end": 2095126,
    "text": "つまり、人間たちに1000クラスを教える必要がある。"
  },
  {
    "start": 2095158,
    "end": 2096074,
    "text": "実のところ、まったく面倒なんだ。"
  },
  {
    "start": 2096112,
    "end": 2097114,
    "text": "決してこんなことはしない。"
  },
  {
    "start": 2097232,
    "end": 2099450,
    "text": "これについては後で詳しく説明しよう。"
  },
  {
    "start": 2099520,
    "end": 2100806,
    "text": "後のことは分からない。"
  },
  {
    "start": 2100848,
    "end": 2108970,
    "text": "画像を1000のカテゴリーに分類するのは、結局は自然な作業ではない。"
  },
  {
    "start": 2109050,
    "end": 2113646,
    "text": "トラツグミが何であるかを知っている人なら、このことを知っているはずだ。"
  },
  {
    "start": 2113668,
    "end": 2116914,
    "text": "これを130犬種とかやると、かなりヤバい。"
  },
  {
    "start": 2116952,
    "end": 2126654,
    "text": "とにかく、私はこのグラフを正しく理解しているのだろうか。最高のモデルは、人間を打ち負かすことによって、分布から人間にマッチしているということだ。"
  },
  {
    "start": 2126702,
    "end": 2127074,
    "text": "いや、申し訳ない。"
  },
  {
    "start": 2127112,
    "end": 2127714,
    "text": "いい指摘だ。"
  },
  {
    "start": 2127752,
    "end": 2128694,
    "text": "ああ、いい指摘だ。"
  },
  {
    "start": 2128812,
    "end": 2133254,
    "text": "このプロットを作ったとき、少し低く切りすぎた。"
  },
  {
    "start": 2133292,
    "end": 2138520,
    "text": "人間はimagenetの最高のネットワークと一致し、imagenetでも彼らを上回った。"
  },
  {
    "start": 2140330,
    "end": 2141400,
    "text": "もう一度言ってくれ。"
  },
  {
    "start": 2143390,
    "end": 2145642,
    "text": "最高のモデルも同様だった。"
  },
  {
    "start": 2145696,
    "end": 2149478,
    "text": "申し訳ないが、最高の人間はimagenetの最高のモデルと同じようにうまくやった。"
  },
  {
    "start": 2149574,
    "end": 2151502,
    "text": "あるいは、少しはマシだが、それほどでもない。"
  },
  {
    "start": 2151556,
    "end": 2155006,
    "text": "imagenetv 2では、彼らは間違いなく彼らを上回っていた。"
  },
  {
    "start": 2155108,
    "end": 2156000,
    "text": "なるほど。"
  },
  {
    "start": 2156370,
    "end": 2157102,
    "text": "了解した。"
  },
  {
    "start": 2157156,
    "end": 2157760,
    "text": "ありがとう。"
  },
  {
    "start": 2161490,
    "end": 2162974,
    "text": "オーケー、いい質問ばかりだ。"
  },
  {
    "start": 2163092,
    "end": 2170462,
    "text": "さて、次のポイントは、分布の変化に対するロバスト性について、多くの人々が取り組んできたということです。"
  },
  {
    "start": 2170526,
    "end": 2172494,
    "text": "このフレームワークの中で、彼らがどの程度通用するのか見てみよう。"
  },
  {
    "start": 2172622,
    "end": 2174434,
    "text": "これが私たちがやったことだ。"
  },
  {
    "start": 2174552,
    "end": 2179162,
    "text": "私たちは基本的に、さらに約100のモデルを走らせ、それらをこのプロットに加えた。"
  },
  {
    "start": 2179246,
    "end": 2184066,
    "text": "ルドウィグ、これは同じ筋書きに見えるだけかもしれない。"
  },
  {
    "start": 2184098,
    "end": 2185714,
    "text": "これは実に正しい反応だ。"
  },
  {
    "start": 2185842,
    "end": 2190842,
    "text": "基本的に、ここにある茶色のモデルはすべて赤のラインに非常に近い。"
  },
  {
    "start": 2190896,
    "end": 2195206,
    "text": "これらは、敵対的訓練、ランダム化平滑化で訓練されたモデルである。"
  },
  {
    "start": 2195238,
    "end": 2201820,
    "text": "敵対的なロバスト性、特別な形のデータ補強などの文脈で、人々が考え出したものすべてだ。"
  },
  {
    "start": 2203310,
    "end": 2207630,
    "text": "本当に際立っているのは、より多くのデータで訓練されたモデルだけだ。"
  },
  {
    "start": 2207700,
    "end": 2210606,
    "text": "全部を強調したわけではないが、赤線より上にあるものもある。"
  },
  {
    "start": 2210628,
    "end": 2212350,
    "text": "そのうちの3つを取り上げた。"
  },
  {
    "start": 2212500,
    "end": 2215070,
    "text": "右上はグーグルの実験。"
  },
  {
    "start": 2215140,
    "end": 2218434,
    "text": "彼らは3億枚の画像と内部データセットでトレーニングを行った。"
  },
  {
    "start": 2218552,
    "end": 2223982,
    "text": "それから、フェイスブックのデータもいくつかあります。フェイスブックは、10億枚の画像からなる社内データセットで訓練しています。"
  },
  {
    "start": 2224126,
    "end": 2227438,
    "text": "次に、全イメージネットで学習された1つのモデル。"
  },
  {
    "start": 2227454,
    "end": 2229054,
    "text": "1000万から2000万の画像"
  },
  {
    "start": 2229102,
    "end": 2234822,
    "text": "これらのデータポイントは、確かに1～2％ポイントほどラインを上回っている。"
  },
  {
    "start": 2234876,
    "end": 2238418,
    "text": "半分が満杯のグラス、半分が空っぽのグラス。"
  },
  {
    "start": 2238594,
    "end": 2242498,
    "text": "少なくとも、より多くのデータが役立つというシグナルはある。"
  },
  {
    "start": 2242594,
    "end": 2245722,
    "text": "これが本当に正しい直感であったことは、すぐにわかるだろう。"
  },
  {
    "start": 2245776,
    "end": 2247178,
    "text": "彼らも画像ネットで訓練されているのか、それとも。"
  },
  {
    "start": 2247184,
    "end": 2248874,
    "text": "いや、それらは素晴らしい点だ。"
  },
  {
    "start": 2248912,
    "end": 2254750,
    "text": "これらの緑色の点は、他のデータセットで事前に訓練され、その後、想像上で微調整される。"
  },
  {
    "start": 2256770,
    "end": 2259726,
    "text": "これはすべて、イメージネットV2の文脈の中でのことだった。"
  },
  {
    "start": 2259828,
    "end": 2262686,
    "text": "戦利品で作ったんだ。"
  },
  {
    "start": 2262708,
    "end": 2263406,
    "text": "V2を想像してみてほしい。"
  },
  {
    "start": 2263428,
    "end": 2265038,
    "text": "他のデータセットにも目を向けるべきかもしれない。"
  },
  {
    "start": 2265054,
    "end": 2265634,
    "text": "私たちはこれをやった。"
  },
  {
    "start": 2265672,
    "end": 2267538,
    "text": "オブジェクティブネットは私が関与していない。"
  },
  {
    "start": 2267624,
    "end": 2275390,
    "text": "objectnetは現在、全体では40％ポイントという大幅な落ち込みを見せているが、それでもかなり良好なリニアトレンドだ。"
  },
  {
    "start": 2275470,
    "end": 2283880,
    "text": "この右上のコーナーで目立つモデル、そして精度の高いレジームは、より多くのデータでトレーニングされたモデルである。"
  },
  {
    "start": 2284330,
    "end": 2286982,
    "text": "このような実験は他にもたくさんある。"
  },
  {
    "start": 2287036,
    "end": 2288470,
    "text": "まだまだ言いたいことはたくさんある。"
  },
  {
    "start": 2288540,
    "end": 2293946,
    "text": "時間の関係上、ここで多くの進歩をもたらしたものをお伝えしたい。"
  },
  {
    "start": 2294048,
    "end": 2296982,
    "text": "これはOpenAIのクリップモデルだ。"
  },
  {
    "start": 2297126,
    "end": 2300886,
    "text": "これは2021年に発表されたもので、テキストと画像をつないでいる。"
  },
  {
    "start": 2300918,
    "end": 2303454,
    "text": "彼らはマルチモーダル学習のために大きなリリースを行った。"
  },
  {
    "start": 2303572,
    "end": 2306558,
    "text": "クリップとダリは同じ日に登場した。"
  },
  {
    "start": 2306724,
    "end": 2315502,
    "text": "ブログ記事でも、クリップを発表した論文でも、私たちのロバストネス・フレームワークで自分たちのモデルを正確に評価していたことだ。"
  },
  {
    "start": 2315566,
    "end": 2322430,
    "text": "彼らは最大のクリップモデルであるクリップVaTlを取り上げ、imagenetで76.2％を獲得した。"
  },
  {
    "start": 2322510,
    "end": 2326478,
    "text": "イメージネットで訓練されたレゾネイト101と比較したが、同じ精度だった。"
  },
  {
    "start": 2326654,
    "end": 2331590,
    "text": "ということは、散布図において、同じX軸の値を持つということである。"
  },
  {
    "start": 2331660,
    "end": 2338674,
    "text": "分布外のテストセットを見るとき、私たちがやっているのは、まさに効果的なロバスト性を見ることだ。"
  },
  {
    "start": 2338802,
    "end": 2351798,
    "text": "そしてクリップはイマジネットV2でプラス6％、イマジネットRでプラス51、オブジェクトネットでプラス40、イマジネットスケッチでプラス35、そしてまだ話していないデータセットでプラス74となる。"
  },
  {
    "start": 2351824,
    "end": 2360522,
    "text": "イメージネットAは、これらのイメージネットのようなデータセットに取り組むと、非常に大きな利益を得ることができる。"
  },
  {
    "start": 2360586,
    "end": 2364206,
    "text": "一桁台の小さな改善なら、喜んで論文に書けるだろう。"
  },
  {
    "start": 2364308,
    "end": 2369794,
    "text": "二桁の大幅な改善と、イメージとB2のプラス6のようなもので、これはかなり小さい。"
  },
  {
    "start": 2369912,
    "end": 2371422,
    "text": "これでもまだ落差の半分だ。"
  },
  {
    "start": 2371486,
    "end": 2373460,
    "text": "これまで見たことがないほどだ。"
  },
  {
    "start": 2373830,
    "end": 2379854,
    "text": "OpenAIとおっしゃったところです。"
  },
  {
    "start": 2379902,
    "end": 2381090,
    "text": "これは少し抽象的だ。"
  },
  {
    "start": 2381170,
    "end": 2383378,
    "text": "これを考案したのは、やはりアレック・ラスフォードだ。"
  },
  {
    "start": 2383394,
    "end": 2386530,
    "text": "彼はGPTを発明した。"
  },
  {
    "start": 2386610,
    "end": 2388114,
    "text": "では、分配シフトを見てみよう。"
  },
  {
    "start": 2388162,
    "end": 2391130,
    "text": "その後、彼はクリップで多くの進歩を遂げた。"
  },
  {
    "start": 2392510,
    "end": 2393594,
    "text": "オーケー、クールだ。"
  },
  {
    "start": 2393712,
    "end": 2396134,
    "text": "これらは非常に広く使われているモデルとなっている。"
  },
  {
    "start": 2396182,
    "end": 2404906,
    "text": "昨日、抱きつきフェーズのモデルリポジトリをチェックしたところ、抱きつきフェーズのモデルで3番目によくダウンロードされているのは、OpenAIのクリップモデルの1つだった。"
  },
  {
    "start": 2405018,
    "end": 2408762,
    "text": "また、抱擁の段階で最も広く使われているコンピュータ・ビジョンのモデルでもある。"
  },
  {
    "start": 2408906,
    "end": 2417130,
    "text": "というのも、直感的に言えば、このようなロバストネス特性を手に入れれば、既製品をそのまま使うだけで、より有用なモデルになるからだ。"
  },
  {
    "start": 2417210,
    "end": 2420450,
    "text": "配給が違うとか、そんなことを気にする必要はない。"
  },
  {
    "start": 2420520,
    "end": 2422210,
    "text": "頻繁に微調整が必要ですか？"
  },
  {
    "start": 2422280,
    "end": 2425380,
    "text": "棚からクリップを取り出すだけでいい。"
  },
  {
    "start": 2426330,
    "end": 2432546,
    "text": "では、散布図の視覚化、X軸イメージネットでもう一度見てみよう。"
  },
  {
    "start": 2432578,
    "end": 2439590,
    "text": "以前と同じように、Y軸の分布のシフト、7つの平均をとった。"
  },
  {
    "start": 2439740,
    "end": 2441814,
    "text": "青い線はimagenetの学習済みモデル。"
  },
  {
    "start": 2441862,
    "end": 2448086,
    "text": "緑色の点は、それ以前のデータモデルで、オレンジ色の線はゼロショット・モードでのクリップだ。"
  },
  {
    "start": 2448118,
    "end": 2456586,
    "text": "クリップは、これらの7つの分布における事前モデルと破線のyとxの間のギャップの半分をほぼ埋めた。"
  },
  {
    "start": 2456618,
    "end": 2462794,
    "text": "だから、さっきも言ったように、これは私にとって非常に印象的なことだった。"
  },
  {
    "start": 2462922,
    "end": 2464782,
    "text": "ここには2つ目の興味深い洞察がある。"
  },
  {
    "start": 2464836,
    "end": 2467086,
    "text": "これは、今グレッグが言ったことを微調整することにつながる。"
  },
  {
    "start": 2467188,
    "end": 2474786,
    "text": "クリップモデルを微調整すれば赤い線が出るが、実は赤い線は前の緑の点よりもそれほど良くはない。"
  },
  {
    "start": 2474888,
    "end": 2477402,
    "text": "覚えておいてほしいのは、これらのグリーンポイントはすべて微調整が施されているということだ。"
  },
  {
    "start": 2477486,
    "end": 2482630,
    "text": "ここで重要なことは、ファインチューニングは純粋なロバストネスの多くを消し去ってしまうということだ。"
  },
  {
    "start": 2483050,
    "end": 2486338,
    "text": "これらのモデルはすべて微調整された。"
  },
  {
    "start": 2486354,
    "end": 2491606,
    "text": "もし、これらのモデルをゼロ・ショット評価で使っていたら、どれほどの結果を残せただろうか？"
  },
  {
    "start": 2491798,
    "end": 2497478,
    "text": "ここから生まれるもう一つの大きな疑問は、ロバスト性を保ちながら微調整ができるかということだ。"
  },
  {
    "start": 2497574,
    "end": 2501706,
    "text": "つ目の質問の答えはイエスだが、この件について話す時間はない。"
  },
  {
    "start": 2501728,
    "end": 2504530,
    "text": "これは、UWDAPの共同研究者との初めての論文だった。"
  },
  {
    "start": 2504630,
    "end": 2508480,
    "text": "その堅牢性がどこから来るのか、これから見ていくことにしよう。"
  },
  {
    "start": 2509970,
    "end": 2514974,
    "text": "ここで詳細を語るにはあまり時間がないが、この論文のタイトルがそれを物語っている。"
  },
  {
    "start": 2515012,
    "end": 2518986,
    "text": "データは分布の頑健性と対照的な言語イメージの事前トレーニングを決定する。"
  },
  {
    "start": 2519108,
    "end": 2528070,
    "text": "私たちがこの論文でやったことは、基本的に多くの異なる仮説を検証し、それぞれの仮説を検証するための実験を設定し、基本的に何が残ったかを見ることだった。"
  },
  {
    "start": 2528140,
    "end": 2530562,
    "text": "ひとつの明白な仮説は、言語監督である。"
  },
  {
    "start": 2530626,
    "end": 2533554,
    "text": "クリップの要点は、テキストと画像をつなげることだった。"
  },
  {
    "start": 2533682,
    "end": 2537346,
    "text": "コンピュータビジョンの言語監視ができるようになった。"
  },
  {
    "start": 2537378,
    "end": 2544122,
    "text": "もう1つの明らかな候補はトレーニング分布で、これは異なるトレーニング分布だからだ。"
  },
  {
    "start": 2544176,
    "end": 2548454,
    "text": "より多くの画像の損失関数で訓練され、教師ありに対してコントラストがある。"
  },
  {
    "start": 2548502,
    "end": 2553962,
    "text": "ほとんどのイメージネットモデルでは、テストタイムプロンプトはクリップとモデルアーキテクチャで行うことができます。"
  },
  {
    "start": 2554026,
    "end": 2560810,
    "text": "最高のクリップモデルはディビジョン・トランスフォーマーであり、それ以前の最高のイマジネットモデルである。"
  },
  {
    "start": 2560890,
    "end": 2563250,
    "text": "この論文では、基本的にそれらすべてに目を通す。"
  },
  {
    "start": 2563320,
    "end": 2571090,
    "text": "この効果的なロバストネスのフレームワークにおけるロバストネスは、すべてトレーニング分布に由来する。"
  },
  {
    "start": 2574310,
    "end": 2582802,
    "text": "トレーニングの分布というと、データの量だけでなく、実際の素晴らしいポイントが言語監督と重なるような気がするのですが？"
  },
  {
    "start": 2582866,
    "end": 2586486,
    "text": "というのも、言語がトレーニング分布に含まれることは、その一部だからだ。"
  },
  {
    "start": 2586668,
    "end": 2587014,
    "text": "そうだね。"
  },
  {
    "start": 2587052,
    "end": 2588674,
    "text": "オーケー、もう少し正確に言おう。"
  },
  {
    "start": 2588722,
    "end": 2589718,
    "text": "本当に素晴らしい質問だ。"
  },
  {
    "start": 2589804,
    "end": 2590642,
    "text": "ありがとう、ジェイコブ。"
  },
  {
    "start": 2590706,
    "end": 2595450,
    "text": "最初にはっきりさせておきたいのだが、トレーニング分布とトレーニングセットのサイズとはどういう意味なのだろうか？"
  },
  {
    "start": 2595600,
    "end": 2603614,
    "text": "ここで散布図に戻って、もしイメージネットのトレーニングデータがもっと多かったり少なかったりしたらどうなるだろうか？"
  },
  {
    "start": 2603812,
    "end": 2611358,
    "text": "これをテストするために、imagenetをエフェクター2 4816 32でサブサンプリングした。"
  },
  {
    "start": 2611444,
    "end": 2617762,
    "text": "興味深いのは、imagenetのデータ量を変えると、赤い線に沿って正確に動いていることだ。"
  },
  {
    "start": 2617816,
    "end": 2624034,
    "text": "トレーニングデータが少なくなるにつれて、精度はかなり低くなっていくが、常に同じラインにとどまっている。"
  },
  {
    "start": 2624072,
    "end": 2631590,
    "text": "だから、直感的に言えば、あなたが乗っている線形トレンドはトレーニング分布の関数なのだ。"
  },
  {
    "start": 2632250,
    "end": 2636070,
    "text": "これが、トレーニング分布とトレーニングセットのサイズを分離できる理由だ。"
  },
  {
    "start": 2636220,
    "end": 2639990,
    "text": "今、言語監督とトレーニングの配分について話したい。"
  },
  {
    "start": 2640150,
    "end": 2642250,
    "text": "ここで行ったのは次のようなことだ。"
  },
  {
    "start": 2642590,
    "end": 2646422,
    "text": "私たちはイマジェネットを採用し、さらに言語監督を加えた。"
  },
  {
    "start": 2646486,
    "end": 2654314,
    "text": "imagenetのいいところは、クリップのような注釈を持つウェブデータソースからimagenetが構築されていることだ。"
  },
  {
    "start": 2654442,
    "end": 2657290,
    "text": "なぜなら、特にimagenetはFlickrから作られたものだからだ。"
  },
  {
    "start": 2657450,
    "end": 2663914,
    "text": "基本的に私たちがしたことは、imagenetの半分を元のFlickrのソースに戻すために、すべてのデータを整理することでした。"
  },
  {
    "start": 2664042,
    "end": 2667150,
    "text": "そうすれば、Flickrから人間が書いたキャプションを手に入れることができる。"
  },
  {
    "start": 2667310,
    "end": 2673790,
    "text": "これを画像側のトレーニング分布がimagenetであるimagenetトレーニングセットとしてまとめる。"
  },
  {
    "start": 2673870,
    "end": 2676054,
    "text": "今は言語監督もいる。"
  },
  {
    "start": 2676172,
    "end": 2679190,
    "text": "これでは堅牢性はゼロだ。"
  },
  {
    "start": 2680970,
    "end": 2683254,
    "text": "逆の実験ももっとある。"
  },
  {
    "start": 2683372,
    "end": 2688706,
    "text": "よりロバストな画像テキストデータセットを、テキストを捨てて、よりロバストにできるか？"
  },
  {
    "start": 2688738,
    "end": 2689900,
    "text": "答えはイエスだ。"
  },
  {
    "start": 2691790,
    "end": 2694074,
    "text": "オーケー、クールだ。"
  },
  {
    "start": 2694112,
    "end": 2697866,
    "text": "トレーニングセットをもっとよく理解する必要があると確信したのは、基本的にこれがきっかけだった。"
  },
  {
    "start": 2697968,
    "end": 2701098,
    "text": "というのも、私はかなり長い間、配給外のシフトに携わっていたからだ。"
  },
  {
    "start": 2701184,
    "end": 2702314,
    "text": "何も役に立たなかったようだ。"
  },
  {
    "start": 2702352,
    "end": 2704720,
    "text": "それなら、クリップで留めて、水面から何もかもやってしまえ。"
  },
  {
    "start": 2705570,
    "end": 2706030,
    "text": "クールだ。"
  },
  {
    "start": 2706100,
    "end": 2710510,
    "text": "テキストガイド付き画像生成についてはもっと言いたいことがあるが、時間がない。"
  },
  {
    "start": 2710660,
    "end": 2717650,
    "text": "より良いプレトレーニング・データセットを構築するために、私たちが行っていることについて少しお話ししましょう。"
  },
  {
    "start": 2719750,
    "end": 2728500,
    "text": "その間に何を入れるのか、私には見当もつかないが、手始めに、一般に公開されている画像データセットをすべてダウンロードする。"
  },
  {
    "start": 2731290,
    "end": 2732422,
    "text": "それはかなり悪いことだ。"
  },
  {
    "start": 2732476,
    "end": 2733080,
    "text": "オーケー。"
  },
  {
    "start": 2733770,
    "end": 2736120,
    "text": "それが答えでないことは分かっている。"
  },
  {
    "start": 2737210,
    "end": 2741258,
    "text": "なるほど、ここには3つのレベルが重なっている。"
  },
  {
    "start": 2741344,
    "end": 2746634,
    "text": "最初のレベルのオーバーラップは、実際にオブジェクト・ネットまたはイメージ・ネットと2つのテスト・セットで訓練することだ。"
  },
  {
    "start": 2746672,
    "end": 2748266,
    "text": "それは非常にまずいことだ。"
  },
  {
    "start": 2748448,
    "end": 2749482,
    "text": "それはない。"
  },
  {
    "start": 2749616,
    "end": 2757534,
    "text": "次のレベルでは、imagenetトレーニングセットと、その他の厳選されたコンピュータビジョントレーニングセットでトレーニングする。"
  },
  {
    "start": 2757572,
    "end": 2758638,
    "text": "それもない。"
  },
  {
    "start": 2758724,
    "end": 2760158,
    "text": "なぜそんなことがわかるのか？"
  },
  {
    "start": 2760244,
    "end": 2761134,
    "text": "私は彼らに尋ねた。"
  },
  {
    "start": 2761172,
    "end": 2761966,
    "text": "そう言われたんだ。"
  },
  {
    "start": 2761988,
    "end": 2767234,
    "text": "今となっては、私たちは彼らを信用していないのかもしれない。"
  },
  {
    "start": 2767352,
    "end": 2770866,
    "text": "トレーニングや重複を取り除けば問題ない。"
  },
  {
    "start": 2770968,
    "end": 2776206,
    "text": "さて、3つ目のレベルは、もっと漠然とした意味的重複という概念はどうだろうか？"
  },
  {
    "start": 2776318,
    "end": 2779778,
    "text": "たぶん、私たちが見たデータセットのひとつは、イメージネットスケッチだと思う。"
  },
  {
    "start": 2779874,
    "end": 2782834,
    "text": "もしかしたら、あなたのクリップ、プレトレーニングデータには、もっとたくさんのスケッチがあるかもしれない。"
  },
  {
    "start": 2782882,
    "end": 2785894,
    "text": "これは間違いなく起こっていることの一部だと思う。"
  },
  {
    "start": 2785932,
    "end": 2795210,
    "text": "実際、このようなことが起こっているのだと思う。数字が上がることは分かっているが、それが何であるかは正確には言えない。"
  },
  {
    "start": 2795280,
    "end": 2800498,
    "text": "カバレッジという概念は、結局のところ、分配の一般化から決まるものだと思う。"
  },
  {
    "start": 2800614,
    "end": 2803278,
    "text": "アルゴリズムの改善はそれとは直交する。"
  },
  {
    "start": 2803364,
    "end": 2811290,
    "text": "このトレーニングの配分がどのような面で役立っているのか、もっと詳しく話すつもりだったのですか？"
  },
  {
    "start": 2811450,
    "end": 2811822,
    "text": "いや。"
  },
  {
    "start": 2811876,
    "end": 2815734,
    "text": "つまり、問題はトレーニングだったので、クリップのトレーニングセットはOpenAIに公開されている。"
  },
  {
    "start": 2815802,
    "end": 2826190,
    "text": "これが発表された後、基本的にオープンソースコミュニティの私たちにとって最初のステップは、比較可能な実験を実際に行えるように、この規模のトレーニングセットを再現しようということでした。"
  },
  {
    "start": 2826270,
    "end": 2827886,
    "text": "これが次に話すことだ。"
  },
  {
    "start": 2828008,
    "end": 2830374,
    "text": "これがライオン5Bのデータセットだ。"
  },
  {
    "start": 2830412,
    "end": 2835350,
    "text": "これは、さまざまな機関から多くの人々が参加したオープンソースの大規模なコラボレーションだった。"
  },
  {
    "start": 2835690,
    "end": 2840498,
    "text": "その名の通り、これは50億人の画像納税者のデータセットである。"
  },
  {
    "start": 2840674,
    "end": 2844006,
    "text": "振り返ってみると、実際には60億人のイメージ納税者がいた。"
  },
  {
    "start": 2844038,
    "end": 2853230,
    "text": "というのも、これは非常に分散された取り組みであり、執筆者グループを通じてこのことが伝わったとは言い難かったからだ。"
  },
  {
    "start": 2853300,
    "end": 2854954,
    "text": "多くのイメージ納税者だ。"
  },
  {
    "start": 2855082,
    "end": 2861614,
    "text": "具体的には、これまでの公開データセットの約50倍のデータ量である。"
  },
  {
    "start": 2861652,
    "end": 2871154,
    "text": "リヨンの前には、1億人の画像納税者がいるYCCの1億件のデータセットがありました。"
  },
  {
    "start": 2871192,
    "end": 2877190,
    "text": "これは、GoogleやOpenAIのようなところのプライベートなデータセットと同程度になる。"
  },
  {
    "start": 2877850,
    "end": 2886134,
    "text": "さて、これがどのように作られたかを少し説明するために、この図を見てみよう。"
  },
  {
    "start": 2886172,
    "end": 2889566,
    "text": "よし、まずは多くの候補者、特に一般的なクロールから始めよう。"
  },
  {
    "start": 2889618,
    "end": 2891510,
    "text": "これは500億の画像テキストペアである。"
  },
  {
    "start": 2891590,
    "end": 2894102,
    "text": "そして、ウェブページのフィルタリングを行う。"
  },
  {
    "start": 2894246,
    "end": 2899082,
    "text": "具体的には、関連するaltテキストを持つ画像のみを取り上げる。"
  },
  {
    "start": 2899216,
    "end": 2906698,
    "text": "HTMLを知っていれば、HTMLの画像タグは、画像が表示されていないときに表示されるaltテキストを持つことができます。"
  },
  {
    "start": 2906874,
    "end": 2910686,
    "text": "これは、画像とテキストが対になったデータの生のソースとして使用するものである。"
  },
  {
    "start": 2910868,
    "end": 2912554,
    "text": "これはとてもとてもうるさい。"
  },
  {
    "start": 2912602,
    "end": 2919854,
    "text": "私たちがひとつの処理として行っているのは、OpenAisクリップモデルVATP 32を使ってフィルタリングを行うことです。"
  },
  {
    "start": 2919902,
    "end": 2931206,
    "text": "画像を画像エンコーダーに通し、テキストをテキストエンコーダーに通し、余弦類似度を調べ、余弦類似度がある閾値以上であれば、その例を残し、そうでなければ捨てる。"
  },
  {
    "start": 2931308,
    "end": 2935062,
    "text": "これが最初のステップで、大規模な画像テキストデータセットを構築した。"
  },
  {
    "start": 2935196,
    "end": 2937334,
    "text": "このステップには多くの疑問がある。"
  },
  {
    "start": 2937372,
    "end": 2944922,
    "text": "たとえば、フィルタリングにB32を使うと、このデータセットで訓練したときにB32以上のモデルができないのではないかという懸念がありました。"
  },
  {
    "start": 2944976,
    "end": 2947946,
    "text": "幸いなことに、そのようなことにはならなかった。"
  },
  {
    "start": 2948048,
    "end": 2950234,
    "text": "私たちは実際に、これを使ってかなり優れたモデルを訓練することができる。"
  },
  {
    "start": 2950272,
    "end": 2951950,
    "text": "つの比較を見てみよう。"
  },
  {
    "start": 2952370,
    "end": 2958586,
    "text": "小型クリップモデルb 32、ビジョントランス、OpenAIとLyon、2つの異なるモデル。"
  },
  {
    "start": 2958618,
    "end": 2963920,
    "text": "トレーニングの精度は基本的に同じで、0.4.3ポイントほど低かった。"
  },
  {
    "start": 2964930,
    "end": 2965874,
    "text": "これは本当にいい。"
  },
  {
    "start": 2965912,
    "end": 2968418,
    "text": "全体として、リヨンは広く使われるデータセットとなった。"
  },
  {
    "start": 2968504,
    "end": 2970686,
    "text": "安定した拡散については、誰もが耳にしたことがあると思う。"
  },
  {
    "start": 2970718,
    "end": 2974050,
    "text": "画像生成モデルは、すべてリヨンでトレーニングされたものだ。"
  },
  {
    "start": 2975030,
    "end": 2988514,
    "text": "ここで悪いニュースは、クラブモデルをスケールアップするにつれて、ライオンのデータセットで得られるものと、OpenAIのモデルで得られるものとの間に、ますます大きなギャップが見られるようになったことだ。"
  },
  {
    "start": 2988562,
    "end": 2994778,
    "text": "イメージネットのL 14クリップは、OpenAIのモデルより3ポイントほど悪い。"
  },
  {
    "start": 2994864,
    "end": 3000314,
    "text": "ということは、objectnetのライオン訓練クリップは、オープンAIモデルよりも9ポイント悪いということになる。"
  },
  {
    "start": 3000352,
    "end": 3005706,
    "text": "クリップ・モデルには、その素晴らしい流通特性から大きな期待を寄せていただけに、これは少々残念だった。"
  },
  {
    "start": 3005818,
    "end": 3011466,
    "text": "今、この分布外のテストされたオブジェクトネットでは、リヨンで訓練されたクリップが突然悪くなった。"
  },
  {
    "start": 3011498,
    "end": 3016138,
    "text": "ライオンの次の明白な疑問は、ライオンを改善できるかということだった。"
  },
  {
    "start": 3016314,
    "end": 3023186,
    "text": "グレッグが求めているような洞察力があれば、より原則的な方法でそれを実現できるだろう。"
  },
  {
    "start": 3023288,
    "end": 3025898,
    "text": "だから、私たちがしたことは、この方向への一歩なのだ。"
  },
  {
    "start": 3025934,
    "end": 3030790,
    "text": "現在、次の大規模なオープンソースコラボレーションとして、データコンプがある。"
  },
  {
    "start": 3030860,
    "end": 3033506,
    "text": "データコンプはデータセット競争を意味する。"
  },
  {
    "start": 3033618,
    "end": 3041558,
    "text": "このアイデアは、基本的には、標準的な機械学習のパラダイム、ベンチマークのパラダイムを逆転させようというものだ。"
  },
  {
    "start": 3041574,
    "end": 3054026,
    "text": "冒頭で述べたように、現在、これは固定されたトレーニングセットのようなもので、研究者はより良い損失関数、ネットワーク・アーキテクチャ、最適化アルゴリズムなど、モデリング側で反復するが、トレーニングデータは常に固定されたままである。"
  },
  {
    "start": 3054058,
    "end": 3056862,
    "text": "データコンプで、よし、これを反転させてみよう。"
  },
  {
    "start": 3056996,
    "end": 3058782,
    "text": "トレーニングコードをお渡しします。"
  },
  {
    "start": 3058836,
    "end": 3060810,
    "text": "モデルのトレーニングコードが修正された。"
  },
  {
    "start": 3060970,
    "end": 3064880,
    "text": "トレーニングセットを改善することで、最終モデルを改善することになっている。"
  },
  {
    "start": 3065810,
    "end": 3075720,
    "text": "こうして多くの人が参加してくれれば、モデル中心のAI研究と同じように、データセットに関する共同研究のプロセスが確立されるだろう。"
  },
  {
    "start": 3076330,
    "end": 3078630,
    "text": "リヨンの予算は？"
  },
  {
    "start": 3079050,
    "end": 3081670,
    "text": "リヨンは非常に安くできた。"
  },
  {
    "start": 3083370,
    "end": 3090530,
    "text": "今はもう少しリソースがあるが、このオープンソースのAIの中には、非常に機知に富んだ人たちもいる。"
  },
  {
    "start": 3090610,
    "end": 3100218,
    "text": "彼らは中国のクラウドで、月々1ドル程度でCPUノードを手に入れられる安価な車輪のようなものを見つけ、できる限り多くのCPUノードを手に入れた。"
  },
  {
    "start": 3100384,
    "end": 3107600,
    "text": "わずかな予算で3点から9点しか取れなかったと自慢することができる。"
  },
  {
    "start": 3108290,
    "end": 3109806,
    "text": "すぐに自慢するつもりだ。"
  },
  {
    "start": 3109908,
    "end": 3110960,
    "text": "待ってくれ。"
  },
  {
    "start": 3112130,
    "end": 3114030,
    "text": "本当にそこにいるとき、私たちは自慢するだろう。"
  },
  {
    "start": 3114180,
    "end": 3115230,
    "text": "よし、いいぞ。"
  },
  {
    "start": 3115380,
    "end": 3116958,
    "text": "励ましをありがとう。"
  },
  {
    "start": 3117134,
    "end": 3117474,
    "text": "いいね。"
  },
  {
    "start": 3117512,
    "end": 3120802,
    "text": "もう時間がないのだから、続けよう。"
  },
  {
    "start": 3120856,
    "end": 3123170,
    "text": "自慢できるようになりたいとは思っている。"
  },
  {
    "start": 3124150,
    "end": 3126578,
    "text": "では、データの一部として何をするのか？"
  },
  {
    "start": 3126664,
    "end": 3140134,
    "text": "誰かのモデルを改良したい場合、その人のパイトーチのコードをダウンロードして5行変えれば、次のイクレオペイへの道半ばだからだ。"
  },
  {
    "start": 3140262,
    "end": 3152730,
    "text": "データセットのリサーチでは、誰かのデータセットをダウンロードするが、実際にデータセットをキュレートするために人々が社内で構築したツールはない。"
  },
  {
    "start": 3152800,
    "end": 3155722,
    "text": "通常、MS Cocoやimagenetのようなデータセットリリース。"
  },
  {
    "start": 3155786,
    "end": 3158426,
    "text": "データダンプだが、周辺のツールは何もない。"
  },
  {
    "start": 3158458,
    "end": 3164686,
    "text": "私たちがデータコムで基本的にやったことは、ライオンをゼロからやり直すことだった。"
  },
  {
    "start": 3164708,
    "end": 3168798,
    "text": "我々は基本的にツールを提供するので、簡単に自分のライオンを作ることができる。"
  },
  {
    "start": 3168974,
    "end": 3174414,
    "text": "これをできるだけ簡単にするために、私たちは画像納税者の大規模なデータプールを提供しています。"
  },
  {
    "start": 3174462,
    "end": 3177310,
    "text": "最大規模の128億円のイメージ納税者。"
  },
  {
    "start": 3177390,
    "end": 3181286,
    "text": "だから、10分の1、3倍にする。"
  },
  {
    "start": 3181388,
    "end": 3185670,
    "text": "ここで必要な計算は、画像モデルを微調整するようなものだ。"
  },
  {
    "start": 3185740,
    "end": 3188070,
    "text": "これは本当にどの学術団体にもできることだ。"
  },
  {
    "start": 3188140,
    "end": 3194966,
    "text": "これは、1億2,800万スケールでゼロからモデルを作るのではなく、画像をトレーニングするようなものだ。"
  },
  {
    "start": 3195158,
    "end": 3205982,
    "text": "そして参加者は、私たちが提供するプールの新しいサブセットかサブセットを提出することができる。"
  },
  {
    "start": 3206116,
    "end": 3215854,
    "text": "これはフィルタリングのトラックで、基本的には、未キュレーションのウェブスケールデータセットから実際のクリップトレーニングセットに移行するための、より良いフィルタリング方法を提案する。"
  },
  {
    "start": 3215972,
    "end": 3221934,
    "text": "あるいは、私はあなたの共通のクロール・プールが好きではない。"
  },
  {
    "start": 3221982,
    "end": 3223780,
    "text": "これは自分のデータトラックを持ち込むためだ。"
  },
  {
    "start": 3224150,
    "end": 3226034,
    "text": "これは私たちが作ったようなものだ。"
  },
  {
    "start": 3226072,
    "end": 3229826,
    "text": "このフレームワークで実際に進歩があるのか？"
  },
  {
    "start": 3229938,
    "end": 3232322,
    "text": "私たち自身、このことをとても知りたかった。"
  },
  {
    "start": 3232466,
    "end": 3239878,
    "text": "このデータコム・フレームワークで、いくつかの簡単なテクニックを使ったベースラインデータセットも提供した。"
  },
  {
    "start": 3239974,
    "end": 3242586,
    "text": "これが新しいデータコム・トレーニング・セットだ。"
  },
  {
    "start": 3242688,
    "end": 3245878,
    "text": "これをOpenAIとLyonのトレーニングセットと比較してみよう。"
  },
  {
    "start": 3245974,
    "end": 3256426,
    "text": "素晴らしいことに、我々はOpenAIのトレーニングセットをイメージネットで4ポイント、オブジェクトネットで5ポイント上回ることができた。"
  },
  {
    "start": 3256458,
    "end": 3262960,
    "text": "特に今、オブジェクトネットのライオンと比較すると、14ポイントの改善が見られる。"
  },
  {
    "start": 3263330,
    "end": 3269874,
    "text": "さて、スコットの励ましの言葉に従って、これが初の公開トレーニングセットであることを自慢しよう。"
  },
  {
    "start": 3269912,
    "end": 3271822,
    "text": "これはOpenAIのものよりいい。"
  },
  {
    "start": 3271966,
    "end": 3276078,
    "text": "これはアンソロピック・アップルなどですでに使われている。"
  },
  {
    "start": 3276174,
    "end": 3279960,
    "text": "CMUの研究者も使っているし、メタの研究者も使っている。"
  },
  {
    "start": 3280330,
    "end": 3286162,
    "text": "先にも述べたように、すべての鍵は規模にある。"
  },
  {
    "start": 3286306,
    "end": 3291734,
    "text": "特に、我々は常にコンピュート倍率の観点から改善を考えるべきだ。"
  },
  {
    "start": 3291862,
    "end": 3299542,
    "text": "これを実行すると、データコムから得られるデータセットの向上は、リヨンに比べて9倍のコンピュート向上となる。"
  },
  {
    "start": 3299606,
    "end": 3307646,
    "text": "クリップモデルをLyonで79％の精度に押し上げることはすでに可能だが、モデルをもっと大きくして、もっと長く訓練する必要があった。"
  },
  {
    "start": 3307748,
    "end": 3323330,
    "text": "これは、研究者たちがすでにデータコムで構築した79％の精度を得るために必要なデータの9倍のコストがかかる。"
  },
  {
    "start": 3323830,
    "end": 3335222,
    "text": "そして4月、アンディ・ジョーンズというアンソロピックの研究者から、C4とデータコンプの早期アクセスに対するお礼として、ギフトバスケットを送りたいという嬉しいメールをもらった。"
  },
  {
    "start": 3335356,
    "end": 3339510,
    "text": "貧乏学究の私にとって、ギフトバスケットを送ってくれる人はいつも嬉しい。"
  },
  {
    "start": 3341710,
    "end": 3342122,
    "text": "クールだ。"
  },
  {
    "start": 3342176,
    "end": 3344940,
    "text": "さて、これで基本的に時間がなくなった。"
  },
  {
    "start": 3347870,
    "end": 3348602,
    "text": "次のレベルへ"
  },
  {
    "start": 3348656,
    "end": 3349162,
    "text": "次のレベルへ"
  },
  {
    "start": 3349216,
    "end": 3352860,
    "text": "今のところはクールでいい。"
  },
  {
    "start": 3353550,
    "end": 3357150,
    "text": "このデータセットをより良いものにするために私たちがしていることについて、もう少しお話ししましょう。"
  },
  {
    "start": 3357220,
    "end": 3359790,
    "text": "スタンドに主義主張があるとは言いたくない。"
  },
  {
    "start": 3359860,
    "end": 3364634,
    "text": "全体的にかなりハチャメチャだった。"
  },
  {
    "start": 3364682,
    "end": 3372366,
    "text": "今、私たちや他の人たちがこれを基に、よりコントロールされた実験を行い、これらのトレーニング前のデータセットで何が本当に重要なのかを理解できることを願っている。"
  },
  {
    "start": 3372478,
    "end": 3377006,
    "text": "特筆すべきは、やはりスケーリング法の重要性だろう。"
  },
  {
    "start": 3377038,
    "end": 3382950,
    "text": "講演の冒頭で、モデル開発には費用がかかるため、スケーリング法は超重要であると述べた。"
  },
  {
    "start": 3383370,
    "end": 3391718,
    "text": "スコットが言ったように、こういったことには予算が必要で、その予算は決して十分なものではないからだ。"
  },
  {
    "start": 3391804,
    "end": 3393526,
    "text": "あなたは少し思慮深くなければならない。"
  },
  {
    "start": 3393638,
    "end": 3396682,
    "text": "私たちがやったことは、先ほども言ったように、データ・カンプに異なるスケールを持たせたことだ。"
  },
  {
    "start": 3396736,
    "end": 3399306,
    "text": "大、中、小がここで強調したものだ。"
  },
  {
    "start": 3399408,
    "end": 3403078,
    "text": "このプロットのすべての点は、1つのフィルタリング方法である。"
  },
  {
    "start": 3403174,
    "end": 3409434,
    "text": "これで、あるフィルタリング手法が小規模なデータ処理と中規模なデータ処理とで、どの程度うまく機能するかを相関させることができる。"
  },
  {
    "start": 3409482,
    "end": 3414250,
    "text": "あるいは、私たちのフィルタリング方法は、中規模データと大規模データの比較において、どの程度の効果があるのだろうか？"
  },
  {
    "start": 3414330,
    "end": 3417474,
    "text": "このカタパルトは不完全だが、かなりいい。"
  },
  {
    "start": 3417512,
    "end": 3431186,
    "text": "基本的に、最先端のデータセットを作成するために私たちが行ったことは、実験の70％は小規模と中規模で、大規模なものは2、3回、そして超大規模なものはごく少数だった。"
  },
  {
    "start": 3431218,
    "end": 3436646,
    "text": "結局、どのようなフィルタリングが最も効果的かという実験は、すべて小型のもので行われた。"
  },
  {
    "start": 3436668,
    "end": 3442182,
    "text": "つまり、小規模や中規模でうまくいけば、次の大規模でもうまくいくだろうという考えだ。"
  },
  {
    "start": 3442246,
    "end": 3444220,
    "text": "大体は、それでうまくいった。"
  },
  {
    "start": 3445150,
    "end": 3449814,
    "text": "現在、CMUのアディティやSQLのグループなど、他のグループによる研究が進んでいる。"
  },
  {
    "start": 3449862,
    "end": 3452854,
    "text": "彼らはこれに取り組み始め、最近、いくつかの素晴らしい改善がなされた。"
  },
  {
    "start": 3452982,
    "end": 3459930,
    "text": "私たちが現在やっていることは、基本的には同じことだが、llmsでは同じ話なので、より良い事前トレーニング・データセットが必要なのだ。"
  },
  {
    "start": 3460010,
    "end": 3462218,
    "text": "時間の都合上、結論を言おう。"
  },
  {
    "start": 3462314,
    "end": 3466402,
    "text": "講演の最初の部分は、AIは計算とデータだ。"
  },
  {
    "start": 3466536,
    "end": 3472942,
    "text": "コンピュート・サイドの多くの重要なイノベーションは、コンピュート・マルチプライヤーと考えることで統一できると思う。"
  },
  {
    "start": 3473006,
    "end": 3477082,
    "text": "大規模な言語モデルの学習効率はどの程度向上しているのだろうか？"
  },
  {
    "start": 3477246,
    "end": 3479702,
    "text": "となると、トランスは超重要だ。"
  },
  {
    "start": 3479836,
    "end": 3485270,
    "text": "結局のところ、1つの言語だけを見ているのであれば、それは本当に大きな計算能力の向上なのかもしれない。"
  },
  {
    "start": 3486890,
    "end": 3491686,
    "text": "トランスフォーマーにとって公平であるように、アーキテクチャーはコンピュート以上の利点をあまり持ち得ない。"
  },
  {
    "start": 3491718,
    "end": 3496170,
    "text": "マルチモダリティは、トランスフォーマーが本当に輝く一例だと思う。"
  },
  {
    "start": 3497630,
    "end": 3509818,
    "text": "次のポイントは、AIの半分を占める計算能力について話したが、少なくとも我々が見てきた実験に基づくと、信頼性の高い汎化には大規模なデータセットが本当に重要だということだ。"
  },
  {
    "start": 3509914,
    "end": 3514382,
    "text": "クラブと同様、テキスト生成におけるこれらすべての改良の背後にあるのもクラブだ。"
  },
  {
    "start": 3514526,
    "end": 3517118,
    "text": "申し訳ないが、画像生成、テキストガイド付き画像生成だ。"
  },
  {
    "start": 3517214,
    "end": 3522322,
    "text": "これもまた、正確なアーキテクチャが実はそれほど重要ではない例である。"
  },
  {
    "start": 3522376,
    "end": 3528158,
    "text": "画像生成におけるこれらのエキサイティングな進歩のように、通常、人々はこれらを拡散モデルとして考えている。"
  },
  {
    "start": 3528254,
    "end": 3532146,
    "text": "標準的な自己回帰モデルでも、基本的にはまったく同じ品質が得られる。"
  },
  {
    "start": 3532178,
    "end": 3538150,
    "text": "画像空間で次のトークンを予測するだけで、安定した拡散と同じように美しい画像が得られる。"
  },
  {
    "start": 3539610,
    "end": 3540070,
    "text": "クールだ。"
  },
  {
    "start": 3540140,
    "end": 3540374,
    "text": "オーケー。"
  },
  {
    "start": 3540412,
    "end": 3544458,
    "text": "そして最後のポイントは、現在のデータセットはまだかなり場当たり的な方法で組み立てられているということだ。"
  },
  {
    "start": 3544544,
    "end": 3548106,
    "text": "トレーニングの飽和プロセスを改善する余地は大いにあると思う。"
  },
  {
    "start": 3548288,
    "end": 3548730,
    "text": "クールだ。"
  },
  {
    "start": 3548800,
    "end": 3550734,
    "text": "残り30秒。"
  },
  {
    "start": 3550772,
    "end": 3552030,
    "text": "聞いてくれてありがとう。"
  },
  {
    "start": 3558130,
    "end": 3560160,
    "text": "質問の時間が少しあります。"
  },
  {
    "start": 3572790,
    "end": 3581510,
    "text": "成功したデータフィルタリングや、特定のデータセットをより成功させた新しいソースについてコメントはありますか？"
  },
  {
    "start": 3582250,
    "end": 3584294,
    "text": "いや、いい質問だから聞いてみたら？"
  },
  {
    "start": 3584332,
    "end": 3586134,
    "text": "オーケー、まったくその通りだ。"
  },
  {
    "start": 3586172,
    "end": 3588386,
    "text": "ライオンのデータはどのように比較されるのか？"
  },
  {
    "start": 3588578,
    "end": 3589682,
    "text": "ソースではない。"
  },
  {
    "start": 3589746,
    "end": 3591670,
    "text": "どちらもコマンドクロール出身だ。"
  },
  {
    "start": 3592650,
    "end": 3595702,
    "text": "データソースを増やすことで改善の余地はあると思う。"
  },
  {
    "start": 3595766,
    "end": 3600890,
    "text": "物事をシンプルに保つために、同じデータソース内でどれだけ改善できるかを確かめたかった。"
  },
  {
    "start": 3601550,
    "end": 3605994,
    "text": "基本的に、ここでやったのはクリップフィルターの微調整だ。"
  },
  {
    "start": 3606122,
    "end": 3610638,
    "text": "それから、英語のフィルタリングを捨てたんだ。"
  },
  {
    "start": 3610724,
    "end": 3613130,
    "text": "下流の評価タスクはすべて英語である。"
  },
  {
    "start": 3613210,
    "end": 3616746,
    "text": "英語のフィルタリングはいいアイデアかもしれない。"
  },
  {
    "start": 3616788,
    "end": 3618226,
    "text": "あなたにはもっと英語のコンテンツがある。"
  },
  {
    "start": 3618408,
    "end": 3620740,
    "text": "それを取り除けば、事態は好転することがわかった。"
  },
  {
    "start": 3621110,
    "end": 3625362,
    "text": "直感的にそう思うのは、言語のフィルタリングは見た目ほど明確ではないからだ。"
  },
  {
    "start": 3625416,
    "end": 3626798,
    "text": "パリの写真をお持ちですね。"
  },
  {
    "start": 3626894,
    "end": 3628002,
    "text": "キャプションはパリ。"
  },
  {
    "start": 3628066,
    "end": 3629910,
    "text": "これは英語ですか、フランス語ですか、それともドイツ語ですか？"
  },
  {
    "start": 3630250,
    "end": 3635240,
    "text": "結局のところ、クリップモデルはキャプションを理解しているかどうかを十分に理解している。"
  },
  {
    "start": 3636330,
    "end": 3641340,
    "text": "そして3つ目は、画質フィルターを追加したことだ。"
  },
  {
    "start": 3643070,
    "end": 3645260,
    "text": "他の人たちはどうなのか、とても興味がある。"
  },
  {
    "start": 3647390,
    "end": 3648700,
    "text": "他に質問は？"
  },
  {
    "start": 3655010,
    "end": 3657806,
    "text": "ええ、たぶん、いい話をありがとうございました。"
  },
  {
    "start": 3657988,
    "end": 3660510,
    "text": "ああ、そのフォローアップのためかな。"
  },
  {
    "start": 3660580,
    "end": 3660926,
    "text": "そんなところだ。"
  },
  {
    "start": 3660948,
    "end": 3672420,
    "text": "一般論として、もし私があなたの直感に基づいてデータセットを構築するとしたら、そのデータセットを良いものにするために念頭に置くべき、第一に重要な特徴は何でしょうか？"
  },
  {
    "start": 3673430,
    "end": 3676146,
    "text": "第一に、やはりスケールが重要だと思う。"
  },
  {
    "start": 3676248,
    "end": 3679238,
    "text": "できるだけ多くのイメージ・テキスト・ペアを手に入れよう。"
  },
  {
    "start": 3679324,
    "end": 3684226,
    "text": "ある時点で計算コストが高くなり、小規模なアブレーション研究を行うことになる。"
  },
  {
    "start": 3684258,
    "end": 3691718,
    "text": "第一に、データは多ければ多いほどいい。"
  },
  {
    "start": 3691804,
    "end": 3698182,
    "text": "というのも、OpenAIのデータセットが優れている理由のひとつは、一般的なクロール以上のものを使用しているからだ。"
  },
  {
    "start": 3698246,
    "end": 3700982,
    "text": "今、私たちは一般的なクロールをするだけで、彼らを凌駕している。"
  },
  {
    "start": 3701046,
    "end": 3702474,
    "text": "実はよくわからないんだ。"
  },
  {
    "start": 3702592,
    "end": 3703090,
    "text": "場合によるね。"
  },
  {
    "start": 3703110,
    "end": 3709898,
    "text": "あるドメインでのパフォーマンスを重視するなら、そのドメインから汎用データセットのデータをもっと入手すべきだ、というようなことだ。"
  },
  {
    "start": 3709994,
    "end": 3711360,
    "text": "今のところ不明だ。"
  },
  {
    "start": 3714930,
    "end": 3725414,
    "text": "でも、もし私たちが多様性を極限まで高めてしまったら、均一な分布になってしまう。"
  },
  {
    "start": 3725452,
    "end": 3730978,
    "text": "その答えを知りたいよ、ジェイコブ。"
  },
  {
    "start": 3731154,
    "end": 3739042,
    "text": "素晴らしい質問だ。"
  },
  {
    "start": 3739106,
    "end": 3739494,
    "text": "その通りだ。"
  },
  {
    "start": 3739532,
    "end": 3743740,
    "text": "また、ガウス分布のような均一な分布も必要ない。"
  },
  {
    "start": 3744190,
    "end": 3748646,
    "text": "今のところ、うまくいっているのは、ただひたすら指標を追い求めることだと思う。"
  },
  {
    "start": 3748678,
    "end": 3757200,
    "text": "例えば、あなたが思いついたすべての革新が、実際にチェックアウトされるかどうかを非常に注意深く測定し、時間が経つにつれて物事が積み重なっていくようなプロセスを設定するのです。"
  },
  {
    "start": 3757970,
    "end": 3758960,
    "text": "ありがとう。"
  },
  {
    "start": 3763010,
    "end": 3769598,
    "text": "因数分解をせずに微調整を行う方法はありますか？"
  },
  {
    "start": 3769694,
    "end": 3771298,
    "text": "そうそう、これはいい質問だね。"
  },
  {
    "start": 3771384,
    "end": 3779986,
    "text": "というのも、この論文では次のように説明しているからだ。"
  },
  {
    "start": 3780018,
    "end": 3785186,
    "text": "ロバスト性は高いが、下流タスクのパフォーマンスはあまり良くない。"
  },
  {
    "start": 3785298,
    "end": 3790134,
    "text": "あなたは、ロバスト性は平凡だが、下流タスクのパフォーマンスは高い、微調整されたモデルを手に入れた。"
  },
  {
    "start": 3790262,
    "end": 3794140,
    "text": "2つのウェイトを足して、それで終わり。"
  },
  {
    "start": 3796670,
    "end": 3797034,
    "text": "オーケー。"
  },
  {
    "start": 3797072,
    "end": 3801462,
    "text": "予測空間で平均化することもできるが、それはアンサンブルだ。"
  },
  {
    "start": 3801526,
    "end": 3807850,
    "text": "アンサンブルの問題点は、2つのモデルを持つことになり、推論時間が少し面倒になることだ。"
  },
  {
    "start": 3807930,
    "end": 3808842,
    "text": "とても興味がある。"
  },
  {
    "start": 3808906,
    "end": 3813230,
    "text": "プリンスタイムで計算オーバーヘッドなしに微調整のようなことができるのだろうか？"
  },
  {
    "start": 3813300,
    "end": 3816106,
    "text": "答えは、ウエイトを平均化するだけでいい。"
  },
  {
    "start": 3816138,
    "end": 3817102,
    "text": "ちょっとばかしね。"
  },
  {
    "start": 3817166,
    "end": 3823138,
    "text": "でも、今のところ、これ以上のものは見つかっていない。"
  },
  {
    "start": 3823224,
    "end": 3826594,
    "text": "最初にこれを見たとき、よし、すごい、ここに信号がある、と思った。"
  },
  {
    "start": 3826632,
    "end": 3827558,
    "text": "もっと良くすることができる。"
  },
  {
    "start": 3827644,
    "end": 3831862,
    "text": "もっと厳密な、最適化ベースのことをすれば、それを上回ることができるだろう。"
  },
  {
    "start": 3831916,
    "end": 3833800,
    "text": "それを上回ることはできなかった。"
  },
  {
    "start": 3835530,
    "end": 3835942,
    "text": "いや。"
  },
  {
    "start": 3835996,
    "end": 3842230,
    "text": "ベースラインと比較したり、異なるデータセットを試したりする実験が30ページもある。"
  },
  {
    "start": 3842650,
    "end": 3844920,
    "text": "でも、実際のアルゴリズムは1行なんだ。"
  },
  {
    "start": 3846090,
    "end": 3847606,
    "text": "他に質問がなければ"
  },
  {
    "start": 3847708,
    "end": 3849742,
    "text": "もう一度ルートヴィヒに感謝しよう。"
  },
  {
    "start": 3849916,
    "end": 3857210,
    "text": "そうだ。"
  }
]