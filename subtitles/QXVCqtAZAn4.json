[
  {
    "start": 26794,
    "end": 32154,
    "text": "ディープラーニングAIのコミュニティチームの一員です。"
  },
  {
    "start": 32314,
    "end": 38658,
    "text": "今日は特別なゲストをお招きして、直接嗜好の最適化についてお話を伺います。"
  },
  {
    "start": 38826,
    "end": 44418,
    "text": "参考までに、このセッションは録画され、スライドとノートはイベント終了後に入手できる。"
  },
  {
    "start": 44586,
    "end": 46874,
    "text": "ご質問はSlidoのリンクにご記入ください。"
  },
  {
    "start": 46914,
    "end": 50454,
    "text": "チャットに質問を投下し、最も答えてほしい質問に投票してください。"
  },
  {
    "start": 50834,
    "end": 60494,
    "text": "本日のワークショップでは、Chachi、BT、Lamaのような大規模な連携モデルの進歩に後押しされ、この1年で劇的に変化したチャットボットの状況に焦点を当てます。"
  },
  {
    "start": 60654,
    "end": 70414,
    "text": "従来の教師ありの微調整がこの分野を支配してきたが、最近の研究では、言語モデルを人間の嗜好に合わせることで、より親切で安全なものになる可能性が強調されている。"
  },
  {
    "start": 70574,
    "end": 79846,
    "text": "このワークショップでは、Zephyrのトレーニングに使用されている強力なテクニックである直接選好最適化DPOについて掘り下げます。"
  },
  {
    "start": 79990,
    "end": 93740,
    "text": "参加者の皆さんは、チャットのためのllmsの微調整、ハギング・フェイス・ツールを使ったDPOの理論と応用を理解し、今日のイベント・パートナーであるハギング・フェイスのチャット・モデルのパフォーマンスを評価するための主要な指標を学ぶことで、実践的な知識を得ることができます。"
  },
  {
    "start": 93852,
    "end": 104468,
    "text": "Hugging faceは、自然言語処理と機械学習を専門とするAI企業であり、オープンソースでの貢献とAIの研究開発への共同アプローチで知られている。"
  },
  {
    "start": 104636,
    "end": 116334,
    "text": "同社は、Transformersライブラリの開発で有名である。このライブラリは、さまざまなNLPテスト用の事前学習済みモデルやツールを幅広く提供しており、研究者や開発者が最先端のAIソリューションを簡単に実装できるようにしている。"
  },
  {
    "start": 116634,
    "end": 127414,
    "text": "Hugging faceはまた、AI愛好家や専門家のための活気あるコミュニティを育成し、モデル、データセット、研究を共有するためのプラットフォームを提供し、AI技術の進歩に大きく貢献している。"
  },
  {
    "start": 128034,
    "end": 130578,
    "text": "今日は、最初のスピーカーを紹介します。"
  },
  {
    "start": 130666,
    "end": 131614,
    "text": "やあ、ルイス。"
  },
  {
    "start": 131994,
    "end": 132586,
    "text": "やあ、みんな。"
  },
  {
    "start": 132650,
    "end": 133482,
    "text": "調子はどうだい？"
  },
  {
    "start": 133658,
    "end": 135094,
    "text": "来てくれて嬉しいよ。"
  },
  {
    "start": 135634,
    "end": 138666,
    "text": "ルイス・タンストールはハギング・フェイスの機械学習エンジニア。"
  },
  {
    "start": 138730,
    "end": 142742,
    "text": "彼の仕事はオープンソースと研究チームの交差点にある。"
  },
  {
    "start": 142878,
    "end": 155406,
    "text": "ベストセラー『NLP with Transformers』の共同著者であり、これまでに自然言語処理、位相データ分析、時系列の各領域で、新興企業や企業向けの機械学習搭載アプリケーションを構築してきた。"
  },
  {
    "start": 155590,
    "end": 163038,
    "text": "理論物理学の博士号を持ち、2010年フルブライト奨学生、オーストラリア、アメリカ、スイスで研究職を歴任。"
  },
  {
    "start": 163206,
    "end": 170934,
    "text": "現在は、強化学習のテクニックを使って、言語モデルを人間やAIの嗜好に合わせるためのツールやレシピの構築に取り組んでいる。"
  },
  {
    "start": 171434,
    "end": 172894,
    "text": "来てくれて嬉しいよ、ルイス。"
  },
  {
    "start": 173314,
    "end": 174858,
    "text": "お招きいただきありがとうございます、ダイアナ。"
  },
  {
    "start": 175026,
    "end": 175938,
    "text": "もちろんだ。"
  },
  {
    "start": 176066,
    "end": 178898,
    "text": "2人目のスピーカーは、エド・ビーチング。"
  },
  {
    "start": 179066,
    "end": 191538,
    "text": "LLM微調整のためのオープンツールの構築、最先端のLLMアライメントアルゴリズムの実装、プリファレンスデータセットの構築に注力している。"
  },
  {
    "start": 191626,
    "end": 197934,
    "text": "EdはOpen LLM Leaderboardの作者であり、Zephyrの共同作者であり、数多くのオープンソースMLライブラリのメンテナでもある。"
  },
  {
    "start": 198094,
    "end": 203126,
    "text": "博士号は、プランニングとナビゲーション、ロボティクスのための深層強化学習アプローチに焦点を当てた。"
  },
  {
    "start": 203270,
    "end": 207766,
    "text": "現在の主な研究テーマは、身体化学習とLLMアライメントである。"
  },
  {
    "start": 207910,
    "end": 209350,
    "text": "本日はお集まりいただき、本当にありがとうございます。"
  },
  {
    "start": 209422,
    "end": 211234,
    "text": "飛び込むのが楽しみですか？"
  },
  {
    "start": 211534,
    "end": 212878,
    "text": "ああ、待ちきれないよ。"
  },
  {
    "start": 213046,
    "end": 214062,
    "text": "もちろんだ。"
  },
  {
    "start": 214238,
    "end": 215394,
    "text": "では、それを取り上げよう。"
  },
  {
    "start": 216094,
    "end": 216478,
    "text": "素晴らしい。"
  },
  {
    "start": 216526,
    "end": 217430,
    "text": "本当にありがとう。"
  },
  {
    "start": 217542,
    "end": 227994,
    "text": "さて、今日はこのllmsにおけるアライメントの考え方について、直接選好最適化と呼ばれるアルゴリズムの具体的なレンズを使ってお話ししましょう。"
  },
  {
    "start": 228614,
    "end": 251194,
    "text": "この言葉を初めて耳にする人のために説明しておくと、これは昨年スタンフォード大学の研究者たちによって発表された大きなブレークスルーで、OpenAIやAn anthropicによって開拓された従来のアライメント技術に見られるような複雑さの多くは、実際にはもっとシンプルでメモリ効率の良い方法で対処できることを示した。"
  },
  {
    "start": 251704,
    "end": 272884,
    "text": "この講演を通してわかるように、理解しておくと便利な技術的な詳細がいくつかあります。私たちは、この種のアルゴリズムに含まれる数学の多くを解明し、ハングリーピースエコシステムのさまざまなツールを使って実際に言語モデルをトレーニングする方法の実践的な例をお見せすることに全力を尽くしたいと思います。"
  },
  {
    "start": 273624,
    "end": 281482,
    "text": "そこで、まず手始めに、そもそもなぜ言語モデルを揃えたいのか？"
  },
  {
    "start": 281538,
    "end": 284934,
    "text": "アライメントとは何か？"
  },
  {
    "start": 285354,
    "end": 290090,
    "text": "ここにいるほとんどの人は、言語モデルについてある程度知っていると思う。"
  },
  {
    "start": 290202,
    "end": 307982,
    "text": "言語モデルを学習する際の最初のステップは、事前学習と呼ばれるプロセスであることはご存じでしょう。このプロセスでは、基本的に変換ニューラル・ネットワークに、通常はインターネットから大量のデータを送り込み、モデルが前に見たトークンを考慮して、基本的にシーケンス内の次のトークンを予測させます。"
  },
  {
    "start": 308158,
    "end": 317030,
    "text": "これは結局、非常に強力なオートコンプリート・モデルのようなもので、特定の答えを生成するためのフューチャー・プロンプトのようなトリックを行うことができる。"
  },
  {
    "start": 317222,
    "end": 324422,
    "text": "オートコンプリターとして、情報探索のような質問をした場合、例えば「ピザにパイナップルは犯罪ですか？"
  },
  {
    "start": 324598,
    "end": 330086,
    "text": "そうすると、この基本言語モデルは、その次に可能性が高いと思われる単語を自動的に完成させる。"
  },
  {
    "start": 330230,
    "end": 340630,
    "text": "この特別な例では、ピザパーティーで答えられる多くの質問のうちのひとつだ、と言うかもしれない。"
  },
  {
    "start": 340822,
    "end": 349954,
    "text": "しかし、チャットモデルとして使用するためには、いくつかのチューニングが必要である。"
  },
  {
    "start": 350334,
    "end": 354598,
    "text": "第2段階は、一般的に「監視下での微調整」と呼ばれる。"
  },
  {
    "start": 354766,
    "end": 369270,
    "text": "このステップで行うことは、ユーザーが質問し、人間が書いた回答か、LLMが生成した回答がその質問に対する答えを提供するという数千の例をモデルに提供することです。"
  },
  {
    "start": 369462,
    "end": 375382,
    "text": "この特別な例では、たぶん人間の注釈者はピザにパイナップルをかけるのが本当に嫌いなのだろう。"
  },
  {
    "start": 375478,
    "end": 378774,
    "text": "彼らは注釈で、これは本当に悪いことだと書いている。"
  },
  {
    "start": 378934,
    "end": 387154,
    "text": "そう、パイナップルとピザを加えることはジュネーブ条約に基づく犯罪なのだ。"
  },
  {
    "start": 387614,
    "end": 400624,
    "text": "このSFTモデルは、基本的に微調整データと事前トレーニングデータから、いくつかのバイアスが存在することを学びました。"
  },
  {
    "start": 401124,
    "end": 413548,
    "text": "さて、あなたはドミノやピザハットのような会社に所属していて、実際に、もしかしたらあなたの顧客は、ピザにパイナップルを乗せるのが好きかもしれない。"
  },
  {
    "start": 413716,
    "end": 428326,
    "text": "ですから、言語モデルを顧客の好みをエンコードするように調整する1つの方法は、基本的に言語モデルにたくさんの質問を提供することです。"
  },
  {
    "start": 428470,
    "end": 437254,
    "text": "いくつかの答え（この場合は「はい」か「いいえ」）を生成し、顧客や人間のラベラーに注釈を付けてもらうことができる。"
  },
  {
    "start": 437334,
    "end": 445874,
    "text": "そうすれば、教師ありデータの事前訓練には直接存在しない、代替的な嗜好をエンコードする方法が得られる。"
  },
  {
    "start": 446174,
    "end": 457754,
    "text": "基本的に、あなたがやろうとしていることは、好ましくない反応よりも好ましい反応の方が可能性が高いという確率を、どのようにモデル化できるかを学ぶ、あるいはモデルに教えようとしているのだ。"
  },
  {
    "start": 458294,
    "end": 471414,
    "text": "そして、このプロセスを正しく行えば、モデルがあなたの好みに近い答えを出す確率を調整することになる。"
  },
  {
    "start": 471534,
    "end": 477034,
    "text": "この場合、パイナップルを加えることは犯罪ではなく、個人の好みの問題だ。"
  },
  {
    "start": 477434,
    "end": 482962,
    "text": "ここで触れておきたいのは、最近のアライメントは検閲のようなものと結び付けられることが多いということだ。"
  },
  {
    "start": 483018,
    "end": 487978,
    "text": "チャットGPTを利用したことがある人なら、たいてい断られる。"
  },
  {
    "start": 488106,
    "end": 497690,
    "text": "言語モデルとして、私はXYZはできないが、それは本当に広い概念である。"
  },
  {
    "start": 497722,
    "end": 502762,
    "text": "これらは多様なものであり、私たちの文化や育ちに左右されるものだ。"
  },
  {
    "start": 502938,
    "end": 507074,
    "text": "これらの嗜好は、原理的にはニューラルネットワークでモデル化できる。"
  },
  {
    "start": 507114,
    "end": 514214,
    "text": "そうすれば、エンドユーザーが望むような振る舞いをするように言語モデルを調整することができる。"
  },
  {
    "start": 515314,
    "end": 522426,
    "text": "DPOに飛び込む前に、OpenAIが開拓したオリジナルのアプローチを紹介するのがいいと思う。"
  },
  {
    "start": 522610,
    "end": 527334,
    "text": "彼らはこれをRLHF（人間のフィードバックからの強化学習）と呼んだ。"
  },
  {
    "start": 527834,
    "end": 533694,
    "text": "彼らが持っていた基本的なアイデアは、微調整を行うこの非常によく似たレシピだった。"
  },
  {
    "start": 534434,
    "end": 542574,
    "text": "この場合、基本的に人間が注釈をつけ、質問と回答の質の高い例を作成した。"
  },
  {
    "start": 543074,
    "end": 548014,
    "text": "そして、そのデータをもとにさまざまなGPTモデルを微調整した。"
  },
  {
    "start": 548754,
    "end": 561660,
    "text": "昨年、chatgptが登場したばかりの頃、オープンソースコミュニティには、監督による微調整に使えるような質の高いデータセットがあまりなかった。"
  },
  {
    "start": 561852,
    "end": 563068,
    "text": "これは本当に変わった。"
  },
  {
    "start": 563116,
    "end": 565756,
    "text": "だから今、いくつか、本当にいい例がある。"
  },
  {
    "start": 565820,
    "end": 571340,
    "text": "テクニウムのオープン・エルメスや、エリック・ハートフォードのドルフィンというのがありますね。"
  },
  {
    "start": 571492,
    "end": 578260,
    "text": "これらは、あらゆる種類の基本言語モデルをチャットモデルに変換するために、今日使用できるデータセットです。"
  },
  {
    "start": 578332,
    "end": 586704,
    "text": "これらのデータセットで訓練すれば、通常、特定の方法で会話できる非常に優れたチャットモデルが得られる。"
  },
  {
    "start": 587604,
    "end": 592084,
    "text": "オープンAIが次にしたことは、よし、このチャットモデルができた。"
  },
  {
    "start": 592164,
    "end": 598384,
    "text": "あとは、アノテーターによる人間の嗜好のようなものをモデル化する方法が必要だ。"
  },
  {
    "start": 598724,
    "end": 608812,
    "text": "その方法は、基本的にプロンプトのコレクションを取り出し、それをチャットモデルに送り、チャットモデルがさまざまな回答を生成するというものだ。"
  },
  {
    "start": 608988,
    "end": 612764,
    "text": "そして、その回答はラベラーによってベストからワーストまで評価される。"
  },
  {
    "start": 612844,
    "end": 617442,
    "text": "これは、どの答えが他の答えより好ましいか、というようなランキングを与えてくれる。"
  },
  {
    "start": 617618,
    "end": 631650,
    "text": "このモデルは基本的に、モデルから何らかの応答が与えられたときに、人間のラベラーがつけたスコアの種類や嗜好スコアが何であるかを予測する方法を学習する。"
  },
  {
    "start": 631842,
    "end": 639810,
    "text": "そうすれば、基本的にあらゆるアウトプットを評価するのに使える回帰モデルのようなものができあがる。"
  },
  {
    "start": 639842,
    "end": 646674,
    "text": "そうすれば、人間のラベルを嗜好モデルにマッピングすることができる。"
  },
  {
    "start": 647134,
    "end": 648214,
    "text": "2つのモデルがある。"
  },
  {
    "start": 648254,
    "end": 650382,
    "text": "私たちにはチャットモデルとリワードモデルがある。"
  },
  {
    "start": 650558,
    "end": 655834,
    "text": "ただ、このようなことが可能な公開データセットもたくさんあることをお伝えしておきたい。"
  },
  {
    "start": 656694,
    "end": 658862,
    "text": "チャットモデルと報酬モデルがある。"
  },
  {
    "start": 658918,
    "end": 671402,
    "text": "最後のステップは、この2つを組み合わせて、OpenAIのような報酬モデルからのシグナルをエンコードする方法を学習した新しいチャットモデルを作成することだ。"
  },
  {
    "start": 671458,
    "end": 674434,
    "text": "anthropicも強化学習を使っている。"
  },
  {
    "start": 674594,
    "end": 679114,
    "text": "ここでの基本的な考え方は、チャットモデルにプロンプトを与えるということだ。"
  },
  {
    "start": 679274,
    "end": 680794,
    "text": "出力が生成される。"
  },
  {
    "start": 680954,
    "end": 695610,
    "text": "そして、そのスコアを使って、基本的に元のチャットモデルの重みを更新し、より好みが一致したり近づいたりすることができる。"
  },
  {
    "start": 695802,
    "end": 706402,
    "text": "そのため、人間のアノテーターが特定の方法で物事を評価する場合、そのような行動や嗜好は、基本的に更新されたチャットモデルにエンコードされます。"
  },
  {
    "start": 706538,
    "end": 710974,
    "text": "その結果、チャットGPTやクロードのようなものになる。"
  },
  {
    "start": 713754,
    "end": 724514,
    "text": "数学的に言うと、あまり怖い話にはしたくないが、基本的な考え方は、モデルが得られる報酬を最大化しようとしているということだ。"
  },
  {
    "start": 724594,
    "end": 731574,
    "text": "このようなプロンプトをすべて与え、アウトプットを採点し、モデルが高い報酬を得るアウトプットを出すのがうまくなるようにする。"
  },
  {
    "start": 732834,
    "end": 749282,
    "text": "リワード・モデルを使うだけなら、リワード・ハッキングと呼ばれるものがあることがわかった。事実上、チャット・モデルは、非常に高いリワードを持つ特定の単語やトークンを置くと、このようなことを延々と続けることができると学習できる。"
  },
  {
    "start": 749418,
    "end": 757854,
    "text": "バックスラッシュや絵文字を出力するような、ゴミのようなモデルになってしまう。"
  },
  {
    "start": 758194,
    "end": 764114,
    "text": "OpenAIが導入したのは、KLペナルティと呼ばれる追加用語だ。"
  },
  {
    "start": 764274,
    "end": 771034,
    "text": "基本的にこれは、元のチャットモデルと現在最適化しているモデルの距離を測定するものだ。"
  },
  {
    "start": 771194,
    "end": 778114,
    "text": "ベータというパラメータがあり、このパラメータでモデル同士をどの程度近づけたいかをコントロールすることができる。"
  },
  {
    "start": 778494,
    "end": 788354,
    "text": "この2つを一緒にして、PPOのような強化学習アルゴリズムで最適化すれば、一般的に非常にパフォーマンスと整合性の高いモデルになる。"
  },
  {
    "start": 788934,
    "end": 791206,
    "text": "さて、これにはある課題がある。"
  },
  {
    "start": 791230,
    "end": 796278,
    "text": "まずひとつは、強化学習をやったことのある人なら誰でも知っていることだが、強化学習は一般的に非常に不安定だ。"
  },
  {
    "start": 796406,
    "end": 798606,
    "text": "チューニングしなければならないハイパーパラメーターはたくさんある。"
  },
  {
    "start": 798750,
    "end": 809388,
    "text": "HuggingFaceの仲間の一人であるコスタは、OpenAIのオリジナルのRLHFパイプラインをゼロから再実装するという英雄的な取り組みを行っている。"
  },
  {
    "start": 809556,
    "end": 816264,
    "text": "アトム・オプティマイザーのパラメーターのひとつを変えるだけで、結果が大きく変わるようなクレイジーなこともあった。"
  },
  {
    "start": 816764,
    "end": 821144,
    "text": "それは非常に、言ってみれば、うまくやるのが難しい獣だ。"
  },
  {
    "start": 821724,
    "end": 827412,
    "text": "もうひとつは、基本的に2つのモデルを扱わなければならないことだ。"
  },
  {
    "start": 827428,
    "end": 833300,
    "text": "チャットモデルと、最適化してより整合性を高めようとするモデルがあり、さらに報酬モデルもある。"
  },
  {
    "start": 833332,
    "end": 836624,
    "text": "あなたは3つの大きな言語モデルを抱えていて、それを両立させなければならない。"
  },
  {
    "start": 836804,
    "end": 840352,
    "text": "最近のコンピュート・ハードウェアはかなり良くなってきている。"
  },
  {
    "start": 840408,
    "end": 848368,
    "text": "もし、300億以上のパラメータを持つような非常に大規模なことをしようとするならば、これをこなすために大量の計算機が必要になるだろう。"
  },
  {
    "start": 848456,
    "end": 855204,
    "text": "これは、エンジニアリングの観点からも、安定性の観点からも、プロセス全体を非常に難しいものにしている。"
  },
  {
    "start": 856864,
    "end": 864302,
    "text": "直接選好最適化の背後にある考え方は、強化学習は必要ないかもしれない、というものだ。"
  },
  {
    "start": 864488,
    "end": 870454,
    "text": "強化学習を使わないとしたら、その代わりとなるものは何か？"
  },
  {
    "start": 870794,
    "end": 894816,
    "text": "彼らが行った基本的な観察は、報酬モデルまたは報酬モデルからのシグナルとKLペナルティの組み合わせを最大化しようとする代わりに、報酬モデルが学習するスコアの種類を直接コード化するか、暗黙的にモデル化するような方法で、目的の種類を変更すればいいのではないかというものだった。"
  },
  {
    "start": 894920,
    "end": 897456,
    "text": "事実上、報酬モデルはもうない。"
  },
  {
    "start": 897560,
    "end": 899984,
    "text": "あなたにはオリジナルの言語モデルがあるだけだ。"
  },
  {
    "start": 900104,
    "end": 907084,
    "text": "あなたの言語モデルは、この最適化のプロセスを通じて、それらの嗜好を直接モデル化する方法を学習する。"
  },
  {
    "start": 907744,
    "end": 920484,
    "text": "この方程式をざっと説明すると、基本的な考え方は、プロンプトを受けて、良い答えと悪い答えの二元的な選好をするということだ。"
  },
  {
    "start": 921034,
    "end": 924970,
    "text": "ピザにパイナップルは犯罪か？"
  },
  {
    "start": 925122,
    "end": 928134,
    "text": "となれば、良い返事は「ノー」、悪い返事は「イエス」である。"
  },
  {
    "start": 928594,
    "end": 933530,
    "text": "では、何をするかというと、基本的に2つのことを計算する。"
  },
  {
    "start": 933602,
    "end": 944698,
    "text": "私たちは基本的に、モデルが選択された反応を正しく予測できる対数確率を計算しようとしている。"
  },
  {
    "start": 944866,
    "end": 948890,
    "text": "ここでは、2つの項の比率を取る。"
  },
  {
    "start": 948962,
    "end": 954294,
    "text": "一番上の用語は、最適化プロセスを通して最適化しているモデルだ。"
  },
  {
    "start": 955034,
    "end": 966694,
    "text": "そしてその下に参照モデルがあり、以前このKLペナルティが機能したのと同じように、参照モデルから離れすぎないように確率を正規化する。"
  },
  {
    "start": 967114,
    "end": 968930,
    "text": "そしてもうひとつ、よく似た言葉がある。"
  },
  {
    "start": 969002,
    "end": 978234,
    "text": "では、望ましい反応を予測する対数確率を計算しようとする代わりに、望ましくない反応の対数確率を予測しようとする。"
  },
  {
    "start": 978854,
    "end": 1000674,
    "text": "この2つの項を一緒に組み合わせると、基本的には2つの項の差を最大化しようとしていることになります。1つは選択された反応、もう1つは拒否された反応です。モデルを選択された反応の予測にもっともっと適したものにしようとしているのです。"
  },
  {
    "start": 1000974,
    "end": 1009104,
    "text": "このアルゴリズムのクールなところは、微分可能なことで、つまり逆伝播を使ってモデルを最適化すればいいということだ。"
  },
  {
    "start": 1009224,
    "end": 1016124,
    "text": "これは強化学習が必要ない理由でもある。なぜなら、前に示した目的は無関心だからだ。"
  },
  {
    "start": 1016624,
    "end": 1025528,
    "text": "アルゴリズムを説明すると、基本的には、プロンプトを受け取って、良い反応と悪い反応の2つがある。"
  },
  {
    "start": 1025656,
    "end": 1027664,
    "text": "2つのモデルを使っている。"
  },
  {
    "start": 1027744,
    "end": 1034294,
    "text": "参照モデルと最適化するモデルがあって、バックプロップをして、基本的にはそれで終わりです。"
  },
  {
    "start": 1034754,
    "end": 1040778,
    "text": "pytorchのコードで言うと、だいたい10行のコードであることがわかるだろう。"
  },
  {
    "start": 1040866,
    "end": 1048094,
    "text": "最初にこれを見たとき、こんな単純なものがうまくいくことにまず驚いた。"
  },
  {
    "start": 1048754,
    "end": 1060794,
    "text": "実際に私たち自身でテストしてみたところ、強化学習パイプラインと比較して、収束し、より整列したモデルを得るのが、以前よりもずっとずっと速いことがわかった。"
  },
  {
    "start": 1061974,
    "end": 1069434,
    "text": "ちょっと余談になるが、ここでの最も重要な考え方のようなものだ。"
  },
  {
    "start": 1069814,
    "end": 1077974,
    "text": "基本的にベータと呼ばれるパラメータがあり、これがDPOを行う際に調整する必要がある主要なハイパーパラメータである。"
  },
  {
    "start": 1078014,
    "end": 1082314,
    "text": "エドはこの後、我々が重要だと判断した他のパラメーターについて少し話してくれるだろう。"
  },
  {
    "start": 1082854,
    "end": 1092204,
    "text": "基本的に、アップデートや勾配アップデートのような計算をするときは、モデルの最適化の方向性を決めるときです。"
  },
  {
    "start": 1092824,
    "end": 1095896,
    "text": "ここでは事実上、3つの用語が使われているようなものだ。"
  },
  {
    "start": 1096000,
    "end": 1104120,
    "text": "最初の項は、基本的に、損失の上にある加重係数のようなものだ。"
  },
  {
    "start": 1104312,
    "end": 1115480,
    "text": "この重み付け係数は、基本的に、不正解を得た場合と正解を得た場合の報酬の種類や暗黙の報酬の違いを測定するものである。"
  },
  {
    "start": 1115672,
    "end": 1138844,
    "text": "そのため、モデルが継続的に不正確な答えを得たり、継続的に不正確なラベルを予測しているような場合、この重み付け係数はシグモイドであるため大きくなり、1に向かって上昇します。"
  },
  {
    "start": 1139224,
    "end": 1147264,
    "text": "この2つの項は、本質的には、モデルが選択された答えを予測する可能性を高めるだけの違いである。"
  },
  {
    "start": 1147344,
    "end": 1151114,
    "text": "不合格になる可能性を減らそうとしている。"
  },
  {
    "start": 1151574,
    "end": 1171114,
    "text": "ベータ・パラメーターを上げると、選択された答えと拒否された答えの差がどんどん大きくなるように、モデルを効果的に訓練するように働きかけていることがわかります。"
  },
  {
    "start": 1171454,
    "end": 1181326,
    "text": "というのも、かなりノイズの多いデータで、選ばれたものと拒否されたもののラベルがあいまいな場合があるからだ。"
  },
  {
    "start": 1181350,
    "end": 1184486,
    "text": "たぶん、その質はまちまちだと思う。"
  },
  {
    "start": 1184670,
    "end": 1192638,
    "text": "このベータ・パラメータを調整することで、選択された回答と拒否された回答にどれだけモデルを集中させるかをコントロールすることができる。"
  },
  {
    "start": 1192806,
    "end": 1198486,
    "text": "そして、一般的には、異なる値で2、3の実験を行い、どれがベストかを見つけなければならない。"
  },
  {
    "start": 1198590,
    "end": 1208118,
    "text": "基本的な考え方は、ベータは基本的にターゲット・マージンをコントロールするということである。"
  },
  {
    "start": 1208286,
    "end": 1212114,
    "text": "それを増やすことで、まったく違った振る舞いができるようになる。"
  },
  {
    "start": 1213134,
    "end": 1223714,
    "text": "数学が苦手なら、ミームのグランドマスターのような存在であるトム・ゴールドスタインが、DPOで起こっていることはだいたいこんな感じだと教えてくれた。"
  },
  {
    "start": 1224254,
    "end": 1230276,
    "text": "基本的には、良いものを勾配降下させ、悪いものを勾配降下させる。"
  },
  {
    "start": 1230390,
    "end": 1245764,
    "text": "というのも、DPOは非常にシンプルなアルゴリズムであり、すべてを暗黙的にモデル化しているため、報酬モデルを実際に独立して訓練することなく、基本的にモデルを訓練することができるからだ。"
  },
  {
    "start": 1247784,
    "end": 1249480,
    "text": "いくつか例を挙げよう。"
  },
  {
    "start": 1249592,
    "end": 1253768,
    "text": "ハグをする段階でトレーニングしたうちのひとつを通り抜けた。"
  },
  {
    "start": 1253856,
    "end": 1254992,
    "text": "ゼフィールと呼ばれている。"
  },
  {
    "start": 1255128,
    "end": 1271380,
    "text": "Zephyrで私たちが興味を持ったのは、DPOの論文で著者たちがDPOとPPOの比較を小さなモデルを使って行っていたことです。"
  },
  {
    "start": 1271572,
    "end": 1276260,
    "text": "彼らは学術的なデータセットのようなものを使っていた。"
  },
  {
    "start": 1276412,
    "end": 1281264,
    "text": "特に、チャット用のデータセットを使っていたのだが、そのデータセットはパフォーマンスのために最適化されていなかった。"
  },
  {
    "start": 1282324,
    "end": 1297964,
    "text": "私たちが考えたアイデアは、コミュニティーの中で、チャットGPTモデルを使って、お互いに会話したり、お互いの回答を評価したりする合成データセットを作成する傾向があることは知っている。"
  },
  {
    "start": 1298504,
    "end": 1308204,
    "text": "これらのデータセットとDPOを併用すれば、もしかしたら人間のフィードバックを完全に取り除くことができるかもしれない。"
  },
  {
    "start": 1308704,
    "end": 1319720,
    "text": "この例では、ultra chatと呼ばれる、教師ありファインチューニングのための一般的なデータセットを使用して、チャットモデル（初期SFTモデル）をトレーニングします。"
  },
  {
    "start": 1319912,
    "end": 1328444,
    "text": "そこで私たちは、合成フィードバックからなる別のデータセットを作成した。"
  },
  {
    "start": 1328744,
    "end": 1336684,
    "text": "そして、この2つのデータセットを使って、基本的には1、2週間前に発表されたばかりのミストラル7 Bのベースモデルを使ったDPOを行う。"
  },
  {
    "start": 1337364,
    "end": 1354184,
    "text": "私たちが発見した驚くべきことのひとつは、このプロセスを適用すると、MtBenchのような様々なチャットベンチマークで、llama 70 Bのようなはるかに大きなモデルと比較して、比較的競争力のある7Bモデルが得られるということです。"
  },
  {
    "start": 1354604,
    "end": 1359820,
    "text": "さて、ここで注意しなければならないのは、これらのベンチマークには一定のバイアスがあるということだ。"
  },
  {
    "start": 1359852,
    "end": 1370464,
    "text": "例えば、GPT4のデータでモデルをトレーニングした場合、GPT4が評価者として機能するため、GPT4は独自のスタイルを好む傾向がある。"
  },
  {
    "start": 1370844,
    "end": 1376084,
    "text": "とはいえ、実際にZeppeモデルと会話してみると、その実力はかなりのものだ。"
  },
  {
    "start": 1376204,
    "end": 1384316,
    "text": "これは、このレシピをより高性能なデータセットでより大きなモデルに適用できることを示す、最初の公開デモンストレーションのひとつだった。"
  },
  {
    "start": 1384500,
    "end": 1390404,
    "text": "それ以来、コミュニティはこのアイデアをさらに発展させた。"
  },
  {
    "start": 1390564,
    "end": 1398794,
    "text": "例えば、アルジオラの何人かは、ミストラル・モデルやミックスドラル・モデル3が出たときに、その上にDPOを行った。"
  },
  {
    "start": 1399534,
    "end": 1404222,
    "text": "ジョン・ダービンという人がいて、とても興味深いDPOデータセットをたくさん作っている。"
  },
  {
    "start": 1404278,
    "end": 1407998,
    "text": "彼はこれを34年B型の微調整に使用した。"
  },
  {
    "start": 1408166,
    "end": 1412590,
    "text": "そしてもちろん、ミストラルはDPOを多用してモデルを揃えている。"
  },
  {
    "start": 1412742,
    "end": 1416994,
    "text": "現在、ヌース・リサーチとテクニウムは、これを自ら実践している。"
  },
  {
    "start": 1417414,
    "end": 1427254,
    "text": "少なくともオープンソースコミュニティでは、DPOは今日、一種の正準整列アルゴリズムになっていると言える。"
  },
  {
    "start": 1427714,
    "end": 1431570,
    "text": "そのために使える主なライブラリーは2つある。"
  },
  {
    "start": 1431602,
    "end": 1435814,
    "text": "TRL（変形強化学習）と呼ばれるものだ。"
  },
  {
    "start": 1436234,
    "end": 1440602,
    "text": "アクソロトルもある。"
  },
  {
    "start": 1440738,
    "end": 1444094,
    "text": "TRLの仕組みについては、後でエドが少しお見せします。"
  },
  {
    "start": 1445754,
    "end": 1462202,
    "text": "このような簡単な概要をまとめると、一般的に言って、学術文献や機械学習で見られるのは、いったん非常に人気のあるアイデアが生まれると、多くの人がそれを拡張したり改良したりする方法を見つけようとする。"
  },
  {
    "start": 1462338,
    "end": 1468666,
    "text": "ディープマインドの研究者たちによって、最初の改良のひとつがなされた。"
  },
  {
    "start": 1468810,
    "end": 1472534,
    "text": "彼らはIPO（アイデンティティ・プリファレンス最適化）と呼ばれるものを開発した。"
  },
  {
    "start": 1472994,
    "end": 1483704,
    "text": "DPOを行う場合、データセットで選択された反応に基本的にオーバーフィットしてしまう危険性があるということだ。"
  },
  {
    "start": 1484084,
    "end": 1493824,
    "text": "それを防ぎたい場合は、回帰と同じように正則化項を追加することができる。"
  },
  {
    "start": 1494284,
    "end": 1499284,
    "text": "彼らは論文で、DPOよりも良い結果を示した。"
  },
  {
    "start": 1499404,
    "end": 1504556,
    "text": "ハグリングフェイズでは、この2つの方法を比較する実験も行った。"
  },
  {
    "start": 1504620,
    "end": 1508914,
    "text": "パラメータによっては、トレードオフになることもある。"
  },
  {
    "start": 1509654,
    "end": 1518350,
    "text": "そしてもう一つのレベルでは、反復的DPO、あるいはオンラインDPOとでも言おうか。"
  },
  {
    "start": 1518542,
    "end": 1533890,
    "text": "Snorkelの素晴らしい例がある。彼らは基本的に、Allen AIから報酬モデルを取り出し、Ultrafeedbackのプロンプトを使ってチャットモデルから応答を生成した。"
  },
  {
    "start": 1533982,
    "end": 1544414,
    "text": "報酬モデルを使って基本的にプロンプトの評価とランク付けを行い、別のDPOモデルを再学習させ、新しいプロンプトで再びプロンプトを生成し、評価とランク付けを行う。"
  },
  {
    "start": 1544714,
    "end": 1549774,
    "text": "これを何度か繰り返すと、どんどん良くなるモデルができる。"
  },
  {
    "start": 1550234,
    "end": 1555058,
    "text": "これがエキサイティングなのは、ラマ2世がどのようにトレーニングされたかに非常に近いことだ。"
  },
  {
    "start": 1555106,
    "end": 1560090,
    "text": "ラマ2世は、徐々に改良されたモデルで訓練された。"
  },
  {
    "start": 1560242,
    "end": 1564898,
    "text": "私たちが理解している限りでは、人間原理モデルがどのように訓練されているかということでもある。"
  },
  {
    "start": 1564946,
    "end": 1568842,
    "text": "このようなオンライン方式でトレーニングされ、新しいデータでモデルを継続的に改善していく。"
  },
  {
    "start": 1568978,
    "end": 1571054,
    "text": "これはかなりエキサイティングな方向性だと思う。"
  },
  {
    "start": 1571634,
    "end": 1581294,
    "text": "KTO（カーネマン・トヴェルスキー・オプティマイゼーション）と呼ばれる新しいアルゴリズムがあります。"
  },
  {
    "start": 1582034,
    "end": 1600400,
    "text": "これはスタンフォード大学の研究者によるもので、文脈的AIの重要なポイントとして、嗜好データを収集したいのであれば、基本的にプロンプトと2つの回答を用意する必要がある。"
  },
  {
    "start": 1600592,
    "end": 1613204,
    "text": "これを人間のフィードバックで行おうとすると、アノテーターがチャットモデルと対話し、何が好ましい反応かをステップごとに選ぶ必要があるため、収集に非常にコストがかかる。"
  },
  {
    "start": 1614104,
    "end": 1633280,
    "text": "もしあなたが彼らのように賢いなら、基本的に良いと判断されたものと悪いと判断されたものを集めるだけでよく、しかもそれらが同じプロンプトに関連している必要はないように、実際に損失を定義することができる。"
  },
  {
    "start": 1633312,
    "end": 1643304,
    "text": "私たちは、これは大いに期待できるものだと考えていますし、おそらく近いうちに、さまざまなスケールでどのように機能するかを示す、私たちからの発表が期待できるものになるでしょう。"
  },
  {
    "start": 1643724,
    "end": 1645276,
    "text": "論文そのものは読む価値がある。"
  },
  {
    "start": 1645340,
    "end": 1646824,
    "text": "彼らは素晴らしい実験をたくさんしている。"
  },
  {
    "start": 1647484,
    "end": 1653664,
    "text": "今のところ欠けていると思われるのは、これを大規模に実行するための、ある種の優れたラベルデータセットである。"
  },
  {
    "start": 1654004,
    "end": 1655664,
    "text": "これはとても、とてもエキサイティングだ。"
  },
  {
    "start": 1656764,
    "end": 1658684,
    "text": "質問を受けるいい機会かもしれない。"
  },
  {
    "start": 1658724,
    "end": 1659304,
    "text": "エド？"
  },
  {
    "start": 1660564,
    "end": 1662424,
    "text": "ああ、そうだ。"
  },
  {
    "start": 1662724,
    "end": 1665624,
    "text": "では、まず1つ目から。"
  },
  {
    "start": 1665764,
    "end": 1672244,
    "text": "オープンソースのアライメントデータセットの質をどのように評価するのですか？"
  },
  {
    "start": 1673424,
    "end": 1675084,
    "text": "ああ、本当にいい質問だね。"
  },
  {
    "start": 1675864,
    "end": 1681248,
    "text": "一般的には、バイブス・テストと呼ばれるものが必要だと思う。"
  },
  {
    "start": 1681296,
    "end": 1684044,
    "text": "自分でデータを見るしかない。"
  },
  {
    "start": 1684504,
    "end": 1694204,
    "text": "ラベルにおかしな間違いがあることが多いからだ。"
  },
  {
    "start": 1695334,
    "end": 1701030,
    "text": "おそらく今日最も一般的な方法は、別の言語モデルを使って評価を行うことだろう。"
  },
  {
    "start": 1701222,
    "end": 1707566,
    "text": "最も一般的な例は、GPT4を使って基本的に回答の質を判断することだろう。"
  },
  {
    "start": 1707750,
    "end": 1720196,
    "text": "一般的にGPT4は、創造的なタスクや、高校生レベルの推論を必要とするようなタスクに強い。"
  },
  {
    "start": 1720390,
    "end": 1730688,
    "text": "数学とかコードに関係するような回答の質を判断するように頼むと、幻覚を見て判断ミスをすることがある。"
  },
  {
    "start": 1730856,
    "end": 1737784,
    "text": "これはgbpoolが悪いということではなく、人間にとっても難しいタスクのように、正しい答えを導き出すのは難しいということだ。"
  },
  {
    "start": 1737904,
    "end": 1743524,
    "text": "現在では、コミュニティがクオリティを評価する方法として、これがあるようだ。"
  },
  {
    "start": 1744544,
    "end": 1745604,
    "text": "もちろんだ。"
  },
  {
    "start": 1746904,
    "end": 1752324,
    "text": "この質問に対して、このプロセス、アライメントにはどれくらいのデータが理想的なのでしょうか？"
  },
  {
    "start": 1753424,
    "end": 1754536,
    "text": "ああ、いい質問だね。"
  },
  {
    "start": 1754600,
    "end": 1776744,
    "text": "SFTとDPOの多段階処理を行う場合、SFTのトレーニングデータは1万から10万例、DPOのステップでは5万例程度で十分です。"
  },
  {
    "start": 1778004,
    "end": 1780220,
    "text": "最近、その例を目にした。"
  },
  {
    "start": 1780252,
    "end": 1788724,
    "text": "データと呼ばれるアルゴリズムがあり、そこではSFTのステップに6000例しか使用しないことが示されている。"
  },
  {
    "start": 1788764,
    "end": 1795004,
    "text": "SFTはおよそ1万例、DPOは10～20例といったところだろう。"
  },
  {
    "start": 1795044,
    "end": 1797144,
    "text": "そうすれば、かなりいいモデルが手に入るだろう。"
  },
  {
    "start": 1797584,
    "end": 1802680,
    "text": "未解決の問題は、私たちが知る限り、他の研究室と同様にOpenAIと人類学的であるということだ。"
  },
  {
    "start": 1802712,
    "end": 1804688,
    "text": "彼らはもっともっと多くの例を使っている。"
  },
  {
    "start": 1804776,
    "end": 1808844,
    "text": "おそらく、このようなことを繰り返し行えば、より良いモデルが得られるだろう。"
  },
  {
    "start": 1809584,
    "end": 1811088,
    "text": "完璧だ。"
  },
  {
    "start": 1811136,
    "end": 1812352,
    "text": "本当にありがとう、ルイス。"
  },
  {
    "start": 1812408,
    "end": 1815044,
    "text": "エドと一緒に第2セクションに飛び込もう。"
  },
  {
    "start": 1815824,
    "end": 1817472,
    "text": "君たちを連れて行くよ。"
  },
  {
    "start": 1817568,
    "end": 1818604,
    "text": "持って行け、エド。"
  },
  {
    "start": 1818944,
    "end": 1819920,
    "text": "ありがとう。"
  },
  {
    "start": 1820032,
    "end": 1827824,
    "text": "今回は、対話データセットに対してDPOでSFTを実際に実行する方法について、より実践的な内容をご紹介します。"
  },
  {
    "start": 1829164,
    "end": 1838464,
    "text": "そのひとつが、会話のデータセットを持っていて、それをトークン化してモデルに送り、微調整するために、実際にどのようにフォーマットするのか、ということだ。"
  },
  {
    "start": 1839724,
    "end": 1844316,
    "text": "このプレゼンテーションでは、SFTとDPOのデータセットへのリンクをいくつか紹介している。"
  },
  {
    "start": 1844380,
    "end": 1847504,
    "text": "もし、あなたが始めたいと思っているなら、良いスタート地点がある。"
  },
  {
    "start": 1848124,
    "end": 1857836,
    "text": "その後、主にDPOのステップで、DPOがうまくトレーニングできているかどうかを評価する方法や、トレーニングの問題点を特定する方法について、いくつかの指標を紹介する。"
  },
  {
    "start": 1858020,
    "end": 1861100,
    "text": "そして、実際にチャットボットを訓練したら、おそらくそれを評価したくなるでしょう。"
  },
  {
    "start": 1861132,
    "end": 1865580,
    "text": "自分でチャットしてテストすることもできるが、自動化された方法が欲しいかもしれない。"
  },
  {
    "start": 1865732,
    "end": 1870904,
    "text": "そこで、どのようにこれを行うか、またこれを行うためのコミュニティで利用可能なツールをいくつか提案しよう。"
  },
  {
    "start": 1871644,
    "end": 1880192,
    "text": "しかし、DPOの話に入る前に、まずSFTステップの監督による微調整について話そう。"
  },
  {
    "start": 1880388,
    "end": 1884032,
    "text": "実際に、このステップをスキップして直接DPOにジャンプする実験をいくつか行った。"
  },
  {
    "start": 1884088,
    "end": 1886840,
    "text": "実際、DPOのステップを実行するのはとてもとても難しい。"
  },
  {
    "start": 1886872,
    "end": 1891032,
    "text": "我々は、LLMというモデルが学習に失敗することを発見した。"
  },
  {
    "start": 1891048,
    "end": 1893684,
    "text": "対話のテンプレートは、私たちが観察していることのひとつだ。"
  },
  {
    "start": 1894704,
    "end": 1895844,
    "text": "さっそく始めよう。"
  },
  {
    "start": 1898504,
    "end": 1898856,
    "text": "そうだ。"
  },
  {
    "start": 1898880,
    "end": 1906164,
    "text": "そこで、このプレゼンテーションに合わせて、SFTとDPOの2つの注釈付きノートを作成した。"
  },
  {
    "start": 1906704,
    "end": 1914884,
    "text": "これらはGoogle Colab GPUで動作するように書かれているので、動くはずだ。"
  },
  {
    "start": 1915424,
    "end": 1923044,
    "text": "当初は、ライブでこの作業を進めようと考えていたのですが、実際には、これらのモデルは列車に何時間もかかるので、オフラインで作業したほうがいいのです。"
  },
  {
    "start": 1924944,
    "end": 1932444,
    "text": "私たちはこれらのノートを更新するつもりはないので、実際には、私たちの例題の最新版である抱擁顔アライメント・ハンドブックがある。"
  },
  {
    "start": 1933154,
    "end": 1933938,
    "text": "それをチェックしてくれ。"
  },
  {
    "start": 1933986,
    "end": 1943802,
    "text": "GPUが貧弱で、コンシューマー向けGPUに24ギガのRAMを搭載している場合、Loraで低リソースの例を実行できる。"
  },
  {
    "start": 1943978,
    "end": 1951294,
    "text": "GPUが豊富であれば、マルチGPU、マルチノードの例があり、アクセラレートとディープスピードで分散トレーニングを行うことができる。"
  },
  {
    "start": 1952034,
    "end": 1958254,
    "text": "このアライメントハンドブックでは、これらの例を実行するための設定やパラメータスクリプトを提供する。"
  },
  {
    "start": 1960094,
    "end": 1970674,
    "text": "私たちが提供するノートブックでは、GPUメモリの少ないGPUで微調整を行う技術であるLoraを使用していることをお伝えしておきます。"
  },
  {
    "start": 1971574,
    "end": 1976074,
    "text": "ローラについてはあまり触れませんが、もっと知りたいなら、それについてのリンクをいくつか貼っておきます。"
  },
  {
    "start": 1978854,
    "end": 1984262,
    "text": "もちろん、SFTを実行しようとする場合、最初のステップはデータセットを見つけることだ。"
  },
  {
    "start": 1984398,
    "end": 1988262,
    "text": "これは、Zephyrの微調整に実際に使っているデータセットだ。"
  },
  {
    "start": 1988318,
    "end": 1991878,
    "text": "これはウルトラ・チャット・データセットのサブセットである。"
  },
  {
    "start": 1992046,
    "end": 1997994,
    "text": "このデータセットには、プロンプトとメッセージの2つのキーが含まれている。"
  },
  {
    "start": 1998814,
    "end": 2007686,
    "text": "プロンプトがどのようなものかの一例を挙げると、ロンドンで有名なランドボックスを訪れるべきか、c言語でプログラムを書きなさい、といったようなものだ。"
  },
  {
    "start": 2007870,
    "end": 2010834,
    "text": "それともYouTubeでケーキの作り方のチュートリアルを作る？"
  },
  {
    "start": 2011144,
    "end": 2013360,
    "text": "には回答が含まれている。"
  },
  {
    "start": 2013392,
    "end": 2017324,
    "text": "これらのプロンプトに対する回答は、マルチターンかもしれないし、シングルターンかもしれない。"
  },
  {
    "start": 2017944,
    "end": 2027924,
    "text": "もしデータセットに興味があれば、私たちは30ほどのSFTデータセットのリポジトリを共有しています。"
  },
  {
    "start": 2028304,
    "end": 2035496,
    "text": "データセットカードを見て、自分のやりたいことに適しているかどうかチェックしてみてください。"
  },
  {
    "start": 2035680,
    "end": 2040234,
    "text": "チャットボット作りに興味があるなら、ここから始めるといいだろう。"
  },
  {
    "start": 2042774,
    "end": 2051142,
    "text": "だから、このデータセットにこれらのメッセージがある場合、実際には、微調整を実行するためにモデルに渡すことができるようにフォーマットする必要がある。"
  },
  {
    "start": 2051278,
    "end": 2055374,
    "text": "そのためには、チャットテンプレートと呼ばれるものを使います。"
  },
  {
    "start": 2055534,
    "end": 2057994,
    "text": "チャットテンプレートはいくつもあります。"
  },
  {
    "start": 2058414,
    "end": 2062742,
    "text": "チャットテンプレートとは、一連のメッセージのようなものです。"
  },
  {
    "start": 2062798,
    "end": 2071244,
    "text": "この場合、数学の問題があり、それはチャットボットに質問された一連の数学の問題で、それをモデルにフィードできるようにフォーマットする必要がある。"
  },
  {
    "start": 2072344,
    "end": 2082064,
    "text": "チャットテンプレートのアイデアは、メッセージの異なるセクションを異なるトークンや異なるもので区切ることです。"
  },
  {
    "start": 2082144,
    "end": 2085672,
    "text": "例えば、システム・プロンプトが緑色で表示されているとする。"
  },
  {
    "start": 2085728,
    "end": 2094044,
    "text": "メッセージのこの部分はユーザーからのもので、この部分はアシスタントからのもので、といった具合だ。"
  },
  {
    "start": 2094764,
    "end": 2098452,
    "text": "言っておくが、この上部にあるシステム・プロンプトは、この設定では空白になっている。"
  },
  {
    "start": 2098588,
    "end": 2109804,
    "text": "例えば、海賊のように振る舞うチャットボットの例をいくつかハブで紹介しています。"
  },
  {
    "start": 2109884,
    "end": 2121624,
    "text": "あるいは、数学ベースのチャットボットを作りたい場合、「あなたは数学の先生だから、数学の先生のような振る舞いをしなさい。"
  },
  {
    "start": 2121704,
    "end": 2125320,
    "text": "あなたのデータセットに、このようなシステムプロンプトが実際に注釈されているかもしれない。"
  },
  {
    "start": 2125352,
    "end": 2133244,
    "text": "モデルをトレーニングするときに、モデルの反応がなぜあるような振る舞いをするのか、その理由をより詳しく説明するのに役立つことがある。"
  },
  {
    "start": 2137384,
    "end": 2139680,
    "text": "先ほども言ったように、テンプレートはいくつもある。"
  },
  {
    "start": 2139792,
    "end": 2142368,
    "text": "どちらかひとつを選ぶとすれば、チャットメルのほうをお勧めする。"
  },
  {
    "start": 2142416,
    "end": 2152104,
    "text": "チャットテンプレートを適用するのはとても簡単です。"
  },
  {
    "start": 2152144,
    "end": 2153328,
    "text": "トランスフォーマーライブラリーで"
  },
  {
    "start": 2153376,
    "end": 2162336,
    "text": "最近、2ヶ月ほど前に、トークナイザーにカスタム・チャット・テンプレートを追加しました。"
  },
  {
    "start": 2162520,
    "end": 2169524,
    "text": "メッセージをテンプレート化するjinja文字列のようなものを定義することができます。"
  },
  {
    "start": 2170144,
    "end": 2173830,
    "text": "これを全部読むとは思っていないが、それならとても簡単だ。"
  },
  {
    "start": 2173862,
    "end": 2180494,
    "text": "メッセージにトークナイザーapplychatテンプレートを呼び出すだけで、このようにフォーマットされます。"
  },
  {
    "start": 2180534,
    "end": 2185434,
    "text": "前のスライドでアシスタント・ユーザーの区切りで示したように。"
  },
  {
    "start": 2188974,
    "end": 2193190,
    "text": "データが適切なチャップ・テンプレートにフォーマットされれば、とても簡単だ。"
  },
  {
    "start": 2193222,
    "end": 2195686,
    "text": "TRlでは、SFtトレーナーをロードするだけ。"
  },
  {
    "start": 2195710,
    "end": 2197334,
    "text": "TRLロードでは。"
  },
  {
    "start": 2197374,
    "end": 2201274,
    "text": "例えば、この場合、ノートブックでは、Mister L seven Bモデルをロードする。"
  },
  {
    "start": 2202254,
    "end": 2208686,
    "text": "学習率、エポック数、バッチサイズなど、学習に関する引数を指定することができる。"
  },
  {
    "start": 2208870,
    "end": 2212662,
    "text": "SFTトレーナーにこの情報を提供し、トレーナートレーニングに連絡してください。"
  },
  {
    "start": 2212838,
    "end": 2218114,
    "text": "この特殊なデータセットを使ってチャットボットを訓練するには、数時間かかるでしょう。"
  },
  {
    "start": 2218934,
    "end": 2223982,
    "text": "アラインメント・ハンドブックでは、異なるデータセットを組み合わせる方法を提供している。"
  },
  {
    "start": 2224038,
    "end": 2234174,
    "text": "興味のあるデータセットがいくつも見つかれば、それらのデータセットを適切な方法で組み合わせることで、さまざまな種類のデータで学習できるようになる。"
  },
  {
    "start": 2240474,
    "end": 2243370,
    "text": "これで最初のSFTチャットボットが完成しました。"
  },
  {
    "start": 2243402,
    "end": 2253634,
    "text": "次に、ある種のアライメントを実行し、基本的に、あなたが特定の製品に対して持っているかもしれないいくつかの好みに合わせたい。"
  },
  {
    "start": 2254614,
    "end": 2258054,
    "text": "パイプラインという点では、手順はほとんど同じだ。"
  },
  {
    "start": 2258094,
    "end": 2265222,
    "text": "データセットをロードする場合、メッセージと選択したプロンプト、拒否したプロンプトに加え、いくつかのキーが追加される。"
  },
  {
    "start": 2265358,
    "end": 2269434,
    "text": "つまり、これがあなた方のような人間が選んだ例であり、拒絶された例なのだ。"
  },
  {
    "start": 2271934,
    "end": 2280720,
    "text": "私たちはこれらのデータセットをいくつも提供しているので、ハブには15ほどの嗜好データセットの例があると思います。"
  },
  {
    "start": 2280752,
    "end": 2285044,
    "text": "スタート地点に立ちたいなら、このリンクをチェックしてほしい。"
  },
  {
    "start": 2287344,
    "end": 2292432,
    "text": "そう、これらのデータセットには、プロンプトが含まれ、選択された回答と拒否された回答が含まれている。"
  },
  {
    "start": 2292488,
    "end": 2297008,
    "text": "先ほどルイスが言っていた損失を振り返ってみただけだ。"
  },
  {
    "start": 2297096,
    "end": 2305764,
    "text": "この場合のx、y W、y Lは、データセットから抽出されるこれらの種類のタプルである。"
  },
  {
    "start": 2309104,
    "end": 2314320,
    "text": "ハブ上のすべてのデータセットが、二値化され、選択され、拒否されただけのこの形式を持っているわけではない。"
  },
  {
    "start": 2314392,
    "end": 2316912,
    "text": "多くの反応があることもある。"
  },
  {
    "start": 2316968,
    "end": 2320204,
    "text": "この場合、1つのプロンプトに対して4つの回答がある。"
  },
  {
    "start": 2320624,
    "end": 2327564,
    "text": "現在のTRLのDPlE実装では、2値化されたプリファレンスしかサポートしていません。"
  },
  {
    "start": 2328024,
    "end": 2333004,
    "text": "ある回答が他の回答より高いということはよくあることだ。"
  },
  {
    "start": 2333044,
    "end": 2335300,
    "text": "なぜ、これらに格付けが適用されたのか？"
  },
  {
    "start": 2335372,
    "end": 2343428,
    "text": "ゼファーの場合、最高ランクの回答を選び、他の回答をランダムに選びました。"
  },
  {
    "start": 2343516,
    "end": 2349932,
    "text": "常に上位2人を選ぶよりも、その方がより多様なデータが得られると思ったからだ。"
  },
  {
    "start": 2350108,
    "end": 2357064,
    "text": "マージンが大きい例と小さい例があり、モデルにとってより多様な学習データが得られる。"
  },
  {
    "start": 2358554,
    "end": 2361298,
    "text": "もちろんマルチターンの例もある。"
  },
  {
    "start": 2361466,
    "end": 2368154,
    "text": "この場合、多くのデータセットがこれを持っており、通常、どちらかである最後のレスポンスが異なる。"
  },
  {
    "start": 2368194,
    "end": 2369930,
    "text": "選ばれるか、拒否されるかだ。"
  },
  {
    "start": 2370122,
    "end": 2375786,
    "text": "他のデータセットもある。例えば、オープンアシスタントのデータセットで、これは会話の大きな木のようなものだ。"
  },
  {
    "start": 2375890,
    "end": 2379490,
    "text": "選ばれる側と拒否される側の組み合わせはさまざまだ。"
  },
  {
    "start": 2379522,
    "end": 2385694,
    "text": "DPOを行うための処理方法があるのですが、それはこの話を少し超えています。"
  },
  {
    "start": 2386834,
    "end": 2394522,
    "text": "チャットのテンプレートを適用するという点では、これら2つの会話は独立したものとして扱われる。"
  },
  {
    "start": 2394578,
    "end": 2406414,
    "text": "これらの会話を個別にトークン化し、モデルを通してフォワードパスを行い、損失を計算するときに初めて、これらの会話が選ばれたものと拒否されたものでリンクしているという事実を意識する必要があるのだ。"
  },
  {
    "start": 2408954,
    "end": 2412938,
    "text": "チャットテンプレートの適用は、SFTの段階と非常によく似ている。"
  },
  {
    "start": 2413106,
    "end": 2417334,
    "text": "選んだメッセージ、拒否されたメッセージ、そしてプロンプトに適用するだけだ。"
  },
  {
    "start": 2417714,
    "end": 2424614,
    "text": "DPOのトレーナーは、選ばれし者、拒否されし者のフォワードパスの複雑さに対処する。"
  },
  {
    "start": 2428714,
    "end": 2434214,
    "text": "DPOを実行する場合は、TRLからDPOトレーナーをロードする。"
  },
  {
    "start": 2434714,
    "end": 2437654,
    "text": "SFT モデルのモデル ID を指定できます。"
  },
  {
    "start": 2438154,
    "end": 2450234,
    "text": "繰り返しになるが、学習レート、勾配、累積ステップ数、エポック数、バッチサイズなど、多くの学習引数を与えることができ、さらにDPOトレーナーに与えるパラメーターが2つある。"
  },
  {
    "start": 2450774,
    "end": 2458494,
    "text": "ひとつはベータ・パラメーターで、このベータ・パラメーターは、大雑把に言えば、ベース・モデルからどれだけ逸脱できるかをコントロールするものだ。"
  },
  {
    "start": 2458614,
    "end": 2466354,
    "text": "もしベータが非常に高ければ、ベースモデルからの逸脱にペナルティーを課し、逆にベータが非常に低ければ、それほどペナルティーを課さない。"
  },
  {
    "start": 2466894,
    "end": 2473812,
    "text": "我々は最近、異なるベータ値と異なる損失タイプを比較したブログ記事を発表した。"
  },
  {
    "start": 2473868,
    "end": 2489584,
    "text": "DPOでは、DPOロス、KTOロス、IPoをDPOトレーナーで実装しています。もうすぐリリースされるKTOの新しいバージョンでは、ペアリングされていないプリファレンス設定で動作することができます。"
  },
  {
    "start": 2490164,
    "end": 2491612,
    "text": "はい、ではこのブログ記事をご覧ください。"
  },
  {
    "start": 2491668,
    "end": 2496952,
    "text": "異なるベータ値を持つこれらの異なるアルゴリズムとそのパフォーマンスの例がいくつか掲載されている。"
  },
  {
    "start": 2497068,
    "end": 2502244,
    "text": "ルイスが言ったように、適切なハブのパラメーターを選べば、ほとんどの場合、両者は同等だ。"
  },
  {
    "start": 2508224,
    "end": 2518560,
    "text": "私はただ、これらのアルゴリズムを実行する際に、どのような点に注目すべきか、いくつかのヒントを強調したかっただけです。"
  },
  {
    "start": 2518672,
    "end": 2521288,
    "text": "そう、ひとつはベータ版そのものなのだ。"
  },
  {
    "start": 2521456,
    "end": 2525724,
    "text": "そのため、通常は0.01から1までテストする。"
  },
  {
    "start": 2527224,
    "end": 2529936,
    "text": "私たちは、非常に小さな値がうまく機能することを発見した。"
  },
  {
    "start": 2530000,
    "end": 2534664,
    "text": "もし一種のスキャンをするのであれば、一様な範囲だけでなく、より良いスキャンをした方がいいだろう。"
  },
  {
    "start": 2534824,
    "end": 2538124,
    "text": "低い値にもう少し集中する。"
  },
  {
    "start": 2539064,
    "end": 2549444,
    "text": "一般的に、学習速度はSFTのステップよりもずっとずっと小さく、例えばゼファーのモデルでは確か5eマイナー7を使った。"
  },
  {
    "start": 2550184,
    "end": 2553804,
    "text": "非常に、非常に小さな学習率が最も適切だと思われる。"
  },
  {
    "start": 2555024,
    "end": 2555840,
    "text": "一般的にはそうだ。"
  },
  {
    "start": 2555872,
    "end": 2563884,
    "text": "プリトレーニングやSFTを行う場合、GPUメモリに収まる最大のバッチサイズを用意しようとします。"
  },
  {
    "start": 2564504,
    "end": 2570304,
    "text": "DPOでわかったのは、グローバルバッチサイズとエポック数の間にトレードオフがあるということだ。"
  },
  {
    "start": 2570344,
    "end": 2576208,
    "text": "グローバルバッチサイズは、すべてのGPUのバッチサイズのようなもので、考慮する必要があります。"
  },
  {
    "start": 2576376,
    "end": 2579528,
    "text": "バッチサイズを最大にすれば収まるというような些細なことではない。"
  },
  {
    "start": 2579536,
    "end": 2583864,
    "text": "GPUオプティマイザについても興味深い質問だ。"
  },
  {
    "start": 2583904,
    "end": 2596884,
    "text": "DPOの論文では、彼らはRMのプロップを使っていたが、私たちはアダムの方がより良いスケジューラーであることを発見した。"
  },
  {
    "start": 2597744,
    "end": 2609244,
    "text": "非常に興味深いのは、SFTモデルを訓練し、自動評価ベンチマークの1つで評価できることです。"
  },
  {
    "start": 2610034,
    "end": 2614362,
    "text": "最適なSFTモデルを選び、データセットに対してDPOを実行する。"
  },
  {
    "start": 2614538,
    "end": 2619050,
    "text": "SFTのベストモデルからDPOのベストモデルが得られるとは限らない。"
  },
  {
    "start": 2619122,
    "end": 2624642,
    "text": "同じデータセットの場合、性能の悪いSFTモデルがより優れたDPOモデルを生み出すこともある。"
  },
  {
    "start": 2624818,
    "end": 2632682,
    "text": "パイプライン全体のSFT DPOをさまざまなハイパーパラメーターで実施し、最良の結果が得られていることを確認することは、本当に意味のあることです。"
  },
  {
    "start": 2632778,
    "end": 2648244,
    "text": "ローラの話に戻るが、ローラを使ったいくつかの実験では、完全な微調整と比較して、トレーニング中にモデルが正則化されることがわかった。"
  },
  {
    "start": 2649384,
    "end": 2650512,
    "text": "どういう意味だろう？"
  },
  {
    "start": 2650528,
    "end": 2659808,
    "text": "申し訳ないのですが、少し画質が悪いのですが、簡単にご紹介します。"
  },
  {
    "start": 2659856,
    "end": 2666934,
    "text": "右側は、ウルトラフィードバックデータセットだと思うが、3エポックにわたるトレーニング中のDPO損失である。"
  },
  {
    "start": 2667634,
    "end": 2677266,
    "text": "緑が訓練したLoraモデルで、ピンクが同じハイパーパラメータを使った完全な微調整である。"
  },
  {
    "start": 2677290,
    "end": 2678614,
    "text": "ロラを使わないだけだ。"
  },
  {
    "start": 2679314,
    "end": 2692654,
    "text": "興味深いのは、最初のエポックの後、ピンクの損失がゼロになり、少なくともトレーニングの損失はゼロになるのに対し、評価の損失はピークに達することだ。"
  },
  {
    "start": 2693274,
    "end": 2704530,
    "text": "Loraモデルは自由パラメータが少ないのに対して、明らかにトレーニングデータセットにオーバーフィットしている。"
  },
  {
    "start": 2704602,
    "end": 2707594,
    "text": "だから、ある意味、規則的なんだ。"
  },
  {
    "start": 2707714,
    "end": 2717294,
    "text": "というのも、自由パラメータが少ないので、トレーニングデータセットに過剰にフィットさせることができず、評価損失が減少し続けるか、少なくともプラトーになるからだ。"
  },
  {
    "start": 2718434,
    "end": 2728090,
    "text": "DPOの精度は、選択された例と拒否された例の報酬を予測するために、あなたのモデルを使用することができるからです。"
  },
  {
    "start": 2728162,
    "end": 2732762,
    "text": "各例に対する精度、またはバッチ内の例の数を計算することができます。"
  },
  {
    "start": 2732898,
    "end": 2745754,
    "text": "ローラを使ったこの正則化ではこの問題は起こらない。"
  },
  {
    "start": 2745874,
    "end": 2750076,
    "text": "これはMtBenchでの性能向上につながり、実に興味深いことだと思った。"
  },
  {
    "start": 2750180,
    "end": 2760424,
    "text": "DPOのトレーニング中に見る指標について少しお話しすると、これを示すのはこれらのプロットだけではありません。"
  },
  {
    "start": 2761524,
    "end": 2785388,
    "text": "そして、選択された例に対する報酬が拒否された例に対する報酬よりも大きい回数を調べ、精度を計算することができます。"
  },
  {
    "start": 2785436,
    "end": 2788904,
    "text": "様々なトレーニングケースで見られることだ。"
  },
  {
    "start": 2789604,
    "end": 2793708,
    "text": "また、トレーニング中のバッチの報酬を見ることもできる。"
  },
  {
    "start": 2793876,
    "end": 2796196,
    "text": "報酬は興味深く、興味深い。"
  },
  {
    "start": 2796260,
    "end": 2800244,
    "text": "不合格の報酬よりも、選ばれた報酬の方が高いはずだ。"
  },
  {
    "start": 2800284,
    "end": 2800820,
    "text": "そうだね。"
  },
  {
    "start": 2800972,
    "end": 2802396,
    "text": "本当に興味があるのはマージンだ。"
  },
  {
    "start": 2802460,
    "end": 2804008,
    "text": "両者の違い"
  },
  {
    "start": 2804156,
    "end": 2806368,
    "text": "そのため、トレーニング中にマージンが増えていくはずだ。"
  },
  {
    "start": 2806416,
    "end": 2811912,
    "text": "それは、選ばれた者がより高い報酬を得、そして一括して拒否されることを識別することであるべきだ。"
  },
  {
    "start": 2812088,
    "end": 2813048,
    "text": "というわけで、これがこの写真である。"
  },
  {
    "start": 2813056,
    "end": 2819084,
    "text": "青が評価データセットで、トレーニング中にマージンが増加しているのがわかります。"
  },
  {
    "start": 2824104,
    "end": 2829352,
    "text": "もちろん、DPO研修の間、すべてがうまくいくとは限らない。"
  },
  {
    "start": 2829408,
    "end": 2831192,
    "text": "うまくいかないこともあるだろう。"
  },
  {
    "start": 2831248,
    "end": 2834174,
    "text": "その理由に目を向けることが重要だ。"
  },
  {
    "start": 2835234,
    "end": 2841570,
    "text": "この場合、何が問題なのかは、この指標からはまったくわからない。"
  },
  {
    "start": 2841682,
    "end": 2848494,
    "text": "だから、精度が上がり、停滞し、そして下がっていくのがわかる。"
  },
  {
    "start": 2850434,
    "end": 2854054,
    "text": "選ばれた報酬は停滞し、下降していく。"
  },
  {
    "start": 2854554,
    "end": 2859788,
    "text": "却下された報酬はプラトーになり、下がっていく。"
  },
  {
    "start": 2859946,
    "end": 2864608,
    "text": "あなたのマージンはゼロ以上ですが、ほんの少しです。"
  },
  {
    "start": 2864776,
    "end": 2866048,
    "text": "うるさいんだ。"
  },
  {
    "start": 2866096,
    "end": 2871764,
    "text": "だから、この走りには明らかに何か問題がある、あるいは何か問題があるように見えるが、それが何なのかはわからなかった。"
  },
  {
    "start": 2872944,
    "end": 2875144,
    "text": "だから、過去にさかのぼって敗戦を振り返ることが本当に重要なんだ。"
  },
  {
    "start": 2875224,
    "end": 2879880,
    "text": "だからここでは、ロスが超高くて、本当にトゲトゲしているのがよくわかる。"
  },
  {
    "start": 2880072,
    "end": 2883208,
    "text": "この場合、学習率が高すぎるのだ。"
  },
  {
    "start": 2883336,
    "end": 2889740,
    "text": "おそらくスケジューラーの使い方に関係しているのだろう。"
  },
  {
    "start": 2889772,
    "end": 2903684,
    "text": "もし1エポックだったら、この問題を解決するのに十分な速さで減衰するだろう。"
  },
  {
    "start": 2903764,
    "end": 2906664,
    "text": "私たちは3人分のトレーニングをしていたのだから。"
  },
  {
    "start": 2907044,
    "end": 2912922,
    "text": "そのため、最適化する際にオーバーステップしてしまい、スパイクが発生してしまうのだ。"
  },
  {
    "start": 2913098,
    "end": 2915642,
    "text": "このような問題があれば、学習率を下げればいい。"
  },
  {
    "start": 2915658,
    "end": 2918934,
    "text": "これは教師あり学習でも典型的な問題だ。"
  },
  {
    "start": 2921474,
    "end": 2921874,
    "text": "そうだね。"
  },
  {
    "start": 2921914,
    "end": 2925610,
    "text": "チャットボットをトレーニングし、いくつかの高いパラメータを見つけました。"
  },
  {
    "start": 2925642,
    "end": 2932974,
    "text": "そのような仕事をすると、損失が減少する。"
  },
  {
    "start": 2933554,
    "end": 2935362,
    "text": "これにはいくつかの方法がある。"
  },
  {
    "start": 2935458,
    "end": 2953728,
    "text": "私たちが社内で見ているのは2つで、1つは空のベンチマークで、推論、数学、要約、創造性などの例をテストする多回数の対話データセットのようなものです。"
  },
  {
    "start": 2953896,
    "end": 2961044,
    "text": "そのコードにもたくさんの例があり、基本的に0点から10点の間で点数がつけられる。"
  },
  {
    "start": 2961544,
    "end": 2967290,
    "text": "GPT 4のような最新のモデルは、この点ではかなり優れている。"
  },
  {
    "start": 2967322,
    "end": 2969402,
    "text": "このベンチマークでは9.4だ。"
  },
  {
    "start": 2969458,
    "end": 2971094,
    "text": "彼らは非常に優秀だ。"
  },
  {
    "start": 2971714,
    "end": 2979746,
    "text": "なお、内部的には、空ベンチ評価ツールはGPT4を使ってモデルの良し悪しを評価している。"
  },
  {
    "start": 2979930,
    "end": 2989734,
    "text": "特にコードや数学の分野では、GPT4というモデルが正しい答えを出し、あなたが評価しているモデルは間違っているとGPT4が言うという例を実際にいくつか見てきました。"
  },
  {
    "start": 2990434,
    "end": 2992018,
    "text": "要点はわかっている。"
  },
  {
    "start": 2992066,
    "end": 2998804,
    "text": "現在、GPT4よりも良い結果を出しているモデルもあり、非常に興味深い。"
  },
  {
    "start": 2998844,
    "end": 3001244,
    "text": "明らかに、平均的にはそうではない。"
  },
  {
    "start": 3001364,
    "end": 3010116,
    "text": "このような例がいくつかあり、ここではベンチスコアが空っぽのモデルをプロットしている。"
  },
  {
    "start": 3010300,
    "end": 3015344,
    "text": "2つ目のベンチマークはアルパカ・エヴァルである。"
  },
  {
    "start": 3015924,
    "end": 3021404,
    "text": "基本的には、GPT4ターボに対するあなたのモデルの評価です。"
  },
  {
    "start": 3021484,
    "end": 3024694,
    "text": "だから、この場合のベストモデルはこうなる。"
  },
  {
    "start": 3024844,
    "end": 3030134,
    "text": "GPTの4ターボで、基本的に50はもらえるだろう。"
  },
  {
    "start": 3030554,
    "end": 3036442,
    "text": "グウェンモデルの勝率は27％で、かなりいい。"
  },
  {
    "start": 3036538,
    "end": 3039010,
    "text": "このプロットはちょっと奇妙だ。"
  },
  {
    "start": 3039042,
    "end": 3040210,
    "text": "リニアではない。"
  },
  {
    "start": 3040242,
    "end": 3041546,
    "text": "丸太の目盛りのようなものだ。"
  },
  {
    "start": 3041730,
    "end": 3043746,
    "text": "これでも実際より近く見える。"
  },
  {
    "start": 3043770,
    "end": 3047614,
    "text": "まだ半分、半分強だが、確実にそこに近づいている。"
  },
  {
    "start": 3049114,
    "end": 3053882,
    "text": "これらのモデルがGT4のクオリティに近づいているスピードには驚かされる。"
  },
  {
    "start": 3053938,
    "end": 3065354,
    "text": "ゼファーは去年の9月くらいに、確か7.3％、15％か10％くらい落ちていたと思う。"
  },
  {
    "start": 3065434,
    "end": 3069082,
    "text": "これで、GPT4位のレベルにずっと、ずっと近づいた。"
  },
  {
    "start": 3069138,
    "end": 3073254,
    "text": "コミュニティがこのようなモデルを作るのは本当に素晴らしいことだ。"
  },
  {
    "start": 3075354,
    "end": 3078130,
    "text": "他にもいくつか触れておくべきベンチマークがある。"
  },
  {
    "start": 3078202,
    "end": 3083888,
    "text": "だから、一番いいのは人間による評価だ。"
  },
  {
    "start": 3083976,
    "end": 3086392,
    "text": "人的評価をする余裕があるなら、そうしているはずだ。"
  },
  {
    "start": 3086448,
    "end": 3102568,
    "text": "最終的なモデルが出来上がり、内部的に良いものだと確信したら、実際に人間に評価してもらい、アライメントやモデルの反応の質、その他いくつかのベンチマークで満足のいくものであることを確認するべきだ。"
  },
  {
    "start": 3102616,
    "end": 3104736,
    "text": "もちろん、オープンLLMのリーダーボードもある。"
  },
  {
    "start": 3104840,
    "end": 3112244,
    "text": "これはチャットボットを評価するために使うことができ、確かにチャットボットが上位にランクインしているが、チャットボットに特化したリーダーボードというわけではない。"
  },
  {
    "start": 3112784,
    "end": 3125648,
    "text": "今、リークに関する多くの疑問がある。意図しているかどうかにかかわらず、リーダーボードからの質問を含むデータセットを使用するのではないか、また、リーダーボードへのオーバーフィットに関する疑問もある。"
  },
  {
    "start": 3125696,
    "end": 3133464,
    "text": "このツールは、時間が経てば経つほど使い勝手が悪くなっていくと思うが、今のところは確かに意味がある。"
  },
  {
    "start": 3133624,
    "end": 3140044,
    "text": "前述したように、ここには空のベンチとアルパカ・エバルのリンクがいくつかあり、これらの評価ツールの実際の使い方も紹介されている。"
  },
  {
    "start": 3141294,
    "end": 3144198,
    "text": "ラナ・インデックスは検索に重点を置いている。"
  },
  {
    "start": 3144286,
    "end": 3148594,
    "text": "検索に興味があるなら、これはモデルを評価するのに非常に良いツールだ。"
  },
  {
    "start": 3149214,
    "end": 3151646,
    "text": "Lmsysのチャットボット・アリーナもある。"
  },
  {
    "start": 3151710,
    "end": 3158182,
    "text": "だから、実際にこのモデルを評価したいのであれば、Lmsysにモデルのプールに追加してもらう必要があると思う。"
  },
  {
    "start": 3158278,
    "end": 3163154,
    "text": "確かに良いモデルであり、自分のモデルを評価するのに使うには良いツールだ。"
  },
  {
    "start": 3166094,
    "end": 3174790,
    "text": "このプロットの作者のツイートにリンクした別のプロットがあるのだが、これはさまざまなベンチマークを比較するのにとても便利なツールだ。"
  },
  {
    "start": 3174942,
    "end": 3178994,
    "text": "ここでは、異なるベンチマーク間の相関関係を見ることができる。"
  },
  {
    "start": 3179414,
    "end": 3180894,
    "text": "だから、こういうのは本当に役に立つ。"
  },
  {
    "start": 3180934,
    "end": 3187606,
    "text": "例えば、左上が人間の評価だとする。"
  },
  {
    "start": 3187710,
    "end": 3191070,
    "text": "ほら、空のベンチは人間の評価と0.9の相関関係がある。"
  },
  {
    "start": 3191142,
    "end": 3193272,
    "text": "これは非常に優れたツールであることを示している。"
  },
  {
    "start": 3193438,
    "end": 3196184,
    "text": "画一的であれば、それは人間の評価のかなり良い代用品だ。"
  },
  {
    "start": 3199484,
    "end": 3205064,
    "text": "このプロットには、他にもさまざまな指標やツールがある。"
  },
  {
    "start": 3208564,
    "end": 3209596,
    "text": "私からは以上だ。"
  },
  {
    "start": 3209620,
    "end": 3211864,
    "text": "質問があれば喜んで答えるよ。"
  },
  {
    "start": 3212884,
    "end": 3213628,
    "text": "驚いたよ。"
  },
  {
    "start": 3213716,
    "end": 3215236,
    "text": "よし、飛び込もう。"
  },
  {
    "start": 3215300,
    "end": 3218864,
    "text": "確かに突っ込みどころはかなりある。"
  },
  {
    "start": 3220614,
    "end": 3221206,
    "text": "完璧だ。"
  },
  {
    "start": 3221310,
    "end": 3232594,
    "text": "では、DPOを画像生成や拡散モデル（lpips、arc based lossなど）のメトリクスを最適化するためにどのように使用し、またはDPOの成功例を見てきましたか？"
  },
  {
    "start": 3234374,
    "end": 3242782,
    "text": "これは私の専門分野ではありませんが、TRLにはDPOを普及モデルに適用するためのDDPOトレーナーがいます。"
  },
  {
    "start": 3242958,
    "end": 3245554,
    "text": "ルイス、もう少し詳しく話してくれないか？"
  },
  {
    "start": 3247324,
    "end": 3255860,
    "text": "いや、DPOラファエロフの生みの親、作者の一人であることは知っていると思う。"
  },
  {
    "start": 3255972,
    "end": 3268940,
    "text": "彼はつい最近、DPOを使って本当に本当に優れた画像を生成する論文を発表した。"
  },
  {
    "start": 3269012,
    "end": 3275774,
    "text": "明らかに完璧なコードとモデルが登場するはずだ。"
  },
  {
    "start": 3278034,
    "end": 3282414,
    "text": "DPOアライメントのために独自のデータセットを作成すべきか、オープンソースにすべきか、どのように判断すればよいのでしょうか？"
  },
  {
    "start": 3283554,
    "end": 3288654,
    "text": "私の意見としては、製品や用途によると思う。"
  },
  {
    "start": 3289314,
    "end": 3293450,
    "text": "だから、例えばコールセンターのような、非常に特殊なアプリケーションがあるかもしれない。"
  },
  {
    "start": 3293522,
    "end": 3304414,
    "text": "チャットボットで通話中の人を助けたり、テキストで話すモデルとして使ったり、クライアントと一緒に仕事をするのにとても便利な場所だと思います。"
  },
  {
    "start": 3304754,
    "end": 3309654,
    "text": "そのような場合は、その用途に特化したアライメントデータセットを用意したほうがよい。"
  },
  {
    "start": 3310994,
    "end": 3313458,
    "text": "そうだね。"
  },
  {
    "start": 3313586,
    "end": 3318378,
    "text": "世の中にはたくさんのデータセットがあり、今後もさらに増えていくだろう。"
  },
  {
    "start": 3318546,
    "end": 3322218,
    "text": "また、多くの合成データセットが生成されている。"
  },
  {
    "start": 3322266,
    "end": 3334222,
    "text": "研究者たちはGPT4やジェミニのような他のモデルを使って、プロンプトに対する合成反応を作っている。"
  },
  {
    "start": 3334238,
    "end": 3341114,
    "text": "このデータセットを完璧なものにするために人間に頼む必要はないかもしれない。"
  },
  {
    "start": 3341934,
    "end": 3350514,
    "text": "metaやNYUの研究のように、自己報酬型言語モデルに特化した、自己学習型報酬llmsのオープンな実装はありますか？"
  },
  {
    "start": 3351174,
    "end": 3352334,
    "text": "ああ、それでこの論文を読んだんだ。"
  },
  {
    "start": 3352414,
    "end": 3357714,
    "text": "本当に興味深い論文で、私たちはこの件について社内で研究を進めている。"
  },
  {
    "start": 3358454,
    "end": 3364598,
    "text": "何か分かり次第、ハンドブックに追加するか、DRLに追加する。"
  },
  {
    "start": 3364726,
    "end": 3365950,
    "text": "でも、うん、超面白い。"
  },
  {
    "start": 3366022,
    "end": 3369358,
    "text": "ああ、もちろんだ。"
  },
  {
    "start": 3369486,
    "end": 3376230,
    "text": "では最後の質問ですが、モデルサイズの拡大がDPO対RLの議論にどのような影響を与えるか、何かお考えはありますか？"
  },
  {
    "start": 3376342,
    "end": 3379794,
    "text": "紙以外では、70億のパラメータでDPOを行っただけだ。"
  },
  {
    "start": 3380954,
    "end": 3383674,
    "text": "これはいい質問だ。"
  },
  {
    "start": 3383754,
    "end": 3389434,
    "text": "その直後に発表されたToluモデルは、700億ものパラメーターを持つモデルだった。"
  },
  {
    "start": 3389474,
    "end": 3391054,
    "text": "10倍はある"
  },
  {
    "start": 3391474,
    "end": 3392266,
    "text": "サイズ。"
  },
  {
    "start": 3392330,
    "end": 3393978,
    "text": "そうだね、ラマモデルかな。"
  },
  {
    "start": 3394066,
    "end": 3396042,
    "text": "ああ、だから大きいんだ。"
  },
  {
    "start": 3396218,
    "end": 3397934,
    "text": "規模的にはうまくいっているようだ。"
  },
  {
    "start": 3398914,
    "end": 3410596,
    "text": "DPOはRLよりはるかに簡単にスケールアップできる。"
  },
  {
    "start": 3410700,
    "end": 3415556,
    "text": "強化学習を使うことが非常に理にかなっている有効なアプリケーションがある。"
  },
  {
    "start": 3415740,
    "end": 3422508,
    "text": "最も明白なのは、最適化するものへのループ内フィードバックが欲しい場合だ。"
  },
  {
    "start": 3422556,
    "end": 3433390,
    "text": "良い例として、もし本当に優れたコード・アシスタントを育てたいのであれば、そのアシスタントに、チャットが行うのと非常に似た方法でコードをデバッグできるようになってもらいたい。"
  },
  {
    "start": 3433542,
    "end": 3446694,
    "text": "プロンプトを与え、コードを生成し、インタープリター上で実際にコードを実行し、スタックトレースからフィードバックを得る。"
  },
  {
    "start": 3446854,
    "end": 3455302,
    "text": "これは、強化学習が、コンパイルして実行されるコードの正しい答えを生成するモデルをより良くするのに役立つ素晴らしい例だ。"
  },
  {
    "start": 3455438,
    "end": 3459668,
    "text": "DPOは通常オフラインなので、DPOでやるのはちょっと難しい。"
  },
  {
    "start": 3459756,
    "end": 3462984,
    "text": "すべての嗜好データを一度に作成し、それをもとにトレーニングを行う。"
  },
  {
    "start": 3464884,
    "end": 3465524,
    "text": "完璧だ。"
  },
  {
    "start": 3465644,
    "end": 3465996,
    "text": "素晴らしい。"
  },
  {
    "start": 3466060,
    "end": 3469564,
    "text": "さて、今日の質問時間はここまでだと思います。"
  },
  {
    "start": 3469604,
    "end": 3471764,
    "text": "ルイスとエド、本当にありがとう。"
  },
  {
    "start": 3471804,
    "end": 3472868,
    "text": "素晴らしいセッションだった。"
  },
  {
    "start": 3472916,
    "end": 3475284,
    "text": "私たちのコミュニティが本当に楽しんでいることは知っている。"
  },
  {
    "start": 3475444,
    "end": 3478516,
    "text": "イベント終了後、スライドとノートをお送りします。"
  },
  {
    "start": 3478580,
    "end": 3482580,
    "text": "YouTubeのビデオの説明文に書いてあるはずだ。"
  },
  {
    "start": 3482732,
    "end": 3483468,
    "text": "みんな、ありがとう。"
  },
  {
    "start": 3483556,
    "end": 3483956,
    "text": "さようなら。"
  },
  {
    "start": 3484020,
    "end": 3484540,
    "text": "みんな、ありがとう。"
  },
  {
    "start": 3484612,
    "end": 3484804,
    "text": "さようなら。"
  }
]