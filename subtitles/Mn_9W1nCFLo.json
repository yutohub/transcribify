[
  {
    "start": 90,
    "end": 2730,
    "text": "こんにちは、ラマについての新しいビデオへようこそ。"
  },
  {
    "start": 2810,
    "end": 15642,
    "text": "このビデオでは、リャマとは何か、どのように作られるのか、トランスフォーマーとは構造的にどう違うのか、そしてリャマを構成する各ブロックを組み立てていく。"
  },
  {
    "start": 15706,
    "end": 28950,
    "text": "それぞれのブロックが何をしているのか、概念的に説明するだけでなく、数学的な観点やコーディングの観点からも探求し、理論と実践を一体化できるようにします。"
  },
  {
    "start": 29370,
    "end": 47820,
    "text": "このビデオを見ていただければ、llamaがどのようなモデルなのかを深く理解することができ、ブロック同士がどのように作用し合っているのかがわかるだけでなく、ブロックがどのように機能しているのか、そもそもなぜこのようなブロックが必要なのかがわかるようになると保証できる。"
  },
  {
    "start": 49150,
    "end": 52978,
    "text": "このビデオでは、多くのトピックをレビューする。"
  },
  {
    "start": 53094,
    "end": 58062,
    "text": "バニラ・トランスとラマ・モデルのアーキテクチャの違いから説明しよう。"
  },
  {
    "start": 58196,
    "end": 69230,
    "text": "我々は、新しい正規化、RMS正規化回転位置埋め込みKVキャッシュマルチクエリ注意、グループ化されたマルチクエリ注意、フィードフォワード層のZwiglu活性化関数とは何であるかを見ていきます。"
  },
  {
    "start": 69390,
    "end": 73886,
    "text": "もちろん、あなたがある程度の予備知識を持っていることは当然のことだ。"
  },
  {
    "start": 73998,
    "end": 80950,
    "text": "まず最初に、トランスフォーマーがどのように機能するのかを知る必要があるので、トランスフォーマーに関する前回のビデオをご覧になることを強くお勧めする。"
  },
  {
    "start": 81100,
    "end": 86150,
    "text": "前回のビデオでは、トランスフォーマーモデルのトレーニングと推論のコンセプトについても説明した。"
  },
  {
    "start": 86300,
    "end": 93270,
    "text": "約45分で、トランスフォーマーについて深く理解することができるので、一見の価値があると思う。"
  },
  {
    "start": 93350,
    "end": 97082,
    "text": "その知識を持ってから、とにかくこのビデオを見ればいい。"
  },
  {
    "start": 97216,
    "end": 106510,
    "text": "すでにビデオを見たが、いくつか忘れてしまったという人のために、トピックを進めながら、ほとんどのコンセプトを復習していこう。"
  },
  {
    "start": 106930,
    "end": 111226,
    "text": "また、あなたが基本的な線形代数の知識を持っていることは当然だと思う。"
  },
  {
    "start": 111258,
    "end": 114970,
    "text": "行列の乗算、ドット積、とにかく基本的なものだ。"
  },
  {
    "start": 115140,
    "end": 121854,
    "text": "また、回転位置埋め込みを使うので、基本的なことではないにせよ、複素数についての知識も必要だ。"
  },
  {
    "start": 121902,
    "end": 127942,
    "text": "複素数やその仕組み、LRSの公式を覚えていなくても問題ない。"
  },
  {
    "start": 128076,
    "end": 130354,
    "text": "数学ではなく、コンセプトを理解することになる。"
  },
  {
    "start": 130482,
    "end": 132994,
    "text": "基本的なことではないんだ。"
  },
  {
    "start": 133122,
    "end": 139290,
    "text": "時には、皆さんがすでに知っているようなトピックを取り上げることもあるので、その部分は読み飛ばしていただいて構わない。"
  },
  {
    "start": 139870,
    "end": 146486,
    "text": "バニラ・トランスとラマとのアーキテクチャーの違いを確認することから、この旅を始めよう。"
  },
  {
    "start": 146678,
    "end": 154320,
    "text": "この写真は、紙の上にあった建築の写真が見つからなかったので、右側に私が建てたものだ。"
  },
  {
    "start": 154690,
    "end": 157034,
    "text": "その違いをおさらいしておこう。"
  },
  {
    "start": 157162,
    "end": 164260,
    "text": "覚えているように、バニラ・トランスにはエンコーダー部とデコーダー部がある。"
  },
  {
    "start": 164630,
    "end": 169870,
    "text": "これがエンコーダーで、右側がデコーダーだ。"
  },
  {
    "start": 170030,
    "end": 173454,
    "text": "一方、リャマにはエンコーダーしかない。"
  },
  {
    "start": 173582,
    "end": 180494,
    "text": "まず、llamaは大規模な言語モデルなので、次の予測トークンタスクで訓練されている。"
  },
  {
    "start": 180622,
    "end": 185586,
    "text": "基本的に、私たちは次のトークンを予測するために自己注意を払うだけでよい。"
  },
  {
    "start": 185618,
    "end": 191020,
    "text": "次の予測タスクが何なのか、それがどのように機能するのか、そしてこの新しい自己注意がどのように機能するのかを見てみよう。"
  },
  {
    "start": 191630,
    "end": 202118,
    "text": "これらの写真からわかる2つ目の違いは、冒頭にエンベッドがあることと、オリジナルのトランスフォーマーにエンベッドがあることだ。"
  },
  {
    "start": 202214,
    "end": 206814,
    "text": "埋め込み直後には、位置エンコーディングはないが、このrmsノルムがある。"
  },
  {
    "start": 206932,
    "end": 211962,
    "text": "実際、すべての規範はブロックの前に移動している。"
  },
  {
    "start": 212026,
    "end": 219074,
    "text": "以前はマルチヘッドが注目され、その後、このプラス記号のようなアドエンドノームがあった。"
  },
  {
    "start": 219112,
    "end": 224686,
    "text": "これは、スキップコネクションとマルチヘッドアテンションとノーマライゼーションの出力を連結したものだ。"
  },
  {
    "start": 224878,
    "end": 238166,
    "text": "正規化とは何か、なぜそのように機能するのかを復習しておこう。"
  },
  {
    "start": 238348,
    "end": 244010,
    "text": "正規化の直後、私たちはこのクエリーキーと値を入力し、自己注意を喚起する。"
  },
  {
    "start": 245470,
    "end": 258634,
    "text": "ひとつ注目すべき点は、位置エンコーディングがトランスフォーマーの位置エンコーディングではなくなり、回転位置エンコーディングになっていること、そしてそれらはクエリーとキーにのみ適用され、値には適用されないことだ。"
  },
  {
    "start": 258682,
    "end": 260400,
    "text": "その理由も見てみよう。"
  },
  {
    "start": 261090,
    "end": 265358,
    "text": "もうひとつは、今、自己注意が自己注意になっていることだ。"
  },
  {
    "start": 265454,
    "end": 269538,
    "text": "kvキャッシュを使って、kvキャッシュとは何か、そしてそれがどのように機能するかを見ていこう。"
  },
  {
    "start": 269624,
    "end": 273090,
    "text": "また、このグループ化されたマルチクエリにも注目している。"
  },
  {
    "start": 274070,
    "end": 277746,
    "text": "もうひとつ変わるのは、このフィードフォワード層だ。"
  },
  {
    "start": 277858,
    "end": 287426,
    "text": "バニラ・トランスのオリジナルのフィードフォワード層では、フィードフォワード・ブロックにrelu活性化関数を持っていた。"
  },
  {
    "start": 287538,
    "end": 291660,
    "text": "llamaではZwiglu関数を使っている。"
  },
  {
    "start": 292750,
    "end": 311230,
    "text": "このnxは、この破線のブロックがn回繰り返され、最後の層の出力がこのrmsノルムに送られ、次に線形層に送られ、そしてソフトマックスに送られることを意味する。"
  },
  {
    "start": 311300,
    "end": 316434,
    "text": "これらのブロックが何をするのか、どのように機能するのか、互いにどのように作用し合うのかを正確にお見せしよう。"
  },
  {
    "start": 316472,
    "end": 319406,
    "text": "その背景にある数学とは何か、彼らが解決しようとした問題とは何か。"
  },
  {
    "start": 319438,
    "end": 328578,
    "text": "これらのモデルについての深い知識を得るために、まずはラマが紹介したモデルを復習することから始めよう。"
  },
  {
    "start": 328754,
    "end": 335798,
    "text": "ラマは2023年2月に発売され、このモデルには4つのディメンションがあった。"
  },
  {
    "start": 335964,
    "end": 341946,
    "text": "あるモデルは67億のパラメーターを持つもので、1332、65。"
  },
  {
    "start": 342048,
    "end": 343290,
    "text": "となると、この数字になる。"
  },
  {
    "start": 343360,
    "end": 344410,
    "text": "どういう意味ですか？"
  },
  {
    "start": 344560,
    "end": 349286,
    "text": "ここでの次元は、埋め込みベクトルのサイズを示す。"
  },
  {
    "start": 349398,
    "end": 354174,
    "text": "ここにあるように、後でレビューする入力エンベッディングがある。"
  },
  {
    "start": 354372,
    "end": 359610,
    "text": "これは基本的に、各トークンをこの次元で示されるサイズのベクトルに変換する。"
  },
  {
    "start": 359690,
    "end": 361514,
    "text": "次にヘッドの数である。"
  },
  {
    "start": 361642,
    "end": 363034,
    "text": "何頭身？"
  },
  {
    "start": 363082,
    "end": 366930,
    "text": "注目はレイヤーの数だ。"
  },
  {
    "start": 367270,
    "end": 371394,
    "text": "オリジナルのトランスフォーマーを思い出してほしい。"
  },
  {
    "start": 371512,
    "end": 373346,
    "text": "頭数は8頭だった。"
  },
  {
    "start": 373448,
    "end": 375700,
    "text": "層の数は6層だったと思う。"
  },
  {
    "start": 376230,
    "end": 381298,
    "text": "そして、各モデルがトレーニングされたトークンの数がわかる。"
  },
  {
    "start": 381394,
    "end": 383650,
    "text": "1,000,000,000,001.4兆ドル。"
  },
  {
    "start": 383730,
    "end": 387026,
    "text": "リャマが2匹になったことで、ほとんどの数字が倍増した。"
  },
  {
    "start": 387058,
    "end": 390390,
    "text": "コンテキストの長さは基本的にシーケンスの長さである。"
  },
  {
    "start": 390470,
    "end": 400646,
    "text": "モデルに与えることができる最長のシーケンスは何であるか、そしてモデルが訓練されたトークンの数も2倍になる。"
  },
  {
    "start": 400678,
    "end": 406670,
    "text": "パラメータはほとんど変わらない。"
  },
  {
    "start": 406820,
    "end": 418786,
    "text": "そして、このGQAという列は、この2つのサイズのモデル、つまり340億と700億がグループ化されたクエリー・アテンションを使用していることを示しています。"
  },
  {
    "start": 418888,
    "end": 423378,
    "text": "まず、ここでエンベッディングレイヤーとは何なのか？"
  },
  {
    "start": 423544,
    "end": 426322,
    "text": "そのために、前回のビデオのスライドを使います。"
  },
  {
    "start": 426376,
    "end": 430150,
    "text": "前回のビデオを覚えていらっしゃるなら、このように埋め込みを紹介しました。"
  },
  {
    "start": 430220,
    "end": 433846,
    "text": "6つの単語からなる文がある。"
  },
  {
    "start": 433948,
    "end": 437554,
    "text": "この文章をトークン化し、トークンに変換するのだ。"
  },
  {
    "start": 437602,
    "end": 443266,
    "text": "トークン化は通常、スペースではなく、BPEのトークナイザーによって行われる。"
  },
  {
    "start": 443298,
    "end": 446342,
    "text": "実際には、各単語はサブワードにも分割される。"
  },
  {
    "start": 446396,
    "end": 455050,
    "text": "わかりやすくするために、ここでは空白を区切り文字として使って文章をトークン化する。"
  },
  {
    "start": 455130,
    "end": 463942,
    "text": "各トークンは他のトークンから空白で区切られ、各トークンは語彙の位置にマッピングされる。"
  },
  {
    "start": 464026,
    "end": 467300,
    "text": "ボキャブラリーは何語ですか？"
  },
  {
    "start": 468070,
    "end": 471970,
    "text": "語彙とは、我々のモデルが認識する単語のリストである。"
  },
  {
    "start": 472390,
    "end": 474258,
    "text": "もちろん、言葉である必要はない。"
  },
  {
    "start": 474344,
    "end": 475406,
    "text": "何でもあり得る。"
  },
  {
    "start": 475528,
    "end": 476866,
    "text": "それらは単なるトークンだ。"
  },
  {
    "start": 476978,
    "end": 486310,
    "text": "各トークンはこの語彙の中の位置を占め、入力idは語彙の中で各トークンが占める番号を示す。"
  },
  {
    "start": 486730,
    "end": 493206,
    "text": "次に、各入力IDを元の変換器のサイズ512のベクトルにマッピングする。"
  },
  {
    "start": 493238,
    "end": 495210,
    "text": "が4096になった。"
  },
  {
    "start": 495280,
    "end": 501062,
    "text": "これらの埋め込みは、学習可能なベクトルである。"
  },
  {
    "start": 501126,
    "end": 503134,
    "text": "これらはモデルのパラメータである。"
  },
  {
    "start": 503252,
    "end": 510830,
    "text": "モデルが学習される間、この埋め込みは、マッピングされる単語の意味を捉えるように変化する。"
  },
  {
    "start": 511330,
    "end": 523106,
    "text": "例えば、catとdogという単語が同じような埋め込みになることを望む。"
  },
  {
    "start": 523288,
    "end": 530498,
    "text": "2つのベクトルを確認すると、houseとbuildingは非常に近い。"
  },
  {
    "start": 530674,
    "end": 533650,
    "text": "これがエンベッディングの背景にある考え方だ。"
  },
  {
    "start": 533730,
    "end": 540330,
    "text": "さて、正規化とは何かを確認しよう。これは埋め込み直後のレイヤーだからだ。"
  },
  {
    "start": 540670,
    "end": 545914,
    "text": "そのために、ニューラルネットワークとその仕組みについて少しおさらいしておこう。"
  },
  {
    "start": 546032,
    "end": 560714,
    "text": "入力、ニューロンからなる隠れ層、さらに5層のニューロンからなる隠れ層があり、それが出力にマッピングされる。"
  },
  {
    "start": 560842,
    "end": 566318,
    "text": "私たちは通常、目標を設定し、出力と目標を比較して損失を出す。"
  },
  {
    "start": 566494,
    "end": 572270,
    "text": "その後、損失は逆伝播によって2つの隠れ層に伝えられる。"
  },
  {
    "start": 572350,
    "end": 583890,
    "text": "この2つの隠れ層の各重みに対する損失の勾配を計算し、それに応じて隠れ層の重みを変更する。"
  },
  {
    "start": 583970,
    "end": 595654,
    "text": "また、設定した学習率に従って、なぜ正規化が必要なのか、正規化の必要性は何なのかを確認するために、ニューラルネットワークを単純化してみる。"
  },
  {
    "start": 595782,
    "end": 601510,
    "text": "神経回路網が実際に工場であり、電話を製造する工場であるとしよう。"
  },
  {
    "start": 601590,
    "end": 610362,
    "text": "つまり、携帯電話を作るには、まずいくつかの原材料を手に入れ、それをハードウェア・チームに渡してハードウェアを製造してもらう。"
  },
  {
    "start": 610426,
    "end": 615454,
    "text": "例えば、ブルートゥース・デバイスを選んだり、ディスプレイを選んだりする。"
  },
  {
    "start": 615502,
    "end": 622066,
    "text": "マイク、カメラ、その他を選択することができ、それらがこの携帯電話のハードウェアを構成している。"
  },
  {
    "start": 622248,
    "end": 628818,
    "text": "ハードウェア・チームはこのプロトタイプをソフトウェア・チームに渡し、ソフトウェア・チームはこのハードウェアのためのソフトウェアを作成する。"
  },
  {
    "start": 628994,
    "end": 635670,
    "text": "となると、ソフトウェア・チームの出力は、ハードウェアとソフトウェアを備えた完全な電話機となり、出力として与えられる。"
  },
  {
    "start": 636010,
    "end": 641020,
    "text": "そしてその出力は、携帯電話の元の設計と比較される。"
  },
  {
    "start": 641390,
    "end": 643062,
    "text": "そして、損失を計算する。"
  },
  {
    "start": 643126,
    "end": 647894,
    "text": "携帯電話の目標台数と実際の生産台数の差は何なのか？"
  },
  {
    "start": 648022,
    "end": 653054,
    "text": "その損失はCEOであり、かなり大きい。"
  },
  {
    "start": 653092,
    "end": 653902,
    "text": "仮にそうだ。"
  },
  {
    "start": 653956,
    "end": 664334,
    "text": "CEOはハードウェア・チームやソフトウェア・チームと話し合い、次回はより目標に近づけるよう戦略を調整するよう指示する。"
  },
  {
    "start": 664452,
    "end": 667022,
    "text": "では、仮にハードウェアが高すぎたとしよう。"
  },
  {
    "start": 667086,
    "end": 679330,
    "text": "CEOはハードウェア・チームに、より小さいディスプレイを使うように、より安いカメラを使うように、ブルートゥースを低レンジのものに変更するように、あるいは無線LANを低エネルギーのものに変更するように、バッテリーを変更するように、等々を指示するだろう。"
  },
  {
    "start": 679410,
    "end": 698678,
    "text": "リファクタリングにあまり集中しないように、トレーニングにあまり集中しないように、インターンをもっと雇うように、コストが高すぎるから従業員にはあまりこだわらないように、とね。"
  },
  {
    "start": 698774,
    "end": 702542,
    "text": "それは、ソフトウェアとハードウェアチームの戦略を調整することになる。"
  },
  {
    "start": 702676,
    "end": 707680,
    "text": "次回はまた原料から始める。"
  },
  {
    "start": 708370,
    "end": 709566,
    "text": "戻ろう。"
  },
  {
    "start": 709668,
    "end": 719090,
    "text": "私たちは再び原材料からスタートし、CEOが定めた新たな戦略に従ってハードウェア・チームが新たなハードウェアを製造する。"
  },
  {
    "start": 719670,
    "end": 721246,
    "text": "ここで問題が発生する。"
  },
  {
    "start": 721358,
    "end": 733122,
    "text": "なぜなら、ディスプレイが変更され、ブルートゥースが変更され、Wifiが変更され、すべてが変更されているからだ。"
  },
  {
    "start": 733186,
    "end": 744570,
    "text": "ソフトウェア・チームは多くの作業をやり直す必要があり、特に、これまでに見たことのないものを扱っているため、戦略を大きく調整する必要がある。"
  },
  {
    "start": 744640,
    "end": 751770,
    "text": "ソフトウェア・チームのアウトプットは、以前とは大きく異なるものになるだろう。"
  },
  {
    "start": 752110,
    "end": 759034,
    "text": "もしかしたら、目標からさらに遠ざかるかもしれない。ソフトウェア・チームがこれらの調整をすべて行う準備ができていなかったからだ。"
  },
  {
    "start": 759082,
    "end": 760414,
    "text": "多くの時間を無駄にしたのかもしれない。"
  },
  {
    "start": 760452,
    "end": 767326,
    "text": "もしかしたら、多くの資源を浪費し、目標に到達できなかったかもしれないし、目標に近づくことさえできなかったかもしれない。"
  },
  {
    "start": 767438,
    "end": 769778,
    "text": "今回の損失はもっと大きいかもしれない。"
  },
  {
    "start": 769944,
    "end": 778758,
    "text": "つまり、おわかりのように、損失関数がハードウェア・チームとソフトウェア・チームの重みを変更するという事実が問題を引き起こしているのだ。"
  },
  {
    "start": 778924,
    "end": 785926,
    "text": "となると、次のイテレーションでは、ソフトウェア・チームは見たこともない入力を受け取ることになる。"
  },
  {
    "start": 786028,
    "end": 794762,
    "text": "この入力によって、以前とは大きく異なる出力が生み出される。"
  },
  {
    "start": 794896,
    "end": 800938,
    "text": "これはモデルを振動させ、一種の損失となり、このトレーニングは非常に遅くなる。"
  },
  {
    "start": 801034,
    "end": 806960,
    "text": "では、正規化の仕組みを理解するために、マットレベルで何が起きているのかを見てみよう。"
  },
  {
    "start": 807330,
    "end": 809258,
    "text": "マットを見直そう。"
  },
  {
    "start": 809434,
    "end": 818050,
    "text": "3つの入力特徴とバイアスを持つ5つの出力特徴を持つ、nn線形と定義される線形層があるとする。"
  },
  {
    "start": 818390,
    "end": 821650,
    "text": "これはpytorchで定義されているリニアレイヤーである。"
  },
  {
    "start": 821990,
    "end": 828738,
    "text": "線形レイヤーは2つの行列を作る。1つはウェイトと呼ばれるw、もう1つはバイアスと呼ばれるbだ。"
  },
  {
    "start": 828914,
    "end": 833330,
    "text": "10行×3列の形状の入力があるとする。"
  },
  {
    "start": 833410,
    "end": 839906,
    "text": "この入力xを持つ線形層の出力は、10行×5列になる。"
  },
  {
    "start": 840018,
    "end": 842198,
    "text": "数学的にはどうなっているのか？"
  },
  {
    "start": 842294,
    "end": 843306,
    "text": "復習しよう。"
  },
  {
    "start": 843408,
    "end": 850890,
    "text": "つまり、10個の項目があり、各項目には10個の特徴があるということだ。"
  },
  {
    "start": 851330,
    "end": 859470,
    "text": "線形層で作られるW行列は5×3になるので、3つの入力特徴で出力特徴になる。"
  },
  {
    "start": 860050,
    "end": 872450,
    "text": "この行のそれぞれを1つのニューロンと考えることができ、それぞれのニューロンは3つの重みを持つ。"
  },
  {
    "start": 873190,
    "end": 882738,
    "text": "バイアス・ベクトルは各ニューロンに対して1つなので、バイアス・ベクトルは各ニューロンに対して1つの重みとなる。"
  },
  {
    "start": 882834,
    "end": 891210,
    "text": "つまり、5つの特徴を持つ10個のアイテムがあるということだ。"
  },
  {
    "start": 891870,
    "end": 896730,
    "text": "これらのマトリックスにおける情報の流れを理解しよう。"
  },
  {
    "start": 897550,
    "end": 901834,
    "text": "情報の流れはこの表現に支配されている。"
  },
  {
    "start": 901882,
    "end": 909120,
    "text": "の場合、出力はxにw行列の転置行列にbを足したものに等しい。"
  },
  {
    "start": 909890,
    "end": 919954,
    "text": "この入力xがあり、1つの項目があり、項目1には3つの特徴、1、2、3があるとする。"
  },
  {
    "start": 920152,
    "end": 923602,
    "text": "wtの転置行列がここにある。"
  },
  {
    "start": 923656,
    "end": 929730,
    "text": "つまり、行と列を入れ替えるのである。数式によれば、行列の転置行列を作る必要があるからだ。"
  },
  {
    "start": 929890,
    "end": 933814,
    "text": "ニューロン1に3つの重み、w1、w2、w3がある。"
  },
  {
    "start": 933932,
    "end": 936802,
    "text": "この2つを掛け合わせると、このような行列ができる。"
  },
  {
    "start": 936866,
    "end": 947750,
    "text": "xにwの転置行列を掛けると、この行列ができる。"
  },
  {
    "start": 947830,
    "end": 949690,
    "text": "この列ベクトルで。"
  },
  {
    "start": 950450,
    "end": 955050,
    "text": "次にb行ベクトルを加える。"
  },
  {
    "start": 955210,
    "end": 972130,
    "text": "ご覧のように、2つの行列を足すには同じ次元である必要があるが、Pytorchではブロードキャストのため、この行がこの行に追加され、さらに独立してこの行に追加され、エトセトラ、エトセトラと追加される。"
  },
  {
    "start": 972550,
    "end": 974926,
    "text": "すると、このような出力になる。"
  },
  {
    "start": 975038,
    "end": 978054,
    "text": "この最初のアイテムはZ1になる。"
  },
  {
    "start": 978172,
    "end": 979334,
    "text": "z oneとは？"
  },
  {
    "start": 979452,
    "end": 984150,
    "text": "まあ、Z1はR1にB1を足したものだ。"
  },
  {
    "start": 984300,
    "end": 985542,
    "text": "R1とは？"
  },
  {
    "start": 985596,
    "end": 990598,
    "text": "R1は、この列とこの行、またはこの行とこの列の内積である。"
  },
  {
    "start": 990694,
    "end": 992474,
    "text": "この表現だ。"
  },
  {
    "start": 992592,
    "end": 999340,
    "text": "アイテム1に対するニューロン1の出力は、アイテム1の特徴にのみ依存する。"
  },
  {
    "start": 999890,
    "end": 1011520,
    "text": "通常、この出力の後にもrelu関数のような非線形性を適用し、relu関数の引数をニューロン1の活性化と呼ぶ。"
  },
  {
    "start": 1012610,
    "end": 1019326,
    "text": "これでわかるように、ニューロン1の出力は各項目の入力特徴にのみ依存する。"
  },
  {
    "start": 1019518,
    "end": 1026110,
    "text": "データ項目に対する誤りの出力は、入力データ項目とニューロン・パラメータの特徴に依存する。"
  },
  {
    "start": 1026270,
    "end": 1030562,
    "text": "誤りの入力は、前の層の出力と考えることができる。"
  },
  {
    "start": 1030626,
    "end": 1049290,
    "text": "つまり、例えば、xの前に見た入力は、勾配降下によって重みが更新された後の前のレイヤーの出力である可能性があるのだ。"
  },
  {
    "start": 1049360,
    "end": 1056346,
    "text": "前のレイヤーであるハードウェア・チームは、これまでとはまったく異なるアウトプットを出すことになる。"
  },
  {
    "start": 1056458,
    "end": 1060042,
    "text": "次のレイヤーの出力も大幅に変更される。"
  },
  {
    "start": 1060186,
    "end": 1067422,
    "text": "つまり、勾配降下の次のステップで、重みを大幅に再調整することを余儀なくされるからだ。"
  },
  {
    "start": 1067486,
    "end": 1084120,
    "text": "我々が好まないのは、前のレイヤーの出力が変化しすぎることで、次のレイヤーも出力を大きく変化させなければならない。"
  },
  {
    "start": 1084650,
    "end": 1092534,
    "text": "ニューロンの内部ノードの分布が変化するこの現象は、内部共変量シフトと呼ばれる。"
  },
  {
    "start": 1092662,
    "end": 1104810,
    "text": "なぜなら、前の層の出力が大幅に変化したために、ニューロンは重みを一方向に大幅に再調整することを余儀なくされるからである。"
  },
  {
    "start": 1104970,
    "end": 1106302,
    "text": "どうする？"
  },
  {
    "start": 1106436,
    "end": 1110186,
    "text": "少なくともバニラのトランスフォーマーでは、レイヤーの正規化を行っている。"
  },
  {
    "start": 1110218,
    "end": 1112720,
    "text": "では、レイヤーの正規化がどのように機能するのか、おさらいしてみよう。"
  },
  {
    "start": 1113110,
    "end": 1118926,
    "text": "入力xが10行×3列で定義されているとしよう。"
  },
  {
    "start": 1119118,
    "end": 1126670,
    "text": "これらの各項目について、それぞれ独立に2つの統計量を計算する。"
  },
  {
    "start": 1126830,
    "end": 1132390,
    "text": "ひとつは平均、もうひとつはシグマ、つまり分散である。"
  },
  {
    "start": 1133290,
    "end": 1138854,
    "text": "そして、この行列の値をこの式に従って正規化する。"
  },
  {
    "start": 1138902,
    "end": 1153342,
    "text": "つまり、各項目からmuを引いたものを分散の平方根で割ったものにεを加えたものである。"
  },
  {
    "start": 1153396,
    "end": 1160794,
    "text": "こうすることで、たとえ分散が非常に小さくても、それぞれの数値に2つのパラメータが掛け合わされる。"
  },
  {
    "start": 1160922,
    "end": 1162974,
    "text": "ひとつはガンマ、もうひとつはベータだ。"
  },
  {
    "start": 1163022,
    "end": 1172500,
    "text": "どちらもモデルによって学習可能であり、モデルがこのガンマとベータを調整して必要な値を増幅することができるため有用である。"
  },
  {
    "start": 1174470,
    "end": 1181842,
    "text": "レイヤー正規化ができる前は、バッチ正規化、バッチ正規化で正規化していた。"
  },
  {
    "start": 1181906,
    "end": 1187554,
    "text": "唯一の違いは、これらの統計を行ごとに計算するのではなく、列ごとに計算したことである。"
  },
  {
    "start": 1187602,
    "end": 1190998,
    "text": "フィーチャー1、フィーチャー2、フィーチャー3。"
  },
  {
    "start": 1191084,
    "end": 1197142,
    "text": "レイヤーの正規化では、行ごとに行うので、各行がそれぞれミューとシグマを持つことになる。"
  },
  {
    "start": 1197206,
    "end": 1210858,
    "text": "レイヤーの正規化を使うことで、基本的には、特徴量の初期分布がどのようなものであっても、平均がゼロで分散が1の正規化された数値に変換する。"
  },
  {
    "start": 1210954,
    "end": 1213774,
    "text": "この公式は、実は確率統計から来ている。"
  },
  {
    "start": 1213822,
    "end": 1218306,
    "text": "覚えているなら、ペンを使わせてくれ。"
  },
  {
    "start": 1218408,
    "end": 1239690,
    "text": "さて、基本的に、平均が例えば5で、分散が36の正規分布のような変数xがあるとすると、xから平均を引くと、5を分散の平方根で割ることになります。"
  },
  {
    "start": 1240670,
    "end": 1251066,
    "text": "36、この変数、ここではzと呼ぶことにする。"
  },
  {
    "start": 1251248,
    "end": 1253930,
    "text": "は標準ガウスになる。"
  },
  {
    "start": 1254090,
    "end": 1256222,
    "text": "これこそ、私たちがここでやっていることだ。"
  },
  {
    "start": 1256276,
    "end": 1265700,
    "text": "私たちは、この値を標準ガウシアンに変換しているので、ほとんどの場合、この値はゼロに近くなる。"
  },
  {
    "start": 1266870,
    "end": 1272450,
    "text": "さて、llamaが使っている二乗平均平方根正規化について話そう。"
  },
  {
    "start": 1274970,
    "end": 1283218,
    "text": "二乗平均平方根正規化（root mean square normalization）は、この2人の研究者から紹介された。"
  },
  {
    "start": 1283394,
    "end": 1286214,
    "text": "一緒に新聞を読もう"
  },
  {
    "start": 1286332,
    "end": 1293466,
    "text": "レイヤーノルムの成功の説明としてよく知られているのは、その最近接不変性と再拡大不変性である。"
  },
  {
    "start": 1293648,
    "end": 1295098,
    "text": "どういう意味なのか？"
  },
  {
    "start": 1295184,
    "end": 1297494,
    "text": "リセンタリングと再スケーリングの不変性とは？"
  },
  {
    "start": 1297542,
    "end": 1306286,
    "text": "特徴量がどのようなものであれ、それらはゼロ平均を中心に再心化され、分散が1になるように再尺度化されるという事実である。"
  },
  {
    "start": 1306468,
    "end": 1318014,
    "text": "前者は入力と重みの両方のシフトノイズに鈍感なモデルを可能にし、後者は入力と重みの両方がランダムにスケーリングされても出力表現をそのまま維持する。"
  },
  {
    "start": 1318062,
    "end": 1327394,
    "text": "さて、この論文では、リカリング分散がレイヤー・ノルムの成功の理由であり、むしろリカリング不変性であるという仮説を立てている。"
  },
  {
    "start": 1327442,
    "end": 1340694,
    "text": "この論文で彼らが主張しているのは、基本的にレイヤー・ノルムの成功は再センタリングや再スケーリングのおかげではなく、ほとんどが再スケーリングのおかげだということだ。"
  },
  {
    "start": 1340742,
    "end": 1345580,
    "text": "つまり、基本的には、分散が1であることが望ましい。"
  },
  {
    "start": 1346350,
    "end": 1355754,
    "text": "つまり、平均値に依存しない別の統計はないか、ということだ。"
  },
  {
    "start": 1355882,
    "end": 1359882,
    "text": "そう、二乗平均平方根統計を使っているんだ。"
  },
  {
    "start": 1359946,
    "end": 1376262,
    "text": "ここで定義された統計量、ここで定義された統計量、そしてこの統計量の式からわかるように、我々はもう平均を使って計算しない。"
  },
  {
    "start": 1376316,
    "end": 1385110,
    "text": "というのも、計算する分散には平均が必要だからである。"
  },
  {
    "start": 1385180,
    "end": 1392682,
    "text": "分散はxからmuを引いたものをnで割った2のべき乗の和に等しい。"
  },
  {
    "start": 1392816,
    "end": 1395942,
    "text": "分散を計算するには平均が必要です。"
  },
  {
    "start": 1396006,
    "end": 1408990,
    "text": "というのも、レイヤー正規化の効果を得るために再センタリングは必要ないという仮説があるからだ。"
  },
  {
    "start": 1409330,
    "end": 1414130,
    "text": "我々は平均に依存しない統計量を求めたいのであり、RMS統計量は平均に依存しない。"
  },
  {
    "start": 1414200,
    "end": 1418590,
    "text": "レイヤーの正規化とまったく同じことをするのだ。"
  },
  {
    "start": 1418670,
    "end": 1427894,
    "text": "行ごとにRMS統計量を計算し、この式に従って正規化する。"
  },
  {
    "start": 1427932,
    "end": 1434866,
    "text": "RMS統計量という統計量で割って、ガンマパラメータをかけるだけだ。"
  },
  {
    "start": 1434978,
    "end": 1439806,
    "text": "では、なぜ二乗平均平方根正規化なのか？"
  },
  {
    "start": 1439938,
    "end": 1450478,
    "text": "レイヤー正規化に比べて計算量が少なくて済むのは、2つの統計量を計算しないからだ。"
  },
  {
    "start": 1450564,
    "end": 1456122,
    "text": "計算上有利になるし、実際にうまくいく。"
  },
  {
    "start": 1456186,
    "end": 1461486,
    "text": "論文の著者が仮説として掲げたことは、実際に真実なのだ。"
  },
  {
    "start": 1461588,
    "end": 1465850,
    "text": "レイヤーの正規化による効果を得るために必要なのは、不変性だけである。"
  },
  {
    "start": 1465930,
    "end": 1467382,
    "text": "リセンタリングは必要ない。"
  },
  {
    "start": 1467466,
    "end": 1470430,
    "text": "少なくともリャマはそうだ。"
  },
  {
    "start": 1470510,
    "end": 1474174,
    "text": "次にお話しするのは、ポジションエンコーディングについてです。"
  },
  {
    "start": 1474302,
    "end": 1480802,
    "text": "回転位置エンコーディングを紹介する前に、バニラ・トランスの位置エンコーディングを復習しておこう。"
  },
  {
    "start": 1480866,
    "end": 1498838,
    "text": "覚えているように、トークンを埋め込みに変換した後、バニラ変換器では512サイズのベクトル、つまり、文中の各トークンの位置を示す埋め込みに別のベクトルを合計します。"
  },
  {
    "start": 1499014,
    "end": 1503950,
    "text": "これらの位置埋め込みは固定されているので、モデルによって学習されることはない。"
  },
  {
    "start": 1504020,
    "end": 1515170,
    "text": "これらのベクトルは一度だけ計算され、訓練と推論の間、すべての文に再利用される。"
  },
  {
    "start": 1515320,
    "end": 1519502,
    "text": "回転位置エンコーディングと呼ばれる新しい種類の位置エンコーディングがある。"
  },
  {
    "start": 1519566,
    "end": 1528338,
    "text": "絶対位置エンコーディングは、文中の絶対位置を表すためにトークンの埋め込みに追加される固定ベクトルである。"
  },
  {
    "start": 1528434,
    "end": 1535906,
    "text": "トークン1番はそれ自身のベクトル、トークン2番はそれ自身のベクトル、トークン3番はそれ自身のベクトルを得る。"
  },
  {
    "start": 1536018,
    "end": 1540246,
    "text": "絶対位置エンコーディングは、一度に1つのトークンを扱う。"
  },
  {
    "start": 1540428,
    "end": 1544454,
    "text": "地図上の緯度と経度のペアと考えればいい。"
  },
  {
    "start": 1544502,
    "end": 1553494,
    "text": "地球上の各点はそれぞれ固有の緯度と経度を持つので、これは地球上の各点の位置を示す絶対的な指標となる。"
  },
  {
    "start": 1553622,
    "end": 1556618,
    "text": "これは、絶対位置エンコーディングで起こることと同じである。"
  },
  {
    "start": 1556634,
    "end": 1563386,
    "text": "バニラ・トランスフォームでは、特定のトークンに追加される位置を正確に表すベクトルが1つある。"
  },
  {
    "start": 1563498,
    "end": 1573710,
    "text": "一方、相対的な位置エンコーディングでは、一度に2つのトークンを扱い、アテンションを計算する際に関与する。"
  },
  {
    "start": 1573870,
    "end": 1587534,
    "text": "注意メカニズムは、2つの単語が互いにどの程度関連しているかの強度をとらえるので、相対位置エンコーディングは、この注意メカニズムに関係する2つの単語間の距離を注意メカニズムに伝える。"
  },
  {
    "start": 1587682,
    "end": 1592870,
    "text": "2つのトークンが与えられたら、その距離を表すベクトルを作成する。"
  },
  {
    "start": 1593030,
    "end": 1598998,
    "text": "これは、2つのトークン間の距離に対する相対的なものなので、相対的と呼ばれる所以である。"
  },
  {
    "start": 1599174,
    "end": 1614846,
    "text": "相対的な位置エンコーディングは、グーグルの以下の論文で初めて紹介されたもので、ヴァスバーニは、現在では絶対的な位置エンコーディングを持つトランスフォーマーモデルと同じ外側であることにお気づきだろう。"
  },
  {
    "start": 1614878,
    "end": 1623198,
    "text": "アテンション・メカニズムでドット積を計算するときに必要なのは、アテンションからのドット積だけだ。"
  },
  {
    "start": 1623294,
    "end": 1627240,
    "text": "注意のメカニズム、公式を覚えているなら、それを書かせてほしい。"
  },
  {
    "start": 1628410,
    "end": 1647642,
    "text": "注目度は、クエリにキーの移調をかけたものをDモデル、Dモデルの平方根で割ったものに等しい。"
  },
  {
    "start": 1647696,
    "end": 1651626,
    "text": "そして、ソフトマックスを行い、それにVを掛ける。"
  },
  {
    "start": 1651658,
    "end": 1656960,
    "text": "この場合は、qにkを転置したものだけに注目する。"
  },
  {
    "start": 1657970,
    "end": 1659950,
    "text": "これがここにあるものだ。"
  },
  {
    "start": 1660100,
    "end": 1676674,
    "text": "このドット積を計算するとき、アテンション・メカニズムは、すでに絶対位置がエンコードされている2つのトークン間のドット積を計算している。"
  },
  {
    "start": 1676802,
    "end": 1681938,
    "text": "このアテンション・メカニズムでは、バニラ・トランスフォーマーから2つのトークンとアテンション・メカニズムを持っている。"
  },
  {
    "start": 1682114,
    "end": 1702030,
    "text": "相対位置エンコーディングでは、トークン1、トークン2、そしてこの2つのトークン間の距離を表すこのベクトルがある。"
  },
  {
    "start": 1703090,
    "end": 1714242,
    "text": "この注意メカニズムには3つのベクトルがあり、このベクトルに基づいて注意メカニズムがトークンをマッチングさせる。"
  },
  {
    "start": 1714296,
    "end": 1723250,
    "text": "このベクトルは、注意メカニズム、つまり、この特定の距離にある2つの単語をどのように関連づけるかを点積に示すことになる。"
  },
  {
    "start": 1724150,
    "end": 1731062,
    "text": "回転位置埋め込みでも同様のことができ、本稿で紹介した。"
  },
  {
    "start": 1731196,
    "end": 1735174,
    "text": "前列、しかも中国企業のものだ。"
  },
  {
    "start": 1735292,
    "end": 1741258,
    "text": "アテンション・メカニズムで使われるドット積は、内積の一種である。"
  },
  {
    "start": 1741424,
    "end": 1753326,
    "text": "線形代数を思い出してほしいのだが、点積はいくつかの性質を持つ演算の一種であり、これらの性質はすべての内積が持たなければならない性質の一種である。"
  },
  {
    "start": 1753428,
    "end": 1757760,
    "text": "内積はドット積の一般化と考えることができる。"
  },
  {
    "start": 1758530,
    "end": 1776290,
    "text": "この論文の著者がやりたかったことは、2つのベクトルそのものと、それらが表すトークンの相対的な距離にのみ依存する、アテンション・メカニズムで使用される2つのベクトル、クエリーとキーの内積を見つけることである。"
  },
  {
    "start": 1776450,
    "end": 1787234,
    "text": "つまり、クエリとキーという2つのベクトルが与えられたとき、それらが表す単語の埋め込みと文中の位置だけを含む。"
  },
  {
    "start": 1787282,
    "end": 1790278,
    "text": "このmは実際には絶対数である。"
  },
  {
    "start": 1790364,
    "end": 1799030,
    "text": "これはScHolarで、文中の単語の位置を表し、このnは文中の2番目の単語の位置を表す。"
  },
  {
    "start": 1799190,
    "end": 1802762,
    "text": "彼らが言いたかったのは、内積を見つけることができるかということだ。"
  },
  {
    "start": 1802816,
    "end": 1816382,
    "text": "この特定の括弧は、xmの埋め込みにのみ依存する関数gのように振る舞う、これら2つのベクトル間の内積である。"
  },
  {
    "start": 1816446,
    "end": 1824420,
    "text": "xnの最初のトークン、2番目のトークン、およびそれらの間の相対距離。"
  },
  {
    "start": 1824790,
    "end": 1834866,
    "text": "この関数には、最初のトークンの埋め込み、2番目のトークンの埋め込み、そしてこれら2つのトークンの相対位置を表す数値だけが与えられる。"
  },
  {
    "start": 1834978,
    "end": 1837270,
    "text": "この2つのトークンの相対距離。"
  },
  {
    "start": 1838970,
    "end": 1843914,
    "text": "はい、そのような関数を見つけることができます。"
  },
  {
    "start": 1844032,
    "end": 1853790,
    "text": "のような関数gを定義することができる。この関数gは、2つの埋め込みベクトルqとkと相対距離に依存するだけでよい。"
  },
  {
    "start": 1854290,
    "end": 1863074,
    "text": "この関数は複素数空間で定義され、オイラーの公式を使ってこの形に変換できる。"
  },
  {
    "start": 1863192,
    "end": 1873762,
    "text": "もうひとつ注意しなければならないのは、この関数は2次元のベクトルに対して定義されているということだ。"
  },
  {
    "start": 1873896,
    "end": 1884982,
    "text": "もちろん、次元が大きくなり、複素数空間にあるこの式を行列の形に変換するとどうなるか、後で見てみよう。"
  },
  {
    "start": 1885036,
    "end": 1890902,
    "text": "ルアーの公式を通して、この行列を回転行列として認識することができる。"
  },
  {
    "start": 1891046,
    "end": 1897610,
    "text": "この行列は基本的にベクトルの回転を表す。"
  },
  {
    "start": 1897680,
    "end": 1910730,
    "text": "この積はベクトルとなり、この回転行列はこのベクトルをmθで表される量だけ空間に回転させる。"
  },
  {
    "start": 1910890,
    "end": 1912494,
    "text": "例を見てみよう。"
  },
  {
    "start": 1912692,
    "end": 1922702,
    "text": "ベクトルvがゼロで、それを角度θだけ回転させてベクトルv primeに到達させたいとする。"
  },
  {
    "start": 1922846,
    "end": 1942038,
    "text": "この行列は、θの余弦からθの正弦を引いた値、θの正弦とθの余弦を計算したもので、結果として得られるベクトルは同じベクトル、つまり同じ長さでこの角度だけ回転したものになる。"
  },
  {
    "start": 1942134,
    "end": 1949610,
    "text": "このベクトルは回転を表すので、回転位置埋め込みと呼ばれる所以である。"
  },
  {
    "start": 1951150,
    "end": 1958062,
    "text": "さて、ベクトルが2次元ではなく、例えばオリジナルのトランスフォーマーモデルではn次元である場合。"
  },
  {
    "start": 1958116,
    "end": 1963120,
    "text": "我々のエンベッディングサイズは512で、llamaでは4096だ。"
  },
  {
    "start": 1963570,
    "end": 1965460,
    "text": "このフォームを使う必要がある。"
  },
  {
    "start": 1965830,
    "end": 1973278,
    "text": "ここで注目してほしいのは、この行列の数値が何であるかではなく、この行列が疎であるという事実である。"
  },
  {
    "start": 1973374,
    "end": 1977214,
    "text": "位置埋め込みを計算するのに使うのは不便だ。"
  },
  {
    "start": 1977262,
    "end": 1987030,
    "text": "というのも、このエンベッディングで掛け算をすると、テンソルフローやGPU、コンピュータは、積のほとんどがゼロになることがわかっているので、無駄な演算をたくさんしてしまうからだ。"
  },
  {
    "start": 1987180,
    "end": 1992198,
    "text": "もっと良い方法、計算効率の良い方法はないのだろうか？"
  },
  {
    "start": 1992294,
    "end": 1995434,
    "text": "さて、ここにこんなフォームがある。"
  },
  {
    "start": 1995552,
    "end": 2006362,
    "text": "つまり、埋め込みベクトルxを持つトークンと、そのトークンの文中での位置mが与えられると、トークンの位置埋め込みはこうして計算される。"
  },
  {
    "start": 2006506,
    "end": 2013242,
    "text": "トークンの次元を取り、この行列を掛け合わせ、次のように計算する。"
  },
  {
    "start": 2013306,
    "end": 2020766,
    "text": "ここで、θは固定、mはトークンの位置、x1、x2、x3は埋め込みの次元である。"
  },
  {
    "start": 2020798,
    "end": 2043290,
    "text": "埋込みの1次元目、埋込みの2次元目、etc、プラスマイナス2次元目の埋込み、このベクトル、次の位置のように計算されるので、マイナスx 2、これは埋込みの2次元目の負の値である、ベクトルxにこの行列を掛けたものである。"
  },
  {
    "start": 2043360,
    "end": 2045766,
    "text": "このマトリックスで学ばなければならないことは何もない。"
  },
  {
    "start": 2045798,
    "end": 2056910,
    "text": "というのも、前のスライドを見れば、このデータは実際には各次元ごとに1つずつこのように計算されていることがわかるからだ。"
  },
  {
    "start": 2057060,
    "end": 2060986,
    "text": "基本的には、絶対位置エンコーディングと同じである。"
  },
  {
    "start": 2061018,
    "end": 2069102,
    "text": "を一度計算すれば、モデルを訓練するすべての文に再利用できる。"
  },
  {
    "start": 2069166,
    "end": 2074130,
    "text": "回転位置埋め込みのもう一つの興味深い性質は、長期的な減衰である。"
  },
  {
    "start": 2074550,
    "end": 2080582,
    "text": "オルターズがやったのは、前に見た内積の上限を計算することだった。"
  },
  {
    "start": 2080636,
    "end": 2084274,
    "text": "2つのトークン間の距離を変化させることで、g関数が変化する。"
  },
  {
    "start": 2084402,
    "end": 2094700,
    "text": "そして、2つのトークンがどのようなものであっても、2つのトークン間の距離が大きくなるにつれて減少する上限が存在することを証明した。"
  },
  {
    "start": 2095070,
    "end": 2119630,
    "text": "この点積は、注目度を計算する2つのトークン間の関係の強さを表し、回転位置埋め込みが行うのは、基本的にこの関係、つまり2つのトークン間の関係の強さを減衰させることです。"
  },
  {
    "start": 2119710,
    "end": 2129010,
    "text": "もし私たちがマッチさせようとしている2つのトークンが、彼らから、互いから遠く離れていて、これが実際に私たちが望んでいることだとしたら。"
  },
  {
    "start": 2129080,
    "end": 2137382,
    "text": "互いに非常に遠い2つの単語はあまり強い関係を持たず、互いに近い2つの単語はより強い関係を持たせたい。"
  },
  {
    "start": 2137516,
    "end": 2142090,
    "text": "これが、回転位置埋め込みに求められる特性である。"
  },
  {
    "start": 2143230,
    "end": 2149702,
    "text": "さて、回転位置埋め込みはクエリーとキーにのみ適用され、値には適用されない。"
  },
  {
    "start": 2149846,
    "end": 2150862,
    "text": "その理由を見てみよう。"
  },
  {
    "start": 2150996,
    "end": 2157386,
    "text": "さて、まず考慮すべきは、基本的にアテンションを計算する際に、これらの要素が絡んでくるということだ。"
  },
  {
    "start": 2157498,
    "end": 2164814,
    "text": "アテンションを計算するとき、スコアを変えるのはアテンション・メカニズムだ。"
  },
  {
    "start": 2164862,
    "end": 2171966,
    "text": "覚えているだろうか、アテンション・メカニズムとは、2つのトークンの関係がどれだけ強いかを示すスコアのようなものだ。"
  },
  {
    "start": 2172078,
    "end": 2185746,
    "text": "この関係は、文中での2つのトークンの位置や、2つのトークン間の相対的な距離によっても強くなったり弱くなったり変化する。"
  },
  {
    "start": 2185858,
    "end": 2198490,
    "text": "もうひとつは、回転位置埋め込みは、注目メカニズムではベクトルqとkがw行列と掛け合わされた後に適用されるが、バニラ変換器ではその前に適用されるということだ。"
  },
  {
    "start": 2198560,
    "end": 2210426,
    "text": "バニラ変換器では、トークンを埋め込みに変換した直後に位置埋め込みが適用されますが、回転位置埋め込みでは、トークンを埋め込みに変換した直後に位置埋め込みが適用されます。"
  },
  {
    "start": 2210458,
    "end": 2218334,
    "text": "LLAMAでは、アテンション・メカニズムでw行列を掛け合わせた直後には、基本的にこのようなことはしない。"
  },
  {
    "start": 2218382,
    "end": 2231506,
    "text": "wマトリックスとは、各アテンションヘッドがラマに持つパラメーターのマトリックスである。"
  },
  {
    "start": 2231538,
    "end": 2238658,
    "text": "基本的には、ベクトルqとkにw行列を掛けた後、回転位置エンコーディングを適用する。"
  },
  {
    "start": 2238754,
    "end": 2244262,
    "text": "さて、ここからが面白いところで、リャマの自己注意がどのように働くかを見てみよう。"
  },
  {
    "start": 2244406,
    "end": 2252430,
    "text": "リャマで使われている自己アテンションについて話す前に、バニラ・トランスフォーマーにおける自己アテンションについて、少なくとも簡単におさらいしておく必要がある。"
  },
  {
    "start": 2253010,
    "end": 2269102,
    "text": "バニラ・トランスフォームの自己注意を思い出せば、行列qから始める。行列qはDモデルによるシーケンス行列であり、行にトークン、列に埋め込みベクトルの次元を持つことを意味する。"
  },
  {
    "start": 2269246,
    "end": 2285990,
    "text": "つまり、6つの行があり、それぞれが512次元のベクトルで、そのトークンの埋め込みを表していると考えることができる。"
  },
  {
    "start": 2286330,
    "end": 2288630,
    "text": "削除させてください。"
  },
  {
    "start": 2290670,
    "end": 2308842,
    "text": "つまり、kを512の平方根で割ったkの転置行列が埋め込みベクトルの次元である。"
  },
  {
    "start": 2308906,
    "end": 2312590,
    "text": "この3つの行列は実際には同じ並びである。"
  },
  {
    "start": 2313730,
    "end": 2316858,
    "text": "そしてソフトマックスを適用し、この行列を得る。"
  },
  {
    "start": 2316954,
    "end": 2323122,
    "text": "6×512の行列と512×6の行列を掛け合わせた。"
  },
  {
    "start": 2323256,
    "end": 2332822,
    "text": "この行列の各項目は、最初のトークンとそれ自身との内積を表す。"
  },
  {
    "start": 2332956,
    "end": 2340726,
    "text": "そして、最初のトークンは2番目のトークンと、最初のトークンは3番目のトークンと、最初のトークンは4番目のトークンと、エトセトラ、エトセトラ、エトセトラ。"
  },
  {
    "start": 2340838,
    "end": 2346570,
    "text": "この行列は、2つのトークン間の関係の強さを表す。"
  },
  {
    "start": 2347950,
    "end": 2357898,
    "text": "そして、このソフトマックスの出力にv行列を掛け合わせ、注目シーケンスを得る。"
  },
  {
    "start": 2357994,
    "end": 2365446,
    "text": "自己注意の出力は、初期行列と同じ次元を持つ別の行列である。"
  },
  {
    "start": 2365578,
    "end": 2381474,
    "text": "埋め込みは、各トークンの意味、各トークンの位置だけでなく、そのトークンと他のトークンとの間の関係も捉えるようになる。"
  },
  {
    "start": 2381602,
    "end": 2389946,
    "text": "もし、このコンセプトを理解していないのであれば、私の以前の変圧器についてのビデオをご覧ください。"
  },
  {
    "start": 2390048,
    "end": 2395286,
    "text": "では、マルチヘッドの注意点を簡単に見てみよう。"
  },
  {
    "start": 2395318,
    "end": 2404554,
    "text": "マルチヘッドアテンションは、基本的に、入力シーケンスがあり、それをQKとvにコピーし、同じ行列にすることを意味する。"
  },
  {
    "start": 2404682,
    "end": 2414762,
    "text": "パラメータ行列を掛け合わせ、各ヘッドごとに複数の小さな行列に分割し、これらのヘッド間の注目度を計算する。"
  },
  {
    "start": 2414826,
    "end": 2417122,
    "text": "ヘッド1、ヘッド2、ヘッド3、ヘッド4。"
  },
  {
    "start": 2417256,
    "end": 2425818,
    "text": "次に、これらのヘッドの出力を連結し、出力マトリックスwoを掛け合わせ、最終的にマルチヘッドアテンションの出力を得る。"
  },
  {
    "start": 2425934,
    "end": 2429510,
    "text": "最初のkvキャッシュとは何か見てみよう。"
  },
  {
    "start": 2430570,
    "end": 2439734,
    "text": "kvキャッシュを紹介する前に、llamaがどのようにトレーニングされたかを理解し、次のトークン予測タスクが何かを理解する必要がある。"
  },
  {
    "start": 2439862,
    "end": 2455610,
    "text": "llamaは、多くの大規模な言語モデルと同様に、次のトークン予測タスクで訓練されている。つまり、シーケンスが与えられると、次のトークン、つまりプロンプトを続ける可能性が最も高い次のトークンを予測しようとする。"
  },
  {
    "start": 2455690,
    "end": 2466686,
    "text": "たとえば、詩の最後の単語を省略して伝えると、おそらくその詩の最後の単語が欠落している。"
  },
  {
    "start": 2466878,
    "end": 2476722,
    "text": "今回は、ダンテ・アリギエリスの非常に有名な一節を使い、イタリア語訳は使わず、ここでは英語訳を使うことにする。"
  },
  {
    "start": 2476776,
    "end": 2478582,
    "text": "私は最初の行だけを扱う。"
  },
  {
    "start": 2478636,
    "end": 2479334,
    "text": "こちらをご覧いただきたい。"
  },
  {
    "start": 2479372,
    "end": 2481960,
    "text": "優しい心を素早くつかむ愛。"
  },
  {
    "start": 2483130,
    "end": 2485590,
    "text": "この文章でラマを訓練しよう。"
  },
  {
    "start": 2486010,
    "end": 2487462,
    "text": "トレーニングはどのように行われるのですか？"
  },
  {
    "start": 2487516,
    "end": 2490258,
    "text": "モデルにインプットを与えるんだ。"
  },
  {
    "start": 2490364,
    "end": 2501238,
    "text": "入力は、まず文頭トークンを前置するように構築され、次に文末トークンを付加するようにターゲットが構築される。"
  },
  {
    "start": 2501414,
    "end": 2502042,
    "text": "なぜですか？"
  },
  {
    "start": 2502176,
    "end": 2517038,
    "text": "なぜならこのモデルは、入力シーケンスの各位置を出力シーケンスの別の位置にマッピングする、シーケンスからシーケンスへの変換モデルだからだ。"
  },
  {
    "start": 2517134,
    "end": 2530754,
    "text": "基本的には、入力シーケンスの最初のトークンは出力シーケンスの最初のトークンにマッピングされ、入力シーケンスの2番目のトークンは出力シーケンスの2番目のトークンにマッピングされる。"
  },
  {
    "start": 2530882,
    "end": 2538362,
    "text": "これはまた、モデルにSOSという入力を与えると、最初のトークンを出力することを意味する。"
  },
  {
    "start": 2538496,
    "end": 2545900,
    "text": "最初の2つのトークンを与えると、2番目のトークンが出力される。"
  },
  {
    "start": 2546830,
    "end": 2554122,
    "text": "最初の3つのトークンを与えると、3番目のトークンが出力される。"
  },
  {
    "start": 2554266,
    "end": 2561118,
    "text": "もちろん、モデルは前の2つのトークンに対する出力も出すが、例で見てみよう。"
  },
  {
    "start": 2561204,
    "end": 2568290,
    "text": "前回のビデオでも推論を行ったが、モデルを訓練するときは、1つのステップしか行わない。"
  },
  {
    "start": 2568360,
    "end": 2582562,
    "text": "入力、ターゲット、損失を計算し、1つの文のモデルを訓練するためにforループは必要ないが、推論はトークンごとに行う必要がある。"
  },
  {
    "start": 2582706,
    "end": 2595114,
    "text": "この推論では、SOSを入力するタイムステップ1から始める。"
  },
  {
    "start": 2595232,
    "end": 2607146,
    "text": "そして、この出力トークンloveを入力に追加し、再びモデルに渡すと、モデルは次のトークンlove debtを生成する。"
  },
  {
    "start": 2607338,
    "end": 2616014,
    "text": "次に、モデルの負債が最後に出力したトークンを取り出し、それを再び入力に加え、モデルが次のトークンを出力する。"
  },
  {
    "start": 2616142,
    "end": 2618782,
    "text": "そしてまた次のトークンを取る。"
  },
  {
    "start": 2618846,
    "end": 2632722,
    "text": "モデルは次のトークンを素早く出力し、文末のトークンに達するまで、必要なすべてのステップを行う。"
  },
  {
    "start": 2632866,
    "end": 2637758,
    "text": "その時、モデルの出力が終了したことがわかる。"
  },
  {
    "start": 2637874,
    "end": 2648460,
    "text": "さて、これはllamaが実際にどのようにトレーニングされたかを示すものではないが、次のトークン予測タスクがどのように機能するかを示す良い例である。"
  },
  {
    "start": 2649310,
    "end": 2653226,
    "text": "さて、このアプローチには問題がある。"
  },
  {
    "start": 2653338,
    "end": 2673454,
    "text": "推論の各ステップにおいて、モデルによって出力された最後のトークンにしか興味がないのは、前のトークンをすでに持っているからである。しかし、モデルはどのトークンを出力するかを決定するために、前のトークンすべてにアクセスする必要がある。"
  },
  {
    "start": 2673502,
    "end": 2681334,
    "text": "つまり、例えばdという単語を出力するためには、モデルはここにあるすべての入力を見なければならないということだ。"
  },
  {
    "start": 2681372,
    "end": 2687782,
    "text": "モデルはこの最後のトークンdを出力するために、すべての入力を見る必要がある。"
  },
  {
    "start": 2687916,
    "end": 2696886,
    "text": "つまり、これはシーケンス対シーケンスのモデルであり、最後のトークンにしか関心がなくても、このシーケンスを出力する。"
  },
  {
    "start": 2696998,
    "end": 2705050,
    "text": "前のタイムステップですでに持っているトークンをもう一度計算するために、不必要な計算をたくさんしているのだ。"
  },
  {
    "start": 2705130,
    "end": 2709230,
    "text": "この無駄な計算をしない方法を見つけよう。"
  },
  {
    "start": 2709810,
    "end": 2713146,
    "text": "これがKVキャッシュの使い方だ。"
  },
  {
    "start": 2713178,
    "end": 2720430,
    "text": "KVキャッシュは、推論中にすでに見たトークンの計算を少なくする方法である。"
  },
  {
    "start": 2720510,
    "end": 2725230,
    "text": "これは、トランスフォーマーモデルの推論時にのみ適用される。"
  },
  {
    "start": 2725400,
    "end": 2735590,
    "text": "というのも、すべてのトランスフォーマーモデルがこのように機能するからだ。"
  },
  {
    "start": 2735660,
    "end": 2743142,
    "text": "これは説明であり、次のトークン予測タスクで自己注意がどのように働くかのイメージである。"
  },
  {
    "start": 2743286,
    "end": 2749894,
    "text": "前のスライドでも見たように、ここにはn個のトークンを持つクエリー行列がある。"
  },
  {
    "start": 2749942,
    "end": 2751654,
    "text": "次に、キーの移調だ。"
  },
  {
    "start": 2751702,
    "end": 2759738,
    "text": "クエリはベクトルの行として考えることができ、最初のベクトルは3番目のトークン、2番目のトークン...を表す。"
  },
  {
    "start": 2759834,
    "end": 2765214,
    "text": "次に、キーの転置は同じトークンだが、行が列になるように転置する。"
  },
  {
    "start": 2765342,
    "end": 2768514,
    "text": "これはn×nの行列を生成する。"
  },
  {
    "start": 2768552,
    "end": 2774290,
    "text": "初期入力行列が9の場合、出力は最大で9×9になる。"
  },
  {
    "start": 2774440,
    "end": 2779030,
    "text": "そして、Vマトリックスと掛け合わせることで、アテンションが生まれる。"
  },
  {
    "start": 2779530,
    "end": 2784466,
    "text": "そして、注目はトランスのリニア層に供給される。"
  },
  {
    "start": 2784578,
    "end": 2804350,
    "text": "そして、線形レイヤーはロジットを生成し、ロジットはソフトマックスに供給される。ソフトマックスによって、語彙の中からどのトークンを選ぶかが決定される。"
  },
  {
    "start": 2805010,
    "end": 2811370,
    "text": "これは、自己の注意の中で一般的なレベルで起こっていることの説明である。"
  },
  {
    "start": 2811450,
    "end": 2813440,
    "text": "では、順を追って見ていこう。"
  },
  {
    "start": 2813810,
    "end": 2818254,
    "text": "推論ステップ1では、最初のトークンしか持っていないとしよう。"
  },
  {
    "start": 2818382,
    "end": 2822526,
    "text": "以前を思い出してほしいのだが、私たちは文頭トークンしか使っていない。"
  },
  {
    "start": 2822638,
    "end": 2826178,
    "text": "文頭トークンを取り、それを掛け算する。"
  },
  {
    "start": 2826264,
    "end": 2829714,
    "text": "を転置すると、1対1の行列ができる。"
  },
  {
    "start": 2829752,
    "end": 2834194,
    "text": "この行列は4096分の1に別の行列を掛けたものである。"
  },
  {
    "start": 2834242,
    "end": 2835974,
    "text": "つまり4096×1である。"
  },
  {
    "start": 2836012,
    "end": 2838150,
    "text": "これは1対1の行列を生成する。"
  },
  {
    "start": 2838810,
    "end": 2840262,
    "text": "なぜ4096なのか？"
  },
  {
    "start": 2840316,
    "end": 2843402,
    "text": "Lamaの埋め込みベクトルは4096だからだ。"
  },
  {
    "start": 2843536,
    "end": 2854030,
    "text": "そして出力は、この1×1にvを掛けて出力トークンを生成し、これが出力の最初のトークンとなる。"
  },
  {
    "start": 2854690,
    "end": 2861150,
    "text": "そして、この出力トークンを取り出し、次のステップで入力に追加する。"
  },
  {
    "start": 2861220,
    "end": 2863658,
    "text": "さて、入力として2つのトークンがある。"
  },
  {
    "start": 2863834,
    "end": 2875294,
    "text": "V行列と乗算され、2つの出力トークンが生成される。"
  },
  {
    "start": 2875342,
    "end": 2878902,
    "text": "モデルによって出力された最後のトークンのみに興味がある。"
  },
  {
    "start": 2878956,
    "end": 2885494,
    "text": "この1つの注意2が、ステップ3の時点で入力マトリックスに追加される。"
  },
  {
    "start": 2885532,
    "end": 2897702,
    "text": "これで、タイムスタンプの3つのトークンは、それ自身の転置バージョンと乗算され、V行列と乗算される3×3の行列を生成する。"
  },
  {
    "start": 2897846,
    "end": 2905422,
    "text": "この3つの行列理論のトークンを出力として持っているが、私たちが興味があるのは、モデルが出力する最後のトークンのみである。"
  },
  {
    "start": 2905476,
    "end": 2923634,
    "text": "この行列は4つのトークンであり、Q行列の入力として再び追加される。Q行列は自身の転置バージョンと掛け合わされ、出力として4×4の行列を生成する。"
  },
  {
    "start": 2923752,
    "end": 2929846,
    "text": "我々は、次のステップの入力に再び追加される最後のアテンションにのみ興味がある。"
  },
  {
    "start": 2929948,
    "end": 2942042,
    "text": "まず第一に、このトークンとこのトークン、このトークンとこのトークンの間のドット積を計算する行列がある。"
  },
  {
    "start": 2942096,
    "end": 2946090,
    "text": "この行列は、これら2つの行列間のすべてのドット積である。"
  },
  {
    "start": 2946430,
    "end": 2947834,
    "text": "何かが見える。"
  },
  {
    "start": 2948032,
    "end": 2953166,
    "text": "まず第一に、これらのドット積は前のステップですでに計算している。"
  },
  {
    "start": 2953268,
    "end": 2954398,
    "text": "キャッシュできるのか？"
  },
  {
    "start": 2954484,
    "end": 2956110,
    "text": "戻ろう。"
  },
  {
    "start": 2956260,
    "end": 2960560,
    "text": "ご覧の通り、このマトリックスは2、3、4と成長している。"
  },
  {
    "start": 2961010,
    "end": 2969534,
    "text": "なぜなら、トランスフォーマーを推論するたびに、私たちはトランスフォーマーに何らかのインプットを与えているからだ。"
  },
  {
    "start": 2969582,
    "end": 2976722,
    "text": "ドット積を再計算することになるが、実は前のタイムステップですでに計算しているので不便だ。"
  },
  {
    "start": 2976776,
    "end": 2979670,
    "text": "もう一度計算しない方法はありますか？"
  },
  {
    "start": 2979820,
    "end": 2981606,
    "text": "キャッシュしておくことはできる？"
  },
  {
    "start": 2981708,
    "end": 2983000,
    "text": "ああ、できる。"
  },
  {
    "start": 2983450,
    "end": 2992682,
    "text": "このモデルは因果的であるため、あるトークンとその後継トークンとの注目度には関心がなく、その前のトークンとの注目度にのみ関心がある。"
  },
  {
    "start": 2992816,
    "end": 2997002,
    "text": "覚えているだろうか、セルフ・アテンションではマスクをする。"
  },
  {
    "start": 2997056,
    "end": 3006654,
    "text": "マスクは基本的に、ある単語とその後に続く単語のドット積を求めず、その前に来る単語だけを求める。"
  },
  {
    "start": 3006692,
    "end": 3016510,
    "text": "基本的に、この行列の主対角より上の数値はすべて不要であり、それが自己注目でマスクを適用した理由である。"
  },
  {
    "start": 3016670,
    "end": 3021458,
    "text": "要は、これらのドット積を計算する必要はないということだ。"
  },
  {
    "start": 3021544,
    "end": 3026334,
    "text": "我々が興味を持っているのは、この最後の行のドット積だけである。"
  },
  {
    "start": 3026382,
    "end": 3039286,
    "text": "というのも、前回のタイムステップと比較して、トークン4を入力として追加したため、トークン4という新しいトークンしか持っておらず、このトークン4が他のすべてのトークンとどのように相互作用しているかを知りたいからだ。"
  },
  {
    "start": 3039398,
    "end": 3052074,
    "text": "また、語彙から単語を選択したいので、最後のトークンにしか関心がない。"
  },
  {
    "start": 3052122,
    "end": 3064050,
    "text": "私たちは最後の行にしか関心がなく、自己の注目の出力シーケンスでこれら3つの注目スコアを出すことには関心がない。"
  },
  {
    "start": 3064120,
    "end": 3068210,
    "text": "これらの冗長な計算をすべて削除する方法はありますか？"
  },
  {
    "start": 3068550,
    "end": 3071342,
    "text": "そう、KVキャッシュでできるんだ。"
  },
  {
    "start": 3071406,
    "end": 3072546,
    "text": "どうやるか見てみよう。"
  },
  {
    "start": 3072728,
    "end": 3074606,
    "text": "KVキャッシュで。"
  },
  {
    "start": 3074638,
    "end": 3092762,
    "text": "基本的には、クエリー、つまりキーと値をキャッシュし、新しいトークンができるたびに、キーと値にそれを追加する。"
  },
  {
    "start": 3092896,
    "end": 3097846,
    "text": "冒頭では、前のステップからの出力がないので、最初のトークンのみを使用する。"
  },
  {
    "start": 3097958,
    "end": 3103418,
    "text": "推論のタイムステップ1は、キャッシュがない場合と同じである。"
  },
  {
    "start": 3103514,
    "end": 3111950,
    "text": "トークン1とトークン1との掛け算で1×1の行列を生成し、それが1つのアテンションを生成します。"
  },
  {
    "start": 3112290,
    "end": 3123250,
    "text": "しかし、ステップ2では、前のクエリに追加するのではなく、前のトークンを新しいトークンに置き換えるだけである。"
  },
  {
    "start": 3123320,
    "end": 3134230,
    "text": "ただし、キーのキャッシュは保持しているので、キーには直前のトークンを保持し、キーには直前の出力を、値には直前の出力を追加する。"
  },
  {
    "start": 3134890,
    "end": 3148698,
    "text": "トークン2とトークン1、トークン2とトークン2の内積が最初の項目となる。"
  },
  {
    "start": 3148864,
    "end": 3157022,
    "text": "Vマトリックスと掛け合わせると、アテンションスコアは1つになる。"
  },
  {
    "start": 3157076,
    "end": 3163790,
    "text": "この注目点を2つ取ると、これが次の推論ステップの入力となる。"
  },
  {
    "start": 3163860,
    "end": 3171790,
    "text": "このトークン3は、以前にキャッシュされたk行列と、以前にキャッシュされたV行列に追加される。"
  },
  {
    "start": 3171950,
    "end": 3176754,
    "text": "この乗算は、ここで見ることができる出力行列を生成する。"
  },
  {
    "start": 3176952,
    "end": 3187862,
    "text": "この出力行列とこのV行列の乗算は、出力に1つのトークンを生成する。"
  },
  {
    "start": 3187996,
    "end": 3190890,
    "text": "そして次の推論ステップの入力として使う。"
  },
  {
    "start": 3190960,
    "end": 3197094,
    "text": "それをキャッシュされたキーに追加し、キャッシュされたVマトリックスに追加することによって。"
  },
  {
    "start": 3197222,
    "end": 3212400,
    "text": "トークン4とトークン1、トークン4とトークン2、トークン4とトークン3、そしてトークン4とそれ自身との内積である。"
  },
  {
    "start": 3212850,
    "end": 3220270,
    "text": "v行列を乗算すると、出力トークンを選択するために必要なアテンションが1つだけ生成される。"
  },
  {
    "start": 3220350,
    "end": 3227646,
    "text": "KVキャッシュと呼ばれる所以は、キーと値のキャッシュを保持しているからだ。"
  },
  {
    "start": 3227758,
    "end": 3241142,
    "text": "KVキャッシュのおかげで、これまで行っていたドット積の計算が減り、推論が高速化された。"
  },
  {
    "start": 3241206,
    "end": 3245222,
    "text": "次に説明するレイヤーは、グループ化されたマルチクエリー・アテンションである。"
  },
  {
    "start": 3245366,
    "end": 3252266,
    "text": "グループ化されたマルチクエリーアテンションについて話す前に、その前身であるマルチクエリーアテンションを紹介する必要がある。"
  },
  {
    "start": 3252378,
    "end": 3253454,
    "text": "見てみよう。"
  },
  {
    "start": 3253652,
    "end": 3256046,
    "text": "まず問題から始めよう。"
  },
  {
    "start": 3256148,
    "end": 3260000,
    "text": "問題はgpusが速すぎることだ。"
  },
  {
    "start": 3260850,
    "end": 3266174,
    "text": "このデータシートを見ると、これはNvidiaのA one gpuのものだ。"
  },
  {
    "start": 3266302,
    "end": 3277410,
    "text": "GPUは計算や演算は非常に速いが、メモリからのデータ転送はそれほど速くないことがわかる。"
  },
  {
    "start": 3277570,
    "end": 3291318,
    "text": "つまり、たとえば100番台の場合、32ビット精度で1秒間に19.5テラの浮動小数点演算ができることになる。"
  },
  {
    "start": 3291494,
    "end": 3307470,
    "text": "毎秒1.9千ギガバイトしか転送できないが、データ転送速度は計算速度の10倍近く遅い。"
  },
  {
    "start": 3308290,
    "end": 3318034,
    "text": "つまり、ボトルネックになるのは操作の回数ではなく、操作に必要なデータ転送量であることがある。"
  },
  {
    "start": 3318152,
    "end": 3323662,
    "text": "これは、計算に関わるテンソルのサイズと量に依存する。"
  },
  {
    "start": 3323806,
    "end": 3336070,
    "text": "例えば、同じテンソルに対して同じ演算をn回計算する場合、同じサイズであっても、n個の異なるトークンに対して同じ演算をするよりも速くなる可能性がある。"
  },
  {
    "start": 3336220,
    "end": 3340140,
    "text": "これは、GPUがテンソルを移動させる必要があるためだ。"
  },
  {
    "start": 3340510,
    "end": 3353918,
    "text": "つまり、アルゴリズムで行う演算の数を最適化するだけでなく、アルゴリズムが実行するメモリアクセスやメモリ転送を最小化することも目標にしなければならない。"
  },
  {
    "start": 3354084,
    "end": 3363530,
    "text": "なぜなら、メモリアクセスとメモリ転送は、計算に比べて時間的に高価だからだ。"
  },
  {
    "start": 3363690,
    "end": 3365486,
    "text": "これはソフトウェアでも起こることだ。"
  },
  {
    "start": 3365518,
    "end": 3374478,
    "text": "例えばコピーをする場合、CPUの中で乗算をしたり、ハードディスクからデータを読み込んだりする。"
  },
  {
    "start": 3374574,
    "end": 3381560,
    "text": "ハードディスクからの読み込みは、CPUで多くの計算をするよりもはるかに遅い。"
  },
  {
    "start": 3381930,
    "end": 3386018,
    "text": "本稿では、マルチクエリ・アテンションを紹介する。"
  },
  {
    "start": 3386194,
    "end": 3391690,
    "text": "この論文は、注目論文の著者の一人でもあるノーム・シャジールによるものだ。"
  },
  {
    "start": 3391760,
    "end": 3393546,
    "text": "注意を払うだけでいい。"
  },
  {
    "start": 3393728,
    "end": 3396010,
    "text": "この論文で彼はこの問題を紹介した。"
  },
  {
    "start": 3396080,
    "end": 3401334,
    "text": "彼は、そうだな、マルチヘッドに注目してみよう、と言った。"
  },
  {
    "start": 3401462,
    "end": 3403514,
    "text": "バッチ・マルチヘッドに注目。"
  },
  {
    "start": 3403642,
    "end": 3407262,
    "text": "これが原論文で発表されたマルチヘッドアテンションである。"
  },
  {
    "start": 3407316,
    "end": 3408622,
    "text": "注意を払うだけでいい。"
  },
  {
    "start": 3408756,
    "end": 3418318,
    "text": "このアルゴリズムを見て、実行される算術演算の回数と、これらの演算に関わるメモリの総量を計算してみよう。"
  },
  {
    "start": 3418494,
    "end": 3432390,
    "text": "ここで、bはバッチサイズ、nはシーケンス長、dは埋め込みベクトルのサイズである。"
  },
  {
    "start": 3432890,
    "end": 3454346,
    "text": "一方、派生的なものも含め、計算に関与するすべてのテンソルの合計で与えられる演算に関与する総メモリは、bとdのoにB-H-Nの2乗を足したものに等しく、hはこのマルチヘッドアテンションのヘッド数にdの2乗を足したものである。"
  },
  {
    "start": 3454538,
    "end": 3465440,
    "text": "ここで、総メモリ量と演算回数の比率を計算すると、kに1を足し、bに1を足した式が得られる。"
  },
  {
    "start": 3466070,
    "end": 3475214,
    "text": "この場合、比率は1よりずっと小さい。つまり、われわれが実行するメモリ・アクセスの回数は、算術演算の回数よりずっと少ないということだ。"
  },
  {
    "start": 3475342,
    "end": 3478870,
    "text": "この場合、メモリアクセスがボトルネックになるわけではない。"
  },
  {
    "start": 3479370,
    "end": 3488642,
    "text": "私が言いたいのは、このアルゴリズムのボトルネックはメモリアクセスではなく、計算回数だということです。"
  },
  {
    "start": 3488786,
    "end": 3494950,
    "text": "以前KVキャッシュを紹介したときに見たように、私たちが解決しようとしている問題は計算回数だ。"
  },
  {
    "start": 3495110,
    "end": 3508986,
    "text": "KVキャッシュを導入することで、新たな問題が発生した。つまり、新たな問題ではないが、新たなボトルネックが発生したのだ。"
  },
  {
    "start": 3509098,
    "end": 3517950,
    "text": "このアルゴリズムは、マルチヘッド自己注意であるが、KVキャッシュを使用することで、実行される操作の数を減らすことができる。"
  },
  {
    "start": 3518030,
    "end": 3524510,
    "text": "実行された算術演算の数を見ると、bとdの2乗である。"
  },
  {
    "start": 3524670,
    "end": 3541530,
    "text": "演算に関わる総メモリはbのn乗、dにnのd乗を足したもので、両者の比はnのo÷d÷1＋bなので、総メモリと演算回数の比となる。"
  },
  {
    "start": 3542590,
    "end": 3555598,
    "text": "つまり、nがdに非常に近いとき、この比率は1になり、bが1に非常に近いとき、あるいは1の極限で、バッチサイズが1になるとき、この比率は1になる。"
  },
  {
    "start": 3555684,
    "end": 3565490,
    "text": "というのも、この条件が検証され、それが真であった場合、メモリアクセスがアルゴリズムのボトルネックになるからだ。"
  },
  {
    "start": 3565990,
    "end": 3586070,
    "text": "このことは、埋め込みベクトルの次元を配列長よりずっと大きくしておくか、埋め込みベクトルの次元をあまり大きくせずに配列長を長くすると、メモリアクセスがボトルネックになることも意味している。"
  },
  {
    "start": 3586490,
    "end": 3596214,
    "text": "私たちにできることは、メモリがボトルネックになった以前のアルゴリズムの問題を解決する、より良い方法を見つけることだ。"
  },
  {
    "start": 3596342,
    "end": 3598694,
    "text": "マルチクエリーアテンションを紹介した。"
  },
  {
    "start": 3598822,
    "end": 3606126,
    "text": "アウターが行ったのは、kとvからH寸法を削除し、qにはH寸法を残すことだった。"
  },
  {
    "start": 3606228,
    "end": 3612766,
    "text": "それはまだマルチヘッドアテンションであるが、qに関してだけである。"
  },
  {
    "start": 3612868,
    "end": 3615102,
    "text": "それがマルチクエリー・アテンションと呼ばれる理由だ。"
  },
  {
    "start": 3615166,
    "end": 3621646,
    "text": "の場合、qだけ複数のヘッドが存在することになるが、kとvはすべてのヘッドが共有することになる。"
  },
  {
    "start": 3621838,
    "end": 3631334,
    "text": "このアルゴリズムで計算すると、dとnの比をdhとbの比で割ったものになる。"
  },
  {
    "start": 3631452,
    "end": 3636278,
    "text": "nをdで割ったものと比較する。"
  },
  {
    "start": 3636364,
    "end": 3638470,
    "text": "今はnをdhで割っている。"
  },
  {
    "start": 3638550,
    "end": 3651118,
    "text": "kとvの頭数をH個取り除いたので、n÷dの倍率をh倍にした。"
  },
  {
    "start": 3651204,
    "end": 3661840,
    "text": "なぜなら、この比率が1になる可能性が低くなったからだ。"
  },
  {
    "start": 3662950,
    "end": 3670878,
    "text": "もちろん、kとvから頭部を取り除くことで、モデルのパラメータも少なくなる。"
  },
  {
    "start": 3670974,
    "end": 3678258,
    "text": "また、自由度や複雑さが少なくなり、モデルの質が低下する可能性もある。"
  },
  {
    "start": 3678344,
    "end": 3682274,
    "text": "実際、モデルの質は落ちるが、ほんのわずかだ。"
  },
  {
    "start": 3682322,
    "end": 3683254,
    "text": "いずれわかるだろう。"
  },
  {
    "start": 3683372,
    "end": 3701900,
    "text": "例えば、英語からドイツ語への翻訳タスクのブルースコアを比較すると、マルチヘッドアテンション、つまり元のアテンション論文にあったアテンションのブルースコアは26.7であるのに対し、マルチクエリのブルースコアは26.5であることがわかる。"
  },
  {
    "start": 3702510,
    "end": 3718478,
    "text": "著者はまた、マルチヘッドローカルやマルチクエリローカルとも比較している。ローカルとは、各トークンの前の31の位置だけに注目して計算することを意味する。"
  },
  {
    "start": 3718654,
    "end": 3720594,
    "text": "ここで見ることができる。"
  },
  {
    "start": 3720712,
    "end": 3733122,
    "text": "kとvのヘッドを減らすことによるパフォーマンス向上は、例えば、元のマルチヘッドアテンションとマルチクエリーアテンションの推論時間を見ることができるので、素晴らしいものです。"
  },
  {
    "start": 3733266,
    "end": 3746870,
    "text": "推論時間は、デコーダーの1.7マイクロ秒＋46マイクロ秒から、デコーダーの1.5マイクロ秒＋3.8マイクロ秒になった。"
  },
  {
    "start": 3746950,
    "end": 3757274,
    "text": "マルチクエリに6マイクロ秒かかるのに対して、ここでは48秒、48マイクロ秒かかっている。"
  },
  {
    "start": 3757322,
    "end": 3765226,
    "text": "これは、推論時のパフォーマンスの観点から見ても大きな利点だ。"
  },
  {
    "start": 3765338,
    "end": 3773742,
    "text": "KVキャッシュとマルチクエリー・アテンションについて説明しよう。"
  },
  {
    "start": 3773886,
    "end": 3780466,
    "text": "マルチクエリーアテンションの次のステップは、グループ化されたマルチクエリーアテンションである。"
  },
  {
    "start": 3780498,
    "end": 3782134,
    "text": "見てみよう。"
  },
  {
    "start": 3782332,
    "end": 3789654,
    "text": "マルチクエリーでは、クエリーのヘッドは複数あるが、キーと値のヘッドは1つしかない。"
  },
  {
    "start": 3789782,
    "end": 3795670,
    "text": "グループ化されたマルチクエリーアテンションでは、基本的にクエリーをグループに分けます。"
  },
  {
    "start": 3795750,
    "end": 3800522,
    "text": "例えば、これがグループ1、これがグループ2、グループ3、グループ4だ。"
  },
  {
    "start": 3800656,
    "end": 3806334,
    "text": "各グループには、kとvの異なる頭が1つずつある。"
  },
  {
    "start": 3806532,
    "end": 3816606,
    "text": "これは、1対1の対応があるマルチヘッドと、n対1の対応があるマルチクエリとの良い妥協点である。"
  },
  {
    "start": 3816718,
    "end": 3826482,
    "text": "この場合でも、キーと値に対して複数のヘッドが存在するが、クエリのヘッドの数に比べれば数値的には少ない。"
  },
  {
    "start": 3826626,
    "end": 3847402,
    "text": "というのも、ここではとにかく、キーと値の頭数を減らすことによる計算上のメリットは享受できるが、品質面ではあまり犠牲にならないからだ。"
  },
  {
    "start": 3847536,
    "end": 3849690,
    "text": "さて、モデルの最後の部分である。"
  },
  {
    "start": 3849840,
    "end": 3855918,
    "text": "ここでわかるように、ラマ・モデルのフィードフォワードは変換されている。"
  },
  {
    "start": 3856084,
    "end": 3860670,
    "text": "その活性化関数はツヴィグル関数と変わったのか？"
  },
  {
    "start": 3860820,
    "end": 3863038,
    "text": "その仕組みを見てみよう。"
  },
  {
    "start": 3863204,
    "end": 3876050,
    "text": "Zwiglu関数は、注目モデルの外側の一人でもあるノーム・シャジール（Noam Shazir）の有名な論文で分析されている。"
  },
  {
    "start": 3876200,
    "end": 3879458,
    "text": "この論文を見てみよう。"
  },
  {
    "start": 3879624,
    "end": 3888902,
    "text": "は、変圧器アーキテクチャのフィードフォワード層に異なる活性化関数を用いて、変圧器モデルの性能を比較した。"
  },
  {
    "start": 3889046,
    "end": 3911578,
    "text": "これは基本的に、ベータが1に等しいzwish関数で、xにパラメータ行列であるw行列を掛けて計算される。"
  },
  {
    "start": 3911674,
    "end": 3923714,
    "text": "元のフィード・フォワード・ネットワークでは2つしかなかったパラメータ行列が、ここでは3つになっている。"
  },
  {
    "start": 3923832,
    "end": 3938274,
    "text": "比較を公平にするため、アウターはこれらの行列のサイズを小さくし、モデルのパラメータ総数がバニラ・トランスと変わらないようにした。"
  },
  {
    "start": 3938322,
    "end": 3942922,
    "text": "バニラ・トランスには、このフィード・フォワード・ネットワークがあった。"
  },
  {
    "start": 3942976,
    "end": 3948822,
    "text": "このmax zeroなどがrelu関数で、パラメータ行列は2つしかなかった。"
  },
  {
    "start": 3948886,
    "end": 3954250,
    "text": "実は、いくつかの後継バージョンのトランスにはバイアスがなかった。"
  },
  {
    "start": 3954330,
    "end": 3959120,
    "text": "私はこの数式を論文から引用したが、実際にはバイアスを除いた多くの実装がある。"
  },
  {
    "start": 3960130,
    "end": 3966558,
    "text": "一方、llamaではこの計算をウォールネットワークのフィールドに使用する。"
  },
  {
    "start": 3966654,
    "end": 3969646,
    "text": "これはlamaのリポジトリから引用したコードだ。"
  },
  {
    "start": 3969678,
    "end": 3972850,
    "text": "見ての通り、モデル通りだ。"
  },
  {
    "start": 3972920,
    "end": 3974274,
    "text": "サイロ機能だ。"
  },
  {
    "start": 3974392,
    "end": 3975522,
    "text": "なぜサイロ機能なのか？"
  },
  {
    "start": 3975576,
    "end": 3978774,
    "text": "なぜなら、それはベータが1に等しいスイス関数だからだ。"
  },
  {
    "start": 3978812,
    "end": 3983766,
    "text": "この式を持つスイス関数の場合、ベータは1に等しい。"
  },
  {
    "start": 3983788,
    "end": 3989970,
    "text": "このグラフはシグモイド線形ユニットと呼ばれるもので、サイロと呼ばれている。"
  },
  {
    "start": 3990050,
    "end": 4000650,
    "text": "これは、xのw 1で評価されたシルエット関数にw 3を掛け、それをw 2に適用したものである。"
  },
  {
    "start": 4000720,
    "end": 4005882,
    "text": "この3つのマトリックスは基本的にリニアレイヤーである。"
  },
  {
    "start": 4005946,
    "end": 4010270,
    "text": "今、彼らはこのリニアレイヤーの並列化バージョンを使っているが、それはリニアレイヤーだ。"
  },
  {
    "start": 4010610,
    "end": 4020206,
    "text": "このサイロ関数のグラフを見ると、リルーのような形になっていることがわかる。"
  },
  {
    "start": 4020238,
    "end": 4026802,
    "text": "ここで、ゼロの前に、すぐに活性化を打ち消すことはしない。"
  },
  {
    "start": 4026866,
    "end": 4036918,
    "text": "ここでは、マイナス側から見てゼロに非常に近い値であっても、関数によって自動的に打ち消されないように、少しテールを残している。"
  },
  {
    "start": 4037004,
    "end": 4038938,
    "text": "さて、どんなパフォーマンスを見せてくれるかな。"
  },
  {
    "start": 4039024,
    "end": 4042090,
    "text": "このZwigluの機能は、実際に非常によく機能している。"
  },
  {
    "start": 4042240,
    "end": 4042602,
    "text": "ここだよ。"
  },
  {
    "start": 4042656,
    "end": 4051566,
    "text": "彼らは、この特定の関数を使用した場合のモデルの対数複雑度を評価する。"
  },
  {
    "start": 4051668,
    "end": 4055402,
    "text": "ここでの当惑は最低であることがわかる。"
  },
  {
    "start": 4055546,
    "end": 4060990,
    "text": "当惑度とは、基本的にモデルがその選択についてどれだけ確信が持てないかを意味する。"
  },
  {
    "start": 4061890,
    "end": 4065874,
    "text": "Zwigluの機能はうまく機能している。"
  },
  {
    "start": 4065992,
    "end": 4076022,
    "text": "さらに、多くのベンチマークで同じ比較を行ったところ、Zwigluの機能が多くのベンチマークで非常に良い結果を出していることがわかった。"
  },
  {
    "start": 4076156,
    "end": 4081142,
    "text": "なぜツヴィグル活性化関数はこれほどうまく機能しているのか？"
  },
  {
    "start": 4081196,
    "end": 4088738,
    "text": "この論文の結論を見てみると、なぜこのアーキテクチャが機能しているように見えるのか、私たちは何も説明していない。"
  },
  {
    "start": 4088844,
    "end": 4093526,
    "text": "私たちは彼らの成功を、他のすべてと同様、神の慈悲によるものだと考えている。"
  },
  {
    "start": 4093718,
    "end": 4104494,
    "text": "というのも、ディープラーニングの研究のほとんどは、物事がなぜそのように機能するのかわかっていないからだ。"
  },
  {
    "start": 4104612,
    "end": 4108378,
    "text": "というのも、700億ものパラメーターを持つモデルがあるとしよう。"
  },
  {
    "start": 4108554,
    "end": 4116530,
    "text": "1つの活性化関数を変更した後、それぞれの関数に何が起こっているかをどうやって証明するのか？"
  },
  {
    "start": 4116600,
    "end": 4122514,
    "text": "モデルが特定の反応を示す理由を説明できるモデルを考え出すのは容易ではない。"
  },
  {
    "start": 4122632,
    "end": 4137270,
    "text": "通常私たちがすることは、モデルを単純化して非常に小さなモデルで作業できるようにし、物事がなぜそのように機能するのかについて仮定を立てるか、あるいは実用的なレベルでそれを行うかだ。"
  },
  {
    "start": 4137340,
    "end": 4143514,
    "text": "あるモデルを使い、それを少し修正し、アブレーション試験を行い、どちらがより良いパフォーマンスかをチェックする。"
  },
  {
    "start": 4143632,
    "end": 4146858,
    "text": "これは機械学習の多くの分野でも起こることだ。"
  },
  {
    "start": 4146944,
    "end": 4166818,
    "text": "例えば、モデルの適切なパラメータを見つけるために、多くのグリッド検索を行う。どれがうまく機能するか、どれを増やしたり減らしたりするかは、使用するアルゴリズムだけでなく、データ、使用する特定の計算、使用する正規化など、多くの要因に依存するため、事前に知ることができないからだ。"
  },
  {
    "start": 4166904,
    "end": 4172450,
    "text": "いろいろな要素があり、すべてを説明する公式は存在しない。"
  },
  {
    "start": 4172600,
    "end": 4183330,
    "text": "だからこそ、ある領域ではうまくいくかもしれないが、他の領域ではうまくいかないようなモデルを考え出すために、研究者はモデルのバリエーションについて多くの研究をする必要がある。"
  },
  {
    "start": 4183410,
    "end": 4189510,
    "text": "この場合、私たちはZwigluを使うが、それは実際にこの種のモデルでうまく機能するからである。"
  },
  {
    "start": 4190510,
    "end": 4192586,
    "text": "長いビデオを見てくれてありがとう。"
  },
  {
    "start": 4192688,
    "end": 4200618,
    "text": "リャマで何が起きているのか、なぜ標準的なトランスモデルと違うのか、より深いレベルで学んでいただけたと思う。"
  },
  {
    "start": 4200704,
    "end": 4205854,
    "text": "映像がかなり長くなってしまい、ついていくのが大変な部分もあったかと思います。"
  },
  {
    "start": 4205972,
    "end": 4215598,
    "text": "特に、あまり馴染みのない部分については、何度も見直すことをお勧めする。"
  },
  {
    "start": 4215694,
    "end": 4220210,
    "text": "あなたが欲しい部分を簡単に見つけられるように、章立てをしておきます。"
  },
  {
    "start": 4220360,
    "end": 4223986,
    "text": "これが必要なことだ。"
  },
  {
    "start": 4224008,
    "end": 4227874,
    "text": "実際にマスターするには、同じコンセプトを何度も見る必要がある。"
  },
  {
    "start": 4227992,
    "end": 4235074,
    "text": "この理論をすべて実践できるように、ゼロからラマ・モデルをコーディングするビデオをまた作りたいと思っている。"
  },
  {
    "start": 4235202,
    "end": 4241142,
    "text": "ご存知のように、私は自由な時間にこれをやっている。"
  },
  {
    "start": 4241196,
    "end": 4251374,
    "text": "私のビデオを見てくれてありがとう。私のチャンネルを購読してください。これが、私がAIと機械学習に関する素晴らしいコンテンツを投稿し続ける最高のモチベーションになるからです。"
  },
  {
    "start": 4251492,
    "end": 4254060,
    "text": "ご視聴ありがとうございました。素晴らしい一日をお過ごしください。"
  }
]