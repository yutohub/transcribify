[
  {
    "start": 170,
    "end": 2686,
    "text": "ハローみんな、僕の新しいコーディング・ビデオへようこそ。"
  },
  {
    "start": 2788,
    "end": 5754,
    "text": "このビデオでは、ラマ2をゼロからコーディングします。"
  },
  {
    "start": 5882,
    "end": 11854,
    "text": "前回のビデオと同じように、私はトランスフォーマーのモデルをゼロからコーディングした。"
  },
  {
    "start": 12052,
    "end": 16346,
    "text": "コーディングしながら、リャマのあらゆる面を説明する。"
  },
  {
    "start": 16378,
    "end": 19466,
    "text": "リャマ・アーキテクチャーの各構成ブロックのすべて。"
  },
  {
    "start": 19578,
    "end": 23434,
    "text": "また、回転位置エンコーディングの背後にある数学についても説明しよう。"
  },
  {
    "start": 23482,
    "end": 27426,
    "text": "グループ化されたクエリー・アテンション、KVキャッシュについても説明する。"
  },
  {
    "start": 27458,
    "end": 32680,
    "text": "これらの概念について理論的な見解を得るだけでなく、実践的な見解も得られるだろう。"
  },
  {
    "start": 33130,
    "end": 38742,
    "text": "もしあなたがトランスフォーマーモデルについてよく知らないのであれば、私が以前に撮影したトランスフォーマーモデルについてのビデオをご覧になることを強くお勧めする。"
  },
  {
    "start": 38796,
    "end": 48700,
    "text": "もし必要であれば、ゼロからTransformerモデルをコーディングする方法についての以前のビデオも見てください。"
  },
  {
    "start": 49470,
    "end": 57870,
    "text": "リャマのアーキテクチャについて説明した前回のビデオをすでにご覧になった方も、そうでない方も、とても参考になると思います。"
  },
  {
    "start": 58850,
    "end": 66862,
    "text": "前回のビデオほど詳しくは説明しないが、すべてのコンセプトを説明しようと思う。"
  },
  {
    "start": 66996,
    "end": 76280,
    "text": "もしお時間があれば、前回のラマ建築のビデオを見てからこのビデオを見てください。"
  },
  {
    "start": 76970,
    "end": 81330,
    "text": "ラマ建築を見直そう"
  },
  {
    "start": 81410,
    "end": 90246,
    "text": "ここでは、\"attention is all you need \"で紹介した標準的なトランスのアーキテクチャと、llamaのアーキテクチャを比較している。"
  },
  {
    "start": 90358,
    "end": 103230,
    "text": "まず最初に気づくのは、トランスフォーマーはエンコーダー・デコーダー・モデルであり、前のビデオでは実際に、たとえば英語からイタリア語への翻訳タスクで訓練した。"
  },
  {
    "start": 103730,
    "end": 106126,
    "text": "llamaは大規模な言語モデルである。"
  },
  {
    "start": 106228,
    "end": 113722,
    "text": "大規模な言語モデルの目標は、実際には次のトークン予測タスクと呼ばれるものに取り組むことである。"
  },
  {
    "start": 113786,
    "end": 121954,
    "text": "プロンプトが与えられると、モデルはこのプロンプトを最も首尾一貫した方法で完成させる次のトークンを考え出そうとする。"
  },
  {
    "start": 122072,
    "end": 125506,
    "text": "答えが納得できるように。"
  },
  {
    "start": 125688,
    "end": 132114,
    "text": "前のトークンに基づいて、連続するトークンをモデルに求め続ける。"
  },
  {
    "start": 132162,
    "end": 134662,
    "text": "これが因果モデルと呼ばれる所以である。"
  },
  {
    "start": 134716,
    "end": 138970,
    "text": "各出力は前のトークンに依存し、これはプロンプトとも呼ばれる。"
  },
  {
    "start": 140830,
    "end": 154058,
    "text": "トランスフォーマーモデルのコーディングに関する前回のビデオで行ったのとは逆に、このビデオでは、ラマを構成する単一のブロックのコーディングから始めて、それから大きな絵を考えようとはしない。"
  },
  {
    "start": 154154,
    "end": 155914,
    "text": "我々は大局的な見地からスタートする。"
  },
  {
    "start": 155962,
    "end": 162090,
    "text": "まずアーキテクチャーの骨格を作り、それから各ブロックを作っていく。"
  },
  {
    "start": 162250,
    "end": 165798,
    "text": "リャマを説明するには、この方がいいと思う。"
  },
  {
    "start": 165834,
    "end": 171278,
    "text": "また、単一の構成要素がはるかに複雑であっても、モデルが単純だからだ。"
  },
  {
    "start": 171374,
    "end": 179426,
    "text": "そのため、まず互いがどのように影響し合っているかを見てから、その内部構造にズームインしたほうがいいのだ。"
  },
  {
    "start": 179618,
    "end": 183202,
    "text": "エンベッディングから旅を始めよう。"
  },
  {
    "start": 183266,
    "end": 186066,
    "text": "このブロック、レーザーを使わせてください。"
  },
  {
    "start": 186178,
    "end": 187382,
    "text": "このブロックはここだ。"
  },
  {
    "start": 187436,
    "end": 192170,
    "text": "入力が与えられ、それを埋め込みに変換したい。"
  },
  {
    "start": 192670,
    "end": 194758,
    "text": "エンベッディングとは何なのかも復習しておこう。"
  },
  {
    "start": 194854,
    "end": 199494,
    "text": "これは前回のトランスフォーマーについてのビデオで使ったスライドだ。"
  },
  {
    "start": 199622,
    "end": 203290,
    "text": "見ての通り、入力文から始める。"
  },
  {
    "start": 203370,
    "end": 206702,
    "text": "例えば、「あなたの猫はかわいい猫です。"
  },
  {
    "start": 206756,
    "end": 207566,
    "text": "それをトークン化する。"
  },
  {
    "start": 207588,
    "end": 209790,
    "text": "トークンに分割する。"
  },
  {
    "start": 210290,
    "end": 213934,
    "text": "各トークンを語彙内の位置にマッピングする。"
  },
  {
    "start": 213982,
    "end": 219330,
    "text": "語彙とは、モデルが認識できるすべての単語のリストである。"
  },
  {
    "start": 219670,
    "end": 224098,
    "text": "これらのトークンは、実はほとんどの場合、単一単語ではない。"
  },
  {
    "start": 224264,
    "end": 230150,
    "text": "私が言いたいのは、このモデルは単に空白で単語を分割して1つの単語にするわけではないということだ。"
  },
  {
    "start": 230300,
    "end": 244410,
    "text": "通常、最も一般的に使用されるトークナイザーはBPEトークナイザーで、これはバイトペアエンコーディングトークナイザーを意味します。"
  },
  {
    "start": 244480,
    "end": 253070,
    "text": "単語の一部かもしれないし、空白かもしれないし、複数の単語かもしれないし、1桁かもしれない。"
  },
  {
    "start": 253490,
    "end": 264158,
    "text": "エンベッディングは、語彙内のトークンの位置を表す入力IDをベクトルにマッピングしたものである。"
  },
  {
    "start": 264334,
    "end": 271554,
    "text": "オリジナルのトランスフォーマーではこのベクトルのサイズは512であったが、ベースモデルではラマであった。"
  },
  {
    "start": 271672,
    "end": 276040,
    "text": "70億モデルでは4096だ。"
  },
  {
    "start": 276570,
    "end": 283174,
    "text": "次元は4096であり、このベクトルには4096個の数値が含まれることになる。"
  },
  {
    "start": 283292,
    "end": 297626,
    "text": "これらのベクトルはそれぞれ、実際にモデルに沿って学習されたパラメータ・ベクトルであり、何らかの形でそれぞれの単語の意味を捉えているからだ。"
  },
  {
    "start": 297728,
    "end": 306750,
    "text": "例えば、catとdogという単語を取り上げると、catとtreeに比べてより類似した埋め込みを持つことになる。"
  },
  {
    "start": 307090,
    "end": 318446,
    "text": "この2つのベクトルの距離を比較すれば、ユークリッド距離ということになる。"
  },
  {
    "start": 318478,
    "end": 321426,
    "text": "だからこそ、私たちは彼らが言葉の意味を捉えていると言うのだ。"
  },
  {
    "start": 321608,
    "end": 323700,
    "text": "では、モデルのコーディングを始めよう。"
  },
  {
    "start": 326550,
    "end": 332818,
    "text": "ビジュアル・スタジオのコードを開き、まず必要なライブラリーがあることを確認する。"
  },
  {
    "start": 332914,
    "end": 335846,
    "text": "このプロジェクトのリポジトリも共有する。"
  },
  {
    "start": 335948,
    "end": 341142,
    "text": "必要なのはtorch sentence pieceだけで、これはllamaで使っているトークナイザーだ。"
  },
  {
    "start": 341206,
    "end": 342650,
    "text": "tkodmです。"
  },
  {
    "start": 343150,
    "end": 347526,
    "text": "もうひとつは、llamaの公式リポジトリから、ダウンロードスクリプトをダウンロードすることだ。"
  },
  {
    "start": 347558,
    "end": 352378,
    "text": "ラマモデルの重さをダウンロードできるのは、このファイルダウンロードshです。"
  },
  {
    "start": 352464,
    "end": 357818,
    "text": "私の場合、トークナイザーと一緒にllama two 70 billionをダウンロードしました。"
  },
  {
    "start": 357994,
    "end": 360640,
    "text": "実はこれが一番小さいモデルなんだ。"
  },
  {
    "start": 361090,
    "end": 365838,
    "text": "私のGpuはパワー不足なので、Gpuを使うことすらできないだろう。"
  },
  {
    "start": 366004,
    "end": 368398,
    "text": "だから、CPUでモデルを走らせる。"
  },
  {
    "start": 368494,
    "end": 375330,
    "text": "強力なGPUを搭載していない限り、普通のコンピューターには少し大きいからね。"
  },
  {
    "start": 375770,
    "end": 377846,
    "text": "さっそくコーディングしてみよう。"
  },
  {
    "start": 377948,
    "end": 384710,
    "text": "新しいファイルモデルpyを作成し、旅を始めましょう。"
  },
  {
    "start": 386090,
    "end": 387850,
    "text": "必要なものを輸入する。"
  },
  {
    "start": 387920,
    "end": 398410,
    "text": "我々はトーチ・インポートとトーチNNを輸入している。"
  },
  {
    "start": 400850,
    "end": 401262,
    "text": "オーケー。"
  },
  {
    "start": 401316,
    "end": 402942,
    "text": "これらは私たちが常に輸入している基本的なものだ。"
  },
  {
    "start": 402996,
    "end": 420178,
    "text": "数学も必要だし、データクラスも必要だ。"
  },
  {
    "start": 420344,
    "end": 426350,
    "text": "コードのほとんどはオリジナルのAMAコードに基づいているので、類似点が多くても驚くことはない。"
  },
  {
    "start": 426510,
    "end": 432070,
    "text": "特に並列化など、不要なものを取り除くために多くの部分を簡略化した。"
  },
  {
    "start": 432410,
    "end": 440498,
    "text": "また、それぞれのテンソルの形がすべて変化することを示すために、たくさんのコメントを加えるようにしている。"
  },
  {
    "start": 440674,
    "end": 442102,
    "text": "それだけだ。"
  },
  {
    "start": 442156,
    "end": 442870,
    "text": "始めよう。"
  },
  {
    "start": 442940,
    "end": 449270,
    "text": "最初に作りたいのは、モデルのパラメータを表すクラスだ。"
  },
  {
    "start": 449420,
    "end": 480274,
    "text": "ここにはすでに2種類のヘッドがある。"
  },
  {
    "start": 480322,
    "end": 483010,
    "text": "ひとつはクエリーのヘッド数。"
  },
  {
    "start": 483090,
    "end": 486338,
    "text": "クエリーのヘッド数。"
  },
  {
    "start": 486434,
    "end": 502842,
    "text": "ここでは、kとv、つまりキーと値に対するヘッドの数を持っています。後でグループ化されたクエリに注目するとわかりますが、元の変換器のように、クエリのキーと値に対して必ずしも同じ数のヘッドを持つ必要はありません。"
  },
  {
    "start": 502906,
    "end": 516126,
    "text": "我々は複数のヘッドを持つことができる。"
  },
  {
    "start": 516228,
    "end": 520510,
    "text": "これは、トークナイザーをロードするときに設定されます。"
  },
  {
    "start": 524610,
    "end": 547798,
    "text": "これら2つのパラメータは、FFN層の隠れ次元を示す。"
  },
  {
    "start": 547814,
    "end": 549398,
    "text": "フィードフォワード層"
  },
  {
    "start": 549494,
    "end": 563690,
    "text": "基本的な考え方は、グループ化されたクエリー・アテンションを導入する際、kとvのヘッダーの数を減らすため、パラメーターの数を維持しようとしたが、フィードフォワード層のパラメーターの数を増やしてしまった。"
  },
  {
    "start": 563770,
    "end": 567522,
    "text": "のように、モデルの全パラメータ数は変わらない。"
  },
  {
    "start": 567576,
    "end": 572686,
    "text": "これにより、完全なベース・トランスを比較することができる。"
  },
  {
    "start": 572718,
    "end": 580770,
    "text": "KNBのヘッド数を減らしたllamaで使用しているもので、クエリ、キー、値のすべてのヘッドを持つ。"
  },
  {
    "start": 580930,
    "end": 583830,
    "text": "これは単なる決定であり、建築上の決定だ。"
  },
  {
    "start": 584410,
    "end": 586610,
    "text": "その後、いくつかのエピソードがある。"
  },
  {
    "start": 586690,
    "end": 591640,
    "text": "これは非常に小さな数字で、なぜそれが必要なのかはこれからわかるだろう。"
  },
  {
    "start": 612850,
    "end": 614798,
    "text": "必要なパラメーターはこれだけだ。"
  },
  {
    "start": 614884,
    "end": 618526,
    "text": "また、ここでは後にKVキャッシュに使用する2つのパラメーターを用意している。"
  },
  {
    "start": 618558,
    "end": 621220,
    "text": "それが何なのか、どのように機能するのかは後で説明しよう。"
  },
  {
    "start": 621670,
    "end": 626950,
    "text": "まずは、さっきも言ったように、モデル全体の骨格を実装するところから始めよう。"
  },
  {
    "start": 627020,
    "end": 629302,
    "text": "そして、各パートを実装する。"
  },
  {
    "start": 629436,
    "end": 636280,
    "text": "各パーツを実装しながら、その背景や仕組み、数学的な裏付けも確認していく。"
  },
  {
    "start": 641690,
    "end": 644710,
    "text": "これはモデル全体を表すメイン・クラスである。"
  },
  {
    "start": 644780,
    "end": 702070,
    "text": "ソフト・マックスを除けば、ここにあるモデルはすべて、モデルの語彙サイズを設定したことを確認できる。"
  },
  {
    "start": 702140,
    "end": 716598,
    "text": "このブロックは、変圧器と同じように、何度も何度も繰り返されている。"
  },
  {
    "start": 716764,
    "end": 719542,
    "text": "ここでは32回繰り返されている。"
  },
  {
    "start": 719676,
    "end": 725990,
    "text": "最後の層の出力は、このrmsノルムに送られ、次に線形に送られる。"
  },
  {
    "start": 726290,
    "end": 756886,
    "text": "モデルの重みをロードするときに、名前が一致していないと、トーチがどこに重みをロードすればいいのかわからないからです。"
  },
  {
    "start": 757068,
    "end": 760022,
    "text": "これが、私が同じ名前を維持しようとしている理由だ。"
  },
  {
    "start": 760156,
    "end": 765720,
    "text": "わかりやすくするために一部の名前を変えただけで、他の名前はほとんど同じだ。"
  },
  {
    "start": 774250,
    "end": 775382,
    "text": "これはモデルリストである。"
  },
  {
    "start": 775436,
    "end": 791650,
    "text": "これは後で作成するレイヤーのリストである。"
  },
  {
    "start": 791720,
    "end": 797762,
    "text": "エンコーダー・ブロックは、ここにある各ブロックである。"
  },
  {
    "start": 797896,
    "end": 799730,
    "text": "これがエンコーダー・ブロックだ。"
  },
  {
    "start": 800070,
    "end": 801902,
    "text": "今は骨格を作るだけだ。"
  },
  {
    "start": 801966,
    "end": 809842,
    "text": "正規化とはRMS正規化のことである。"
  },
  {
    "start": 809906,
    "end": 811400,
    "text": "後で実施する。"
  },
  {
    "start": 812170,
    "end": 815686,
    "text": "特徴の大きさを伝える必要がある。"
  },
  {
    "start": 815878,
    "end": 824220,
    "text": "EPSは正規化計算に必要な非常に小さな数字で、基本的にゼロで割ることはない。"
  },
  {
    "start": 828850,
    "end": 849342,
    "text": "次に、出力層、回転位置エンコーディングの周波数がある。"
  },
  {
    "start": 849406,
    "end": 852520,
    "text": "そうしよう"
  },
  {
    "start": 855130,
    "end": 859720,
    "text": "私はこのメソッドを作り、それを実装することにした。"
  },
  {
    "start": 876770,
    "end": 877520,
    "text": "そうだ。"
  },
  {
    "start": 895110,
    "end": 895618,
    "text": "チェックする。"
  },
  {
    "start": 895704,
    "end": 897454,
    "text": "括弧があると思う。"
  },
  {
    "start": 897582,
    "end": 901042,
    "text": "さて、これがベースとなるトランスのモデルだ。"
  },
  {
    "start": 901096,
    "end": 903294,
    "text": "まず最初に、我々はn個のレイヤーを持つ。"
  },
  {
    "start": 903422,
    "end": 906002,
    "text": "まず、トークン、入力エンベッディングがある。"
  },
  {
    "start": 906066,
    "end": 910002,
    "text": "を埋め込みに変換する。"
  },
  {
    "start": 910066,
    "end": 912002,
    "text": "そして、それをレイヤーのリストに通す。"
  },
  {
    "start": 912066,
    "end": 915878,
    "text": "最後のレイヤーの出力は正規化され、次に出力に送られる。"
  },
  {
    "start": 915974,
    "end": 917802,
    "text": "このロジックはより明確になるだろう。"
  },
  {
    "start": 917856,
    "end": 941378,
    "text": "フォワード・メソッドでは、これまでの変圧器と違う点は、欲しいシーケンス長が常に1であることだ。"
  },
  {
    "start": 941544,
    "end": 946386,
    "text": "これはKVキャッシュを使っているからで、その理由は後ほど説明する。"
  },
  {
    "start": 946488,
    "end": 950020,
    "text": "では、ここでおさらいをしよう。"
  },
  {
    "start": 952390,
    "end": 959618,
    "text": "私たちが入力を与えると、プロンプトが表示され、モデルは次のトークンのソフトマックスを表示する。"
  },
  {
    "start": 959794,
    "end": 966946,
    "text": "KVキャッシュを使えば、以前のトークンをすべて渡す必要はない。"
  },
  {
    "start": 967058,
    "end": 972982,
    "text": "最新のトークンを与えるだけで、モデルは次のトークンを出力し、その間に中間トークンを出力する。"
  },
  {
    "start": 973046,
    "end": 976774,
    "text": "前のトークンのキャッシュは、モデルによってそのキャッシュに保持される。"
  },
  {
    "start": 976822,
    "end": 978186,
    "text": "なぜなら、ここにはkvキャッシュがあるからだ。"
  },
  {
    "start": 978218,
    "end": 979742,
    "text": "このメカニズムについては後述する。"
  },
  {
    "start": 979796,
    "end": 985120,
    "text": "とりあえず、ここで得られる入力は一度にひとつのトークンであることを覚えておいてほしい。"
  },
  {
    "start": 989750,
    "end": 993330,
    "text": "シーケンス長を持つバッチが得られる。"
  },
  {
    "start": 996070,
    "end": 1006280,
    "text": "バッチ・サイズ、シーケンス長はトークン形状で、シーケンス長が実際に1であることを確認する。"
  },
  {
    "start": 1015450,
    "end": 1025766,
    "text": "なぜなら、トレーニングにはもちろんkvキャッシュが必要ないし、複数のトークンを処理できる必要があるからだ。"
  },
  {
    "start": 1025958,
    "end": 1029870,
    "text": "私たちの目標は、実際には事前に訓練されたラマの重みを使用することです。"
  },
  {
    "start": 1041750,
    "end": 1044210,
    "text": "トークンを埋め込みに変換する。"
  },
  {
    "start": 1047750,
    "end": 1055622,
    "text": "ご覧の通り、dim次元を追加しているので、埋め込み次元はベースモデルの4096次元となる。"
  },
  {
    "start": 1055676,
    "end": 1058200,
    "text": "モデルのサイズによって異なる場合があります。"
  },
  {
    "start": 1090930,
    "end": 1095618,
    "text": "このラインとこのラインについては、もっと詳しく説明すると約束しよう。"
  },
  {
    "start": 1095704,
    "end": 1099346,
    "text": "あと2分で終わるから、全部一緒に説明するよ。"
  },
  {
    "start": 1099448,
    "end": 1107490,
    "text": "これは基本的に、位置エンコーディングに関する何かを事前に計算し、それを連続するレイヤーに与えるというものだ。"
  },
  {
    "start": 1107650,
    "end": 1115480,
    "text": "それを書き終えてから、この方法とこの方法を広げれば、すべてがクリアになる。"
  },
  {
    "start": 1116170,
    "end": 1123590,
    "text": "このレイヤーで、位置エンコーディングを計算するために必要なものを取得し、それを次のレイヤーに送るとする。"
  },
  {
    "start": 1126010,
    "end": 1127040,
    "text": "君だ。"
  },
  {
    "start": 1137810,
    "end": 1149220,
    "text": "すべてのエンコーダー・ブロックを連続して適用し、最後にここと同じように正規化を適用する。"
  },
  {
    "start": 1149910,
    "end": 1153062,
    "text": "これらのブロックを何度も何度も適用していく。"
  },
  {
    "start": 1153116,
    "end": 1163922,
    "text": "次に正規化を適用し、線形層を使って出力を計算し、最後に出力を返す。"
  },
  {
    "start": 1164066,
    "end": 1166746,
    "text": "これがモデルの骨格である。"
  },
  {
    "start": 1166848,
    "end": 1170010,
    "text": "入力を埋め込みに変換する。"
  },
  {
    "start": 1170350,
    "end": 1171978,
    "text": "この部分は後で説明する。"
  },
  {
    "start": 1172064,
    "end": 1178410,
    "text": "私たちは、これらのブロックに、位置エンコーディングに関する何かを持つ入力埋め込みを次々に与える。"
  },
  {
    "start": 1178480,
    "end": 1181518,
    "text": "最後の出力をRM規範に渡す。"
  },
  {
    "start": 1181604,
    "end": 1188270,
    "text": "RMsノルムの出力を線形層に与え、推論中にソフトマックスを適用する。"
  },
  {
    "start": 1188610,
    "end": 1191822,
    "text": "では、ポジションエンコーディングに集中しよう。"
  },
  {
    "start": 1191886,
    "end": 1195310,
    "text": "まず、オリジナルのトランスフォーマーでどのように機能したかをおさらいしておこう。"
  },
  {
    "start": 1195470,
    "end": 1197646,
    "text": "覚えているように、オリジナルのトランスフォーマーでは。"
  },
  {
    "start": 1197678,
    "end": 1200420,
    "text": "注目のトランスフォーマーは必要なものだけだ。"
  },
  {
    "start": 1200790,
    "end": 1202334,
    "text": "まず、この文章を取り上げる。"
  },
  {
    "start": 1202382,
    "end": 1207030,
    "text": "埋め込みベクトルに変換するので、サイズ512のベクトル。"
  },
  {
    "start": 1207180,
    "end": 1215798,
    "text": "次に、同じ大きさの別のベクトルを追加し、512が文中のそのトークンの位置を表す。"
  },
  {
    "start": 1215894,
    "end": 1222358,
    "text": "すべての文において、文の最初の位置にあるすべてのトークンはこのベクトルを受け取る。"
  },
  {
    "start": 1222534,
    "end": 1234654,
    "text": "文の2番目の位置にあるすべてのトークンにこのベクトルが追加され、文の3番目の位置にあるすべてのトークンにこのベクトルが追加される。"
  },
  {
    "start": 1234772,
    "end": 1242034,
    "text": "これらのベクトルは、適用される単語ではなく位置にのみ依存するため、あらかじめ計算されている。"
  },
  {
    "start": 1242152,
    "end": 1254390,
    "text": "このように、絶対位置エンコーディングと呼ばれるのは、文中の単語の位置にのみ厳密に依存するからであり、逆に回転位置エンコーディングでは、少し異なる。"
  },
  {
    "start": 1254460,
    "end": 1255478,
    "text": "見に行こう"
  },
  {
    "start": 1255564,
    "end": 1268540,
    "text": "まず、回転位置エンコーディングまたはエンベッディングだが、これらは注目度の計算の直前に計算され、q行列とk行列にのみ適用され、v行列には適用されない。"
  },
  {
    "start": 1269150,
    "end": 1270474,
    "text": "その理由を見てみよう。"
  },
  {
    "start": 1270672,
    "end": 1275370,
    "text": "まず理解しなければならないのは、絶対位置エンコーディングと相対位置エンコーディングの違いだ。"
  },
  {
    "start": 1275440,
    "end": 1288058,
    "text": "絶対位置エンコーディングは、先ほど見たように、一度に1つのトークンを扱い、それぞれのトークンは独自のエンベッディングを得る。一方、相対位置エンコーディングは、注目度の計算中に登場する。"
  },
  {
    "start": 1288234,
    "end": 1298658,
    "text": "注目度の計算は、キーの移調をDモデルの平方根で割ったものにクエリーをかけたものなので、基本的にはドット積を使って行われる。"
  },
  {
    "start": 1298744,
    "end": 1312070,
    "text": "相対位置エンコーディングでは、このドット積を変更し、ドット積に含まれる2つのトークン間の距離を示す新しいベクトルを導入する。"
  },
  {
    "start": 1312220,
    "end": 1317914,
    "text": "例えば、オリジナルのトランスフォーマーでは、このような式がある。"
  },
  {
    "start": 1317952,
    "end": 1326934,
    "text": "クエリにキーの転置率を乗じ、Dモデルの平方根で割ったものである。"
  },
  {
    "start": 1326982,
    "end": 1329066,
    "text": "これは単なる紹介だ。"
  },
  {
    "start": 1329258,
    "end": 1338810,
    "text": "相対位置エンコーディングでは、この2つのトークン間の距離を表す別のベクトルがあり、このように注意メカニズムを計算する。"
  },
  {
    "start": 1338980,
    "end": 1359522,
    "text": "llamaで使用されている回転位置埋め込みは、各トークンが独自の埋め込みを取得するため、絶対的な埋め込みと相対的な埋め込みの中間のようなものですが、注目メカニズムは2つのトークン間の相対的な距離を使って評価されるため、相対的なものです。"
  },
  {
    "start": 1359666,
    "end": 1374362,
    "text": "さて、回転位置埋め込みは、この会社、Jueとこの論文の著者の論文で紹介されている。"
  },
  {
    "start": 1374496,
    "end": 1379594,
    "text": "彼らは、このように機能するインナープロダクトを見つけたかったのだ。"
  },
  {
    "start": 1379632,
    "end": 1381694,
    "text": "まず、内積とは何か？"
  },
  {
    "start": 1381892,
    "end": 1384254,
    "text": "ドット積はよく知られている。"
  },
  {
    "start": 1384452,
    "end": 1388810,
    "text": "内積はドット積の一般化と考えることができる。"
  },
  {
    "start": 1388900,
    "end": 1395378,
    "text": "これは、ドット積とは何かを反映するいくつかの特性を持つ演算である。"
  },
  {
    "start": 1395544,
    "end": 1409798,
    "text": "この論文の著者は、2つのベクトル、クエリとキーの間の内積を求めようとした。"
  },
  {
    "start": 1409884,
    "end": 1414322,
    "text": "この内積は、関係する2つのトークンの埋め込みにのみ依存する。"
  },
  {
    "start": 1414386,
    "end": 1420038,
    "text": "xmとxn、そしてこれら2つのトークンの相対距離。"
  },
  {
    "start": 1420134,
    "end": 1422346,
    "text": "両者の距離"
  },
  {
    "start": 1422448,
    "end": 1437774,
    "text": "例えば、最初のトークンが2の位置にあり、2番目のトークンが5の位置にある場合、mは2、nは5に等しく、2つの間の距離は順序に従って3またはマイナス3となる。"
  },
  {
    "start": 1437972,
    "end": 1449202,
    "text": "彼らは、最初のトークンの埋め込み、2番目のトークンの埋め込み、そしてそれらの間の相対距離にのみ依存する、この性質を持つドット積を見つけたいと考えた。"
  },
  {
    "start": 1449336,
    "end": 1456370,
    "text": "そして、この関数gをこのように作れば、その目的は達成されると考えたのだ。"
  },
  {
    "start": 1456530,
    "end": 1463010,
    "text": "つまり、最初のトークンを取り出し、例えばクエリにw行列を掛ける。"
  },
  {
    "start": 1463090,
    "end": 1465158,
    "text": "これは実はバニラ・トランスでも行われている。"
  },
  {
    "start": 1465174,
    "end": 1467786,
    "text": "よし、ここにw行列がないとしよう。"
  },
  {
    "start": 1467968,
    "end": 1482320,
    "text": "この形で複素数に変換し、キーベクトルを取り、この形で複素数に変換し、この形で内積を定義する。"
  },
  {
    "start": 1483170,
    "end": 1489386,
    "text": "この内積は基本的に2つのトークン間の距離のみに依存する。"
  },
  {
    "start": 1489498,
    "end": 1499922,
    "text": "彼らは、内積であるドット積に基づく注意メカニズムがこのように振る舞うような符号化メカニズムを見つけたいと考えた。"
  },
  {
    "start": 1499976,
    "end": 1504360,
    "text": "それはベクトルの埋め込みとその間の距離にのみ依存する。"
  },
  {
    "start": 1504810,
    "end": 1520086,
    "text": "例えば、この定式化を2次元のベクトルに適用すると、2次元だけの埋め込みを考えた場合、このような形になります。"
  },
  {
    "start": 1520118,
    "end": 1528670,
    "text": "各複素数は、LRの公式のおかげで、正弦＋余弦＋正弦と書くことができる。"
  },
  {
    "start": 1529570,
    "end": 1535626,
    "text": "この行列は回転行列を連想させる。"
  },
  {
    "start": 1535738,
    "end": 1537070,
    "text": "例を挙げよう。"
  },
  {
    "start": 1537220,
    "end": 1551086,
    "text": "元のベクトルがここにあるとすると、このベクトルvゼロにこの行列を掛けると、結果のベクトルは角度θだけ回転する。"
  },
  {
    "start": 1551198,
    "end": 1558210,
    "text": "回転位置埋め込みと呼ばれるのは、この行列がベクトルの回転を表しているからである。"
  },
  {
    "start": 1558370,
    "end": 1572806,
    "text": "ロータリー・ポジショナル・エンベッディングがどのように機能するかを視覚化したい場合、ベクター空間にマッピングし、各単語を基本角度の倍数になる角度に回転させると考えなければならない。"
  },
  {
    "start": 1572838,
    "end": 1590530,
    "text": "つまり、同じような位置にある2つのトークンは同じような傾きを持ち、異なる位置にある2つのトークンは異なる傾きを持つことになる。"
  },
  {
    "start": 1591110,
    "end": 1594094,
    "text": "これがロータリー・ポジション・エンベッディングの背景にある考え方だ。"
  },
  {
    "start": 1594222,
    "end": 1600046,
    "text": "パイトーチのコードでは、実際にどのように計算するのですか？"
  },
  {
    "start": 1600158,
    "end": 1604418,
    "text": "それを計算するには、次のような行列を作る必要がある。"
  },
  {
    "start": 1604584,
    "end": 1609042,
    "text": "ご覧のように、この行列は実際にはゼロでいっぱいである。"
  },
  {
    "start": 1609186,
    "end": 1619530,
    "text": "そのため、この方法で埋め込みを計算する場合、行列の乗算を行うと、これらの項目のほとんどがゼロであるため、無駄な演算をたくさん行うことになる。"
  },
  {
    "start": 1619680,
    "end": 1624790,
    "text": "この論文の著者たちは、より計算効率の高い別の形式を提案した。"
  },
  {
    "start": 1624950,
    "end": 1632042,
    "text": "この形式は基本的に、位置エンコーディングを適用したいベクトルの埋め込みを取るというものだ。"
  },
  {
    "start": 1632106,
    "end": 1634474,
    "text": "例えば、これはベクターだ。"
  },
  {
    "start": 1634522,
    "end": 1638458,
    "text": "最初の次元、2番目の次元、3番目の次元、そして最後の次元である。"
  },
  {
    "start": 1638554,
    "end": 1644740,
    "text": "これが例えばバニラ・トランスであれば、xdとなり、512となるはずだ。"
  },
  {
    "start": 1645190,
    "end": 1657122,
    "text": "要素ごとにこの行列と、実際にはこのベクトルに基づくが位置と符号を変えた別のベクトルを掛け合わせる。"
  },
  {
    "start": 1657186,
    "end": 1661730,
    "text": "これは、実際には最初の位置に、符号を変えた2番目の次元がある。"
  },
  {
    "start": 1661810,
    "end": 1664642,
    "text": "番目のポジションには、実際に1つ目の次元がある。"
  },
  {
    "start": 1664706,
    "end": 1668646,
    "text": "3つ目のポジションには4つ目の次元があるが、符号の変化などがある。"
  },
  {
    "start": 1668678,
    "end": 1676374,
    "text": "それは単語の埋め込みにのみ依存し、符号や位置の変化によって変化する。"
  },
  {
    "start": 1676502,
    "end": 1682590,
    "text": "次に、この要素に別の行列を掛け合わせる。"
  },
  {
    "start": 1683410,
    "end": 1688446,
    "text": "そうすると、これが今話しているトークンのエンコーディングになる。"
  },
  {
    "start": 1688628,
    "end": 1703190,
    "text": "というのも、この行列は適用するトークンに依存しないし、この行列は適用するトークンに依存しないからだ。"
  },
  {
    "start": 1703340,
    "end": 1707506,
    "text": "シータとは何ですか？"
  },
  {
    "start": 1707618,
    "end": 1711800,
    "text": "シータはこのように定義された一連の数値である。"
  },
  {
    "start": 1713850,
    "end": 1719638,
    "text": "まず、これとこれを事前に計算するコードを作ろう。"
  },
  {
    "start": 1719804,
    "end": 1720950,
    "text": "そうしよう。"
  },
  {
    "start": 1721100,
    "end": 1725800,
    "text": "最初にコードを書き、後でそれがどのように機能するかをお見せします。"
  },
  {
    "start": 1726290,
    "end": 1755126,
    "text": "それは、このデータ・パラメーター10,000は論文に由来する。"
  },
  {
    "start": 1755308,
    "end": 1757560,
    "text": "ここに10,000と書いてある。"
  },
  {
    "start": 1762010,
    "end": 1784160,
    "text": "論文では、この回転位置エンコーディングは、od次元を持つエンベッディングには適用できないと書かれているので、513ではだめで、512か514か他の偶数でなければならない。"
  },
  {
    "start": 1789990,
    "end": 1794500,
    "text": "これは論文に書かれている通りである。"
  },
  {
    "start": 1802950,
    "end": 1827918,
    "text": "ここで、シータパラメータを構築する。シータパラメータはシーケンスであり、このデータの形状はヘッドの次元を2で割ったものになる。"
  },
  {
    "start": 1828004,
    "end": 1832782,
    "text": "エンベッディングの直後ではなく、マルチヘッドに分割した後だ。"
  },
  {
    "start": 1832916,
    "end": 1842162,
    "text": "各トークン、各ヘッドのトークンは、各ヘッドの次元の大きさをチェックし、それを2で割る。"
  },
  {
    "start": 1842216,
    "end": 1843794,
    "text": "なぜビットを2で割るのか？"
  },
  {
    "start": 1843832,
    "end": 1846562,
    "text": "論文でも2で割っているからだ。"
  },
  {
    "start": 1846616,
    "end": 1861894,
    "text": "dを2で割ると、こういうことになる。"
  },
  {
    "start": 1861932,
    "end": 1883680,
    "text": "計算式は、Iのシータは10,000のマイナス2乗に等しく、Iのマイナス1乗を4次元で割ったもので、Iは1、2など、2次元で割ったものまで等しい。"
  },
  {
    "start": 1884290,
    "end": 1887278,
    "text": "今はこの部分を計算しているところだ。"
  },
  {
    "start": 1887364,
    "end": 1905110,
    "text": "この部分は級数なので、Iは1から始まるが、我々はゼロから始めるので、Iマイナス1をする必要はなく、θはθの上の1に等しい。"
  },
  {
    "start": 1905610,
    "end": 1912390,
    "text": "10,000をその次元で割ったシータ分子のべき乗。"
  },
  {
    "start": 1916110,
    "end": 1918182,
    "text": "なぜθよりoneなのか？"
  },
  {
    "start": 1918246,
    "end": 1921626,
    "text": "まあ、これはマイナス2のべき乗だからね。"
  },
  {
    "start": 1921728,
    "end": 1930110,
    "text": "負の数の1乗は、正の指数の1乗に相当する。"
  },
  {
    "start": 1931650,
    "end": 1940434,
    "text": "この場合、形状、ヘッド寸法を2で割った行列になる。"
  },
  {
    "start": 1940552,
    "end": 1945860,
    "text": "形状はヘッド寸法を2で割ったもの。"
  },
  {
    "start": 1946390,
    "end": 1948126,
    "text": "次にポジションを構築する。"
  },
  {
    "start": 1948158,
    "end": 1949438,
    "text": "ポジションは？"
  },
  {
    "start": 1949534,
    "end": 1953070,
    "text": "この2つの行列を作りたいので、これらはθに依存する。"
  },
  {
    "start": 1953150,
    "end": 1959382,
    "text": "シータ1からシータ次元までを2で割ったシータの級数で、すでに持っている。"
  },
  {
    "start": 1959516,
    "end": 1965226,
    "text": "トークンの位置はいくつでもあり得るからだ。"
  },
  {
    "start": 1965408,
    "end": 1977546,
    "text": "プロンプトも長いかもしれないので、この関数への入力は、基本的に、余裕のある最大シーケンス長に2を掛けたものを与える。"
  },
  {
    "start": 1977648,
    "end": 1985806,
    "text": "では、モデルが見る可能性のあるすべての位置について、可能性のあるすべてのθとmをあらかじめ計算しておこう。"
  },
  {
    "start": 1985908,
    "end": 1990370,
    "text": "すべての可能な位置は、このパラメータ配列長で与えられる。"
  },
  {
    "start": 1991350,
    "end": 1994290,
    "text": "これでポジションを構築した。"
  },
  {
    "start": 2000470,
    "end": 2005400,
    "text": "形状はシーケンスの長さであり、mである。"
  },
  {
    "start": 2012090,
    "end": 2021174,
    "text": "ここで、mに一連のテータをすべて掛け合わせる必要があるが、それぞれのmにはすべてのテータを掛け合わせる。"
  },
  {
    "start": 2021222,
    "end": 2029370,
    "text": "例えば、mが1に等しい場合、m one theta one m one theta two m one theta divided d divided by twoが必要である。"
  },
  {
    "start": 2029440,
    "end": 2032830,
    "text": "そうすると、m2シータ1 m2シータ2 m2シータ3が必要になる。"
  },
  {
    "start": 2032900,
    "end": 2035760,
    "text": "そのためには外積を使う。"
  },
  {
    "start": 2036210,
    "end": 2048130,
    "text": "外積とは、後で説明するが、基本的には、1つ目のベクトルのすべての要素と2つ目のベクトルのすべての要素、つまり可能なすべての組み合わせを掛け合わせることを意味する。"
  },
  {
    "start": 2058170,
    "end": 2070090,
    "text": "例えば、ここでは周波数はトーチの外積mとθに等しい。"
  },
  {
    "start": 2071310,
    "end": 2078438,
    "text": "つまり、位置とシータを掛け合わせたm内の外積をやっているわけだ。"
  },
  {
    "start": 2078534,
    "end": 2085274,
    "text": "これは基本的に、最初のベクトルの最初の要素を取り出し、2番目のベクトルのすべての要素と乗算する。"
  },
  {
    "start": 2085402,
    "end": 2091310,
    "text": "次に、最初のベクトルの2番目の要素を取り出し、2番目のベクトルのすべての要素と掛け合わせる。"
  },
  {
    "start": 2091470,
    "end": 2099490,
    "text": "形状から始めるとすると、mの形状はシーケンスの長さである。"
  },
  {
    "start": 2101290,
    "end": 2108566,
    "text": "例えば、ヘッド寸法を2で割ったアウター・プロダクトとしよう。"
  },
  {
    "start": 2108668,
    "end": 2116038,
    "text": "この結果、配列の長さをヘッドの次元で割って2で割ったテンソルになる。"
  },
  {
    "start": 2116124,
    "end": 2118286,
    "text": "各ポジションについて、すべてのシータを得ることになる。"
  },
  {
    "start": 2118338,
    "end": 2120502,
    "text": "そして2つ目のポジションでは、すべてのシータが揃うことになる。"
  },
  {
    "start": 2120566,
    "end": 2122790,
    "text": "3番目のポジションには、すべてのシータがいる。"
  },
  {
    "start": 2122950,
    "end": 2124380,
    "text": "そうだ。"
  },
  {
    "start": 2126290,
    "end": 2154950,
    "text": "さて、これらの数字を複素数の形に書きたいのだが、mにθを掛けてrを1にした理由を次のように説明しよう。"
  },
  {
    "start": 2156730,
    "end": 2158470,
    "text": "を計算する。"
  },
  {
    "start": 2172030,
    "end": 2176540,
    "text": "形も書いてから、どのように機能するかを説明しよう。"
  },
  {
    "start": 2180450,
    "end": 2183920,
    "text": "ここにもある。"
  },
  {
    "start": 2187410,
    "end": 2191466,
    "text": "よし、いくつか数式を書いてみよう。"
  },
  {
    "start": 2191658,
    "end": 2194382,
    "text": "また、すべての証明を説明することもできなかった。"
  },
  {
    "start": 2194446,
    "end": 2199858,
    "text": "これからの数分間は、その背後にある数学をすべて説明することになるので、少し退屈になることは分かっている。"
  },
  {
    "start": 2200024,
    "end": 2207494,
    "text": "もちろん、僕と同じように、コードだけを見て、オーケー、こうやるんだ、と言うのが好きな人ばかりではないと思う。"
  },
  {
    "start": 2207532,
    "end": 2212006,
    "text": "いや、私たちが行うすべての作戦の背景にある動機を実際に示したいんだ。"
  },
  {
    "start": 2212028,
    "end": 2217142,
    "text": "それは、あなたがメタ・リポジトリからコードを読むだけでなく、このビデオを見ている理由のひとつだと思う。"
  },
  {
    "start": 2217286,
    "end": 2221530,
    "text": "ちょっと計算してみよう。"
  },
  {
    "start": 2222830,
    "end": 2227230,
    "text": "まず復習しなければならないのは、複素数の仕組みである。"
  },
  {
    "start": 2227380,
    "end": 2239646,
    "text": "さて、複素数とはa＋Iにbを掛けた形の数で、aは実数部、bは虚数部と呼ばれる。"
  },
  {
    "start": 2239828,
    "end": 2245698,
    "text": "Iは、Iの2乗がマイナス1に等しい数である。"
  },
  {
    "start": 2245784,
    "end": 2253746,
    "text": "複素数は、負の数の平方根を含むすべての数を表すために導入された。"
  },
  {
    "start": 2253928,
    "end": 2257442,
    "text": "学校で習ったように、あらゆる数の平方根は計算できない。"
  },
  {
    "start": 2257506,
    "end": 2263430,
    "text": "だから、マイナス1の平方根である負の数である定数Iを導入したのである。"
  },
  {
    "start": 2263580,
    "end": 2271110,
    "text": "そのため、負の数の平方根を表すことができ、ベクトル計算にも役立つ。"
  },
  {
    "start": 2271190,
    "end": 2277434,
    "text": "オイラーの公式があるからだ。"
  },
  {
    "start": 2277482,
    "end": 2291522,
    "text": "オイラーの公式は、eのI乗×xは、xの余弦＋I乗×xの正弦に等しいとしている。"
  },
  {
    "start": 2291656,
    "end": 2301954,
    "text": "この関数によって、指数形式の複素数を2つの三角関数、余弦と正弦の和で表すことができる。"
  },
  {
    "start": 2302082,
    "end": 2316006,
    "text": "というのも、我々の目的はこれらの行列、すなわちmθの余弦とmθの正弦を計算することだからである。"
  },
  {
    "start": 2316198,
    "end": 2320282,
    "text": "最初にやったのは、すべてのシータを計算することだった。"
  },
  {
    "start": 2320416,
    "end": 2322246,
    "text": "そして、すべてのポジションを計算した。"
  },
  {
    "start": 2322278,
    "end": 2327378,
    "text": "そして、ポジションとシータの可能な組み合わせをすべて計算した。"
  },
  {
    "start": 2327494,
    "end": 2334190,
    "text": "私たちが行ったのは、シータを表すベクトルを計算することだ。"
  },
  {
    "start": 2334930,
    "end": 2342242,
    "text": "シータ1 シータ2 シータdを2で割ったものまで。"
  },
  {
    "start": 2342376,
    "end": 2345330,
    "text": "そして、可能なすべてのMを計算した。"
  },
  {
    "start": 2346150,
    "end": 2349378,
    "text": "Mは1でも2でも何でもいい。"
  },
  {
    "start": 2349464,
    "end": 2351460,
    "text": "シークエンスの長さだ。"
  },
  {
    "start": 2352170,
    "end": 2359522,
    "text": "そして、可能なすべてのθについて、それぞれの積を計算した。"
  },
  {
    "start": 2359586,
    "end": 2395140,
    "text": "たとえば、m 1 θ 1 m 1 θ 2 m 1 θ 3 までを m 1 θ d を 2 で割った新しい行列を作り、m 2 θ 1 m 2 θ 2 m 2 θ 3 までを m 2 θ d を 2 で割った新しい行列を作った。"
  },
  {
    "start": 2396710,
    "end": 2399234,
    "text": "これらの数字はまだ複素数ではない。"
  },
  {
    "start": 2399272,
    "end": 2401842,
    "text": "シータが実数なので、これらは単なる実数である。"
  },
  {
    "start": 2401896,
    "end": 2404070,
    "text": "Mは実数だが、複素数ではない。"
  },
  {
    "start": 2404140,
    "end": 2406134,
    "text": "それから複素数に変換する。"
  },
  {
    "start": 2406252,
    "end": 2414566,
    "text": "この最後の操作で何をするかというと、それぞれの数値を極形に変換するのである。"
  },
  {
    "start": 2414668,
    "end": 2429818,
    "text": "極座標形式の数とは、rにeをかけてθのI乗と書くことができる数であり、θのr余弦にθのI正弦を加えたものと書くことができる。"
  },
  {
    "start": 2429994,
    "end": 2430430,
    "text": "なぜですか？"
  },
  {
    "start": 2430500,
    "end": 2439150,
    "text": "なぜなら、それはグラフィカルな、例えばグラフィカルな平面xyで表現できるからだ。"
  },
  {
    "start": 2440050,
    "end": 2450466,
    "text": "ご存知のように、複素数は2次元平面xyに表すことができ、実数部がx、虚数部がyになる。"
  },
  {
    "start": 2450648,
    "end": 2471078,
    "text": "というのも、このベクトルの実数部への投影はr cos thetaであり、y軸への投影はθの正弦だからである。"
  },
  {
    "start": 2471254,
    "end": 2474042,
    "text": "を忘れていた。"
  },
  {
    "start": 2474096,
    "end": 2478258,
    "text": "そうだ、ここにθのsinのrを忘れていた。"
  },
  {
    "start": 2478374,
    "end": 2481870,
    "text": "これは複素数を表す別の方法である。"
  },
  {
    "start": 2481940,
    "end": 2488858,
    "text": "私たちがやっているのは、この行列を計算し、すべての数値を複素数に変換することだ。"
  },
  {
    "start": 2488964,
    "end": 2494500,
    "text": "この行列は、rが1に等しい別の行列に変換される。"
  },
  {
    "start": 2495110,
    "end": 2511926,
    "text": "この数値は、例えばこの項目は、m1θ1の余弦＋m1θ1の正弦となる。"
  },
  {
    "start": 2512108,
    "end": 2515660,
    "text": "この数字が別の数字になる。"
  },
  {
    "start": 2516590,
    "end": 2518380,
    "text": "これは1つの数字に過ぎない。"
  },
  {
    "start": 2519230,
    "end": 2532122,
    "text": "これは、m1θ2の余弦にm1θ2の正弦を加えた別の複素数になる。"
  },
  {
    "start": 2532186,
    "end": 2541362,
    "text": "すべての数、つまり総数を増やすわけではないので、テンソルの形も変わらず、より複雑な数になるだけだ。"
  },
  {
    "start": 2541416,
    "end": 2547458,
    "text": "mシータ1ではなく、mシータ1の余弦＋I mシータ1になる。"
  },
  {
    "start": 2547544,
    "end": 2549362,
    "text": "なぜこのフォームが必要なのか？"
  },
  {
    "start": 2549416,
    "end": 2556166,
    "text": "なぜなら、サインとコサインが必要だからである。"
  },
  {
    "start": 2556348,
    "end": 2564098,
    "text": "さて、ポイントは、これらの位置エンコーディングをベクトルに適用したいので、ベクトルが与えられたと想像することである。"
  },
  {
    "start": 2564274,
    "end": 2565718,
    "text": "どのように適用するのか？"
  },
  {
    "start": 2565804,
    "end": 2572262,
    "text": "なぜなら、ベクトルはx1から最後の次元までの次元のリストとして与えられるからである。"
  },
  {
    "start": 2572326,
    "end": 2576682,
    "text": "オリジナルのトランスフォーマーと同じように、512サイズのベクトルがある。"
  },
  {
    "start": 2576736,
    "end": 2580638,
    "text": "この場合、各ヘッドの寸法なので、もっと小さくなる。"
  },
  {
    "start": 2580724,
    "end": 2588078,
    "text": "覚えているように、各ヘッドは埋め込みベクトルの全次元を見ているわけではなく、その一部を見ているのだ。"
  },
  {
    "start": 2588244,
    "end": 2590622,
    "text": "頭だけだと想像してくれ。"
  },
  {
    "start": 2590676,
    "end": 2593214,
    "text": "頭だけなら、フル次元で見る。"
  },
  {
    "start": 2593262,
    "end": 2596002,
    "text": "今のところ、マルチヘッドは考えていない。"
  },
  {
    "start": 2596056,
    "end": 2598402,
    "text": "仮にヘッドが1つしかないとする。"
  },
  {
    "start": 2598456,
    "end": 2601550,
    "text": "トークンを完全な寸法で持っているとしよう。"
  },
  {
    "start": 2601630,
    "end": 2610134,
    "text": "llamaの場合は4096次元、バニラトランスの場合は512次元ですが、どのように適用するのですか？"
  },
  {
    "start": 2610332,
    "end": 2611762,
    "text": "少し計算してみよう。"
  },
  {
    "start": 2611906,
    "end": 2614174,
    "text": "もう少し計算してみよう。"
  },
  {
    "start": 2614322,
    "end": 2627260,
    "text": "なぜなら、我々は計算をしたいし、狂わないようにしたいからだ。"
  },
  {
    "start": 2627790,
    "end": 2631466,
    "text": "そうでなければ、4096を証明するのは少し難しい。"
  },
  {
    "start": 2631658,
    "end": 2640338,
    "text": "このベクトルに対して、この形にたどり着くまでの操作のリストを作りたい。"
  },
  {
    "start": 2640504,
    "end": 2641554,
    "text": "始めよう。"
  },
  {
    "start": 2641592,
    "end": 2651300,
    "text": "埋め込みベクトルが、x1、x2、x3、x4の4次元でできているとしよう。"
  },
  {
    "start": 2653690,
    "end": 2661862,
    "text": "さて、最初にすることは、いくつかの変換を行い、後でそれをコードに変換することだ。"
  },
  {
    "start": 2661916,
    "end": 2664490,
    "text": "とりあえず、僕がやっている変身についてきてくれ。"
  },
  {
    "start": 2664640,
    "end": 2666860,
    "text": "これが変身その1だ。"
  },
  {
    "start": 2667790,
    "end": 2676022,
    "text": "連続するトークン、連続する次元を別の次元にグループ化したい。"
  },
  {
    "start": 2676086,
    "end": 2687390,
    "text": "x 1とx 2はこのテンソルの別の次元になり、x 3とx 4は、おっと、非常にひどい書き方になってしまった。"
  },
  {
    "start": 2689270,
    "end": 2692606,
    "text": "Xの4つは、このテンソルの別の次元となる。"
  },
  {
    "start": 2692638,
    "end": 2703320,
    "text": "アイテムの総数はまだ4つだが、もう1つ次元を追加し、これは4つを1つずつ大きくした。"
  },
  {
    "start": 2704330,
    "end": 2708134,
    "text": "こちらは2本×2本×1本。"
  },
  {
    "start": 2708172,
    "end": 2711990,
    "text": "私はそれを複数のテンソルに分割した。"
  },
  {
    "start": 2713530,
    "end": 2728638,
    "text": "さて、次にすることは、この部分の最初の数字を複素数の実数部、この数字を複素数の虚数部とみなし、2番目のベクトルについても同様とする。"
  },
  {
    "start": 2728804,
    "end": 2745940,
    "text": "x1にIを足したものをx2とし、x3にIを足したものをx2とする。"
  },
  {
    "start": 2747030,
    "end": 2752920,
    "text": "このベクトルは、2つの数が1つの複素数になったため、項目が少なくなった。"
  },
  {
    "start": 2754090,
    "end": 2761766,
    "text": "次に、この要素ごとに、前に計算したベクトルと掛け合わせる。"
  },
  {
    "start": 2761948,
    "end": 2773226,
    "text": "覚えているように、この1つのm 1の余弦、m 1のθ1プラスI、m 1のθ1余弦、θ2プラスIを事前に計算した。"
  },
  {
    "start": 2773408,
    "end": 2781518,
    "text": "というのも、この位置、このトークン、仮に彼の位置がmだとすると、mも必要だからだ。"
  },
  {
    "start": 2781604,
    "end": 2786080,
    "text": "このトークン、彼のポジションはM1だとする。"
  },
  {
    "start": 2786450,
    "end": 2794740,
    "text": "この行のmをすべて取ると、これが新しい行列になる。"
  },
  {
    "start": 2795830,
    "end": 2797678,
    "text": "私たちには4つの次元しかない。"
  },
  {
    "start": 2797774,
    "end": 2805174,
    "text": "4次元ということは、シータ1とシータ2があるということだ。"
  },
  {
    "start": 2805212,
    "end": 2843940,
    "text": "要素的には、m1の余弦θ1とm1の正弦θ1のIを足したもの、そしてm1の余弦θ2とm1の正弦θ2のIを足したものがある。"
  },
  {
    "start": 2846660,
    "end": 2853440,
    "text": "これで、この行列の最初の項目とこの行列の最初の項目との間の要素ごとの積ができた。"
  },
  {
    "start": 2853520,
    "end": 2855104,
    "text": "実際には、2つのベクトルがある。"
  },
  {
    "start": 2855232,
    "end": 2862932,
    "text": "この複素数とこの複素数の積がある。"
  },
  {
    "start": 2863066,
    "end": 2870980,
    "text": "あまり長い式は書きたくないので、2つの複素数の積を計算する方法を見てみよう。"
  },
  {
    "start": 2871060,
    "end": 2874392,
    "text": "これはF1と呼ぶことにしよう。"
  },
  {
    "start": 2874446,
    "end": 2883150,
    "text": "f 1はm 1の余弦、θ 1、f 2はm 1の正弦、θ 1である。"
  },
  {
    "start": 2884240,
    "end": 2890850,
    "text": "同じ理由で、これをF3とF4と呼ぶことにする。"
  },
  {
    "start": 2891780,
    "end": 2899840,
    "text": "では、このベクトルの最初の項目と、このベクトルの最初の項目の積を計算してみよう。"
  },
  {
    "start": 2900180,
    "end": 2939100,
    "text": "x one + ix two に f one + if two を掛けると、これは x one f one + I x one f two + I x two f one に等しい。"
  },
  {
    "start": 2940750,
    "end": 2947290,
    "text": "すると、I×2にI×2を掛けた積が、Iの2乗になる。"
  },
  {
    "start": 2947370,
    "end": 2949546,
    "text": "X two f two I squared."
  },
  {
    "start": 2949578,
    "end": 2956720,
    "text": "マイナス1に等しいことが分かっているので、マイナス×2 f 2となる。"
  },
  {
    "start": 2957350,
    "end": 2960226,
    "text": "これは実数部として書くことができる。"
  },
  {
    "start": 2960328,
    "end": 2983980,
    "text": "Iを持たないすべての項 x one f one マイナス x two f two プラス I x one f two x one f two プラス x two f oneを掛ける。"
  },
  {
    "start": 2985870,
    "end": 2990250,
    "text": "さて、これが2つの複素数の積を計算する方法だ。"
  },
  {
    "start": 2990320,
    "end": 2991178,
    "text": "そうしよう。"
  },
  {
    "start": 2991264,
    "end": 3023160,
    "text": "この要素ごとの掛け算から得られる行列の最初の数字は、x one f one x one f one マイナス x two f two x two f two プラス I の x one f two プラス x two f one となる。"
  },
  {
    "start": 3025610,
    "end": 3031734,
    "text": "つ目の要素は、1つ目の要素と同じような構造を持っているので、この掛け算をする必要はない。"
  },
  {
    "start": 3031772,
    "end": 3035818,
    "text": "x1をx3に、x2をx4に変えるだけだ。"
  },
  {
    "start": 3035984,
    "end": 3043242,
    "text": "これはx4であり、f1とf3、f2とf4である。"
  },
  {
    "start": 3043296,
    "end": 3071360,
    "text": "その結果、行列は x three f three マイナス x four f four プラス I x three f four プラス x four f three となる。"
  },
  {
    "start": 3073990,
    "end": 3076082,
    "text": "これは分割して戻すことができる。"
  },
  {
    "start": 3076136,
    "end": 3080082,
    "text": "この複素数は、実部と虚部に分けることができる。"
  },
  {
    "start": 3080136,
    "end": 3084100,
    "text": "これを変身3号と呼ぶことにする。"
  },
  {
    "start": 3086710,
    "end": 3090450,
    "text": "これを2次元のテンソルに分割することができる。"
  },
  {
    "start": 3090530,
    "end": 3093238,
    "text": "ひとつは実質的な部分、もうひとつは複雑な部分だ。"
  },
  {
    "start": 3093324,
    "end": 3103900,
    "text": "ここで、x1 f1マイナスx2 f2である。"
  },
  {
    "start": 3104670,
    "end": 3114560,
    "text": "つまり、x1 f 2 + x2 f 1となる。"
  },
  {
    "start": 3116290,
    "end": 3117994,
    "text": "これが最初のテンソルだ。"
  },
  {
    "start": 3118042,
    "end": 3139080,
    "text": "番目のテンソルは、x3 f 3からx4 f 4を引いたものになり、2番目のテンソルは、x3 f 4にx4 f 3を足したものになる。"
  },
  {
    "start": 3140890,
    "end": 3146840,
    "text": "字が汚くて本当に申し訳ないんだけど、僕のタッチパッドはあまり良くないんだ。"
  },
  {
    "start": 3147690,
    "end": 3152470,
    "text": "次に、これらの値をすべて平坦化する別の変換を行う。"
  },
  {
    "start": 3152550,
    "end": 3155702,
    "text": "これが最初の項目になる。"
  },
  {
    "start": 3155766,
    "end": 3157526,
    "text": "これが2つ目、3つ目、そして4つ目だ。"
  },
  {
    "start": 3157558,
    "end": 3160030,
    "text": "私たちはこの次元、内なる次元を取り除く。"
  },
  {
    "start": 3160690,
    "end": 3171690,
    "text": "この行列を平らにすると、x one f one - x two f two となる。"
  },
  {
    "start": 3171860,
    "end": 3189720,
    "text": "2つ目の項目は、x one f two plus x two、x one f two plus two x two f oneとなる。"
  },
  {
    "start": 3191130,
    "end": 3199900,
    "text": "そうすると、x3、f3からx4を引いてf4となる。"
  },
  {
    "start": 3201470,
    "end": 3212640,
    "text": "そうすると、x 3 f 4 ＋ x 4 f 3 となる。"
  },
  {
    "start": 3213890,
    "end": 3217854,
    "text": "出来上がったマトリックスを論文にあるものと比較してみよう。"
  },
  {
    "start": 3217972,
    "end": 3220880,
    "text": "これと比較してみよう。"
  },
  {
    "start": 3224590,
    "end": 3226330,
    "text": "少しズームしてみよう。"
  },
  {
    "start": 3226400,
    "end": 3227020,
    "text": "オーケー。"
  },
  {
    "start": 3228270,
    "end": 3233098,
    "text": "出来上がったマトリックスは、論文にあるものとまったく同じである。"
  },
  {
    "start": 3233184,
    "end": 3240126,
    "text": "x 1にf 1を掛けたもので、これはコサインである。"
  },
  {
    "start": 3240228,
    "end": 3251282,
    "text": "x 1にf 1を掛けたものにマイナスx 2を足したもので、マイナスx 2にf 2を掛けたものが正弦となる。"
  },
  {
    "start": 3251336,
    "end": 3253186,
    "text": "m 1の正弦、θ 1。"
  },
  {
    "start": 3253288,
    "end": 3256002,
    "text": "ここでは、一般的なmに対応するため、m1ではない。"
  },
  {
    "start": 3256056,
    "end": 3258580,
    "text": "mを1としている。"
  },
  {
    "start": 3259510,
    "end": 3261698,
    "text": "第二の次元も正しい。"
  },
  {
    "start": 3261784,
    "end": 3269462,
    "text": "x one f twoだから、x one、ちょっと待って、x oneはここ、ここは合計だから順番は関係ない。"
  },
  {
    "start": 3269596,
    "end": 3276140,
    "text": "x one f twoだから、x oneに正弦をかけると、x two f one x two f oneとなる。"
  },
  {
    "start": 3276910,
    "end": 3284810,
    "text": "第3の次元はx 3 f 3だ。"
  },
  {
    "start": 3284880,
    "end": 3291354,
    "text": "x 3に余弦からx 4の正弦を引いたものを掛ける。"
  },
  {
    "start": 3291482,
    "end": 3293550,
    "text": "すると、x three f fourとなる。"
  },
  {
    "start": 3293620,
    "end": 3304210,
    "text": "x three f four f fourはθ2の正弦であり、x four f three, x four f three, f threeはθ2の余弦である。"
  },
  {
    "start": 3304280,
    "end": 3310580,
    "text": "また、この場合、内部で合計を持っているので、順番は関係ない。"
  },
  {
    "start": 3311270,
    "end": 3320562,
    "text": "見ての通り、我々は4次元のベクトルから始めたが、n次元のベクトルもあり得る。"
  },
  {
    "start": 3320626,
    "end": 3324198,
    "text": "そして、ここで事前に計算した行列と掛け合わせた。"
  },
  {
    "start": 3324364,
    "end": 3329846,
    "text": "その後、別の変換を行ったが、最終結果はこの操作とまったく同じだった。"
  },
  {
    "start": 3329958,
    "end": 3338602,
    "text": "埋め込みベクトルをこのベクトルに適用するために必要なものだからだ。"
  },
  {
    "start": 3338656,
    "end": 3344746,
    "text": "このトークンに、どのようにエンベッディングを適用するか、この一連の変換を通して回転位置エンベッディングを適用するか。"
  },
  {
    "start": 3344938,
    "end": 3347742,
    "text": "私はコードを書いても何も言わなかったかもしれない。"
  },
  {
    "start": 3347796,
    "end": 3356594,
    "text": "私がやっていることが実際に論文に記載されていることがわかるように、私は自分のやっていることに証拠を示したい。"
  },
  {
    "start": 3356632,
    "end": 3359218,
    "text": "私たちは論文に従って実際にやっている。"
  },
  {
    "start": 3359384,
    "end": 3362722,
    "text": "論文には、本当に役に立つ視覚化もある。"
  },
  {
    "start": 3362786,
    "end": 3378262,
    "text": "例えば、埋め込みベクトルを変換して、半分の次元を持つ新しいテンソルに分割する。"
  },
  {
    "start": 3378326,
    "end": 3382700,
    "text": "x1とx2、x3とx4の2つの連続した次元。"
  },
  {
    "start": 3383470,
    "end": 3387210,
    "text": "そして掛け算をし、複素数に変換する。"
  },
  {
    "start": 3387280,
    "end": 3391618,
    "text": "事前に計算したmシータを掛けた。"
  },
  {
    "start": 3391814,
    "end": 3400180,
    "text": "なぜこのようなことをするのかについては、論文、特にこちらの図に視覚化されている。"
  },
  {
    "start": 3401750,
    "end": 3411410,
    "text": "ここでは、n個の次元を持つ単語があれば、当然d個の次元が必要だと言われている。"
  },
  {
    "start": 3411490,
    "end": 3415362,
    "text": "もちろん、シータはシータの半分になる。"
  },
  {
    "start": 3415506,
    "end": 3431302,
    "text": "シータ1、シータ2、シータdハーフまであるので、連続する次元を新しい複素数にグループ化し、複素平面上に投影すると、このベクトルになる。"
  },
  {
    "start": 3431366,
    "end": 3438800,
    "text": "ここに見えるx1x2のベクトルに、複素数mシータ1を掛ける。"
  },
  {
    "start": 3439490,
    "end": 3445326,
    "text": "この結果、数字はmθ1で示される角度だけ回転することになる。"
  },
  {
    "start": 3445428,
    "end": 3449950,
    "text": "これがエンコードされた番号であり、これがエンコードされたトークンである。"
  },
  {
    "start": 3450030,
    "end": 3455700,
    "text": "これこそが、いまお見せしている行列変換でやっていることなのです。"
  },
  {
    "start": 3456310,
    "end": 3458660,
    "text": "では、これをコードに変換してみよう。"
  },
  {
    "start": 3465530,
    "end": 3467910,
    "text": "回転埋め込みを適用する。"
  },
  {
    "start": 3471290,
    "end": 3475690,
    "text": "Xは回転埋め込みを適用したいトークンである。"
  },
  {
    "start": 3480590,
    "end": 3485670,
    "text": "Frixコンプレックスは、この関数の出力であるが、このトークンの位置についてのみである。"
  },
  {
    "start": 3485750,
    "end": 3492058,
    "text": "というのも、このトークンのすべての位置に対してのみ、すべての可能な位置のデータを持つことになるからである。"
  },
  {
    "start": 3492074,
    "end": 3495710,
    "text": "必要なのは、この特定のトークンのポジションだけだ。"
  },
  {
    "start": 3496290,
    "end": 3498350,
    "text": "それなら、その装置が必要だ。"
  },
  {
    "start": 3502710,
    "end": 3507922,
    "text": "私たちが最初に行うのは、\"トランスフォーメーション・ナンバーワン \"と呼んでいるものだ。"
  },
  {
    "start": 3507976,
    "end": 3512022,
    "text": "ああ、これと1番と2番だ。"
  },
  {
    "start": 3512076,
    "end": 3521190,
    "text": "まず最初にすることは、連続する2つの次元を新しいテンソルに変換し、それを複素数として可視化することである。"
  },
  {
    "start": 3521340,
    "end": 3524674,
    "text": "これらの操作はPytorchによってサポートされている。"
  },
  {
    "start": 3524722,
    "end": 3525880,
    "text": "我々はそれを行う。"
  },
  {
    "start": 3527370,
    "end": 3536320,
    "text": "に等しい複合体を作る。"
  },
  {
    "start": 3550070,
    "end": 3555540,
    "text": "さて、この操作は基本的に、連続する2つの次元を取り出してグループ化することを意味している。"
  },
  {
    "start": 3555910,
    "end": 3564150,
    "text": "そしてこの中間テンソルを、トーチのview as complex演算を使って複素テンソルに変換する。"
  },
  {
    "start": 3565450,
    "end": 3566882,
    "text": "いくつかコメントを書かせてほしい。"
  },
  {
    "start": 3566946,
    "end": 3587514,
    "text": "というのも、このxは実は元のベクトルではなく、すでに頭の次元で分割されたものだからだ。"
  },
  {
    "start": 3587642,
    "end": 3592794,
    "text": "マルチヘッドがない場合は、このヘッド寸法がトークンの全寸法となる。"
  },
  {
    "start": 3592842,
    "end": 3604674,
    "text": "つまり、4096のテンソルがここにあるわけだが、このテンソルはこのテンソルより2次元小さい。"
  },
  {
    "start": 3604712,
    "end": 3608254,
    "text": "バッチ寸法がなく、ヘッド寸法もない。"
  },
  {
    "start": 3608302,
    "end": 3609314,
    "text": "追加する必要がある。"
  },
  {
    "start": 3609352,
    "end": 3614390,
    "text": "固定された複合体に、欠けている2つの次元を加える。"
  },
  {
    "start": 3621290,
    "end": 3623562,
    "text": "私たちはここでやっている。"
  },
  {
    "start": 3623616,
    "end": 3626730,
    "text": "よし、ここにすべての変形を書こう。"
  },
  {
    "start": 3626800,
    "end": 3635466,
    "text": "ここから2で割っていく。"
  },
  {
    "start": 3635488,
    "end": 3636254,
    "text": "なぜ2で割るのか？"
  },
  {
    "start": 3636292,
    "end": 3640126,
    "text": "なぜなら、連続する2つのペアが1つの複素数になるからだ。"
  },
  {
    "start": 3640228,
    "end": 3647040,
    "text": "ここでは、シークエンスの長さからヘッド寸法を2で割ったものになる。"
  },
  {
    "start": 3647490,
    "end": 3660100,
    "text": "これはバッチ・ディメンション、シーケンス長、ヘッド・ディメンション1、ヘッド・ディメンションを2で割ったものであるため、1にマッピングしている。"
  },
  {
    "start": 3660630,
    "end": 3662258,
    "text": "今度はそれを掛け合わせる。"
  },
  {
    "start": 3662344,
    "end": 3663934,
    "text": "私たちはここでこの作戦を実行する。"
  },
  {
    "start": 3663992,
    "end": 3668982,
    "text": "要素の賢明な乗算は、先の図で見たように回転をもたらす。"
  },
  {
    "start": 3669036,
    "end": 3677430,
    "text": "だから私はこれをx回転と呼んでいるのだが、これはxの複素数に周波数の複素数を掛けたものに等しい。"
  },
  {
    "start": 3678490,
    "end": 3686480,
    "text": "この場合、シークエンスの長さ、hヘッド寸法を2で割ったものを合計している。"
  },
  {
    "start": 3703190,
    "end": 3711380,
    "text": "それを掛け算してこの結果を得て、まずそれを変換する。"
  },
  {
    "start": 3712070,
    "end": 3724570,
    "text": "複素数をテンソルに変換し、その最初の項目は複素数の実数部、次に複素数部、虚数部とし、それを平坦化する。"
  },
  {
    "start": 3724640,
    "end": 3725980,
    "text": "そうしよう"
  },
  {
    "start": 3734590,
    "end": 3739920,
    "text": "この演算を実数として見ると、テンソルはこのように変換される。"
  },
  {
    "start": 3757990,
    "end": 3768726,
    "text": "それは、複素数を2次元のテンソルに変換する変換である。"
  },
  {
    "start": 3768908,
    "end": 3770600,
    "text": "そして、それを平らにする。"
  },
  {
    "start": 3777050,
    "end": 3798820,
    "text": "元のテンソルの形状で平らにすればいいのだ。"
  },
  {
    "start": 3807990,
    "end": 3810414,
    "text": "これがエンベッディングの計算方法である。"
  },
  {
    "start": 3810462,
    "end": 3830380,
    "text": "トークンまたはトークンのリストを表すテンソルが与えられると、バッチ次元があるので、埋め込みをこのように適用することができる。"
  },
  {
    "start": 3831950,
    "end": 3838526,
    "text": "あとはトランスフォーマーを実装していくだけだ。"
  },
  {
    "start": 3838708,
    "end": 3848614,
    "text": "次に実装できるのは、このRMSノルムです。RMSノルムはトランスの出力にも存在しますが、入力にも存在します。"
  },
  {
    "start": 3848682,
    "end": 3851374,
    "text": "もう一度、アーキテクチャを見直してみよう。"
  },
  {
    "start": 3851502,
    "end": 3859538,
    "text": "正規化、RMS正規化はここにもありますが、こことここにもあります。"
  },
  {
    "start": 3859624,
    "end": 3861250,
    "text": "それを実行に移そう。"
  },
  {
    "start": 3861400,
    "end": 3868374,
    "text": "正規化の仕組みを深く理解したいなら、RMSノルムの仕組みも可視化してみよう。"
  },
  {
    "start": 3868492,
    "end": 3878300,
    "text": "前回のラマについてのビデオでは、なぜノーマライゼーションが必要なのか、ノーマライゼーションが歴史的にどのように行われてきたのか、そしてそれがオートグラッドレベルでもどのように機能するのかを説明した。"
  },
  {
    "start": 3878750,
    "end": 3881210,
    "text": "ここで同じ講義を繰り返すつもりはない。"
  },
  {
    "start": 3881360,
    "end": 3884058,
    "text": "その仕組みを簡単に紹介しよう。"
  },
  {
    "start": 3884144,
    "end": 3887694,
    "text": "より理解を深めたい方は、私の以前のビデオをご覧ください。"
  },
  {
    "start": 3887812,
    "end": 3893934,
    "text": "覚えているように、オリジナルのトランスフォーマーではレイヤー正規化を使い、レイヤー正規化は次のように機能した。"
  },
  {
    "start": 3894052,
    "end": 3897402,
    "text": "入力項目がいくつかある。"
  },
  {
    "start": 3897466,
    "end": 3900162,
    "text": "アイテム1、アイテム2、アイテム10までとする。"
  },
  {
    "start": 3900296,
    "end": 3904434,
    "text": "各アイテムには3つの特徴があり、1、2、3となる。"
  },
  {
    "start": 3904552,
    "end": 3927658,
    "text": "レイヤーの正規化で行ったのは、各項目について1つずつ、つまりミューとシグマ、つまり平均とシグマの2つの統計量を計算し、各項目を標準化し、この式を使って入力行列の各要素を正規化することです。"
  },
  {
    "start": 3927744,
    "end": 3931526,
    "text": "この公式は確率統計に由来する。"
  },
  {
    "start": 3931638,
    "end": 3950914,
    "text": "ご存知のように、ミューとシグマを持つ確率変数があれば、その平均を標準偏差で割った値、つまり分散の平方根を引くと、平均ゼロ、標準、分散1のガウス型になります。"
  },
  {
    "start": 3951112,
    "end": 3956882,
    "text": "これにガンマ・パラメーターを掛け合わせ、さらにベータ・パラメーターも加える。"
  },
  {
    "start": 3956936,
    "end": 3959250,
    "text": "これはレイヤーの正規化で行われた。"
  },
  {
    "start": 3961050,
    "end": 3969026,
    "text": "llamaではRMS正規化を使用しており、RMS正規化の違いを見てみよう。"
  },
  {
    "start": 3969138,
    "end": 3976342,
    "text": "RMS正規化の論文は、レイヤー正規化の効果を得る必要はないと主張している。"
  },
  {
    "start": 3976406,
    "end": 3980518,
    "text": "平均と分散という2つの統計量を計算する必要はない。"
  },
  {
    "start": 3980614,
    "end": 3995002,
    "text": "実際には、レイヤー正規化によって得られる効果は、値を再中心化することなく、つまりゼロの平均値を中心に再中心化することなく、ただスケーリングすることによって得られると主張している。"
  },
  {
    "start": 3995146,
    "end": 4008770,
    "text": "というのも、分散の公式は、xから分布の平均を引いて2のべき乗をnで割ったものだからだ。"
  },
  {
    "start": 4008920,
    "end": 4017094,
    "text": "分散を計算するためには平均が必要だが、平均は必要ないので計算を避けたかった。"
  },
  {
    "start": 4017132,
    "end": 4019506,
    "text": "これはRMSの論文の主張とは異なる。"
  },
  {
    "start": 4019538,
    "end": 4023794,
    "text": "RMSの論文では、平均値は必要ないし、再中心化も必要ないと主張している。"
  },
  {
    "start": 4023842,
    "end": 4027366,
    "text": "平均に依存しない統計量を計算する必要がある。"
  },
  {
    "start": 4027468,
    "end": 4038590,
    "text": "そのため、平均に依存しない二乗平均平方根という統計量を導入し、実際にはレイヤーの正規化と同じ正規化効果を与えている。"
  },
  {
    "start": 4039010,
    "end": 4044362,
    "text": "ガンマ・パラメーターも学習可能で、これは乗算される。"
  },
  {
    "start": 4044426,
    "end": 4065266,
    "text": "ご覧のように、レイヤー正規化とRMS正規化の唯一の違いは、値を再正規化しないことです。この論文では、再正規化は必要なかったように見えます。"
  },
  {
    "start": 4065378,
    "end": 4070722,
    "text": "RMS統計に従って値を再スケーリングしているだけだ。"
  },
  {
    "start": 4070786,
    "end": 4073158,
    "text": "これが我々のコードで行うことである。"
  },
  {
    "start": 4073244,
    "end": 4075190,
    "text": "このブロックを作ろう"
  },
  {
    "start": 4099810,
    "end": 4107010,
    "text": "ここに見えるAPSの値が分母として使われる。"
  },
  {
    "start": 4107590,
    "end": 4112178,
    "text": "ここで話を戻すが、分母に足されるものとして使われている。"
  },
  {
    "start": 4112274,
    "end": 4114520,
    "text": "ゼロによる除算を避けるためである。"
  },
  {
    "start": 4119210,
    "end": 4133958,
    "text": "それからガンマ、ガンマ・パラメーター、それだけだ。"
  },
  {
    "start": 4134044,
    "end": 4151250,
    "text": "ここでxはバッチシーケンス長次元である。"
  },
  {
    "start": 4152390,
    "end": 4164950,
    "text": "では、xにトーチRSスクウェアT-R-SスクウェアTは平方根上の1を表す。"
  },
  {
    "start": 4185570,
    "end": 4211910,
    "text": "ガンマとディムを掛ける。"
  },
  {
    "start": 4216490,
    "end": 4228098,
    "text": "おわかりのように、ウェイトとは数値であり、次元dimにbのシーケンス長を掛けた次元を持つもののリストである。"
  },
  {
    "start": 4228194,
    "end": 4234570,
    "text": "Dimの結果は、b個のシーケンス長チームとなり、bはバッチ次元である。"
  },
  {
    "start": 4235150,
    "end": 4244990,
    "text": "ここで我々がやっていることは、rsqrtはxのsqrtの1乗に等しいということだ。"
  },
  {
    "start": 4245490,
    "end": 4259250,
    "text": "ここでの寸法は、p個の長さを掛け合わせたもので、p個の長さの寸法になる。"
  },
  {
    "start": 4259590,
    "end": 4263542,
    "text": "私たちがやっていることは、まさにこの公式そのものなのだ。"
  },
  {
    "start": 4263596,
    "end": 4271640,
    "text": "rmsを1倍し、ガンマをかける。"
  },
  {
    "start": 4273050,
    "end": 4279734,
    "text": "rmsノルムも構築できたので、次のビルディング・ブロック、つまりこのエンコーダー・ブロックをチェックしてみよう。"
  },
  {
    "start": 4279782,
    "end": 4281046,
    "text": "エンコーダー・ブロックとは何ですか？"
  },
  {
    "start": 4281078,
    "end": 4283910,
    "text": "変圧器の話に戻ろう。"
  },
  {
    "start": 4284070,
    "end": 4286474,
    "text": "ここにエンコーダー・ブロックがある。"
  },
  {
    "start": 4286522,
    "end": 4296010,
    "text": "このブロックはすべて正規化を含み、自己注意を含み、スキップコネクションを含む。"
  },
  {
    "start": 4296090,
    "end": 4302660,
    "text": "ここに別の正規化、スキップ接続、フィードフォワード層があるのがわかるだろう。"
  },
  {
    "start": 4303110,
    "end": 4309106,
    "text": "一番簡単なのはフィードフォワードだと思う。"
  },
  {
    "start": 4309208,
    "end": 4312870,
    "text": "よし、始めよう。"
  },
  {
    "start": 4312940,
    "end": 4319414,
    "text": "最初にエンコーダー・ブロックを構築し、次にアテンション、最後にフィードフォワードを構築する。"
  },
  {
    "start": 4319532,
    "end": 4324038,
    "text": "まずこの骨格を作り、次に注目、そしてこの骨格を作る。"
  },
  {
    "start": 4324124,
    "end": 4376886,
    "text": "頭の次元とは、ベクトルの次元を頭の数で割ったものである。"
  },
  {
    "start": 4376918,
    "end": 4389786,
    "text": "4096をここで32で割ると、エンベッディング・ベクトルの次元は4096だが、ヘッドは32個になる。"
  },
  {
    "start": 4389818,
    "end": 4395390,
    "text": "各ヘッドは各トークンから4096÷32のアイテムを見ることになる。"
  },
  {
    "start": 4404870,
    "end": 4407086,
    "text": "そして、自己注意ブロックがある。"
  },
  {
    "start": 4407198,
    "end": 4410922,
    "text": "私はそれを定義するが、今すぐには作らず、骨格だけを定義する。"
  },
  {
    "start": 4411006,
    "end": 4412520,
    "text": "次にフィードフォワードだ。"
  },
  {
    "start": 4419770,
    "end": 4425050,
    "text": "そして、自己注目の前に正常化がある。"
  },
  {
    "start": 4425950,
    "end": 4432540,
    "text": "これがRMSノルムだ。"
  },
  {
    "start": 4437090,
    "end": 4439834,
    "text": "これがこの議論の動機である。"
  },
  {
    "start": 4439882,
    "end": 4441070,
    "text": "ノーム・アプス"
  },
  {
    "start": 4444130,
    "end": 4458020,
    "text": "フィードフォワードの後ではなく、アテンションの後だ。"
  },
  {
    "start": 4459190,
    "end": 4462310,
    "text": "フィードフォワードブロックの前"
  },
  {
    "start": 4479710,
    "end": 4483930,
    "text": "では、フォワード・メソッドを実装してみよう。"
  },
  {
    "start": 4493010,
    "end": 4494874,
    "text": "トークンの位置を示す。"
  },
  {
    "start": 4495002,
    "end": 4498270,
    "text": "変数番号は元のコードと同じにした。"
  },
  {
    "start": 4498340,
    "end": 4503842,
    "text": "トークンは一度に1つしか扱わないからだ。"
  },
  {
    "start": 4503896,
    "end": 4508100,
    "text": "開始ポーズは、扱っているトークンの位置を示す。"
  },
  {
    "start": 4511210,
    "end": 4514550,
    "text": "これらは事前に計算された周波数である。"
  },
  {
    "start": 4529630,
    "end": 4537070,
    "text": "さて、隠されたものはxに注目度を足したものだ。"
  },
  {
    "start": 4537650,
    "end": 4542954,
    "text": "この入力の正規化されたバージョンの注目度を計算する。"
  },
  {
    "start": 4543082,
    "end": 4552046,
    "text": "まず正規化を行い、それからこの注目度を計算する。"
  },
  {
    "start": 4552158,
    "end": 4565110,
    "text": "というのも、回転位置エンコーディングは、アテンションを計算するときに、ある種の役割を果たすからだ。"
  },
  {
    "start": 4565930,
    "end": 4583690,
    "text": "これは、xにスキップ接続を足したものに、注目のbシーケンス長次元の出力を足したものである。"
  },
  {
    "start": 4584910,
    "end": 4592880,
    "text": "次に、フィードフォワードをスキップ接続で適用すると、アウトはhプラスに等しくなる。"
  },
  {
    "start": 4600390,
    "end": 4610782,
    "text": "をフィードフォワードに送る前に、前回と同様に正規化を適用し、これが出力となる。"
  },
  {
    "start": 4610926,
    "end": 4614118,
    "text": "あとは自己の注意力とフィードフォワードを高める必要がある。"
  },
  {
    "start": 4614284,
    "end": 4616710,
    "text": "まずは難しいところから始めよう。"
  },
  {
    "start": 4616780,
    "end": 4628380,
    "text": "自己注意を構築する前に、オリジナルのトランスフォーマーで自己注意がどのように機能したか、そしてここでそれがどのように機能するかをおさらいしておこう。"
  },
  {
    "start": 4629710,
    "end": 4636090,
    "text": "よし、これは変圧器の原紙だから、注意はこれだけでいい。"
  },
  {
    "start": 4636160,
    "end": 4641710,
    "text": "オリジナルのトランスフォーマーにおける自己注視のメカニズムをおさらいし、次にそれがリャマでどのように機能するかを見てみよう。"
  },
  {
    "start": 4642130,
    "end": 4646702,
    "text": "注目すべきは、dモデルによるシーケンスである入力だ。"
  },
  {
    "start": 4646756,
    "end": 4651506,
    "text": "トークンのシーケンスで、各トークンはサイズdのベクトルでモデル化される。"
  },
  {
    "start": 4651688,
    "end": 4656526,
    "text": "同じ入力であるクエリーキーと値に変換する。"
  },
  {
    "start": 4656718,
    "end": 4665080,
    "text": "パラメータ行列であるw行列を掛け合わせ、その結果、dモデルによるシーケンスの次元を持つ新しい行列ができる。"
  },
  {
    "start": 4665610,
    "end": 4675030,
    "text": "次に、トークンを表す各ベクトルが分割されるような頭数に分割する。"
  },
  {
    "start": 4675100,
    "end": 4682358,
    "text": "仮にヘッドが4つあるとすると、各ベクトル、各ヘッドは各トークンのエンベッディングの一部を見ることになる。"
  },
  {
    "start": 4682454,
    "end": 4692874,
    "text": "トークンのサイズが512であった場合、例えば埋め込みベクトルであれば、最初のヘッドはこのベクトルの120 I 8次元を見ることになる。"
  },
  {
    "start": 4693002,
    "end": 4701490,
    "text": "番目の頭は次の108次元を、次の頭は次の108次元を、エトセトラ、エトセトラ、エトセトラ。"
  },
  {
    "start": 4701830,
    "end": 4707220,
    "text": "次に、これらすべての小さな行列間の注目度を計算し、q、k、vとした。"
  },
  {
    "start": 4707590,
    "end": 4710734,
    "text": "その結果、ヘッド1、ヘッド2、ヘッド3、ヘッド4となる。"
  },
  {
    "start": 4710872,
    "end": 4713400,
    "text": "そして、それらを連結する。"
  },
  {
    "start": 4713850,
    "end": 4722146,
    "text": "w行列と乗算し、これがマルチヘッドアテンションの出力となる。"
  },
  {
    "start": 4722338,
    "end": 4729686,
    "text": "この場合、キーと値としてクエリーとして機能するのは同じ入力なので、自己注意と呼ばれる。"
  },
  {
    "start": 4729798,
    "end": 4735482,
    "text": "クエリがある場所から来たもので、キーと値が別の場所から来た場合。"
  },
  {
    "start": 4735536,
    "end": 4737418,
    "text": "その場合、クロス・アテンションと呼ばれる。"
  },
  {
    "start": 4737514,
    "end": 4742970,
    "text": "このような注意は、マルチモーダルアーキテクチャで使われている。"
  },
  {
    "start": 4743050,
    "end": 4751890,
    "text": "例えば、写真とキャプション、音楽とテキストを組み合わせたり、ある言語から別の言語に翻訳したい場合などだ。"
  },
  {
    "start": 4751960,
    "end": 4755954,
    "text": "マルチモダリティのようなもので、その2つを結びつけたい。"
  },
  {
    "start": 4756152,
    "end": 4761320,
    "text": "私たちの場合、言語をモデルにしているので、自己注意が必要なのだ。"
  },
  {
    "start": 4761930,
    "end": 4763958,
    "text": "実際、私たちに必要なのは注意力だけだ。"
  },
  {
    "start": 4764124,
    "end": 4769110,
    "text": "リャマでどう動くか見てみよう。"
  },
  {
    "start": 4769690,
    "end": 4770534,
    "text": "オーケー。"
  },
  {
    "start": 4770732,
    "end": 4774738,
    "text": "リャマでは、セルフ・アテンションを高める前に、いろいろなことを話す必要がある。"
  },
  {
    "start": 4774834,
    "end": 4784682,
    "text": "llamaの自己注意がどのように働くのか、kvキャッシュとは何か、グループ化されたクエリー注意とは何か、そして実際に推論がどのように働くのかを復習する必要がある。"
  },
  {
    "start": 4784736,
    "end": 4787182,
    "text": "コードを進める前に、これらすべてを見直す必要がある。"
  },
  {
    "start": 4787236,
    "end": 4789774,
    "text": "そうでなければ、コードを追うのは非常に難しくなる。"
  },
  {
    "start": 4789972,
    "end": 4796110,
    "text": "まず、与えられた推論について話そう。"
  },
  {
    "start": 4796260,
    "end": 4801474,
    "text": "仮に、この特定のラインでトレーニングされたモデルがあるとしよう。"
  },
  {
    "start": 4801592,
    "end": 4806260,
    "text": "その一線は、優しい心を素早くつかむ愛である。"
  },
  {
    "start": 4806630,
    "end": 4809326,
    "text": "これはダンテリゲリのセリフだ。"
  },
  {
    "start": 4809358,
    "end": 4814506,
    "text": "このことはパソ、インフェルノ第5カントからわかるだろう。"
  },
  {
    "start": 4814638,
    "end": 4818678,
    "text": "最初の行ではないですが、パオロ・フランチェスカです。"
  },
  {
    "start": 4818844,
    "end": 4825640,
    "text": "私たちには、このラインで訓練されたモデルがいる。"
  },
  {
    "start": 4826670,
    "end": 4834474,
    "text": "さて、次のトークン予測を使ってこの特定の行で訓練されたモデルは、このように構築された入力を持つはずである。"
  },
  {
    "start": 4834512,
    "end": 4840074,
    "text": "文頭のトークン、そして文を表すトークン。"
  },
  {
    "start": 4840122,
    "end": 4844138,
    "text": "そうすると、ターゲットは文末と同じ文になるはずだ。"
  },
  {
    "start": 4844314,
    "end": 4852100,
    "text": "トランスフォーマーはシーケンスからシーケンスへのモデルなので、1つの入力シーケンスを同じサイズの出力シーケンスにマッピングする。"
  },
  {
    "start": 4852550,
    "end": 4856814,
    "text": "これは、最初のトークンが出力の最初のトークンにマッピングされることを意味する。"
  },
  {
    "start": 4856862,
    "end": 4862194,
    "text": "入力の2番目のトークンは出力の2番目のトークンにマッピングされる。"
  },
  {
    "start": 4862242,
    "end": 4877690,
    "text": "入力の3番目のトークンは出力の3番目のトークンにマッピングされるが、自己注意の際に適用する因果関係のマスクのせいで、1対1の対応にはならない。"
  },
  {
    "start": 4878830,
    "end": 4890186,
    "text": "例えば、この特定のトークンkenを予測するために、モデルは入力中の同じトークンだけを見るのではなく、過去のすべてのトークンも見る。"
  },
  {
    "start": 4890378,
    "end": 4905140,
    "text": "そして、因果関係のマスクを持つ自己注意メカニズムは、前のトークンすべてにアクセスするが、次のトークンにはアクセスしない。"
  },
  {
    "start": 4905590,
    "end": 4908802,
    "text": "つまり、推論を行う際には、次のように行うべきだということだ。"
  },
  {
    "start": 4908856,
    "end": 4912606,
    "text": "文頭から始めて、モデルは最初の単語を出力する。"
  },
  {
    "start": 4912728,
    "end": 4918360,
    "text": "次のトークンを出力するには、前の出力トークンも入力として与える必要がある。"
  },
  {
    "start": 4918730,
    "end": 4924978,
    "text": "連続するトークンを予測するために、常に出力の最後のトークンを入力に追加する。"
  },
  {
    "start": 4925074,
    "end": 4938410,
    "text": "例えば、次のトークンを出力するためにSOSの愛を与える必要があることを出力するために、この負債を取り、次のトークンを出力するために次の単語を得ることができるように入力に置く。"
  },
  {
    "start": 4938490,
    "end": 4943520,
    "text": "新しい出力を素早く得るためには、前の出力を入力に追加する必要がある。"
  },
  {
    "start": 4944370,
    "end": 4956850,
    "text": "さて、この仕事をするとき、モデルは実際には4つのトークンの並びである入力を与え、モデルは出力として4つのトークンの並びを生成する。"
  },
  {
    "start": 4957670,
    "end": 4968200,
    "text": "というのも、推論を行う際、モデルはすでに構築された、必要のない多くのドット積を行うからだ。"
  },
  {
    "start": 4968730,
    "end": 4982714,
    "text": "例えば、私が言いたいのは、この最後のトークンを素早く取得するためには、ここにある以前のすべてのコンテキストにアクセスする必要がある。"
  },
  {
    "start": 4982752,
    "end": 4985898,
    "text": "私たちはすでにこれらのトークンを持っている。"
  },
  {
    "start": 4985984,
    "end": 4990714,
    "text": "しかし、前のトークンを出力しないようにトランスフォーマーモデルに指示すればよい。"
  },
  {
    "start": 4990762,
    "end": 5000590,
    "text": "変圧器の出力で受け取るトークンは1つだけで、他のトークンは計算すらされないように、計算を変更する必要がある。"
  },
  {
    "start": 5000670,
    "end": 5002978,
    "text": "これで推論が速くなる。"
  },
  {
    "start": 5003064,
    "end": 5005310,
    "text": "これはKVキャッシュの仕事である。"
  },
  {
    "start": 5005390,
    "end": 5007490,
    "text": "図を使って説明しよう。"
  },
  {
    "start": 5011590,
    "end": 5018710,
    "text": "おわかりのように、トークンの各ステップにおいて、我々はモデルによって出力された最後のトークンにしか興味がない。"
  },
  {
    "start": 5018780,
    "end": 5030438,
    "text": "しかし、モデルは次のトークンを出力するためにすべてのプロンプトにアクセスする必要があるため、どのトークンを出力するかを決定するために前のトークンにアクセスする必要がある。"
  },
  {
    "start": 5030534,
    "end": 5034710,
    "text": "KVキャッシュを使って計算量を減らしている。"
  },
  {
    "start": 5034870,
    "end": 5042334,
    "text": "では、前と同じようにモデルを推論すると仮定して、いくつかの例を挙げてみよう。"
  },
  {
    "start": 5042532,
    "end": 5043834,
    "text": "最初のトークンを渡す。"
  },
  {
    "start": 5043882,
    "end": 5047754,
    "text": "SOS、これは自己注意である。"
  },
  {
    "start": 5047802,
    "end": 5050554,
    "text": "にキーのトランスポーズが掛けられる。"
  },
  {
    "start": 5050602,
    "end": 5053346,
    "text": "これでこのマトリックスができあがる。"
  },
  {
    "start": 5053368,
    "end": 5054574,
    "text": "寸法を確認してください。"
  },
  {
    "start": 5054622,
    "end": 5061860,
    "text": "1×4096と4006×1の掛け算は、1×1の行列を出力する。"
  },
  {
    "start": 5062790,
    "end": 5067202,
    "text": "これに値が掛け合わされ、出力トークンとなる。"
  },
  {
    "start": 5067266,
    "end": 5072226,
    "text": "これは推論ステップ1で、トークンは文頭だけである。"
  },
  {
    "start": 5072338,
    "end": 5077610,
    "text": "そして、このトークンを取り出し、トークンを出力する。"
  },
  {
    "start": 5078510,
    "end": 5082678,
    "text": "なぜなら、これはリニアレイヤーにマッピングされなければならないからだ。"
  },
  {
    "start": 5082694,
    "end": 5093034,
    "text": "これがすでにトークンであり、それを入力に追加することで、入力の2番目の入力になるとする。"
  },
  {
    "start": 5093082,
    "end": 5096410,
    "text": "これがSOSで、これが最後の出力だ。"
  },
  {
    "start": 5096570,
    "end": 5099230,
    "text": "それにキーの移調を掛ける。"
  },
  {
    "start": 5099390,
    "end": 5109620,
    "text": "このマトリックスに値を掛け合わせ、出力として2つの出力トークンを得る。"
  },
  {
    "start": 5110310,
    "end": 5116946,
    "text": "次に、前の出力を入力として追加し、キーによる移調を掛ける。"
  },
  {
    "start": 5116978,
    "end": 5121954,
    "text": "このマトリックスにVを掛けると、3つのトークンが出力される。"
  },
  {
    "start": 5122082,
    "end": 5125238,
    "text": "そして、最後の出力をqに追加する。"
  },
  {
    "start": 5125324,
    "end": 5127522,
    "text": "キーの移調を掛け合わせた。"
  },
  {
    "start": 5127586,
    "end": 5132038,
    "text": "この行列を得、これにvを掛け、出力としてこのシーケンスを得る。"
  },
  {
    "start": 5132134,
    "end": 5133546,
    "text": "いくつかの問題がある。"
  },
  {
    "start": 5133648,
    "end": 5138238,
    "text": "以前お話ししたように、私たちは必要のない計算をたくさんしている。"
  },
  {
    "start": 5138404,
    "end": 5145674,
    "text": "まず第一に、私たちがここで計算しているこれらのドットプロダクトは、自己注目である。"
  },
  {
    "start": 5145722,
    "end": 5152126,
    "text": "qにキーの転置行列を掛けると、多くのドット積がこの行列になる。"
  },
  {
    "start": 5152318,
    "end": 5162226,
    "text": "ここで紫色でハイライトされているこれらのドット積は、すでに前のステップで計算されたものである。なぜなら、我々はステップ番号4にいるからだが、これらはすでに前のステップで計算されている。"
  },
  {
    "start": 5162328,
    "end": 5171794,
    "text": "しかも、それらはすでに計算されているだけでなく、入力として追加した最新のトークンが何であるかにしか興味がないため、必要ないのだ。"
  },
  {
    "start": 5171842,
    "end": 5179286,
    "text": "プロンプトに、このトークンは他のすべてのトークンとどのような関係にあるのですか？"
  },
  {
    "start": 5179318,
    "end": 5188262,
    "text": "なぜなら、このトークンは他のすべてのトークンとの点積の結果、最後のトークンが出力されるからだ。"
  },
  {
    "start": 5188416,
    "end": 5200302,
    "text": "また、常に最新のトークンにアクセスするため、実際には必要のない以前のトークンすべてを出力しない方法もある。"
  },
  {
    "start": 5200446,
    "end": 5202718,
    "text": "はい、KVキャッシュを使うだけです。"
  },
  {
    "start": 5202814,
    "end": 5212046,
    "text": "KVキャッシュでは、常に最後のトークンを入力として使用する。"
  },
  {
    "start": 5212158,
    "end": 5216870,
    "text": "クエリに追加するのではなく、クエリとして直接使用する。"
  },
  {
    "start": 5217450,
    "end": 5223714,
    "text": "クエリーは以前のトークンすべてにアクセスする必要があるため、キーと値を保持する。"
  },
  {
    "start": 5223762,
    "end": 5233290,
    "text": "最後の入力をキーと値に追加しますが、クエリには追加せず、クエリで完全に置き換えます。"
  },
  {
    "start": 5233630,
    "end": 5235100,
    "text": "例で見てみよう。"
  },
  {
    "start": 5235790,
    "end": 5238502,
    "text": "例えば、これは推論の最初のステップだ。"
  },
  {
    "start": 5238566,
    "end": 5241130,
    "text": "これはセンテンス・トークンの始まりに過ぎない。"
  },
  {
    "start": 5241290,
    "end": 5247054,
    "text": "トークンが1つしかないので、それにキーの移調をかけると、1×1になる。"
  },
  {
    "start": 5247092,
    "end": 5248970,
    "text": "出力されるトークンは1つだけである。"
  },
  {
    "start": 5249130,
    "end": 5259762,
    "text": "前のケースでは、このトークンはクエリーに追加されたため、次のステップでは4096×2次元の行列となった。"
  },
  {
    "start": 5259816,
    "end": 5270054,
    "text": "この場合、ステップ2の時点では、キーと値の末尾に追加するのではなく、クエリーだけをここに残しておく。"
  },
  {
    "start": 5270252,
    "end": 5275734,
    "text": "今、この製品をもう一度やってみると、この行だけが私たちの関心事であることがわかるだろう。"
  },
  {
    "start": 5275772,
    "end": 5278614,
    "text": "前の図で紫色でなかった方。"
  },
  {
    "start": 5278742,
    "end": 5284650,
    "text": "このドット積を行なうと、最後のトークンだけが得られる。"
  },
  {
    "start": 5284720,
    "end": 5290640,
    "text": "この仕事を続けるたびに、キーと価値が高まっていくのがわかるだろう。"
  },
  {
    "start": 5291010,
    "end": 5294794,
    "text": "クエリーは常に最後のトークンになる。"
  },
  {
    "start": 5294922,
    "end": 5299822,
    "text": "推論中に行うドット積の数はずっと少なくなる。"
  },
  {
    "start": 5299876,
    "end": 5302610,
    "text": "以前のようなドット製品は必要ない。"
  },
  {
    "start": 5302680,
    "end": 5305074,
    "text": "比較せよ、これがタイムステップ4だ。"
  },
  {
    "start": 5305272,
    "end": 5307042,
    "text": "これは4つのドット積である。"
  },
  {
    "start": 5307176,
    "end": 5309378,
    "text": "前回のステップ4と比較する。"
  },
  {
    "start": 5309464,
    "end": 5316040,
    "text": "ここでは、16個のドット積があるので、それを4分の1にする。"
  },
  {
    "start": 5318090,
    "end": 5322738,
    "text": "だから、KVキャッシュを使った推論の方がずっと速いのだ。"
  },
  {
    "start": 5322914,
    "end": 5324550,
    "text": "もう一度おさらいしよう。"
  },
  {
    "start": 5324620,
    "end": 5330714,
    "text": "ご覧のように、行列はコクとなり、トークンをキューに追加するたびに増えていきますよね？"
  },
  {
    "start": 5330832,
    "end": 5334762,
    "text": "KVキャッシュを使用することで、これらの以前の値を再計算することはない。"
  },
  {
    "start": 5334816,
    "end": 5336454,
    "text": "だからこっちの方が速いんだ"
  },
  {
    "start": 5336502,
    "end": 5341658,
    "text": "必要なものだけを計算し、出力としてトークンを1つだけ得る。"
  },
  {
    "start": 5341754,
    "end": 5351490,
    "text": "このメカニズムがよくわからない場合は、私が以前に撮影したラメインについてのビデオをご覧いただきたい。"
  },
  {
    "start": 5351910,
    "end": 5353940,
    "text": "さあ、これを作ろう。"
  },
  {
    "start": 5355430,
    "end": 5363090,
    "text": "構築する前にもうひとつお見せしたいものがある。"
  },
  {
    "start": 5363240,
    "end": 5370434,
    "text": "私はこれをグループ化されたマルチクエリー・アテンションと呼んでいる。"
  },
  {
    "start": 5370482,
    "end": 5373582,
    "text": "マルチヘッドアテンションとマルチクエリーアテンションの中間のようなものだ。"
  },
  {
    "start": 5373666,
    "end": 5376374,
    "text": "実際、実名はグループ化されたクエリである。"
  },
  {
    "start": 5376422,
    "end": 5378970,
    "text": "この論文では、グループ化されたクエリー・アテンション（grouped query attention）とも呼ばれている。"
  },
  {
    "start": 5380350,
    "end": 5387754,
    "text": "さて、グループ化されたクエリーアテンションを導入した理由は、まずマルチクエリーアテンションがあったからだ。"
  },
  {
    "start": 5387802,
    "end": 5391886,
    "text": "マルチクエリ・アテンションは基本的に、ある問題を解決するために導入された。"
  },
  {
    "start": 5391988,
    "end": 5396270,
    "text": "つまり、まずマルチヘッドに注目してもらった。"
  },
  {
    "start": 5396610,
    "end": 5402266,
    "text": "先ほどのマルチヘッド注目のKVキャッシュを紹介した。"
  },
  {
    "start": 5402388,
    "end": 5407442,
    "text": "問題は、マルチヘッドでドットプロダクトをやりすぎていたことだ。"
  },
  {
    "start": 5407496,
    "end": 5409940,
    "text": "KVキャッシュを使えば、ドット積みは少なくなる。"
  },
  {
    "start": 5410390,
    "end": 5416754,
    "text": "その結果、計算量は大幅に減ったが、アルゴリズムにとって新たなボトルネックとなった。"
  },
  {
    "start": 5416802,
    "end": 5425206,
    "text": "ボトルネックは、計算回数ではなく、テンソルにアクセスするためのメモリアクセス回数だった。"
  },
  {
    "start": 5425398,
    "end": 5436294,
    "text": "GPUでは、テンソルをメモリ内で移動させるよりも、計算を行う方がはるかに速いからだ。"
  },
  {
    "start": 5436422,
    "end": 5447034,
    "text": "アルゴリズムを最適化するとき、どれだけの演算を行うかだけでなく、どれだけのテンソルにアクセスするのか、そのテンソルはどこにあるのかを考慮する必要がある。"
  },
  {
    "start": 5447082,
    "end": 5457486,
    "text": "テンソルをある場所から別の場所にコピーし続けるのは良いアイデアではありません。GPUは、ある場所から別の場所にメモリをコピーするのが、演算処理よりもはるかに遅いからです。"
  },
  {
    "start": 5457678,
    "end": 5462638,
    "text": "これはGPUのデータシートで確認できる。"
  },
  {
    "start": 5462734,
    "end": 5474410,
    "text": "たとえば、演算速度は1秒間に19.5テラ浮動小数点演算であるのに対し、メモリ帯域幅は40倍も遅い。"
  },
  {
    "start": 5475470,
    "end": 5484454,
    "text": "我々は、アクセスするテンソルの数と、それらをメモリ上でどのように移動させるかを管理するアルゴリズムも最適化する必要がある。"
  },
  {
    "start": 5484582,
    "end": 5488106,
    "text": "これが、マルチクエリ・アテンションを導入した理由である。"
  },
  {
    "start": 5488138,
    "end": 5497866,
    "text": "マルチ・クエリー・アテンションとは、基本的に、クエリー用のヘッドはたくさんあるが、キーと値用のヘッドは1つしかないことを意味する。"
  },
  {
    "start": 5498058,
    "end": 5513574,
    "text": "KVキャッシュは、確かにドット積の数は減らしたが、新たなボトルネックがあった。"
  },
  {
    "start": 5513692,
    "end": 5522898,
    "text": "このアルゴリズムでは、メモリ・アクセスを最適化することもできるが、キーと値のヘッ ドの数を減らすことになるため、品質が多少損なわれる。"
  },
  {
    "start": 5522994,
    "end": 5525980,
    "text": "モデルのパラメーターの数を減らしている。"
  },
  {
    "start": 5526430,
    "end": 5546702,
    "text": "この方法では、注目メカニズムに関わるパラメーターの数を減らしているため、もちろんモデルの質は落ちるが、実質的にはそれほど質は落ちないことがわかった。"
  },
  {
    "start": 5546836,
    "end": 5550562,
    "text": "その結果、品質の劣化はほとんど見られなかった。"
  },
  {
    "start": 5550616,
    "end": 5557074,
    "text": "26.7から26.5になったが、パフォーマンスの向上は非常に重要だった。"
  },
  {
    "start": 5557272,
    "end": 5564722,
    "text": "トークンあたり48マイクロ秒だったのが、5マイクロ秒か6マイクロ秒になった。"
  },
  {
    "start": 5564786,
    "end": 5566150,
    "text": "より速くなった。"
  },
  {
    "start": 5567210,
    "end": 5573350,
    "text": "それでは、グループ化されたクエリー・アテンション、またはグループ・マルチ・クエリー・アテンションを紹介しよう。"
  },
  {
    "start": 5574110,
    "end": 5582662,
    "text": "マルチ・ヘッド・アテンションでは、クエリにn個のヘッド、キーにn個のヘッド、値にn個のヘッドを持っていた。"
  },
  {
    "start": 5582806,
    "end": 5590618,
    "text": "マルチクエリでは、キーのヘッドはn個あるが、キーと値のヘッドは1個しかない。"
  },
  {
    "start": 5590794,
    "end": 5600686,
    "text": "グループ化された複数のクエリの注意またはグループ化されたクエリの注意では、我々は、キーと値のためのヘッドの数が少ない。"
  },
  {
    "start": 5600798,
    "end": 5603214,
    "text": "クエリーの2ヘッドごと。"
  },
  {
    "start": 5603262,
    "end": 5607650,
    "text": "この場合、例えば、キーと値を1つのヘッドにする。"
  },
  {
    "start": 5608950,
    "end": 5617910,
    "text": "ヘッド数が少ないので、もちろん最速はこの1本になる。"
  },
  {
    "start": 5618330,
    "end": 5622774,
    "text": "もちろん、品質という観点からはこれがベストだ。"
  },
  {
    "start": 5622812,
    "end": 5626374,
    "text": "この2つの妥協点は、クオリティを落とさないことだ。"
  },
  {
    "start": 5626492,
    "end": 5631366,
    "text": "同時に、マルチヘッドアテンションに比べてスピードも最適化される。"
  },
  {
    "start": 5631558,
    "end": 5635018,
    "text": "さて、このコンセプトをすべて確認したところで、実際に作ってみよう。"
  },
  {
    "start": 5635104,
    "end": 5640734,
    "text": "もし、あなたがあまり詳しく理解していないのであれば、この部分を読んでください。"
  },
  {
    "start": 5640772,
    "end": 5645534,
    "text": "リャマについては、私の別のビデオでもっと詳しく説明しているので、そちらをご覧いただきたい。"
  },
  {
    "start": 5645732,
    "end": 5651840,
    "text": "そうでなければ、前のビデオと同じ内容を繰り返さなければならないとしたら、今のビデオは10時間になってしまう。"
  },
  {
    "start": 5652370,
    "end": 5653374,
    "text": "さあ、作ろう。"
  },
  {
    "start": 5653412,
    "end": 5677000,
    "text": "大丈夫だ。"
  },
  {
    "start": 5677850,
    "end": 5686254,
    "text": "ラマやフェイスブックやメタのオリジナル・コードと比べると、私はまず並列化を取り除いた。"
  },
  {
    "start": 5686292,
    "end": 5687674,
    "text": "複数のGpusは持っていない。"
  },
  {
    "start": 5687722,
    "end": 5693840,
    "text": "実際、私はそれほど強力なGPUを持っていないので、コードをかなり単純化している。"
  },
  {
    "start": 5711050,
    "end": 5720470,
    "text": "また、KV headsmは、キーと値のヘッダーの数を示しており、これらはクエリのヘッダーの数と異なる場合があるからである。"
  },
  {
    "start": 5721610,
    "end": 5724242,
    "text": "だから、私たちはNヘッドも持っている。"
  },
  {
    "start": 5724306,
    "end": 5757646,
    "text": "Qこの値は、クエリのヘッダーの数と、キーと値のヘッダーの数の比率を表している。"
  },
  {
    "start": 5757678,
    "end": 5760130,
    "text": "後で注目度を計算するときに使う。"
  },
  {
    "start": 5760810,
    "end": 5819990,
    "text": "これは、各ヘッドによって視覚化されるエンベッディングの部分を示している。"
  },
  {
    "start": 5820060,
    "end": 5823118,
    "text": "ご存知のように、エンベディングは複数のヘッドに分かれている。"
  },
  {
    "start": 5823154,
    "end": 5828780,
    "text": "各ヘッドは全文を見るが、各単語の埋め込みの一部を見ることになる。"
  },
  {
    "start": 5837730,
    "end": 5845390,
    "text": "そして、通常のバニラ・トランスと同じように、w行列wqwk、wv、woがある。"
  },
  {
    "start": 5886050,
    "end": 5891200,
    "text": "なぜ本当のことを書いたのか？"
  },
  {
    "start": 5908950,
    "end": 5910986,
    "text": "そしてキャッシュを作成する。"
  },
  {
    "start": 5911038,
    "end": 5912662,
    "text": "それがどのように使われるかは、後で見てみよう。"
  },
  {
    "start": 5912716,
    "end": 5917990,
    "text": "キー用と値用を作ったところだ。"
  },
  {
    "start": 5954790,
    "end": 5959570,
    "text": "さて、最後にフォワード・メソッドを実装する。"
  },
  {
    "start": 5959640,
    "end": 5969640,
    "text": "self xは、あなたのためにコードを単純化する。"
  },
  {
    "start": 5970010,
    "end": 5978330,
    "text": "各演算について、その演算に関与するテンソルの次元と、各演算から得られるテンソルを書く。"
  },
  {
    "start": 5982910,
    "end": 5991150,
    "text": "この開始位置は、文中のトークンの位置を示しており、これが計算した頻度である。"
  },
  {
    "start": 5996210,
    "end": 6015750,
    "text": "では、まずbのシークエンスの長さと次元を抽出してみよう。"
  },
  {
    "start": 6016250,
    "end": 6021880,
    "text": "配列の長さは1であることが分かっている。"
  },
  {
    "start": 6023450,
    "end": 6027126,
    "text": "そして、オリジナルのトランスフォーマーと同じように乗算を行う。"
  },
  {
    "start": 6027158,
    "end": 6028726,
    "text": "クエリー、キー、値を受け取ります。"
  },
  {
    "start": 6028758,
    "end": 6032822,
    "text": "これにWq、Wq、wq行列を掛ける。"
  },
  {
    "start": 6032966,
    "end": 6036490,
    "text": "x qはself dot wqに等しい。"
  },
  {
    "start": 6039730,
    "end": 6049262,
    "text": "つまり、b1次元からb1次元のヘッドになるということだ。"
  },
  {
    "start": 6049326,
    "end": 6058610,
    "text": "クエリのヘッダーの数にディメンジョンを掛けたものです。"
  },
  {
    "start": 6059990,
    "end": 6093854,
    "text": "Kvのヘッド数はqより少ないかもしれない。"
  },
  {
    "start": 6094052,
    "end": 6103170,
    "text": "この行列の最後の次元はxqより小さく、xpも同様である。"
  },
  {
    "start": 6106950,
    "end": 6109246,
    "text": "ここで少しコメントを書かせてほしい。"
  },
  {
    "start": 6109438,
    "end": 6124502,
    "text": "WqWk行列とWv行列をクエリのキーと値に適用する。"
  },
  {
    "start": 6124566,
    "end": 6127980,
    "text": "クエリのキーと値は常にxである。"
  },
  {
    "start": 6130270,
    "end": 6140222,
    "text": "xqはxqビューベッドサイズに等しくなる。"
  },
  {
    "start": 6140276,
    "end": 6141760,
    "text": "私たちはこのようにしている。"
  },
  {
    "start": 6142130,
    "end": 6144240,
    "text": "シークエンスの長さは1。"
  },
  {
    "start": 6149990,
    "end": 6166706,
    "text": "b、1Hqにヘッド・ディメンションを掛けたものを、b、1ヘッドHqとヘッド・ディメンションに分ける。"
  },
  {
    "start": 6166898,
    "end": 6237350,
    "text": "それをクエリーのh個のヘッドに分け、キーと値も同じようにする。"
  },
  {
    "start": 6237430,
    "end": 6239580,
    "text": "コードを確認してみよう。"
  },
  {
    "start": 6239970,
    "end": 6244650,
    "text": "覚えているように、我々は入力を受け取り、WQWKとWvを掛ける。"
  },
  {
    "start": 6244730,
    "end": 6246846,
    "text": "その結果、ここにこれらの行列ができる。"
  },
  {
    "start": 6246948,
    "end": 6249482,
    "text": "それを頭数で割っていく。"
  },
  {
    "start": 6249546,
    "end": 6252270,
    "text": "グループ化されたクエリーアテンションの場合、それらは異なるかもしれない。"
  },
  {
    "start": 6252340,
    "end": 6258786,
    "text": "これは4頭身かもしれないし、これは2頭身かもしれないし、これは2頭身かもしれない。"
  },
  {
    "start": 6258968,
    "end": 6272402,
    "text": "次にやることは、クエリーとキーには回転位置エンコーディングを適用するが、値には適用しないことだ。"
  },
  {
    "start": 6272466,
    "end": 6275340,
    "text": "そうしよう。"
  },
  {
    "start": 6285230,
    "end": 6289226,
    "text": "これが位置エンコーディングを適用する方法である。"
  },
  {
    "start": 6289418,
    "end": 6292266,
    "text": "これによってベクターのサイズが変わることはない。"
  },
  {
    "start": 6292458,
    "end": 6299230,
    "text": "最終的に元の入力ベクトルと同じ形になっているのだから。"
  },
  {
    "start": 6327470,
    "end": 6330938,
    "text": "今度はkvキャッシュの部分だ。"
  },
  {
    "start": 6331024,
    "end": 6333050,
    "text": "もう一度スライドを見てみよう。"
  },
  {
    "start": 6336590,
    "end": 6340106,
    "text": "ここでわかるように、入力出力トークンがあるたびに"
  },
  {
    "start": 6340218,
    "end": 6351906,
    "text": "例えば、ここに注目してほしいのは、トークン番号2をキーと値の最後に追加することだ。"
  },
  {
    "start": 6352088,
    "end": 6379210,
    "text": "というのも、xの各反復では、前の反復から出力された最新のトークンのみを受け取り、それをkとvに追加し、すべてのk、すべてのvの間のアテンションを計算するが、クエリとして単一のトークンのみを受け取るからである。"
  },
  {
    "start": 6381070,
    "end": 6382378,
    "text": "そうしよう"
  },
  {
    "start": 6382464,
    "end": 6398510,
    "text": "まずトークンの位置を置き換える。"
  },
  {
    "start": 6400310,
    "end": 6404500,
    "text": "シーケンスの長さは常に1なので、これは1であるべきだ。"
  },
  {
    "start": 6405510,
    "end": 6412530,
    "text": "このコードは、メタのllamaのコードと同じにするよう心がけている。"
  },
  {
    "start": 6416870,
    "end": 6424470,
    "text": "これは基本的に、多くのバッチから1つのトークンを得た場合、つまりすべてのバッチに対して1つのトークンを得たことを意味する。"
  },
  {
    "start": 6424910,
    "end": 6431222,
    "text": "複数バッチを処理できるため、私たちはそれらを交換する。"
  },
  {
    "start": 6431366,
    "end": 6436090,
    "text": "バッチごとに、この特定のポジションのエントリーを置き換える。"
  },
  {
    "start": 6446850,
    "end": 6451426,
    "text": "今度は、この位置だけを置き換える。"
  },
  {
    "start": 6451528,
    "end": 6455054,
    "text": "kvキャッシュを使ってアテンションを計算する場合。"
  },
  {
    "start": 6455102,
    "end": 6456820,
    "text": "もう一度見に行こう。"
  },
  {
    "start": 6458630,
    "end": 6466914,
    "text": "積を計算する必要がある。1つのトークンだけでなく、すべてのキーの間のドット積である。"
  },
  {
    "start": 6467042,
    "end": 6472402,
    "text": "この場合、すべての値を掛け合わせる必要があり、結果として出力されるトークンは1つになる。"
  },
  {
    "start": 6472466,
    "end": 6482594,
    "text": "このキャッシュから、キーとしてすべてのトークンを、値としてこの位置までのすべてのトークンを抽出する必要がある。"
  },
  {
    "start": 6482642,
    "end": 6501102,
    "text": "すべての人に平等だ。"
  },
  {
    "start": 6501156,
    "end": 6512840,
    "text": "ゼロから始まり、スタートポーズにシーケンス長を足した値がそれである。"
  },
  {
    "start": 6529230,
    "end": 6537302,
    "text": "さて、何が起こるかというと、サイズも書いてみよう。"
  },
  {
    "start": 6537366,
    "end": 6544750,
    "text": "入力の配列長は常に1であるため、ここではkとvの配列長をbとする。"
  },
  {
    "start": 6544820,
    "end": 6556386,
    "text": "しかし、キャッシュのシーケンス長とは、開始位置までのすべてのキャッシュされたキーと値を意味する。"
  },
  {
    "start": 6556488,
    "end": 6565320,
    "text": "このシークエンスの長さは、実際には開始位置と等しく、実際には開始位置に1を足したものとなる。"
  },
  {
    "start": 6569290,
    "end": 6575080,
    "text": "次の次元は、キーとvのヘッドの数であり、次に各ヘッドの次元である。"
  },
  {
    "start": 6577390,
    "end": 6586262,
    "text": "さて、キーと値のヘッダーの数は、クエリのヘッダーの数と一致しないかもしれない。"
  },
  {
    "start": 6586406,
    "end": 6589786,
    "text": "llamaのオリジナルコードではどのように計算するのか？"
  },
  {
    "start": 6589818,
    "end": 6595166,
    "text": "彼らがしたことは、基本的に、ここのコードをチェックしようということだった。"
  },
  {
    "start": 6595348,
    "end": 6604702,
    "text": "グループ化されたクエリーでは、キーと値のヘッダーの数はクエリーのヘッダーの数と同じではありません。"
  },
  {
    "start": 6604846,
    "end": 6606242,
    "text": "方法は2つある。"
  },
  {
    "start": 6606296,
    "end": 6610270,
    "text": "ひとつは、このことを実際に考慮した最適化されたアルゴリズムを作ることだ。"
  },
  {
    "start": 6610350,
    "end": 6624294,
    "text": "他の方法としては、このシングルヘッドをマルチヘッドにコピーして、マルチヘッドと同じように計算することもできる。"
  },
  {
    "start": 6624412,
    "end": 6642106,
    "text": "これは最適化された解決策ではないが、llamaのコードで使われているものであり、また、グループ化されたクエリの注意をサポートする唯一のモデルがllamaの最大のものであるため、他のコードをテストする方法がないため、私が固執するものでもある。"
  },
  {
    "start": 6642138,
    "end": 6649806,
    "text": "しかし、私のコンピューターはそのモデルをロードすることができない。"
  },
  {
    "start": 6649828,
    "end": 6654334,
    "text": "だから、実際にグループ化されたクエリーのアテンションを計算するコードも最適化しなかった。"
  },
  {
    "start": 6654382,
    "end": 6660180,
    "text": "この1つの頭を何度も繰り返すことで、このような状況になるのだ。"
  },
  {
    "start": 6661510,
    "end": 6697386,
    "text": "この機能も繰り返す。"
  },
  {
    "start": 6697418,
    "end": 6704818,
    "text": "Kvは、この回数に達するまでキーを繰り返すだけだ。"
  },
  {
    "start": 6704904,
    "end": 6706802,
    "text": "nrepさん、これは何ですか？"
  },
  {
    "start": 6706856,
    "end": 6711598,
    "text": "これは、クエリのヘッダーの数とキーのヘッダーの数の比である。"
  },
  {
    "start": 6711694,
    "end": 6720360,
    "text": "キーのヘッダーの数が4で、クエリーのヘッダーの数が8であれば、それぞれのヘッダーの数を2回繰り返す必要がある。"
  },
  {
    "start": 6720810,
    "end": 6737840,
    "text": "せっかくだから、このメソッドも作ってみよう。"
  },
  {
    "start": 6749570,
    "end": 6751246,
    "text": "オーケー、繰り返す必要はない。"
  },
  {
    "start": 6751268,
    "end": 6756186,
    "text": "繰り返しは1回だけなので、基本テンソルを返すだけだ。"
  },
  {
    "start": 6756218,
    "end": 6758160,
    "text": "それ以外はn回繰り返す。"
  },
  {
    "start": 6759650,
    "end": 6779180,
    "text": "まず最初にすることは、新しいディメンションを追加することだ。"
  },
  {
    "start": 6782670,
    "end": 6788830,
    "text": "参加者の長さ、頭数、そして何もない。"
  },
  {
    "start": 6788900,
    "end": 6794960,
    "text": "そうなれば、このポジションに新たな次元が加わることになる。"
  },
  {
    "start": 6795490,
    "end": 6826860,
    "text": "そして、それを拡大し、形を変える。"
  },
  {
    "start": 6828670,
    "end": 6831622,
    "text": "基本的に、私たちは新しい次元を導入する。"
  },
  {
    "start": 6831766,
    "end": 6836362,
    "text": "この次元のシークエンスをすべて、何回繰り返すか。"
  },
  {
    "start": 6836416,
    "end": 6843280,
    "text": "この次元に沿ってNWaP回Nwapし、あとは平らにするだけだ。"
  },
  {
    "start": 6845490,
    "end": 6854366,
    "text": "こうしてキーと値を繰り返す。"
  },
  {
    "start": 6854398,
    "end": 6855650,
    "text": "あとは繰り返すだけだ。"
  },
  {
    "start": 6866170,
    "end": 6874306,
    "text": "では、マルチヘッドの標準的な計算と同じように進めていきます。"
  },
  {
    "start": 6874418,
    "end": 6887850,
    "text": "つまり、各ヘッドはすべてのシーケンスを見るが、各トークンのエンベッディングの一部を見ることになるため、まずヘッドの次元をシーケンスの次元より先に移動させる。"
  },
  {
    "start": 6891730,
    "end": 6902830,
    "text": "というのも、1というのは、クエリのシーケンス長、クエリのヘッド数、ヘッド・ディメンションだからである。"
  },
  {
    "start": 6905730,
    "end": 6911170,
    "text": "バッチヘッドシーケンスの長さとヘッド寸法。"
  },
  {
    "start": 6913130,
    "end": 6915590,
    "text": "キーと値についても同様である。"
  },
  {
    "start": 6929050,
    "end": 6956820,
    "text": "次に、キーの転置を各ヘッドの次元の平方根で割ったものをクエリーにかけるという標準的な計算式を行う。つまり、xキューブなので、キーの転置をクエリーにかけ、これを各ヘッドの次元の平方根で割ったものだ。"
  },
  {
    "start": 6957190,
    "end": 6975190,
    "text": "そして、ソフトマックスを適用し、クエリの形状を決定する。"
  },
  {
    "start": 6975630,
    "end": 6998350,
    "text": "1つの追加ディメンションにソフトマックスを掛けても、ディメンションは変わらない。"
  },
  {
    "start": 6998690,
    "end": 7012030,
    "text": "そして、その値を掛け合わせ、キーの移調と出力を掛け合わせた式になる。"
  },
  {
    "start": 7012110,
    "end": 7052842,
    "text": "次にソフトマックスを行い、出力に値を掛け合わせる。"
  },
  {
    "start": 7052906,
    "end": 7058994,
    "text": "ヘッドをすべて取り除く前に、もう一度連結する。"
  },
  {
    "start": 7059112,
    "end": 7124022,
    "text": "ここでは、すべてのヘッドの出力を取り、それらを連結し、それをWO行列itと掛け合わせる。その結果、b one dim、b one dimとなり、これはB-H-Qの1つのヘッドの次元が、転置のためb one hqのヘッドの次元となる。"
  },
  {
    "start": 7124076,
    "end": 7131350,
    "text": "これはKVキャッシュを使った自己アテンションである。"
  },
  {
    "start": 7131430,
    "end": 7134842,
    "text": "ここで私たちがやったことを復習しておこう。"
  },
  {
    "start": 7134896,
    "end": 7140278,
    "text": "だから色が違うんだ。"
  },
  {
    "start": 7140374,
    "end": 7145882,
    "text": "さて、自己の注意を計算するときにやったことを復習しよう。"
  },
  {
    "start": 7145946,
    "end": 7147294,
    "text": "このコードが機能するのは"
  },
  {
    "start": 7147332,
    "end": 7154978,
    "text": "推論にはKVキャッシュを使うことができる。KVキャッシュを使えば、不要なドット積の数を節約できる。"
  },
  {
    "start": 7155144,
    "end": 7155570,
    "text": "なぜですか？"
  },
  {
    "start": 7155640,
    "end": 7165266,
    "text": "というのも、オリジナルのトランスフォーマーでは、トークンのドット積を大量に計算していたからだ。"
  },
  {
    "start": 7165448,
    "end": 7169682,
    "text": "この場合、トークンを1つだけ出力するようにメカニズムを単純化した。"
  },
  {
    "start": 7169746,
    "end": 7172470,
    "text": "ご覧の通り、自己アテンションの出力はbである。"
  },
  {
    "start": 7172540,
    "end": 7179420,
    "text": "トークンの埋め込みサイズは4096である。"
  },
  {
    "start": 7180110,
    "end": 7184870,
    "text": "トークンを何個も出力するのではなく、トークンを1個だけ出力する。"
  },
  {
    "start": 7184950,
    "end": 7188170,
    "text": "トークンを1つだけ入力し、トークンを1つだけ出力する。"
  },
  {
    "start": 7188910,
    "end": 7196122,
    "text": "というのも、そのトークンを以前のすべてのトークンと関連づける必要があるため、キーと値のキャッシュを保持するのだ。"
  },
  {
    "start": 7196266,
    "end": 7200334,
    "text": "トークンがあるたびに、このようにキャッシュに入れる。"
  },
  {
    "start": 7200452,
    "end": 7207726,
    "text": "次に、キャッシュから以前に保存されたトークンをすべて取得し、すべての以前のトークン間のアテンションを計算する。"
  },
  {
    "start": 7207758,
    "end": 7213410,
    "text": "キーと値、そして単一のトークンをクエリーとして入力する。"
  },
  {
    "start": 7213770,
    "end": 7216454,
    "text": "気になるのは出力トークンだけだ。"
  },
  {
    "start": 7216572,
    "end": 7228502,
    "text": "これはKVキャッシュの背後にある考え方であり、グループ化されたクエリの注意は、キーと値に対して異なる数のヘッドを持っているという事実である。"
  },
  {
    "start": 7228646,
    "end": 7242154,
    "text": "私たちの場合、キーとクエリで異なる数のヘッドを持っているが、アテンションを計算するために、足りないヘッドを繰り返すだけだ。"
  },
  {
    "start": 7242202,
    "end": 7255566,
    "text": "この注目度は、前のトランスフォーマーと同じように、通常のマルチヘッドアテンションのように計算されるが、実際にアルゴリズムを最適化するのではなく、欠落しているキーと値のヘッドを繰り返すことによって計算される。"
  },
  {
    "start": 7255598,
    "end": 7261570,
    "text": "これはmetaの公式実装でも行われており、私もここで行った。"
  },
  {
    "start": 7261720,
    "end": 7265138,
    "text": "最大の理由は、他の改造をテストできないからだ。"
  },
  {
    "start": 7265234,
    "end": 7270754,
    "text": "この計算を実際に最適化しようとする他のアルゴリズムをテストすることはできない。"
  },
  {
    "start": 7270882,
    "end": 7276742,
    "text": "もし他にうまくいっている実装を見つけたら、みんなと共有するよ。"
  },
  {
    "start": 7276796,
    "end": 7281878,
    "text": "そうでなければ、colabでやってみて、より良い解決策を思いつくかどうか試してみるつもりだ。"
  },
  {
    "start": 7281974,
    "end": 7283530,
    "text": "今はそれを繰り返すだけだ。"
  },
  {
    "start": 7283600,
    "end": 7287382,
    "text": "少なくとも、グループ化されたクエリーのコンセプトは理解できた。"
  },
  {
    "start": 7287446,
    "end": 7298590,
    "text": "つまり、ヘッド数を少なくし、マルチクエリー・アテンションとマルチヘッド・アテンションの中間に位置するもので、クオリティを犠牲にすることなく、スピードを向上させる。"
  },
  {
    "start": 7299170,
    "end": 7303226,
    "text": "最後に、フィードフォワード層を実装しなかった。"
  },
  {
    "start": 7303258,
    "end": 7308820,
    "text": "フィード層フォワード層については、ここで見ることができるツヴィグル活性化関数だけを見直す必要がある。"
  },
  {
    "start": 7309430,
    "end": 7318070,
    "text": "この活性化関数は、バニラ変換器で使用されていた以前の活性化関数（relu関数）と比べて変更されている。"
  },
  {
    "start": 7318220,
    "end": 7322358,
    "text": "交換した唯一の理由は、こちらの方が性能が良いからだ。"
  },
  {
    "start": 7322524,
    "end": 7335722,
    "text": "なぜなら、700億ものパラメーターがあるような大きなモデルでは、ちょっとした修正で他のものよりうまくいく理由を説明するのは難しいからだ。"
  },
  {
    "start": 7335856,
    "end": 7340922,
    "text": "私たちは、その種のモデルやその種のアプリケーションのために、より効果的に機能するものがあることを知っているだけだ。"
  },
  {
    "start": 7341056,
    "end": 7344590,
    "text": "これは私の意見ではなく、実際に新聞に書かれていることだ。"
  },
  {
    "start": 7344660,
    "end": 7351486,
    "text": "論文の結論にあるように、なぜこのようなアーキテクチャが機能するのかについて、私たちは何の説明もしていない。"
  },
  {
    "start": 7351508,
    "end": 7354666,
    "text": "彼らの成功は、他のすべてと同様、神の慈悲によるものだと考えている。"
  },
  {
    "start": 7354778,
    "end": 7363282,
    "text": "つまり、大きなモデルを持っていて、ちょっとしたことを変えたらうまくいったとしても、それがなぜうまくいったのかを説明するパターンを思いつくとは限らないということだ。"
  },
  {
    "start": 7363336,
    "end": 7369990,
    "text": "その方がうまくいくのが当たり前だと思っていて、実際によりうまくいくから使っているだけなのだ。"
  },
  {
    "start": 7370730,
    "end": 7374120,
    "text": "そこで、Zwigluの機能を実装するために必要なのが、アプリケーションだ。"
  },
  {
    "start": 7374490,
    "end": 7377254,
    "text": "これはオリジナルのトランスフォーマーからの公式である。"
  },
  {
    "start": 7377302,
    "end": 7381034,
    "text": "ここには2つの行列がある。"
  },
  {
    "start": 7381072,
    "end": 7385590,
    "text": "これは、第一のリニアレイヤーと第二のリニアレイヤーのrelu関数である。"
  },
  {
    "start": 7385670,
    "end": 7393742,
    "text": "llamaでは、3つの行列を含むZwiglu関数を使う。ここではパラメータの数を増やしたからだ。"
  },
  {
    "start": 7393796,
    "end": 7397514,
    "text": "また、グループ化されたクエリーの実験もしていた。"
  },
  {
    "start": 7397562,
    "end": 7398270,
    "text": "注目してほしい。"
  },
  {
    "start": 7398610,
    "end": 7415350,
    "text": "llamaのアーキテクチャには、このフィードフォワード層のパラメータ数を調整するためのいくつかのパラメータがあり、いくつかの制約を尊重するようになっている。"
  },
  {
    "start": 7415500,
    "end": 7431830,
    "text": "変圧器モデルを修正し、パラメーターの数を減らしたり増やしたりする場合、研究者が最初に行うことは、フィードフォワード層のパラメーターの数を調整することである。"
  },
  {
    "start": 7431990,
    "end": 7440160,
    "text": "もちろん、同じ構造を使うつもりだ。そうしないと、事前学習したモデルからウェイトをロードできないからだ。"
  },
  {
    "start": 7441330,
    "end": 7442800,
    "text": "そうしよう"
  },
  {
    "start": 7444130,
    "end": 7448170,
    "text": "非表示のサイズはこのように計算されるので、寸法の4倍となる。"
  },
  {
    "start": 7448330,
    "end": 7459790,
    "text": "そして、この次元の3分の2を行い、さらに倍率をかける。"
  },
  {
    "start": 7459870,
    "end": 7494314,
    "text": "もしそれが指定されていれば、彼らはこの修正を使って非表示を丸くすると言う。"
  },
  {
    "start": 7494442,
    "end": 7503582,
    "text": "だから、このように隠れた次元を計算することは、この隠れた次元がこの数字の倍数であるとは限らない。"
  },
  {
    "start": 7503636,
    "end": 7509278,
    "text": "隠れ層のサイズを256パスの倍数にしたいのかもしれない。"
  },
  {
    "start": 7509294,
    "end": 7511730,
    "text": "このように計算すると、そうではないかもしれない。"
  },
  {
    "start": 7511800,
    "end": 7518150,
    "text": "つまり、パラメータの倍数の次の倍数に切り上げるのだ。"
  },
  {
    "start": 7537530,
    "end": 7538638,
    "text": "例を挙げよう。"
  },
  {
    "start": 7538724,
    "end": 7542462,
    "text": "実際に書くよりも、例で示す方が簡単だ。"
  },
  {
    "start": 7542516,
    "end": 7552382,
    "text": "例えば、非表示のサイズを7とし、それを5倍にして表示させたいとする。"
  },
  {
    "start": 7552436,
    "end": 7554914,
    "text": "非表示のサイズを5の倍数にしたい場合。"
  },
  {
    "start": 7554952,
    "end": 7556130,
    "text": "ごきげんよう。"
  },
  {
    "start": 7556280,
    "end": 7561922,
    "text": "まあ、僕らがやることは基本的に、この場合は隠し＋4だ。"
  },
  {
    "start": 7561976,
    "end": 7565270,
    "text": "7＋4で11になる。"
  },
  {
    "start": 7566650,
    "end": 7578442,
    "text": "これを5で割って2とし、この2に5を掛けると、2×5は10になる。"
  },
  {
    "start": 7578496,
    "end": 7583820,
    "text": "その結果、この数値より大きいか等しい最初の倍数がここに表示される。"
  },
  {
    "start": 7584350,
    "end": 7585660,
    "text": "そういうことだ。"
  },
  {
    "start": 7586670,
    "end": 7590622,
    "text": "そうすると、ツヴィグル関数の行列はこうなる。"
  },
  {
    "start": 7590756,
    "end": 7591920,
    "text": "とても簡単だ。"
  },
  {
    "start": 7592610,
    "end": 7598110,
    "text": "ここにあるジグラー関数の計算式に従うだけだ。"
  },
  {
    "start": 7598180,
    "end": 7602430,
    "text": "願いとは何か？"
  },
  {
    "start": 7603970,
    "end": 7612594,
    "text": "スウィッシュはサイロ関数で、ベータが1に等しいスウィッシュは、実はこのグラフのサイロ関数なのだ。"
  },
  {
    "start": 7612792,
    "end": 7616758,
    "text": "そして、ここで別のパラメータ行列と掛け合わせる。"
  },
  {
    "start": 7616844,
    "end": 7620022,
    "text": "次に、それを別のリニアレイヤーw 2に適用する。"
  },
  {
    "start": 7620076,
    "end": 7622598,
    "text": "合計で3つの行列がある。"
  },
  {
    "start": 7622684,
    "end": 7639418,
    "text": "私たちはW2、W3と呼んでいるが、これらに偏りはない。"
  },
  {
    "start": 7639594,
    "end": 7664360,
    "text": "おっと、それだ。"
  },
  {
    "start": 7667370,
    "end": 7669990,
    "text": "では、フォワード・メソッドを実装しよう。"
  },
  {
    "start": 7677950,
    "end": 7680780,
    "text": "最初にすることは、このウィッシュ関数を計算することだ。"
  },
  {
    "start": 7691570,
    "end": 7692880,
    "text": "お見せしましょう。"
  },
  {
    "start": 7694850,
    "end": 7698650,
    "text": "このxwのzwishを計算している。"
  },
  {
    "start": 7698810,
    "end": 7701150,
    "text": "そして、このxvを計算する。"
  },
  {
    "start": 7709430,
    "end": 7712206,
    "text": "そして、計算式と同じように、それらを掛け合わせる。"
  },
  {
    "start": 7712238,
    "end": 7716422,
    "text": "スウィッシュにx vを掛ける。"
  },
  {
    "start": 7716556,
    "end": 7729420,
    "text": "そして、最後の線形レイヤーであるw 2を適用し、その結果、w 2の行列による乗算となり、xを返す。"
  },
  {
    "start": 7730750,
    "end": 7733770,
    "text": "これがフィードフォワード層である。"
  },
  {
    "start": 7734270,
    "end": 7740986,
    "text": "これですべての構成要素が揃ったので、推論に入る必要がある。"
  },
  {
    "start": 7741178,
    "end": 7743982,
    "text": "推論コードを作り始めよう。"
  },
  {
    "start": 7744116,
    "end": 7748638,
    "text": "推論py最初のコード。"
  },
  {
    "start": 7748804,
    "end": 7754594,
    "text": "まずモデルをロードするコードを構築し、次にモデルを推論するコードを構築する。"
  },
  {
    "start": 7754712,
    "end": 7761060,
    "text": "実際に、世の中にあるすべての推論テクニックと、どれを適用するのか、そしてその理由も紹介する。"
  },
  {
    "start": 7761430,
    "end": 7765510,
    "text": "まず、モデルをロードするコードから作り始めよう。"
  },
  {
    "start": 7765580,
    "end": 7768760,
    "text": "まず、必要なものを輸入する。"
  },
  {
    "start": 7782250,
    "end": 7794000,
    "text": "パラメータを読み込むにはJSonが必要で、トークナイザーを読み込むにはセンテンス・ピースが必要です。センテンス・ピースは使用されているトークナイザーで、Googleのライブラリだからです。"
  },
  {
    "start": 7799650,
    "end": 7844590,
    "text": "モデルのインポート、モデルのアーク、トランスフォーマークラスから、モデルがトランスフォーマー、トークン・プロセッサー、そしてモデルの引数アークを受け取るクラスllamaを定義します。"
  },
  {
    "start": 7846050,
    "end": 7850074,
    "text": "モデル・アーク、そう、モデル・アークだ。"
  },
  {
    "start": 7850202,
    "end": 7865614,
    "text": "さて、ここで静的メソッド、staticメソッドをビルドして、llamaのオリジナル・コードと同じようにbuildと呼び、チェックポイントが保存されるディレクトリを渡す。"
  },
  {
    "start": 7865662,
    "end": 7872790,
    "text": "この場合、ディレクトリ名は私の場合llama two seven bですが、どのサイズのモデルをダウンロードしたかによります。"
  },
  {
    "start": 7873210,
    "end": 7877622,
    "text": "次にトークナイザーのパス。"
  },
  {
    "start": 7877686,
    "end": 7880410,
    "text": "これがダウンロードしたトークナイザーのファイルです。"
  },
  {
    "start": 7881470,
    "end": 7897950,
    "text": "そして、ロードモデルレイヤーの最大シーケンス長、最大パッチサイズ、そしてデバイスがある。"
  },
  {
    "start": 7901590,
    "end": 7906020,
    "text": "さて、これはモデルのロードにかかる時間を表示するためだけのものだ。"
  },
  {
    "start": 7907510,
    "end": 7913242,
    "text": "モデルをロードする場合は、チェックポイントもロードする。"
  },
  {
    "start": 7913326,
    "end": 7917670,
    "text": "チェックポイントはソートされたものと等しい。"
  },
  {
    "start": 7921450,
    "end": 7938880,
    "text": "グローバル・メソッドは、このフィルターに一致するすべてのファイルを見つけることができる。"
  },
  {
    "start": 7950050,
    "end": 7971942,
    "text": "チェックポイントをロードし、実際にロードして保存する。"
  },
  {
    "start": 7971996,
    "end": 8009390,
    "text": "CPU上では、モデルのすべてのパラメーターをロードするのにかかる時間を示すことができる。"
  },
  {
    "start": 8009460,
    "end": 8027750,
    "text": "次に、JSONファイルにパラメータをロードし、読み取り専用ファイルとして開く。"
  },
  {
    "start": 8036810,
    "end": 8050838,
    "text": "そして、argumentsquenceを構築する。"
  },
  {
    "start": 8050874,
    "end": 8071430,
    "text": "長さは我々が指定したもので、次に最大パッチ・サイズは我々が指定したもので、次にJSONファイルからロードされたすべてのパラメーターがあります。"
  },
  {
    "start": 8073930,
    "end": 8076150,
    "text": "次にトークナイザーをロードした。"
  },
  {
    "start": 8085070,
    "end": 8090594,
    "text": "トークナイザーを使用することで、モデルアークのボキャブサイズを入力することができます。"
  },
  {
    "start": 8090742,
    "end": 8095630,
    "text": "語彙のサイズは、実際にはトークナイザー内のトークンの数です。"
  },
  {
    "start": 8103430,
    "end": 8112574,
    "text": "これはPytorchのデフォルトのテンソルでもある。"
  },
  {
    "start": 8112622,
    "end": 8118034,
    "text": "Pytorchが新しいテンソルを作りたいときはいつでも、どんな型を使うべきかが定義されている。"
  },
  {
    "start": 8118082,
    "end": 8119026,
    "text": "これはメタによるものだ。"
  },
  {
    "start": 8119058,
    "end": 8123480,
    "text": "彼らはCuDAに、ここでお見せするこのタイプを使うことを望んでいる。"
  },
  {
    "start": 8124170,
    "end": 8134246,
    "text": "デフォルトのテンソル・タイプ torch cuda hard tensor これはテンソルがサポートする精度を変更します。"
  },
  {
    "start": 8134278,
    "end": 8137530,
    "text": "それがメモリ上でどれだけのスペースを占めているか。"
  },
  {
    "start": 8139090,
    "end": 8152420,
    "text": "そうでなければ、実際のモデルを作ることになる。"
  },
  {
    "start": 8167610,
    "end": 8172066,
    "text": "実際には、チェックポイントはキーと値のリストである。"
  },
  {
    "start": 8172178,
    "end": 8175462,
    "text": "各キーはモデル内のマトリックスである。"
  },
  {
    "start": 8175596,
    "end": 8181034,
    "text": "例えば、リニアレイヤーの重さとか、リニアレイヤーのバイアスとか。"
  },
  {
    "start": 8181232,
    "end": 8196798,
    "text": "ここで変数名とマトリックスに使った名前、たとえばWqWKは、チェックポイントにある名前と一致している。"
  },
  {
    "start": 8196884,
    "end": 8205282,
    "text": "正しい名前を使ったことを確認するために、strictをtrueに等しくしてチェックポイントをロードする。"
  },
  {
    "start": 8205336,
    "end": 8210830,
    "text": "Strict equal trueは、少なくとも1つでも一致しない名前があればエラーを投げることを意味する。"
  },
  {
    "start": 8210990,
    "end": 8223542,
    "text": "ロード・モデル、ロード・ステートにチェックが入っていれば、trueに等しい。"
  },
  {
    "start": 8223596,
    "end": 8231958,
    "text": "読み込まれたファイルの中に、モデルで作成したクラスの名前と一致しない名前が1つでもあれば、エラーを投げます。"
  },
  {
    "start": 8232054,
    "end": 8243274,
    "text": "テンソルを作るたびに計算している回転位置埋め込み用の周波数だ。"
  },
  {
    "start": 8243322,
    "end": 8246830,
    "text": "ここではこの関数を使って作成している。"
  },
  {
    "start": 8246900,
    "end": 8249630,
    "text": "モデルからロードする必要はない。"
  },
  {
    "start": 8249700,
    "end": 8253838,
    "text": "モデルからもチェックポイントからも外すことができる。"
  },
  {
    "start": 8253934,
    "end": 8260610,
    "text": "チェックポイントは辞書なので、これを削除すればいい。"
  },
  {
    "start": 8261830,
    "end": 8265400,
    "text": "そうすれば、モデルのロードにかかった時間を表示することができる。"
  },
  {
    "start": 8280990,
    "end": 8289950,
    "text": "で、llamaモデルのトークナイザーとモデルの引数を返します。"
  },
  {
    "start": 8290370,
    "end": 8295790,
    "text": "さて、先に進む前に、モデルが正常にロードできるかどうかテストしてみよう。"
  },
  {
    "start": 8296690,
    "end": 8298702,
    "text": "そうしよう"
  },
  {
    "start": 8298836,
    "end": 8307618,
    "text": "もし名前が先なら、私は手動シードをゼロに設定する。"
  },
  {
    "start": 8307704,
    "end": 8310690,
    "text": "後でそれを推論に使う。"
  },
  {
    "start": 8312230,
    "end": 8315718,
    "text": "それから、私のGPUはCuDAをサポートしていないので、CuDAは使いたくない。"
  },
  {
    "start": 8315724,
    "end": 8327634,
    "text": "私は、CUDAがストレージと同等で、CUDAが利用可能で、それ以外のCPUにCUDAを許可すると言っている。"
  },
  {
    "start": 8327762,
    "end": 8332982,
    "text": "次回、CuDAでモデルをロードしたい場合は、この変数をtrueに設定すればよい。"
  },
  {
    "start": 8333036,
    "end": 8339360,
    "text": "私の場合、cudaをロードしたくないので、常にfalseのままにしておく。"
  },
  {
    "start": 8363130,
    "end": 8369720,
    "text": "1024の最大バッチサイズを設定した。"
  },
  {
    "start": 8371290,
    "end": 8375990,
    "text": "デバイス、デバイス。"
  },
  {
    "start": 8377690,
    "end": 8389150,
    "text": "さあ、実行してみよう。うまくいけばクラッシュしないかもしれない。"
  },
  {
    "start": 8390450,
    "end": 8395738,
    "text": "うわぁ、もうテンソルじゃなくてテンソルだ。"
  },
  {
    "start": 8395914,
    "end": 8398960,
    "text": "もう一度やってみよう。"
  },
  {
    "start": 8400050,
    "end": 8403060,
    "text": "コードを書いていると、いつも誤字脱字が多い。"
  },
  {
    "start": 8404310,
    "end": 8417538,
    "text": "もうひとつの問題は、ストレージではなくテンソルである。"
  },
  {
    "start": 8417634,
    "end": 8418374,
    "text": "そうだね。"
  },
  {
    "start": 8418572,
    "end": 8419800,
    "text": "もう一度やってみよう。"
  },
  {
    "start": 8423560,
    "end": 8424500,
    "text": "隠れている。"
  },
  {
    "start": 8424920,
    "end": 8425684,
    "text": "何を隠した？"
  },
  {
    "start": 8425722,
    "end": 8426544,
    "text": "隠された次元。"
  },
  {
    "start": 8426592,
    "end": 8443070,
    "text": "もちろんだ。"
  },
  {
    "start": 8448020,
    "end": 8450880,
    "text": "ああ、すべて、オーケー、素晴らしい。"
  },
  {
    "start": 8451700,
    "end": 8456400,
    "text": "少なくとも何かをしていて、クラッシュしていないということだ。"
  },
  {
    "start": 8456550,
    "end": 8459572,
    "text": "次のステップは、実際に推論コードを構築することだ。"
  },
  {
    "start": 8459626,
    "end": 8467764,
    "text": "私たちがやりたいことは、モデルにいくつかのプロンプトを与え、そのプロンプトに対する出力をチェックすることです。"
  },
  {
    "start": 8467812,
    "end": 8486168,
    "text": "ここでいくつかのプロンプトを定義し、例えばプロンプトのサイズを渡す。"
  },
  {
    "start": 8486254,
    "end": 8495020,
    "text": "推論を始める前に、モデルを推論するためのコードを構築する必要がある。次のトークンを選択するための戦略を見つける必要があるからだ。"
  },
  {
    "start": 8495100,
    "end": 8500080,
    "text": "推論がどのように機能するのか、推論のための様々な戦略とは何なのか、復習してみよう。"
  },
  {
    "start": 8500500,
    "end": 8509888,
    "text": "さて、次のトークン予測タスクを扱う場合、推論を行う場合、通常はプロンプトを与えてからトークンを予測する。"
  },
  {
    "start": 8509984,
    "end": 8512244,
    "text": "トークンを1つずつ与える。"
  },
  {
    "start": 8512362,
    "end": 8516976,
    "text": "さらにトークンを1つ与えるたびに、モデルは出力としてさらに1つのトークンを出力する。"
  },
  {
    "start": 8517008,
    "end": 8518420,
    "text": "私たちは最後の1本だけを残す。"
  },
  {
    "start": 8518490,
    "end": 8522616,
    "text": "KVキャッシュでは、常に一度に1つのトークンを与える。"
  },
  {
    "start": 8522718,
    "end": 8529240,
    "text": "KVキャッシュは、キーと値のキャッシュを保持し、トークンを1つだけ出力する。"
  },
  {
    "start": 8529660,
    "end": 8537112,
    "text": "要は、ボキャブラリーにあるすべてのトークンの中から、このトークンを選択するストラテジーを見つける必要があるということだ。"
  },
  {
    "start": 8537176,
    "end": 8540360,
    "text": "これはロジクールとソフトマックスの仕事だ。"
  },
  {
    "start": 8540440,
    "end": 8542144,
    "text": "その仕組みをおさらいしておこう。"
  },
  {
    "start": 8542262,
    "end": 8549740,
    "text": "では、私が人間としてあなたに次の仕事を与えると想像して、次の文章を完成させてください。"
  },
  {
    "start": 8549820,
    "end": 8554320,
    "text": "原発はそうだと思うし、それから言葉を選ばなければならない。"
  },
  {
    "start": 8554470,
    "end": 8564036,
    "text": "今、人間であるあなたは、クリーン、危険、安い、高い、安全、難しい、あるいは他の何かであるかもしれない次の可能性のあるトークンを考えたかもしれない。"
  },
  {
    "start": 8564218,
    "end": 8572024,
    "text": "あなたの頭の中にある次のトークンを選ぶかどうかは、あなたの教育、原子力発電の経験、そしてこの問題に対するあなたの意見にかかっている。"
  },
  {
    "start": 8572222,
    "end": 8575144,
    "text": "大規模な言語モデルも同じ問題に直面している。"
  },
  {
    "start": 8575262,
    "end": 8589020,
    "text": "プロンプトを与えると、モデルは次の単語を選択しなければならないが、その選択の不確実性は、モデルのトレーニングプロセスと、次のトークンを選択するために使用するストラテジーに完全に由来する。"
  },
  {
    "start": 8589440,
    "end": 8590888,
    "text": "戦略はたくさんある。"
  },
  {
    "start": 8590984,
    "end": 8597200,
    "text": "例えば、我々は貪欲な戦略を持っている、ビーム検索温度は、パラメータランダムサンプリングトップk、トップpです。"
  },
  {
    "start": 8597270,
    "end": 8601632,
    "text": "このビデオでは、これらすべての戦略とその効果を検証する。"
  },
  {
    "start": 8601766,
    "end": 8604240,
    "text": "まず、ロジストとは何かを理解する必要がある。"
  },
  {
    "start": 8604580,
    "end": 8608776,
    "text": "ラマのトランスフォーマーモデルを見てみよう。"
  },
  {
    "start": 8608908,
    "end": 8613520,
    "text": "自己注意の出力はシーケンスである。"
  },
  {
    "start": 8613680,
    "end": 8616800,
    "text": "kvキャッシュの場合、トークンは1つだけである。"
  },
  {
    "start": 8616960,
    "end": 8619124,
    "text": "その後、リニアレイヤーに通す。"
  },
  {
    "start": 8619172,
    "end": 8621860,
    "text": "正規化後、リニアレイヤーに通す。"
  },
  {
    "start": 8621940,
    "end": 8634808,
    "text": "線形層は、ここで自己注目から出力される埋め込みを、確率の種類を表す数字のリストに変換する。"
  },
  {
    "start": 8634904,
    "end": 8641704,
    "text": "これらは確率とは言えないが、そのトークンが語彙に含まれる確率と考えることはできる。"
  },
  {
    "start": 8641752,
    "end": 8662384,
    "text": "この線形レイヤーは100個の数値を出力し、ソフトマックスを適用した後、この100個の数値が、入力に与えられたプロンプトに対して、そのトークンが次に確率の高いトークンである確率になる。"
  },
  {
    "start": 8662512,
    "end": 8670650,
    "text": "入力、つまりプロンプトが与えられると、モデルは次にどのトークンを選ぶべきかの確率を導き出す。"
  },
  {
    "start": 8671900,
    "end": 8675796,
    "text": "リニアレイヤーとソフトマックスの役割は何ですか？"
  },
  {
    "start": 8675828,
    "end": 8690680,
    "text": "線形層はトークンの埋め込みを数値のリストに変換し、各数値は後にソフトマックスでその特定のトークンが語彙に含まれる確率を表すスコアを表す。"
  },
  {
    "start": 8690840,
    "end": 8695536,
    "text": "ソフトマックスの仕事は、ロジットの和が1になるようにスケーリングすることだ。"
  },
  {
    "start": 8695558,
    "end": 8700636,
    "text": "だからソフトマックスでは確率の話ができるが、ロジットではできないのだ。"
  },
  {
    "start": 8700828,
    "end": 8706588,
    "text": "ソフトマックスの出力は、語彙内のすべての単語の確率分布にすぎない。"
  },
  {
    "start": 8706684,
    "end": 8710724,
    "text": "つまり、語彙の各単語には確率が関連付けられている。"
  },
  {
    "start": 8710762,
    "end": 8716304,
    "text": "さて、これらの単語とそれぞれの確率が与えられたら、次のトークンをどのように選ぶか？"
  },
  {
    "start": 8716352,
    "end": 8717780,
    "text": "戦略はたくさんある。"
  },
  {
    "start": 8718460,
    "end": 8720388,
    "text": "一番簡単なのは貪欲さだ。"
  },
  {
    "start": 8720484,
    "end": 8725524,
    "text": "貪欲戦略は基本的に、最大の確率でトークンを選択するというものだ。"
  },
  {
    "start": 8725652,
    "end": 8729900,
    "text": "推論しているところを想像してほしい。"
  },
  {
    "start": 8729970,
    "end": 8732524,
    "text": "貪欲な戦略では、プロンプトはこうなる。"
  },
  {
    "start": 8732642,
    "end": 8734508,
    "text": "セリア、あなたは私の心を傷つけている。"
  },
  {
    "start": 8734594,
    "end": 8735628,
    "text": "あなたは私の心を揺さぶっている。"
  },
  {
    "start": 8735714,
    "end": 8747228,
    "text": "さて、これはシモーネ・アーガン・ガーファンクルのとても有名な歌の一節だが、知る人ぞ知る次の言葉は「自信」だろう。"
  },
  {
    "start": 8747324,
    "end": 8750960,
    "text": "だから、セリア、あなたは私の心を焼き、私の自信を揺さぶっている。"
  },
  {
    "start": 8751620,
    "end": 8755584,
    "text": "ソフトマックスの出力がこの分布だとする。"
  },
  {
    "start": 8755702,
    "end": 8763750,
    "text": "この単語の確率は40％、この単語の確率は20％、この単語の確率は15％、この単語の確率は10％である。"
  },
  {
    "start": 8764520,
    "end": 8769136,
    "text": "貪欲な戦略では、常に最大の確率でトークンを選ぶ。"
  },
  {
    "start": 8769328,
    "end": 8771892,
    "text": "そして、それを入力に追加する。"
  },
  {
    "start": 8771956,
    "end": 8777684,
    "text": "次の推論ステップの入力は、「セリア、あなたは私の心を傷つけ、私の自信を揺るがしている」となる。"
  },
  {
    "start": 8777732,
    "end": 8783048,
    "text": "そして、モデルは次の単語を考えなければならない。"
  },
  {
    "start": 8783224,
    "end": 8787960,
    "text": "貪欲な戦略を使えば、最も確率の高いものを選ぶ。"
  },
  {
    "start": 8788040,
    "end": 8790924,
    "text": "この場合、それは毎日であり、正しいものでもある。"
  },
  {
    "start": 8791042,
    "end": 8793436,
    "text": "これが貪欲な戦略のやり方だ。"
  },
  {
    "start": 8793538,
    "end": 8801440,
    "text": "ステップごとに、最大確率のトークンを選択し、それを入力に追加して次のトークンを生成する。"
  },
  {
    "start": 8801590,
    "end": 8809348,
    "text": "最初のトークンがたまたま間違ったものだった場合、最初のトークンだけでなく、最初の2つ、3つのトークンも間違ったものになる。"
  },
  {
    "start": 8809434,
    "end": 8815188,
    "text": "モデルに間違ったプロンプトを与えているため、次のトークンもすべて間違っている可能性が高い。"
  },
  {
    "start": 8815274,
    "end": 8823412,
    "text": "ステップ1では自信を選択しないが、モデルはなぜかレバーに高得点をつけた。"
  },
  {
    "start": 8823556,
    "end": 8824996,
    "text": "あなたは私の肝臓を揺さぶっている。"
  },
  {
    "start": 8825028,
    "end": 8834568,
    "text": "という歌は存在しないからだ。"
  },
  {
    "start": 8834664,
    "end": 8840444,
    "text": "貪欲な初期の段階でミスを犯せば、次のトークンもおそらくすべてミスを犯すだろう。"
  },
  {
    "start": 8840562,
    "end": 8848450,
    "text": "実装は非常に簡単なのだが、実際には性能が低いため、あまり使われていない。"
  },
  {
    "start": 8849060,
    "end": 8851440,
    "text": "もう一つの戦略はビーム・サーチだ。"
  },
  {
    "start": 8851590,
    "end": 8865824,
    "text": "ビームサーチでは、kと呼ばれるパラメータがあり、これは、すべてのステップで上位のものを選ぶだけでなく、すべてのステップで上位kのものを選び、常に上位2つの最高のパフォーマンスを持つトークンを保持することを意味する。"
  },
  {
    "start": 8865952,
    "end": 8869384,
    "text": "この場合、例えば、私たちがステップ1のテンプルにいるとしよう。"
  },
  {
    "start": 8869422,
    "end": 8872410,
    "text": "セリア、あなたは私の心を打ち砕き、揺さぶっている。"
  },
  {
    "start": 8872780,
    "end": 8876116,
    "text": "トップ2は「ピザ」と「自信」。"
  },
  {
    "start": 8876308,
    "end": 8887480,
    "text": "ピザの方が確率が高いのは、モデルがこの曲を見たことがないからかもしれない。"
  },
  {
    "start": 8887640,
    "end": 8897388,
    "text": "モデルはこれらの確率を出力するかもしれないが、私たちは最も高い確率を持つ2つのトークンを選ぶ。"
  },
  {
    "start": 8897484,
    "end": 8907884,
    "text": "次のタイムステップでは、2つのプロンプトを作成する。1つは最初のトークンを選択した場合、もう1つは2番目のトークンを選択した場合である。"
  },
  {
    "start": 8907932,
    "end": 8912448,
    "text": "最初のトークンを使用した場合、次にどのような選択肢が考えられるかを見る。"
  },
  {
    "start": 8912544,
    "end": 8915600,
    "text": "番目のトークンを使う場合、次の選択肢は？"
  },
  {
    "start": 8915680,
    "end": 8920468,
    "text": "最初のプロンプトと2番目のプロンプトに対するモデル出力をチェックする。"
  },
  {
    "start": 8920644,
    "end": 8932088,
    "text": "例えば最初のプロンプトを使用した場合、モデルはこれらの確率を出力し、2番目のプロンプトを使用した場合、モデルはこれらの確率を出力する。"
  },
  {
    "start": 8932264,
    "end": 8937436,
    "text": "そこで私たちが行うのは、可能性のあるパートごとに累積得点を計算することだ。"
  },
  {
    "start": 8937538,
    "end": 8947824,
    "text": "例えばピザの場合、確率は40％であったが、ピザの後、モデルは例えばマルガリータの確率を0.1セントと算出した。"
  },
  {
    "start": 8947862,
    "end": 8955350,
    "text": "ピザ、マルガリータ、この部分は0.4、確率は0.4%だ。"
  },
  {
    "start": 8955720,
    "end": 8962390,
    "text": "ピザのアンチョビは0.2％か0.2％だろう。"
  },
  {
    "start": 8963160,
    "end": 8969956,
    "text": "しかし、信頼があれば、毎日でも毎月でも新しい次のトークンを手に入れることができる。"
  },
  {
    "start": 8970148,
    "end": 8975640,
    "text": "日次の累積スコアは0.16点、月次は0.2点である。"
  },
  {
    "start": 8975710,
    "end": 8988316,
    "text": "ステップ2の時点でわかるように、ステップ1の時点ではピザが最も可能性の高い単語であったとしても、ステップ2の時点ではピザが最も可能性の高い単語であった。"
  },
  {
    "start": 8988338,
    "end": 8989304,
    "text": "貪欲に。"
  },
  {
    "start": 8989432,
    "end": 8991820,
    "text": "レーザーを使わせてくれ。"
  },
  {
    "start": 8992240,
    "end": 9007808,
    "text": "この信頼度によって、次のトークンが非常に高い確率で生成されることがわかります。なぜなら、モデルはより多くのプロンプトを持っており、非常に高い信頼度で次のトークンのより具体的な選択肢を思いつくことができるからです。"
  },
  {
    "start": 9007904,
    "end": 9017024,
    "text": "これらすべてのパートの累積スコアを計算し、上位の選択肢を持つ2つのパートを残す。"
  },
  {
    "start": 9017072,
    "end": 9021112,
    "text": "ピッツァは後回しだ。"
  },
  {
    "start": 9021166,
    "end": 9025896,
    "text": "最初にピザを選んだのは、なぜかモデルがピザだと思ったからだが、その後、見つけられなかった。"
  },
  {
    "start": 9025998,
    "end": 9028936,
    "text": "モデルは次の言葉に自信がなかった。"
  },
  {
    "start": 9029118,
    "end": 9033704,
    "text": "このトークンの場合、モデルは2番目のスコアに非常に自信を持っていた。"
  },
  {
    "start": 9033752,
    "end": 9042264,
    "text": "最後のトークンに辿り着くまで、私たちはこのパスをすべて潰し、このパスをキープした。"
  },
  {
    "start": 9042312,
    "end": 9054210,
    "text": "これが、ビームサーチを使った推論ストラテジーの出力であり、最後のトークンにたどり着くまで、すべての連続したトークンに対して、最後のスライドの最後のステップのステップを繰り返す。"
  },
  {
    "start": 9054840,
    "end": 9063008,
    "text": "ビーム・サーチでは、各ステップで上位のkポットを生かし、他のkポットはすべて殺す。"
  },
  {
    "start": 9063184,
    "end": 9068056,
    "text": "すべてのステップでk個の可能な選択肢を探らなければならないので、推論時間が長くなる。"
  },
  {
    "start": 9068158,
    "end": 9073160,
    "text": "一般的に、今示したような理由で、貪欲な戦略よりも成績が良い。"
  },
  {
    "start": 9074140,
    "end": 9078100,
    "text": "もうひとつ、推論で興味深いのは気温だ。"
  },
  {
    "start": 9078260,
    "end": 9086536,
    "text": "温度の考え方は、モデルをより確信のあるものにしたり、そうでないものにしたりすることができるということだからだ。"
  },
  {
    "start": 9086728,
    "end": 9096204,
    "text": "例えば、ロジットを計算するとき、これは確率ではないので、ソフトマックスを適用した後の確率になる。"
  },
  {
    "start": 9096252,
    "end": 9111030,
    "text": "ソフトマックスを適用する前に、ロジットをスケーリングすることができます。例えば、このようなロジットを使用する場合、ソフトマックスの確率は妥当な数字になります。"
  },
  {
    "start": 9112520,
    "end": 9114160,
    "text": "これがロジットである。"
  },
  {
    "start": 9114320,
    "end": 9123588,
    "text": "ソフトマックスを適用する前のこれらのロジットを、温度が低いほど低い数値で割ると、この数値は温度と呼ばれる。"
  },
  {
    "start": 9123764,
    "end": 9131096,
    "text": "より大きな確率をより大きく、より小さな確率をより小さくすることで、モデルの信頼性を高めることができる。"
  },
  {
    "start": 9131128,
    "end": 9135528,
    "text": "低確率と高確率の差が大きくなる。"
  },
  {
    "start": 9135704,
    "end": 9142904,
    "text": "例えば、温度をかけずに、最も高いロジックが80％の確率を得ることがわかる。"
  },
  {
    "start": 9143032,
    "end": 9149116,
    "text": "0.4の温度を適用すると、最高のロジットは98％の確率となる。"
  },
  {
    "start": 9149308,
    "end": 9153052,
    "text": "高温を適用すると、モデルの信頼性が低下する。"
  },
  {
    "start": 9153116,
    "end": 9156260,
    "text": "低確率と高確率の差が小さくなる。"
  },
  {
    "start": 9157720,
    "end": 9171050,
    "text": "この温度は、モデルの信頼性を高めたい場合にも、そうでない場合にも重要である。なぜなら、この温度は、後で説明する貪欲な戦略やトップk、トップvのような他の戦略と組み合わせて使うことができるからである。"
  },
  {
    "start": 9173020,
    "end": 9175604,
    "text": "もう一つの戦略は、無作為抽出である。"
  },
  {
    "start": 9175732,
    "end": 9182488,
    "text": "先ほど見たように、ロジットは確率分布ではないが、ソフトマックスを適用すると分布になる。"
  },
  {
    "start": 9182664,
    "end": 9187704,
    "text": "分布なので、この分布からサンプリングすることもできる。"
  },
  {
    "start": 9187832,
    "end": 9201036,
    "text": "たとえば、このロジットから得られる分布では、12％の確率で選ばれるトークンが1つ、7％の確率で選ばれるトークンが1つ、80％の確率で選ばれるトークンが1つある。"
  },
  {
    "start": 9201148,
    "end": 9209584,
    "text": "コインをひっくり返すと、80％の確率でこのトークンを選び、12％の確率でこのトークンを選び、7％の確率でこのトークンを選ぶ。"
  },
  {
    "start": 9209632,
    "end": 9212464,
    "text": "つまり、この分布からサンプルを採取する。"
  },
  {
    "start": 9212592,
    "end": 9219800,
    "text": "つまり、この分布から、その重みに応じて、つまり確率に応じて数値を取り出すということである。"
  },
  {
    "start": 9220220,
    "end": 9231224,
    "text": "このサンプリング戦略には問題がある。わずかな確率で、まったくくだらないトークンを選んでしまう可能性があるのだ。"
  },
  {
    "start": 9231352,
    "end": 9255780,
    "text": "例えば、このシナリオでは、例えば、貪欲戦略やブリムサーチの前に、ランダムサンプリングを使用すると、40％の確率でピザという単語を選択し、20％の確率で自信という単語を選択しますが、ごくわずかな確率で、10％の確率でポケモンという単語を選択する可能性があります。"
  },
  {
    "start": 9256840,
    "end": 9261764,
    "text": "もちろん、確率は低いので、私たちが間違った選択をする確率も低い。"
  },
  {
    "start": 9261802,
    "end": 9263920,
    "text": "この確率がある。"
  },
  {
    "start": 9264080,
    "end": 9266580,
    "text": "これはランダムサンプリングの問題である。"
  },
  {
    "start": 9267980,
    "end": 9270104,
    "text": "次の戦略はトップKだ。"
  },
  {
    "start": 9270222,
    "end": 9275800,
    "text": "トップKでは、くだらないトークンを選ばないようにするために、トークンを取り除くだけだ。"
  },
  {
    "start": 9275870,
    "end": 9282172,
    "text": "すべてのロジットを取って並べ替え、最も高いkだけを残す。"
  },
  {
    "start": 9282306,
    "end": 9283804,
    "text": "あれはくだらないやつだ。"
  },
  {
    "start": 9283842,
    "end": 9291340,
    "text": "この分布からそれらを取り除き、残りの分布について計算する。"
  },
  {
    "start": 9291410,
    "end": 9295600,
    "text": "生き残ったものだけにソフトマックスを適用する。"
  },
  {
    "start": 9296500,
    "end": 9309652,
    "text": "問題はここにもある。この2つの分布に従った場合、確率の低いトークンもトップkに入る可能性がある。"
  },
  {
    "start": 9309786,
    "end": 9311844,
    "text": "図解の例を挙げよう。"
  },
  {
    "start": 9312042,
    "end": 9316500,
    "text": "非常に平坦な分布があるとしよう。"
  },
  {
    "start": 9317660,
    "end": 9320488,
    "text": "仮にこの分布がここにあるとする。"
  },
  {
    "start": 9320574,
    "end": 9326040,
    "text": "これが私たちのボキャブラリーだ。"
  },
  {
    "start": 9326860,
    "end": 9328664,
    "text": "これが各単語の確率である。"
  },
  {
    "start": 9328702,
    "end": 9333224,
    "text": "番、1番、2番、3番、4番、エトセトラ、エトセトラ、エトセトラ。"
  },
  {
    "start": 9333352,
    "end": 9336568,
    "text": "多かれ少なかれ、すべての単語が同じ確率を持つ。"
  },
  {
    "start": 9336744,
    "end": 9339308,
    "text": "上位10語を想像してみよう。"
  },
  {
    "start": 9339394,
    "end": 9342524,
    "text": "これらのトークンをすべて選択するのですね？"
  },
  {
    "start": 9342722,
    "end": 9343324,
    "text": "オーケー。"
  },
  {
    "start": 9343442,
    "end": 9349570,
    "text": "トークン番号1、トークン番号2、トークン番号3、トークン番号4、そしてここにあるトークンまでが選択されます。"
  },
  {
    "start": 9349940,
    "end": 9352992,
    "text": "このような別のディストリビューションがあるとしよう。"
  },
  {
    "start": 9353046,
    "end": 9356480,
    "text": "ボキャブラリーはまだある。"
  },
  {
    "start": 9356560,
    "end": 9362756,
    "text": "我々はまだ確率分布を持っており、その分布はこのように作られている。"
  },
  {
    "start": 9362778,
    "end": 9372970,
    "text": "というのも、ソートされているため、トップ10をキープしているため、分布が非常に偏っているのだ。"
  },
  {
    "start": 9373340,
    "end": 9377252,
    "text": "ご覧のように、このトークン、このトークン、このトークン、このトークンを選択します。"
  },
  {
    "start": 9377316,
    "end": 9381784,
    "text": "このトークンは、このトークンに比べると、とてもしょぼい。"
  },
  {
    "start": 9381902,
    "end": 9385416,
    "text": "それでもセレクションには入るだろう。"
  },
  {
    "start": 9385528,
    "end": 9387404,
    "text": "これは私たちが望んでいることではない。"
  },
  {
    "start": 9387442,
    "end": 9401580,
    "text": "私たちはくだらないトークンを選択することは避けたいが、それでもある程度のランダム性は欲しい。"
  },
  {
    "start": 9401660,
    "end": 9409744,
    "text": "また、プロンプトがかなり曖昧な場合もあるので、人間であっても次に選ばれる単語がどれなのかわからないこともある。"
  },
  {
    "start": 9409792,
    "end": 9415040,
    "text": "ある程度のランダム性は欲しいが、非常に低い確率のトークンも欲しくない。"
  },
  {
    "start": 9415120,
    "end": 9421700,
    "text": "このトップケース戦略では、低確率のトークンもセレクションに入れることができる。"
  },
  {
    "start": 9422300,
    "end": 9425128,
    "text": "この問題はトップPで解決する。"
  },
  {
    "start": 9425294,
    "end": 9435230,
    "text": "トップpでは、累積確率がパラメータp以上となるような、最も高い確率を持つトークンのみを保持する。"
  },
  {
    "start": 9435680,
    "end": 9437276,
    "text": "これは何を意味するのか？"
  },
  {
    "start": 9437458,
    "end": 9449004,
    "text": "つまり、前の分布があったとして、例えば、かなり平坦な分布と最頻値を持つ分布がある。"
  },
  {
    "start": 9449052,
    "end": 9456230,
    "text": "例えば、これはほぼ90％で、他のはゼロポイントだ。"
  },
  {
    "start": 9456680,
    "end": 9462470,
    "text": "これは多かれ少なかれ、すべての選手が0.2％のようなもので、その後は下がっていく。"
  },
  {
    "start": 9463960,
    "end": 9468824,
    "text": "この場合、pが仮に0.5だとしよう。"
  },
  {
    "start": 9469022,
    "end": 9477530,
    "text": "この場合、曲線下の面積が0.5になるようなトークンをすべて選択する。"
  },
  {
    "start": 9477900,
    "end": 9489644,
    "text": "ここで、最初のトークンはすでに0.9なので、実際には1つのトークンしか選択されず、すべてのくだらないトークンは選択されない。"
  },
  {
    "start": 9489842,
    "end": 9492336,
    "text": "これがトップPの考え方だ。"
  },
  {
    "start": 9492438,
    "end": 9502912,
    "text": "分布がより平坦である場合、どのトークンを選択するかについてより不確実であることを意味するため、より多くのトークンを選択する。"
  },
  {
    "start": 9502966,
    "end": 9507840,
    "text": "大きなモードでは、少ないトークンを選択する。"
  },
  {
    "start": 9508000,
    "end": 9513988,
    "text": "こうすることで、確率の低いものを避けることができる。"
  },
  {
    "start": 9514154,
    "end": 9519944,
    "text": "トークンを選択するためのすべての戦略を見直したので、次はそれを実行に移そう。"
  },
  {
    "start": 9519982,
    "end": 9526040,
    "text": "llamaの場合、公式コードでも、実際にトップP戦略を実装している。"
  },
  {
    "start": 9526940,
    "end": 9531588,
    "text": "私の場合、ビーム・サーチは合理的な選択だと思う。"
  },
  {
    "start": 9531684,
    "end": 9538284,
    "text": "別のビデオで、ビーム・サーチの実装方法を紹介するかもしれない。"
  },
  {
    "start": 9538482,
    "end": 9539900,
    "text": "さあ、作ろう。"
  },
  {
    "start": 9539970,
    "end": 9541432,
    "text": "メソッドを実装する。"
  },
  {
    "start": 9541496,
    "end": 9548560,
    "text": "これをtextcompolationと呼ぶことにしよう。これはllamaのオリジナルのコードで使われているのと同じ名前だ。"
  },
  {
    "start": 9549700,
    "end": 9559350,
    "text": "気温が0.6度であることを示す。"
  },
  {
    "start": 9561800,
    "end": 9565460,
    "text": "0.6は、モデルの信頼性を高めたいことを意味する。"
  },
  {
    "start": 9568860,
    "end": 9578008,
    "text": "トップPとは、累積確率が少なくとも0.9になるようなトークンをすべて欲しいという意味である。"
  },
  {
    "start": 9578094,
    "end": 9579290,
    "text": "90%."
  },
  {
    "start": 9591220,
    "end": 9596044,
    "text": "よし、ここは小文字にしよう。"
  },
  {
    "start": 9596172,
    "end": 9605380,
    "text": "最大生成長を指定しなかった場合は、最大トークンを生成することになる。"
  },
  {
    "start": 9605960,
    "end": 9614580,
    "text": "最大アーギュメントは、シーケンスの長さまでトークンを生成する。"
  },
  {
    "start": 9615240,
    "end": 9642370,
    "text": "次に、まずプロンプトの各トークンを、トークナイザーを使ってトークンに変換する。"
  },
  {
    "start": 9642820,
    "end": 9680228,
    "text": "そして、前に見たように、推論用のモデルに入力を渡すときに文頭を追加する必要があるが、kvキャッシュ用にモデルを構築するときに最大バッチも指定したため、文末は追加しない。"
  },
  {
    "start": 9680244,
    "end": 9684330,
    "text": "プロンプトのバッチサイズが大きすぎないことを確認する必要がある。"
  },
  {
    "start": 9691620,
    "end": 9708240,
    "text": "最大プロンプト長は、プロンプトで使用できる最大のプロンプト長である。"
  },
  {
    "start": 9736000,
    "end": 9741940,
    "text": "でも、僕らにとっては基本的なデバッグなんだ。"
  },
  {
    "start": 9742120,
    "end": 9774440,
    "text": "次に、合計lenはモデルから取得したいトークンの数です。"
  },
  {
    "start": 9811720,
    "end": 9821240,
    "text": "これは、各項目が実際にはパディングトークンである、全長でバッチサイズのテンソルを作成することを意味する。"
  },
  {
    "start": 9822060,
    "end": 9826040,
    "text": "その後、初期トークンをプロンプトトークンで埋める。"
  },
  {
    "start": 9862280,
    "end": 9894064,
    "text": "また、どのプロンプトでも文末に達したら、この位置にあるトークンがパディングトークンかどうかを示す変数も必要である。"
  },
  {
    "start": 9894102,
    "end": 9901040,
    "text": "トークンがプロンプトトークンであればtrue、そうでなければfalse。"
  },
  {
    "start": 9902420,
    "end": 9939100,
    "text": "トークンを生成するためのforループは、1つずつ生成していく。"
  },
  {
    "start": 9939170,
    "end": 9941630,
    "text": "モデルを前進させる。"
  },
  {
    "start": 9942180,
    "end": 9944530,
    "text": "トークンを1つずつパスする必要がある。"
  },
  {
    "start": 9945620,
    "end": 9946604,
    "text": "どのトークン？"
  },
  {
    "start": 9946652,
    "end": 9956480,
    "text": "現在出力したいのは、カレントからポーズを1つ引いたものなので、トークンは1つだけだ。"
  },
  {
    "start": 9956980,
    "end": 9966890,
    "text": "また、kvキャッシュと温度を使用する場合は光を適用するため、このトークンの位置をモデルに伝える。"
  },
  {
    "start": 9986080,
    "end": 9991496,
    "text": "おわかりのように、推論するときはいつも最後のトークンを選択する。"
  },
  {
    "start": 9991608,
    "end": 9995920,
    "text": "キャリー・キャッシュを使っているため、実際にはモデルは一度に1つのトークンしか出力しない。"
  },
  {
    "start": 9995990,
    "end": 10008832,
    "text": "次のトークンは、トップP戦略に従って選択される。"
  },
  {
    "start": 10008896,
    "end": 10010240,
    "text": "確率はこうだ。"
  },
  {
    "start": 10010320,
    "end": 10019050,
    "text": "ここで、ソフトP、トップPを適用します。トップPのサンプルはここで定義します。"
  },
  {
    "start": 10022060,
    "end": 10026280,
    "text": "温度を指定しなかった場合は、欲張りな温度を使うだけだ。"
  },
  {
    "start": 10060920,
    "end": 10066660,
    "text": "さて、次のトークンは、この戦略、あるいはこの度合いに従ったものだ。"
  },
  {
    "start": 10067020,
    "end": 10074212,
    "text": "そして、トークンがパディングトークンである場合にのみ、トークンを置き換える。"
  },
  {
    "start": 10074356,
    "end": 10083148,
    "text": "問題は、プロンプトに由来するトークンをすでにいくつか持っているにもかかわらず、プロンプトをモデルに与える必要があることだ。"
  },
  {
    "start": 10083234,
    "end": 10087256,
    "text": "初期キャッシュを構築するために、モデルには一度に一つのトークンしか渡さない。"
  },
  {
    "start": 10087288,
    "end": 10101804,
    "text": "最初のプロンプトトークンがモデルに渡されるのは、そのトークンに対してモデルが何を出力するかを気にするからではなく、それらの位置に対してkvキャッシュを構築したいからにほかならない。"
  },
  {
    "start": 10101932,
    "end": 10108384,
    "text": "プロンプトの最後のトークンを与えた後、モデルが何を出力しているかが気になる。"
  },
  {
    "start": 10108512,
    "end": 10114992,
    "text": "次のトークンがパディングトークンであった場合にのみ置換されるように、どのトークンがパディングトークンなのか？"
  },
  {
    "start": 10115056,
    "end": 10118164,
    "text": "最初のプロンプトトークンではないもの。"
  },
  {
    "start": 10118212,
    "end": 10135368,
    "text": "というのも、ここではパディングでいっぱいのトークンを作るが、その後、初期トークンのプロンプトトークンをパディングトークンに置き換える。"
  },
  {
    "start": 10135454,
    "end": 10163952,
    "text": "つまり、このマスクをチェックすることだ。"
  },
  {
    "start": 10164016,
    "end": 10164864,
    "text": "このマスクは何ですか？"
  },
  {
    "start": 10164912,
    "end": 10170728,
    "text": "もしtrueなら、そのトークンがプロンプトトークンであれば、このトークンに置き換える。"
  },
  {
    "start": 10170814,
    "end": 10174730,
    "text": "プロンプトトークンでない場合は、現在のトークンのままでよい。"
  },
  {
    "start": 10184140,
    "end": 10192370,
    "text": "オーケー、それならそれでいい。"
  },
  {
    "start": 10195940,
    "end": 10209440,
    "text": "最初のプロンプトトークンに対してモデルが何を出力するかは気にせず、最後のプロンプトトークンのみを気にするので、それらのトークンの文末位置を見つけるかどうかは気にしない。"
  },
  {
    "start": 10209520,
    "end": 10254870,
    "text": "文末に到達するのは、kvキャッシュを構築するためだけにモデルに送るトークンではなく、実際に推論したいトークンの1つに対して見つけた場合のみである。"
  },
  {
    "start": 10258040,
    "end": 10270132,
    "text": "つまり、特定のプロンプトがパディングトークンであった場合にのみ、そのプロンプトの文末に到達するということだ。"
  },
  {
    "start": 10270276,
    "end": 10271972,
    "text": "パディングトークンの場合のみ。"
  },
  {
    "start": 10272036,
    "end": 10274340,
    "text": "プロンプト・トークンではなかった。"
  },
  {
    "start": 10274420,
    "end": 10283188,
    "text": "これは、モデル出力からエンドス・トークンを発見したことを意味する。"
  },
  {
    "start": 10283364,
    "end": 10290120,
    "text": "すべてのプロンプトが文末トークンに到達したら、このforループを停止する。"
  },
  {
    "start": 10290460,
    "end": 10292480,
    "text": "では、出力をペアにしてみよう。"
  },
  {
    "start": 10351380,
    "end": 10358844,
    "text": "つまり、あるプロンプトの文末トークンが見つかったら、そのプロンプトの出力をカットすればいいのだ。"
  },
  {
    "start": 10358892,
    "end": 10362172,
    "text": "その特定のトークンでのモデル出力がある。"
  },
  {
    "start": 10362236,
    "end": 10385950,
    "text": "これが出力テキストで、次にトークンとテキストを出力するのであれば、次に何を出力するかは気にしない。"
  },
  {
    "start": 10389600,
    "end": 10393592,
    "text": "誤字やミスが少なければいいのだが。"
  },
  {
    "start": 10393656,
    "end": 10397792,
    "text": "さて、サンプルのトップPを作る必要がある。"
  },
  {
    "start": 10397846,
    "end": 10400848,
    "text": "モデルの出力であるロジットがある。"
  },
  {
    "start": 10400934,
    "end": 10404188,
    "text": "ソフトマックスを使って確率に変換する。"
  },
  {
    "start": 10404284,
    "end": 10417688,
    "text": "これらの確率が与えられたら、sample top p戦略を使用して、累積確率がtop pに等しくなるようにすべてのトークンを選択する必要がある。"
  },
  {
    "start": 10417774,
    "end": 10434410,
    "text": "90％死亡のサンプルb......よし、まず最初にすることは、これらの確率を降順に並べ替えることだ。"
  },
  {
    "start": 10450340,
    "end": 10452800,
    "text": "そして、累計を計算する。"
  },
  {
    "start": 10462760,
    "end": 10468392,
    "text": "そして、どのトークンを残し、どのトークンを残したくないかを示すマスクを作る。"
  },
  {
    "start": 10468446,
    "end": 10476408,
    "text": "マスクは、確率の和からp以上の確率の並べ替えを引いたものに等しい。"
  },
  {
    "start": 10476494,
    "end": 10479388,
    "text": "なぜマイナス確率ソートをするのか？"
  },
  {
    "start": 10479474,
    "end": 10481260,
    "text": "シフトしたいからだ。"
  },
  {
    "start": 10481760,
    "end": 10487230,
    "text": "スライドでお見せしましょう。"
  },
  {
    "start": 10489760,
    "end": 10492664,
    "text": "例えば、累積確率をご覧いただきたい。"
  },
  {
    "start": 10492712,
    "end": 10499648,
    "text": "確率はこの10ポイント、444％、44％、40％、6％、4％、3％である。"
  },
  {
    "start": 10499814,
    "end": 10501420,
    "text": "そして累積を計算した。"
  },
  {
    "start": 10501500,
    "end": 10505232,
    "text": "つまり、ここまでは44％だ。"
  },
  {
    "start": 10505286,
    "end": 10507680,
    "text": "それなら、これとこれを足して85％だ。"
  },
  {
    "start": 10507750,
    "end": 10510580,
    "text": "それなら、これとこれとこれを足して91％だ。"
  },
  {
    "start": 10510650,
    "end": 10514580,
    "text": "これとこれとこれとこれで96％、などなど。"
  },
  {
    "start": 10515240,
    "end": 10524920,
    "text": "ゼロポイント990パーセントの確率、つまり0.5パーセントの確率があるとしよう。"
  },
  {
    "start": 10525900,
    "end": 10531192,
    "text": "これだけでは不十分なので、このトークンまで続ける必要がある。"
  },
  {
    "start": 10531246,
    "end": 10531896,
    "text": "ゼロ点だ。"
  },
  {
    "start": 10531918,
    "end": 10533484,
    "text": "私たちはこの一件に取り組む必要がある。"
  },
  {
    "start": 10533522,
    "end": 10539836,
    "text": "はp以下の最初の数で、これがpの場合はこれとなる。"
  },
  {
    "start": 10539858,
    "end": 10541004,
    "text": "だからシフトするんだ"
  },
  {
    "start": 10541042,
    "end": 10543360,
    "text": "私たちはこのトークンにも包括的なものを求めている。"
  },
  {
    "start": 10545620,
    "end": 10549330,
    "text": "だから、私たちはこのマイナス確率ソートを行うのです。"
  },
  {
    "start": 10551060,
    "end": 10553040,
    "text": "私たちが選ばなかったものをすべて送る。"
  },
  {
    "start": 10553110,
    "end": 10559270,
    "text": "ゼロをゼロにする。"
  },
  {
    "start": 10561480,
    "end": 10564352,
    "text": "そして、確率を再配分する。"
  },
  {
    "start": 10564416,
    "end": 10568996,
    "text": "もちろん、ここからいくつかの項目を取り除けば、合計が1ではなくなるからだ。"
  },
  {
    "start": 10569028,
    "end": 10571320,
    "text": "確率を再配分する必要がある。"
  },
  {
    "start": 10572940,
    "end": 10574970,
    "text": "これはとても簡単だ。"
  },
  {
    "start": 10591620,
    "end": 10604624,
    "text": "次のトークンは、基本的に最初の2つのトークンを保持し、そこからサンプリングする。"
  },
  {
    "start": 10604662,
    "end": 10607548,
    "text": "最初のトークンは44％の確率でゼロ点。"
  },
  {
    "start": 10607644,
    "end": 10610480,
    "text": "2つ目のトークンは、ゼロポイント40％の確率である。"
  },
  {
    "start": 10610560,
    "end": 10617270,
    "text": "それぞれの確率を再分配すると、実はこの確率は40％より少し高く、この確率は40％より少し高くなる。"
  },
  {
    "start": 10619820,
    "end": 10620852,
    "text": "それからサンプリングする。"
  },
  {
    "start": 10620916,
    "end": 10628440,
    "text": "つまり、最初のトークンは選ばれる確率が少し高く、2番目のトークンは選ばれる確率が少し低くなる。"
  },
  {
    "start": 10638960,
    "end": 10644908,
    "text": "というのも、テキスト・トークンではなく、1つのトークンが欲しいからです。"
  },
  {
    "start": 10645084,
    "end": 10651280,
    "text": "これは、どのインデックスを選択するかを示しているからである。"
  },
  {
    "start": 10651350,
    "end": 10656464,
    "text": "そして、そのインデックスを語彙の中の実際の数字に対応させる必要がある。"
  },
  {
    "start": 10656592,
    "end": 10661700,
    "text": "というのも、ソートしたために、これらの数字の順番はすでに変わってしまったからだ。"
  },
  {
    "start": 10661770,
    "end": 10677064,
    "text": "当初、ロジットは、1番目のロジックが語彙の1番目の番号に対応し、2番目のロジックが語彙の2番目の番号に対応するように構築されていたが、降順でソートしたため、この順序はなくなってしまった。"
  },
  {
    "start": 10677102,
    "end": 10677832,
    "text": "私たちは知らない。"
  },
  {
    "start": 10677886,
    "end": 10684616,
    "text": "さて、選択されたトークンが、どの番号のボキャブラリーにマッピングされるかはわからない。"
  },
  {
    "start": 10684728,
    "end": 10687496,
    "text": "そのため、sortメソッドは2つの引数を返す。"
  },
  {
    "start": 10687528,
    "end": 10691228,
    "text": "ひとつはソートされた数字、もうひとつは変更されたインデックスである。"
  },
  {
    "start": 10691314,
    "end": 10696400,
    "text": "各ポジションについて、そのポジションの元のポジションアイテムが何であったかを教えてくれる。"
  },
  {
    "start": 10696550,
    "end": 10700000,
    "text": "これが、実際にgatherを使ってクエリーを行う理由である。"
  },
  {
    "start": 10700820,
    "end": 10701756,
    "text": "集まってくれ。"
  },
  {
    "start": 10701948,
    "end": 10711270,
    "text": "このインデックスがあれば、ある要素から元の要素を取り出すことができる。"
  },
  {
    "start": 10711720,
    "end": 10720280,
    "text": "そうすれば、次のトークンを返し、その語彙に直接マッピングされる。"
  },
  {
    "start": 10721740,
    "end": 10723912,
    "text": "これでいいのだ。"
  },
  {
    "start": 10724046,
    "end": 10727544,
    "text": "では、プロンプトを作ってコードを実行してみよう。"
  },
  {
    "start": 10727662,
    "end": 10730568,
    "text": "ここにコピー＆ペーストしたプロンプトがある。"
  },
  {
    "start": 10730664,
    "end": 10734812,
    "text": "では、推論コードを作ってみよう。"
  },
  {
    "start": 10734866,
    "end": 10746708,
    "text": "だから、トークンも教科書もいらない。"
  },
  {
    "start": 10746904,
    "end": 10750000,
    "text": "最大64トークンを生成したい。"
  },
  {
    "start": 10752340,
    "end": 10757488,
    "text": "出力テキストのlenは、実際にはプロンプトのlenと等しいことを保証する。"
  },
  {
    "start": 10757584,
    "end": 10760980,
    "text": "レンジ内ではそうあるべきだ。"
  },
  {
    "start": 10764760,
    "end": 10767190,
    "text": "このモデルがうまくいくことを願っている。"
  },
  {
    "start": 10775100,
    "end": 10782904,
    "text": "次に、各プロンプトの出力テキストを表示する。"
  },
  {
    "start": 10783032,
    "end": 10786350,
    "text": "コードを実行し、最善を祈ろう。"
  },
  {
    "start": 10788960,
    "end": 10793984,
    "text": "なるほど、自己注意は必要な前進機能が欠けている。"
  },
  {
    "start": 10794182,
    "end": 10795410,
    "text": "その理由を見てみよう。"
  },
  {
    "start": 10804660,
    "end": 10806064,
    "text": "おっと、前方だ。"
  },
  {
    "start": 10806182,
    "end": 10807810,
    "text": "それは前方であるべきだ。"
  },
  {
    "start": 10808820,
    "end": 10810210,
    "text": "もう一度走ろう。"
  },
  {
    "start": 10813230,
    "end": 10814650,
    "text": "受取額"
  },
  {
    "start": 10815950,
    "end": 10824010,
    "text": "これはdivではなくdimであるべきなので間違っている。"
  },
  {
    "start": 10825170,
    "end": 10834390,
    "text": "もう一度、bフロート16で走ってみよう。"
  },
  {
    "start": 10836490,
    "end": 10838120,
    "text": "その理由を見てみよう。"
  },
  {
    "start": 10841230,
    "end": 10844630,
    "text": "Eosトークン。"
  },
  {
    "start": 10844710,
    "end": 10845820,
    "text": "確認してみよう。"
  },
  {
    "start": 10857960,
    "end": 10860036,
    "text": "よし、次はトレーニングだ。"
  },
  {
    "start": 10860138,
    "end": 10864916,
    "text": "私はこのテンソルを大文字のtから小文字のtに変えただけだ。"
  },
  {
    "start": 10865098,
    "end": 10866950,
    "text": "その理由を調査するつもりだ。"
  },
  {
    "start": 10869910,
    "end": 10872422,
    "text": "さて、出力ができたので確認してみよう。"
  },
  {
    "start": 10872556,
    "end": 10874370,
    "text": "まず、プロンプトを確認しよう。"
  },
  {
    "start": 10874530,
    "end": 10887386,
    "text": "簡単に言えば、相対性理論では、時間は観測者にとって相対的なものであり、質量は観測者にとって相対的なものであり、速度は観測者にとって相対的なものであり、エネルギーは観測者にとって相対的なものであるとしている。"
  },
  {
    "start": 10887488,
    "end": 10889740,
    "text": "見た目は悪くない。"
  },
  {
    "start": 10890350,
    "end": 10901194,
    "text": "もしグーグルがミラノに本社を置くイタリア企業だとしたら、ミラノ証券取引所に上場するだろう。"
  },
  {
    "start": 10901242,
    "end": 10907198,
    "text": "グーグルは米国企業であり、ナスダック証券取引所に上場している。"
  },
  {
    "start": 10907364,
    "end": 10908874,
    "text": "フーゾットのプロンプトを試してみよう。"
  },
  {
    "start": 10908922,
    "end": 10914574,
    "text": "これは、私が実際にラマコードからコピーしたので、彼らは英語からフランス語に翻訳するように頼む。"
  },
  {
    "start": 10914622,
    "end": 10919790,
    "text": "チーズの次はフロマージュ、オニオン。"
  },
  {
    "start": 10919870,
    "end": 10921326,
    "text": "玉ねぎ、などなど。"
  },
  {
    "start": 10921358,
    "end": 10922518,
    "text": "それが正しいように見える。"
  },
  {
    "start": 10922684,
    "end": 10926354,
    "text": "スペースが確保されていることもわかる。"
  },
  {
    "start": 10926482,
    "end": 10933350,
    "text": "これらのスペースは私が追加したのではなく、実際にモデルが追加したものだ。"
  },
  {
    "start": 10933930,
    "end": 10936454,
    "text": "そして、ゼロショットのプロンプトを作成した。"
  },
  {
    "start": 10936502,
    "end": 10940170,
    "text": "次の人物が本当に人間に変装したライモンドなのかどうか教えてください。"
  },
  {
    "start": 10940240,
    "end": 10945434,
    "text": "その名はウマル・ジャミルで、彼はあらゆる意味でヒーローだ。"
  },
  {
    "start": 10945472,
    "end": 10947438,
    "text": "彼はあらゆる意味でヒーローだ。"
  },
  {
    "start": 10947604,
    "end": 10950382,
    "text": "私はとても幸せだ。"
  },
  {
    "start": 10950436,
    "end": 10953822,
    "text": "さて、これが手動シードゼロのモデルの出力である。"
  },
  {
    "start": 10953876,
    "end": 10962258,
    "text": "シードを他の数字に変えてモデルを再実行すれば、出力はまったく違うものになるか、あるいはわずかに違うものになるだろう。"
  },
  {
    "start": 10962344,
    "end": 10964338,
    "text": "そうでないことを願うが、違うかもしれない。"
  },
  {
    "start": 10964504,
    "end": 10967250,
    "text": "とにかく、僕のビデオを見てくれてありがとう。"
  },
  {
    "start": 10967400,
    "end": 10980258,
    "text": "トレーニングコードを構築するのはかなり複雑なので、トレーニングコードを構築しなかったとしても、llamaの内部のアーキテクチャがどのようなものであるかのアイデアを伝えようとしました。"
  },
  {
    "start": 10980434,
    "end": 10987420,
    "text": "膨大なテキストコーパスが必要で、それをトークン化する必要がある。"
  },
  {
    "start": 10988270,
    "end": 10997066,
    "text": "将来的には、言語モデルの学習方法について、より少ないデータセットと、より軽量なアーキテクチャで、別のビデオを作りたいと思っている。"
  },
  {
    "start": 10997258,
    "end": 11008790,
    "text": "私は、すべての選択肢の背後にあるすべての数学と、KVキャッシュの内部動作とグループ化されたクエリの注意を伝えようとした。"
  },
  {
    "start": 11008970,
    "end": 11011470,
    "text": "質問があれば、コメントに書いてください。"
  },
  {
    "start": 11011550,
    "end": 11020882,
    "text": "また、私が以前このために作成した、ここに書いたものよりもはるかにコメントの多いコードもリポジトリで共有します。"
  },
  {
    "start": 11021016,
    "end": 11026754,
    "text": "もっと詳しく書かれているので、誰もがここに関係するすべての次元をステップ・バイ・ステップで理解することができる。"
  },
  {
    "start": 11026792,
    "end": 11031980,
    "text": "最も重要な次元を書こうとしたが、時間の関係で全部は書けなかった。"
  },
  {
    "start": 11032910,
    "end": 11034682,
    "text": "見てくれてありがとう。"
  },
  {
    "start": 11034736,
    "end": 11038410,
    "text": "長い旅だったが、多くのことを学んだと断言できる。"
  },
  {
    "start": 11038480,
    "end": 11048598,
    "text": "ディープラーニングについて、Pytorchについて、コーディングについて、そして私たちがAIで愛しているものすべてについて、もっと多くのビデオを見るために、また私のチャンネルを訪れてくれることを願っています。"
  },
  {
    "start": 11048694,
    "end": 11049898,
    "text": "見てくれてありがとう。"
  },
  {
    "start": 11049984,
    "end": 11050600,
    "text": "良い一日を。"
  }
]