[
  {
    "start": 810,
    "end": 1694,
    "text": "皆さん、こんにちは。"
  },
  {
    "start": 1892,
    "end": 5034,
    "text": "今日もマケモアの導入を続けている。"
  },
  {
    "start": 5162,
    "end": 15070,
    "text": "さて、前回の講義では、Bigram言語モデルを実装し、カウントを使った実装と、1つの線形層を持つ超シンプルなニューラルネットワークを使った実装の両方を行った。"
  },
  {
    "start": 15490,
    "end": 29026,
    "text": "さて、これは前回の講義で作成したJupyterノートブックですが、私たちのアプローチ方法は、1つ前の文字だけを見て、次に続く文字の分布を予測するというものでした。"
  },
  {
    "start": 29138,
    "end": 36360,
    "text": "そのために、カウントをとって確率に正規化し、各行の合計が1になるようにした。"
  },
  {
    "start": 36730,
    "end": 43686,
    "text": "今までの文脈が1文字しかなく、これがうまく機能し、親しみやすいのであれば、これはこれで結構なことだ。"
  },
  {
    "start": 43798,
    "end": 51462,
    "text": "もちろん、このモデルの問題点は、文脈を一文字しかとらないため、このモデルからの予測があまりよくないことだ。"
  },
  {
    "start": 51606,
    "end": 55200,
    "text": "このモデルからは、あまりナメた音は出なかった。"
  },
  {
    "start": 56130,
    "end": 64702,
    "text": "しかし、このアプローチの問題点は、次の登場人物を予測する際に、より多くの文脈を考慮に入れようとすると、事態が一気に爆発してしまうことだ。"
  },
  {
    "start": 64756,
    "end": 70958,
    "text": "このテーブルのサイズは、コンテキストの長さに応じて指数関数的に大きくなる。"
  },
  {
    "start": 71134,
    "end": 75710,
    "text": "というのも、一度に一人のキャラクターを取り上げるだけなら、27のコンテクストの可能性があることになるからだ。"
  },
  {
    "start": 75870,
    "end": 85302,
    "text": "過去に登場した2人のキャラクターから3人目のキャラクターを予測しようとすると、この行列の行数は突然、27×27になる。"
  },
  {
    "start": 85356,
    "end": 89926,
    "text": "この文脈で何が出てくるか、729の可能性がある。"
  },
  {
    "start": 90118,
    "end": 97286,
    "text": "仮に3文字を文脈とすると、突然、2万通りの文脈の可能性が出てくる。"
  },
  {
    "start": 97478,
    "end": 101238,
    "text": "だから、この行列の行数は多すぎる。"
  },
  {
    "start": 101334,
    "end": 108320,
    "text": "それぞれの可能性のカウント数が少なすぎるし、全体が爆発してしまって、あまりうまく機能していない。"
  },
  {
    "start": 108930,
    "end": 118142,
    "text": "というわけで、今日はこの箇条書きに移り、シーケンスの次の文字を予測するための多層パーセプトロン・モデルを実装します。"
  },
  {
    "start": 118286,
    "end": 123700,
    "text": "私たちが採用するこのモデリング・アプローチは、Benjuatal 2003という論文に従ったものである。"
  },
  {
    "start": 124230,
    "end": 126210,
    "text": "新聞はここにある。"
  },
  {
    "start": 126360,
    "end": 138070,
    "text": "この論文は、マルチリオペセプトロンやニューラルネットワークを使って次の文字やトークンを予測することを提案した最初の論文ではないが、当時大きな影響力を持った論文であることは間違いない。"
  },
  {
    "start": 138140,
    "end": 142626,
    "text": "この考えを代弁するために引用されることが非常に多いが、とてもいい文章だと思う。"
  },
  {
    "start": 142668,
    "end": 146614,
    "text": "というわけで、まずはこの論文を見て、それから実行に移そうと思っている。"
  },
  {
    "start": 146742,
    "end": 148374,
    "text": "さて、この論文は19ページある。"
  },
  {
    "start": 148422,
    "end": 153082,
    "text": "この論文について詳しく説明する時間はないが、ぜひ読んでいただきたい。"
  },
  {
    "start": 153216,
    "end": 156846,
    "text": "とても読みやすく、興味深く、興味深いアイデアがたくさん詰まっている。"
  },
  {
    "start": 157028,
    "end": 160394,
    "text": "イントロダクションの中で、彼らは今私が説明したのとまったく同じ問題を説明している。"
  },
  {
    "start": 160522,
    "end": 163520,
    "text": "そこで、彼らは次のようなモデルを提案した。"
  },
  {
    "start": 163890,
    "end": 167922,
    "text": "今、私たちは文字レベルの言語モデルを構築していることを心に留めておいてほしい。"
  },
  {
    "start": 167976,
    "end": 170866,
    "text": "私たちはこの論文で登場人物のレベルに取り組んでいる。"
  },
  {
    "start": 170968,
    "end": 176754,
    "text": "彼らは17,000語の語彙を持ち、その代わりに単語レベルの言語モデルを構築する。"
  },
  {
    "start": 176872,
    "end": 180790,
    "text": "キャラクターにはこだわるつもりだが、同じモデリング・アプローチを取るつもりだ。"
  },
  {
    "start": 181290,
    "end": 192170,
    "text": "さて、彼らがやっていることは、基本的には、17,000語の単語をひとつひとつ取り出して、それぞれの単語に30次元の特徴ベクトルを関連づけるというものだ。"
  },
  {
    "start": 192670,
    "end": 197690,
    "text": "すべての単語が30次元の空間に埋め込まれたのだ。"
  },
  {
    "start": 197760,
    "end": 199034,
    "text": "そう考えることもできる。"
  },
  {
    "start": 199152,
    "end": 203678,
    "text": "30次元空間に17,000の点またはベクトルがある。"
  },
  {
    "start": 203844,
    "end": 206506,
    "text": "混雑しているのは想像がつくだろう。"
  },
  {
    "start": 206538,
    "end": 208480,
    "text": "狭いスペースにたくさんの点数だ。"
  },
  {
    "start": 208930,
    "end": 214506,
    "text": "さて、最初の段階では、これらの単語は完全にランダムに初期化されるため、ランダムに広がっていく。"
  },
  {
    "start": 214698,
    "end": 219582,
    "text": "次に、バックプロパゲーションを使って単語の埋め込みを調整する。"
  },
  {
    "start": 219726,
    "end": 225650,
    "text": "ニューラルネットワークのトレーニングの過程で、これらの点またはベクトルは基本的にこの空間を動き回ることになる。"
  },
  {
    "start": 225800,
    "end": 233974,
    "text": "例えば、非常によく似た意味を持つ単語や、同義語である単語が、非常によく似たスペースに配置されることを想像するかもしれない。"
  },
  {
    "start": 234092,
    "end": 238440,
    "text": "逆に、まったく異なる意味を持つ単語は、スペースの別の場所に置かれることになる。"
  },
  {
    "start": 239290,
    "end": 242578,
    "text": "さて、それ以外の点では、彼らのモデリング・アプローチは我々と同じである。"
  },
  {
    "start": 242674,
    "end": 247178,
    "text": "彼らは多層ニューラルネットワークを使って、前の単語から次の単語を予測している。"
  },
  {
    "start": 247344,
    "end": 252662,
    "text": "ニューラルネットワークを訓練するために、私たちと同じように訓練データの対数尤度を最大化しているのだ。"
  },
  {
    "start": 252816,
    "end": 255594,
    "text": "モデリングのアプローチ自体は同じである。"
  },
  {
    "start": 255722,
    "end": 258190,
    "text": "さて、この直感を具体的に示す例がある。"
  },
  {
    "start": 258770,
    "end": 260078,
    "text": "なぜうまくいくのか？"
  },
  {
    "start": 260244,
    "end": 265230,
    "text": "基本的には、例えば、犬が空白の中を走っているのを予測しようとしているとする。"
  },
  {
    "start": 265570,
    "end": 271602,
    "text": "ここで、「犬が走っていた」というフレーズが訓練データに一度も出てこなかったとする。"
  },
  {
    "start": 271736,
    "end": 281170,
    "text": "モデルがどこかに配備され、文章を作ろうとしているときだ。"
  },
  {
    "start": 281330,
    "end": 293834,
    "text": "というのも、トレーニングセットでこの正確なフレーズに遭遇したことがないからだ。"
  },
  {
    "start": 294032,
    "end": 300522,
    "text": "というのも、もしかしたら、犬が何かの中で走っているフレーズを正確に見ていなかったかもしれないからだ。"
  },
  {
    "start": 300656,
    "end": 302390,
    "text": "似たようなフレーズを見たことがあるかもしれない。"
  },
  {
    "start": 302470,
    "end": 306074,
    "text": "もしかしたら、「犬は空白の時間を走っていた」というフレーズを見たことがあるかもしれない。"
  },
  {
    "start": 306202,
    "end": 311790,
    "text": "もしかしたら、あなたのネットワークはaとtheが頻繁に入れ替わることを学習したのかもしれない。"
  },
  {
    "start": 311860,
    "end": 318274,
    "text": "ということは、aの埋め込みとtheの埋め込みを取り込んで、実際に空間の中で互いの近くに配置したのだろう。"
  },
  {
    "start": 318392,
    "end": 323250,
    "text": "だから、その埋め込みによって知識を伝達することができるし、そのようにして一般化することができる。"
  },
  {
    "start": 323400,
    "end": 329554,
    "text": "同様に、ネットワークはネコとイヌが動物であり、非常によく似た文脈で出現することを知っている可能性がある。"
  },
  {
    "start": 329682,
    "end": 341974,
    "text": "だから、この正確なフレーズを見たことがなくても、あるいは正確に歩いたり走ったりするのを見たことがなくても、埋め込み空間を通して知識を伝達し、新しいシナリオに一般化することができる。"
  },
  {
    "start": 342102,
    "end": 344822,
    "text": "では、ニューラルネットワークの図までスクロールしてみよう。"
  },
  {
    "start": 344966,
    "end": 346940,
    "text": "ここに素敵な図がある。"
  },
  {
    "start": 347310,
    "end": 355310,
    "text": "この例では、3つ前の単語から4つ目の単語を予測しようとしている。"
  },
  {
    "start": 355890,
    "end": 362714,
    "text": "さて、先ほどの3つの単語だが、17,000語の語彙がある。"
  },
  {
    "start": 362852,
    "end": 369074,
    "text": "これらはすべて、基本的に入力される単語のインデックスである。"
  },
  {
    "start": 369272,
    "end": 376420,
    "text": "17,000語あるので、これは0から16,999までの整数となる。"
  },
  {
    "start": 377210,
    "end": 380694,
    "text": "さて、cと呼ばれるルックアップテーブルもある。"
  },
  {
    "start": 380892,
    "end": 385400,
    "text": "このルックアップテーブルは、例えば17,000×30の行列である。"
  },
  {
    "start": 385850,
    "end": 389526,
    "text": "基本的にここでやっていることは、これをルックアップテーブルとして扱うということだ。"
  },
  {
    "start": 389638,
    "end": 402080,
    "text": "そのため、各インデックスは、その単語の埋め込みベクトルに対応する30次元のベクトルに変換されるように、この埋め込み行列の行を抜き出すことになる。"
  },
  {
    "start": 402850,
    "end": 410160,
    "text": "ここでは、3つの単語に対して30ニューロン、合計90ニューロンの入力層がある。"
  },
  {
    "start": 410610,
    "end": 414738,
    "text": "ここで彼らは、このマトリックスCはすべての単語で共有されていると言っているのだ。"
  },
  {
    "start": 414824,
    "end": 420980,
    "text": "ということは、これらの単語のそれぞれについて、常に同じ行列Cに何度もインデックスを付けていることになる。"
  },
  {
    "start": 421910,
    "end": 424494,
    "text": "次はこのニューラルネットワークの隠れ層だ。"
  },
  {
    "start": 424622,
    "end": 429086,
    "text": "このニューラルネットの隠れ神経層のサイズがハイパーパラメータである。"
  },
  {
    "start": 429198,
    "end": 433590,
    "text": "ハイパーパラメーターという言葉を使うのは、ニューラルネットの設計者が自由に選択できるようにするためだ。"
  },
  {
    "start": 433660,
    "end": 436854,
    "text": "これは、好きなだけ大きくしてもいいし、小さくしてもいい。"
  },
  {
    "start": 436892,
    "end": 439046,
    "text": "だから、例えば、サイズは100かもしれない。"
  },
  {
    "start": 439228,
    "end": 445580,
    "text": "この隠れ層のサイズについて複数の選択肢を検討し、それらがどの程度機能するかを評価する。"
  },
  {
    "start": 445950,
    "end": 448074,
    "text": "ここに100のニューロンがあったとしよう。"
  },
  {
    "start": 448192,
    "end": 455534,
    "text": "そのすべてが、この3つの単語を構成する90の単語、あるいは90の数字と完全に結びついている。"
  },
  {
    "start": 455732,
    "end": 457802,
    "text": "これは完全連結層である。"
  },
  {
    "start": 457946,
    "end": 461898,
    "text": "それから10時間ほどのレイヤーがあり、そしてこの出力レイヤーがある。"
  },
  {
    "start": 461994,
    "end": 474130,
    "text": "というのも、次に来る可能性のある単語は1万7,000個あり、この層には1万7,000個のニューロンがあり、そのすべてが隠れ層のこれらのニューロンすべてに完全に接続されているからだ。"
  },
  {
    "start": 474630,
    "end": 478038,
    "text": "ここにはたくさんの言葉があるから、たくさんのパラメーターがある。"
  },
  {
    "start": 478204,
    "end": 479574,
    "text": "ほとんどの計算はここにある。"
  },
  {
    "start": 479612,
    "end": 481190,
    "text": "これは高価な層だ。"
  },
  {
    "start": 481610,
    "end": 484374,
    "text": "今、ここには17,000のロジットがある。"
  },
  {
    "start": 484492,
    "end": 489158,
    "text": "その上にソフトマックス・レイヤーがある。"
  },
  {
    "start": 489244,
    "end": 499530,
    "text": "これらのロジットが指数化され、すべての和が1になるように正規化される。"
  },
  {
    "start": 499870,
    "end": 505390,
    "text": "もちろん、トレーニング中は、ラベル、シーケンス内の次の単語の識別情報を持っている。"
  },
  {
    "start": 505730,
    "end": 512526,
    "text": "その単語、あるいはそのインデックスを使って、その単語の確率を抜き出す。"
  },
  {
    "start": 512708,
    "end": 519650,
    "text": "この場合、ニューラルネットのパラメータに関して、その単語の確率を最大化することになる。"
  },
  {
    "start": 519800,
    "end": 529046,
    "text": "パラメータは、出力層の重みとバイアス、隠れ層の重みとバイアス、埋め込みルックアップテーブルCである。"
  },
  {
    "start": 529148,
    "end": 531830,
    "text": "そのすべてがバックプロパゲーションを使って最適化される。"
  },
  {
    "start": 532330,
    "end": 539686,
    "text": "これらの破線の矢印は、ニューラルネットのバリエーションを示すもので、このビデオでは説明しない。"
  },
  {
    "start": 539868,
    "end": 542330,
    "text": "では、それを実装してみよう。"
  },
  {
    "start": 542480,
    "end": 545430,
    "text": "それで、この講義のために新しいノートを作ったんだ。"
  },
  {
    "start": 545590,
    "end": 550410,
    "text": "Pytorchをインポートし、図を作成できるようにmatplotlibをインポートしています。"
  },
  {
    "start": 550750,
    "end": 557600,
    "text": "そして、先ほどと同じように、すべての名前を単語のリストに読み込んでいる。"
  },
  {
    "start": 558050,
    "end": 560974,
    "text": "合計で3万2,000人いることを覚えておいてほしい。"
  },
  {
    "start": 561092,
    "end": 562606,
    "text": "これらは最初の8つに過ぎない。"
  },
  {
    "start": 562788,
    "end": 570770,
    "text": "そしてここでは、文字の語彙と、文字列としての文字から整数へのマッピング、あるいはその逆を構築している。"
  },
  {
    "start": 571270,
    "end": 575374,
    "text": "さて、最初にやりたいことは、ニューラルネットワーク用のデータセットをコンパイルすることだ。"
  },
  {
    "start": 575502,
    "end": 577538,
    "text": "私はこのコードを書き直さなければならなかった。"
  },
  {
    "start": 577704,
    "end": 579800,
    "text": "どんなものか、すぐにお見せしましょう。"
  },
  {
    "start": 581050,
    "end": 584146,
    "text": "これはデータセット作成のために作成したコードです。"
  },
  {
    "start": 584178,
    "end": 587640,
    "text": "まずそれを実行させてから、この仕組みについて簡単に説明しよう。"
  },
  {
    "start": 588490,
    "end": 596874,
    "text": "これは基本的に、次の文字を予測するのに何文字かかるかというコンテキストの長さだ。"
  },
  {
    "start": 596992,
    "end": 600698,
    "text": "この例では、3つの文字を使って4つ目の文字を予測している。"
  },
  {
    "start": 600784,
    "end": 602414,
    "text": "ブロックサイズは3。"
  },
  {
    "start": 602452,
    "end": 605950,
    "text": "これが予測を支えるブロックの大きさだ。"
  },
  {
    "start": 606610,
    "end": 609870,
    "text": "そして、ここでXとYを組み立てている。"
  },
  {
    "start": 610020,
    "end": 616980,
    "text": "xはニューラルネットへの入力で、yはx内の各例に対するラベルである。"
  },
  {
    "start": 617510,
    "end": 619858,
    "text": "そして、最初の5単語を反復する。"
  },
  {
    "start": 619944,
    "end": 623858,
    "text": "すべてのコードを開発している間、効率化のために最初の5つをやっている。"
  },
  {
    "start": 623944,
    "end": 628280,
    "text": "そして後で、ここに来てこれを消去し、トレーニングセット全体を使うようにする。"
  },
  {
    "start": 629050,
    "end": 640810,
    "text": "ここではEmmaという単語を印刷して、基本的にEmmaという単語から生成できる5つの例を示しています。"
  },
  {
    "start": 641150,
    "end": 647466,
    "text": "ドット、ドット、ドットという文脈が与えられた場合、最初の文字はeとなる。"
  },
  {
    "start": 647648,
    "end": 650138,
    "text": "この文脈では、ラベルはmである。"
  },
  {
    "start": 650304,
    "end": 654258,
    "text": "文脈がこれであれば、ラベルはm、といった具合だ。"
  },
  {
    "start": 654374,
    "end": 658990,
    "text": "まず、トークンがゼロのパディングされたコンテキストから始める。"
  },
  {
    "start": 659730,
    "end": 661674,
    "text": "それから、全キャラクターを反復処理する。"
  },
  {
    "start": 661802,
    "end": 671598,
    "text": "シーケンス内の文字を取得し、基本的にはこの現在の文字を配列yとし、現在の実行中のコンテキストを格納する配列xを構築する。"
  },
  {
    "start": 671774,
    "end": 673746,
    "text": "それで、ほら、全部印刷するんだ。"
  },
  {
    "start": 673848,
    "end": 678542,
    "text": "ここで文脈を切り取り、新しい文字をシーケンスに入力する。"
  },
  {
    "start": 678606,
    "end": 681350,
    "text": "これはコンテクストのローリングウィンドウのようなものだ。"
  },
  {
    "start": 682650,
    "end": 685526,
    "text": "ここで、ブロックサイズを例えば4つに変更することができる。"
  },
  {
    "start": 685708,
    "end": 690070,
    "text": "この場合、前の4文字から5文字目を予測することになる。"
  },
  {
    "start": 690220,
    "end": 693340,
    "text": "あるいは5つで、次のようになる。"
  },
  {
    "start": 693870,
    "end": 697946,
    "text": "あるいは、たとえば10であれば、次のようになる。"
  },
  {
    "start": 697968,
    "end": 703010,
    "text": "10文字で11文字目を予測し、常にドットで水増ししている。"
  },
  {
    "start": 703190,
    "end": 709040,
    "text": "この新聞に書いてあることを確認するために、これを3つに戻します。"
  },
  {
    "start": 709970,
    "end": 712670,
    "text": "最後に、現在のデータセットは以下のようになっている。"
  },
  {
    "start": 713410,
    "end": 717726,
    "text": "これら5つの単語から、32の例からなるデータセットを作成した。"
  },
  {
    "start": 717918,
    "end": 720926,
    "text": "ニューラルネットへの各入力は3つの整数である。"
  },
  {
    "start": 721038,
    "end": 724450,
    "text": "ラベルも整数である。"
  },
  {
    "start": 724600,
    "end": 731030,
    "text": "xはこんな感じで、これが個々の例、そしてyがラベルだ。"
  },
  {
    "start": 732410,
    "end": 739282,
    "text": "これを前提に、xを受け取ってyを予測するニューラルネットワークを書いてみよう。"
  },
  {
    "start": 739346,
    "end": 742380,
    "text": "まず、埋め込みルックアップテーブルcを作ってみよう。"
  },
  {
    "start": 743150,
    "end": 755662,
    "text": "この論文では、17,000の単語があり、30という小さな次元の空間に埋め込まれている。"
  },
  {
    "start": 755796,
    "end": 760382,
    "text": "30次元の空間に17,000語を詰め込んでいるのだ。"
  },
  {
    "start": 760516,
    "end": 763306,
    "text": "私たちの場合、可能な文字は27文字しかない。"
  },
  {
    "start": 763418,
    "end": 768274,
    "text": "例えば、2次元空間から始めるような小さなものにそれらを詰め込もう。"
  },
  {
    "start": 768472,
    "end": 776454,
    "text": "このルックアップテーブルはランダムな数字で、27行、2列あるよね？"
  },
  {
    "start": 776492,
    "end": 781350,
    "text": "27文字のそれぞれが2次元の埋め込みを持つことになる。"
  },
  {
    "start": 782010,
    "end": 787714,
    "text": "これが埋め込み行列Cで、ランダムに初期化される。"
  },
  {
    "start": 787842,
    "end": 801594,
    "text": "さて、このルックアップテーブルCを使って入力xの中にすべての整数を埋め込む前に、実際に1つの整数を埋め込んでみよう。"
  },
  {
    "start": 801792,
    "end": 811454,
    "text": "さて、これがうまくいく方法のひとつは、もちろん、cをとって5行目にインデックスを作れば、Cの5行目のベクトルが得られるということだ。"
  },
  {
    "start": 811652,
    "end": 814622,
    "text": "これもひとつの方法だ。"
  },
  {
    "start": 814756,
    "end": 820542,
    "text": "前回の講義で紹介したもうひとつの方法は、一見違うようで実は同じなのだ。"
  },
  {
    "start": 820686,
    "end": 826786,
    "text": "前回の講義では、これらの整数をまず1ホットエンコーディングでエンコードした。"
  },
  {
    "start": 826888,
    "end": 828514,
    "text": "fドット、1ホット。"
  },
  {
    "start": 828712,
    "end": 833670,
    "text": "整数5をエンコードし、クラス数が27であることを伝えたい。"
  },
  {
    "start": 833740,
    "end": 839100,
    "text": "これは26次元のベクトルで、5ビット目がオンになっている以外はすべてゼロである。"
  },
  {
    "start": 839950,
    "end": 842460,
    "text": "しかし、これは実際には機能しない。"
  },
  {
    "start": 843070,
    "end": 846970,
    "text": "なぜなら、この入力は実際にはトーチテンソルでなければならないからだ。"
  },
  {
    "start": 847870,
    "end": 852538,
    "text": "いくつかのエラーとその修正方法を知ってもらうために、私は意図的にこれらのエラーをいくつか作っている。"
  },
  {
    "start": 852704,
    "end": 854382,
    "text": "これはintではなくテンソルでなければならない。"
  },
  {
    "start": 854436,
    "end": 856030,
    "text": "修正するのは簡単だ。"
  },
  {
    "start": 856180,
    "end": 861520,
    "text": "ホットなベクトルが1つ、5番目の次元が1つ、そしてこの形状は27である。"
  },
  {
    "start": 862210,
    "end": 876920,
    "text": "前のビデオで少し触れたように、この1つのホットベクトルにcを掛けると、どうなるだろうか？"
  },
  {
    "start": 877370,
    "end": 887682,
    "text": "まず、スカラー型のlongを期待していたのにfloatが見つかったので、少し混乱した。"
  },
  {
    "start": 887746,
    "end": 893754,
    "text": "ここでの問題は、ホットなデータ型がlongであることだ。"
  },
  {
    "start": 893872,
    "end": 897654,
    "text": "これは64ビットの整数だが、これはfloatテンソルだ。"
  },
  {
    "start": 897702,
    "end": 901242,
    "text": "だから、Pytorchはintとfloatの掛け算を知らない。"
  },
  {
    "start": 901306,
    "end": 905950,
    "text": "そのため、乗算できるように明示的にfloatにキャストする必要があった。"
  },
  {
    "start": 906290,
    "end": 915058,
    "text": "さて、ここでの出力は実際には同一であり、行列の乗算が機能するため同一である。"
  },
  {
    "start": 915144,
    "end": 919762,
    "text": "Cの列を乗算する1つのホットファクターがある。"
  },
  {
    "start": 919896,
    "end": 926740,
    "text": "というのも、すべてのゼロがあるため、実際には、5列目が抜かれる以外は、Cのすべてをマスクしてしまうからだ。"
  },
  {
    "start": 927190,
    "end": 929606,
    "text": "だから、実際には同じ結果になる。"
  },
  {
    "start": 929788,
    "end": 935586,
    "text": "ということは、この最初の部分、つまり整数の埋め込みを解釈することができるということだ。"
  },
  {
    "start": 935698,
    "end": 939762,
    "text": "これは、ルックアップテーブルcへの整数インデックスと考えることもできる。"
  },
  {
    "start": 939916,
    "end": 946186,
    "text": "同様に、この小さな断片を、より大きなニューラルネットの第1層と考えることもできる。"
  },
  {
    "start": 946368,
    "end": 954800,
    "text": "このレイヤーには、非線形性を持たないニューロンがあり、10個のhもない、ただの線形ニューロンで、重み行列はcである。"
  },
  {
    "start": 955410,
    "end": 960030,
    "text": "ということは、整数を1つのホットにエンコードし、それをニューラルネットに入力していることになる。"
  },
  {
    "start": 960100,
    "end": 962510,
    "text": "この最初のレイヤーは、基本的にそれらを埋め込む。"
  },
  {
    "start": 962660,
    "end": 965486,
    "text": "これらは同じことをする2つの等価な方法である。"
  },
  {
    "start": 965588,
    "end": 973262,
    "text": "その方がはるかに速いので、インデックスを作成し、ニューラルネットへの1つのホットな入力の解釈を捨てることにする。"
  },
  {
    "start": 973326,
    "end": 977330,
    "text": "ここでは、整数のインデックスを作成し、埋め込みテーブルを作成して使用するだけだ。"
  },
  {
    "start": 977410,
    "end": 980502,
    "text": "5という整数を埋め込むのは簡単だ。"
  },
  {
    "start": 980636,
    "end": 987000,
    "text": "Pytorchにcの5行目、またはCの行インデックス5を取得するよう依頼すればよい。"
  },
  {
    "start": 987690,
    "end": 994060,
    "text": "配列xに格納された32×3の整数を同時に埋め込むには？"
  },
  {
    "start": 994670,
    "end": 998022,
    "text": "幸いなことに、Pytorchのインデックス付けはかなり柔軟で、かなり強力だ。"
  },
  {
    "start": 998166,
    "end": 1006250,
    "text": "このように5つの要素を求めるだけでなく、リストを使ってインデックスを作成することもできる。"
  },
  {
    "start": 1006330,
    "end": 1009342,
    "text": "例えば、5行目、6行目、7行目を得ることができる。"
  },
  {
    "start": 1009476,
    "end": 1011614,
    "text": "このように動作する。"
  },
  {
    "start": 1011652,
    "end": 1013200,
    "text": "リストでインデックスを作ることができる。"
  },
  {
    "start": 1013730,
    "end": 1020354,
    "text": "それはリストである必要はなく、実際には整数のテンソルであることもでき、それを使ってインデックスを作ることができる。"
  },
  {
    "start": 1020552,
    "end": 1025460,
    "text": "これは5、6、7の整数テンソルであり、これも同様に機能する。"
  },
  {
    "start": 1025990,
    "end": 1030790,
    "text": "実際、たとえば7行目を繰り返して何度も取り出すこともできる。"
  },
  {
    "start": 1030940,
    "end": 1035720,
    "text": "同じインデックスがここに何度も埋め込まれることになる。"
  },
  {
    "start": 1036090,
    "end": 1040598,
    "text": "ここでは、整数の1次元テンソルでインデックスを作っている。"
  },
  {
    "start": 1040694,
    "end": 1045142,
    "text": "整数の多次元テンソルでもインデックスを作成できることがわかった。"
  },
  {
    "start": 1045206,
    "end": 1048518,
    "text": "ここでは整数の2次元テンソルがある。"
  },
  {
    "start": 1048614,
    "end": 1053680,
    "text": "をxの位置でcとするだけでいい。"
  },
  {
    "start": 1054530,
    "end": 1060010,
    "text": "この形は32×3であり、これが本来の形である。"
  },
  {
    "start": 1060090,
    "end": 1065220,
    "text": "この32×3の整数のひとつひとつについて、埋め込みベクトルを求めた。"
  },
  {
    "start": 1065910,
    "end": 1073570,
    "text": "基本的には、13番目、つまり例のインデックス13を例として挙げている。"
  },
  {
    "start": 1074310,
    "end": 1078120,
    "text": "つ目の次元は、例として整数の次元である。"
  },
  {
    "start": 1078650,
    "end": 1083506,
    "text": "ここでxのcを実行すると、配列が得られる。"
  },
  {
    "start": 1083618,
    "end": 1087042,
    "text": "そして、その配列の2つずつを13にインデックスする。"
  },
  {
    "start": 1087186,
    "end": 1098860,
    "text": "そして、この埋め込みが得られ、その位置での整数である1におけるcが、実際にこれと等しいことが確認できる。"
  },
  {
    "start": 1099790,
    "end": 1101210,
    "text": "彼らは平等なんだ。"
  },
  {
    "start": 1101630,
    "end": 1105022,
    "text": "要するに、手短に言えば、パイトーチ・インデックスは素晴らしいということだ。"
  },
  {
    "start": 1105156,
    "end": 1112950,
    "text": "のすべての整数を同時に埋め込むには、単純にxのcを埋め込めばよい。"
  },
  {
    "start": 1113130,
    "end": 1114786,
    "text": "その通りだ。"
  },
  {
    "start": 1114968,
    "end": 1118290,
    "text": "では、このレイヤー、隠れレイヤーを作ってみよう。"
  },
  {
    "start": 1118710,
    "end": 1125838,
    "text": "この重みはランダムに初期化する。"
  },
  {
    "start": 1126014,
    "end": 1130982,
    "text": "さて、このレイヤーへの入力数は2の3倍になるよね？"
  },
  {
    "start": 1131036,
    "end": 1133574,
    "text": "2次元の埋め込みがあり、3次元の埋め込みがあるからだ。"
  },
  {
    "start": 1133692,
    "end": 1140214,
    "text": "入力数は6であり、この層のニューロン数は任意である。"
  },
  {
    "start": 1140332,
    "end": 1147340,
    "text": "100ニューロンを例にして、バイアスもランダムに初期化する。"
  },
  {
    "start": 1148350,
    "end": 1150460,
    "text": "100個あればいい。"
  },
  {
    "start": 1151070,
    "end": 1162362,
    "text": "さて、この場合の問題点は、単純にはできないことだ。通常は、この場合はエンベッディングである入力を受け取り、それにこれらの重みを掛け合わせ、さらにバイアスを加えたい。"
  },
  {
    "start": 1162426,
    "end": 1164238,
    "text": "これが私たちがやりたいことの大まかな流れだ。"
  },
  {
    "start": 1164404,
    "end": 1169742,
    "text": "ここで問題なのは、これらの埋め込みがこの入力テンソルの次元で積み重ねられていることだ。"
  },
  {
    "start": 1169886,
    "end": 1176898,
    "text": "というのも、これは32×3×2の形状で、これに6×100を掛けることはできないからだ。"
  },
  {
    "start": 1177064,
    "end": 1184774,
    "text": "どうにかして、これらの入力を連結して、この線で何かできるようにする必要がある。"
  },
  {
    "start": 1184972,
    "end": 1194138,
    "text": "この32×3×2を32×6に変換して、実際に掛け算ができるようにするにはどうすればいいのだろう。"
  },
  {
    "start": 1194224,
    "end": 1203654,
    "text": "トーチでやりたいことを実現する方法はたくさんあり、その中にはより速く、より良く、より短く、などといったものもあることをお見せしたい。"
  },
  {
    "start": 1203782,
    "end": 1208650,
    "text": "というのも、トーチは非常に大きなライブラリで、たくさんの関数があるからだ。"
  },
  {
    "start": 1208730,
    "end": 1213978,
    "text": "ドキュメントでトーチをクリックすると、このスライダーがとても小さいことがわかるだろう。"
  },
  {
    "start": 1214074,
    "end": 1223860,
    "text": "というのも、テンソルに対して変換、作成、乗算、加算など、あらゆる種類の演算を実行するために呼び出せる関数がたくさんあるからだ。"
  },
  {
    "start": 1224870,
    "end": 1230966,
    "text": "これは可能性の空間のようなものだ。"
  },
  {
    "start": 1231148,
    "end": 1235730,
    "text": "さて、ここでできることのひとつは、concatenateのfをコントロールすることだ。"
  },
  {
    "start": 1235810,
    "end": 1240306,
    "text": "トルク・キャットという関数がある。"
  },
  {
    "start": 1240498,
    "end": 1244410,
    "text": "これは、与えられた次元のテンソル列を連結する。"
  },
  {
    "start": 1245070,
    "end": 1247990,
    "text": "これらのテンソルは同じ形をしていなければならない。"
  },
  {
    "start": 1248070,
    "end": 1255610,
    "text": "を使えば、素朴な方法で、各入力に対してこれらの自由埋め込みを連結することができる。"
  },
  {
    "start": 1255950,
    "end": 1260250,
    "text": "この場合、形状はmである。"
  },
  {
    "start": 1260330,
    "end": 1264850,
    "text": "本当にやりたいことは、この3つの部分を取り出して連結することだ。"
  },
  {
    "start": 1265000,
    "end": 1268286,
    "text": "我々はすべての例をつかみたい。"
  },
  {
    "start": 1268478,
    "end": 1277320,
    "text": "まず0番目のインデックスを取得し、次にこれをすべて取得したい。"
  },
  {
    "start": 1277770,
    "end": 1285560,
    "text": "これは、ここで最初の単語だけの32×2の埋め込みを抜き出す。"
  },
  {
    "start": 1286330,
    "end": 1295770,
    "text": "だから基本的には、この男が欲しい、一番目の次元が欲しい、二番目の次元が欲しい。"
  },
  {
    "start": 1296350,
    "end": 1301642,
    "text": "ということは、これをシーケンスとして扱い、そのシーケンス上でトーチキャットをしたいのだ。"
  },
  {
    "start": 1301706,
    "end": 1302960,
    "text": "これがそのリストだ。"
  },
  {
    "start": 1303490,
    "end": 1310986,
    "text": "トーチキャットはテンソルの列を受け取り、どの次元のテンソルを連結するかを指示しなければならない。"
  },
  {
    "start": 1311178,
    "end": 1317906,
    "text": "この場合、これらはすべて32×2であり、0次元ではなく1次元で連結したい。"
  },
  {
    "start": 1318088,
    "end": 1321300,
    "text": "をパスすることで結果が得られる。"
  },
  {
    "start": 1321910,
    "end": 1325250,
    "text": "この形は32×6で、われわれの希望通りだ。"
  },
  {
    "start": 1325400,
    "end": 1331366,
    "text": "これは、基本的に32を6つずつ連結して32にしたものだ。"
  },
  {
    "start": 1331548,
    "end": 1337174,
    "text": "というのも、このコードは、後でブロック・サイズを変更したい場合、一般化できないからだ。"
  },
  {
    "start": 1337292,
    "end": 1342058,
    "text": "今は3つのインプット、3つの言葉がある。"
  },
  {
    "start": 1342224,
    "end": 1345702,
    "text": "そうなると、直接インデックスを作成しているので、コードを変更しなければならない。"
  },
  {
    "start": 1345846,
    "end": 1353610,
    "text": "というのも、unbindという関数があり、これはテンソルの次元を削除してくれるからだ。"
  },
  {
    "start": 1355230,
    "end": 1361518,
    "text": "はテンソルの次元を取り除き、その次元を除いた、与えられた次元に沿ったすべてのスライスのタプルを返す。"
  },
  {
    "start": 1361684,
    "end": 1363698,
    "text": "これこそ私たちが必要としているものだ。"
  },
  {
    "start": 1363864,
    "end": 1382310,
    "text": "基本的に、トーチ・アンバインドを呼び出すと、mをトーチ・アンバインドし、1次元、インデックス1を渡すと、これとまったく等価なテンソルのリストが得られる。"
  },
  {
    "start": 1382460,
    "end": 1388970,
    "text": "このリストがまさにそうだ。"
  },
  {
    "start": 1389040,
    "end": 1396140,
    "text": "トーチ・キャットを呼び出すことができる。"
  },
  {
    "start": 1396830,
    "end": 1398938,
    "text": "この形状は同じである。"
  },
  {
    "start": 1399104,
    "end": 1404586,
    "text": "ブロックサイズが3だろうが5だろうが10だろうが関係ない。"
  },
  {
    "start": 1404768,
    "end": 1406462,
    "text": "これもひとつの方法だ。"
  },
  {
    "start": 1406516,
    "end": 1411326,
    "text": "この場合、実はもっと効率的で優れた方法があることがわかった。"
  },
  {
    "start": 1411428,
    "end": 1416206,
    "text": "これは、トーチ・テンソルの内部についてヒントを与えてくれる。"
  },
  {
    "start": 1416398,
    "end": 1422462,
    "text": "ここで0から17までの要素からなる配列を作成しよう。"
  },
  {
    "start": 1422606,
    "end": 1425274,
    "text": "この形はちょうど18歳だ。"
  },
  {
    "start": 1425342,
    "end": 1427240,
    "text": "18個の数字からなる1つのベクトルだ。"
  },
  {
    "start": 1427850,
    "end": 1434386,
    "text": "これは、異なる大きさのn次元テンソルとしてすぐに表現できることがわかった。"
  },
  {
    "start": 1434498,
    "end": 1451470,
    "text": "ビューを呼び出すことで、これは実際には18の単一ベクトルではなく、2×9のテンソルである、あるいは9×2のテンソルである、あるいは3×3×2のテンソルである、と言うことができる。"
  },
  {
    "start": 1451810,
    "end": 1457934,
    "text": "ここにある要素の総数が同じになるように掛け算する限り、これでうまくいく。"
  },
  {
    "start": 1458132,
    "end": 1463862,
    "text": "PyTorchでは、このOperation Callingビューは非常に効率的です。"
  },
  {
    "start": 1464026,
    "end": 1469570,
    "text": "その理由は、それぞれのテンソルには基礎となるストレージと呼ばれるものがあるからだ。"
  },
  {
    "start": 1470390,
    "end": 1474718,
    "text": "ストレージは数字だけで、常に1次元のベクトルである。"
  },
  {
    "start": 1474814,
    "end": 1478194,
    "text": "これが、このテンソルがコンピュータのメモリ上でどのように表現されるかである。"
  },
  {
    "start": 1478242,
    "end": 1480470,
    "text": "常に一面的なベクターだ。"
  },
  {
    "start": 1481690,
    "end": 1493366,
    "text": "ビューを呼び出すとき、私たちはテンソルの属性のいくつかを操作していることになる。"
  },
  {
    "start": 1493558,
    "end": 1498054,
    "text": "つまり、ここで起こっているのは、メモリが変更されたり、コピーされたり、移動されたり、作成されたりすることではないのだ。"
  },
  {
    "start": 1498102,
    "end": 1501558,
    "text": "ビューを呼び出すとき、ストレージは同じである。"
  },
  {
    "start": 1501654,
    "end": 1509402,
    "text": "を呼び出すと、このテンソルのビューの内部属性の一部が操作され、変更される。"
  },
  {
    "start": 1509466,
    "end": 1519970,
    "text": "特に、ストレージ、オフセットストライド、シェイプと呼ばれるものがあり、これらを操作することで、この1次元のバイト列を異なるn次元の配列と見なすことができる。"
  },
  {
    "start": 1520550,
    "end": 1530162,
    "text": "EricのPyTorch Internalsというブログポストがあり、Tensorとテンソルのビューがどのように表現されるかについて、この記事のいくつかを紹介している。"
  },
  {
    "start": 1530306,
    "end": 1535270,
    "text": "これは、物理的なメモリーを表現する論理的な構成のようなものだ。"
  },
  {
    "start": 1535690,
    "end": 1539398,
    "text": "というわけで、このブログはかなりいい記事だ。"
  },
  {
    "start": 1539484,
    "end": 1545130,
    "text": "また、トーチ・テンソルの内部と、これがどのように機能するかについてのビデオも作成するかもしれない。"
  },
  {
    "start": 1545200,
    "end": 1547878,
    "text": "ただ、これは非常に効率的なオペレーションであることに留意したい。"
  },
  {
    "start": 1548054,
    "end": 1562000,
    "text": "これを削除してmに戻ると、mの形が32×3×2であることがわかる。"
  },
  {
    "start": 1563010,
    "end": 1573630,
    "text": "32×6の配列に平らになるのは、たまたまこの2つが1列に積み重なるからだ。"
  },
  {
    "start": 1573710,
    "end": 1576660,
    "text": "これが基本的な連結操作だ。"
  },
  {
    "start": 1577110,
    "end": 1581400,
    "text": "これで、以前とまったく同じ結果が得られることが確認できる。"
  },
  {
    "start": 1582090,
    "end": 1589900,
    "text": "この2つのテンソルの要素はすべて同じで、まったく同じ結果が得られることがわかるだろう。"
  },
  {
    "start": 1590750,
    "end": 1604110,
    "text": "要するに、ここに来て、これを32×6と見なせば、掛け算がうまくいき、私たちが求めている隠された状態を得ることができるのだ。"
  },
  {
    "start": 1604260,
    "end": 1615282,
    "text": "もしこれがhであれば、hの形状は32の例のすべての100次元の活性化であり、これは望ましい結果を与える。"
  },
  {
    "start": 1615416,
    "end": 1616882,
    "text": "ここで2つのことをさせてほしい。"
  },
  {
    "start": 1617016,
    "end": 1618850,
    "text": "その1、32を使うのはやめよう。"
  },
  {
    "start": 1618920,
    "end": 1627458,
    "text": "例えば、これらの数字をハードコードしないように、ゼロのドットシェイプのようにすることができる。"
  },
  {
    "start": 1627544,
    "end": 1630150,
    "text": "これは、このMのどのサイズでも使えるだろう。"
  },
  {
    "start": 1630300,
    "end": 1632486,
    "text": "あるいは、マイナスにすることもできる。"
  },
  {
    "start": 1632588,
    "end": 1640266,
    "text": "負の数を入力すると、ピトーチは要素の数が同じでなければならないので、これがどうあるべきかを推論する。"
  },
  {
    "start": 1640368,
    "end": 1645900,
    "text": "PyTorchは、mが異なるサイズである場合、これが32でなければならない、または他の何であるかを導出する。"
  },
  {
    "start": 1646830,
    "end": 1648300,
    "text": "もうひとつはここだ。"
  },
  {
    "start": 1649550,
    "end": 1662282,
    "text": "もうひとつ指摘しておきたいのは、ここで連結を行う場合、連結はまったく新しいテンソルを作成し、まったく新しいストレージを作成することになるので、実際にははるかに効率が悪いということだ。"
  },
  {
    "start": 1662346,
    "end": 1668302,
    "text": "ビューの属性を操作するだけではテンソルを連結する方法がないため、新しいメモリが生成されているのだ。"
  },
  {
    "start": 1668446,
    "end": 1671490,
    "text": "これは非効率的で、あらゆる種類の新しいメモリーを生み出す。"
  },
  {
    "start": 1672470,
    "end": 1673986,
    "text": "削除させてください。"
  },
  {
    "start": 1674008,
    "end": 1676934,
    "text": "今、これは必要ない。"
  },
  {
    "start": 1677132,
    "end": 1683720,
    "text": "ここでhを計算するために、このhの10乗を計算する。"
  },
  {
    "start": 1684410,
    "end": 1686200,
    "text": "おっと、私たちのHを手に入れるためにね。"
  },
  {
    "start": 1686970,
    "end": 1690354,
    "text": "hが10個あるため、これらの数字はマイナス1と1の間の数字になる。"
  },
  {
    "start": 1690492,
    "end": 1693786,
    "text": "の場合、形状は32×100である。"
  },
  {
    "start": 1693968,
    "end": 1700134,
    "text": "これは基本的に、32の例の一つ一つに対する、この隠れ層の活性化である。"
  },
  {
    "start": 1700262,
    "end": 1705134,
    "text": "さて、もう一つ、非常に注意しなければならないことがある。"
  },
  {
    "start": 1705172,
    "end": 1710494,
    "text": "それにここでは特に、放送が自分たちの好きなようにやってくれることを確認したい。"
  },
  {
    "start": 1710692,
    "end": 1715300,
    "text": "この形状は32×100であり、b oneの形状は100である。"
  },
  {
    "start": 1715670,
    "end": 1718946,
    "text": "ここで追加されるのは、この2つである。"
  },
  {
    "start": 1719048,
    "end": 1723300,
    "text": "特に32×100を100に放送している。"
  },
  {
    "start": 1724230,
    "end": 1728934,
    "text": "放送は右側に整列し、ここに偽の寸法を作る。"
  },
  {
    "start": 1729052,
    "end": 1738738,
    "text": "これは100行×1列のベクトルとなり、32行ごとに縦方向にコピーされ、要素ごとに加算される。"
  },
  {
    "start": 1738914,
    "end": 1747734,
    "text": "この場合、この行列のすべての行に同じバイアス・ベクトルが加えられるので、正しいことが起こる。"
  },
  {
    "start": 1747862,
    "end": 1748842,
    "text": "その通りだ。"
  },
  {
    "start": 1748896,
    "end": 1749866,
    "text": "それが私たちの望みだ。"
  },
  {
    "start": 1749968,
    "end": 1754686,
    "text": "自分で自分の足を撃たないようにするために、常に確認することはいい練習になる。"
  },
  {
    "start": 1754788,
    "end": 1757294,
    "text": "最後に、ここに最後のレイヤーを作成しよう。"
  },
  {
    "start": 1757492,
    "end": 1761360,
    "text": "w twoとb twoを作ろう。"
  },
  {
    "start": 1762610,
    "end": 1771586,
    "text": "今の入力は100で、次に来る可能性のある文字が27個なので、ニューロンの出力数は27となる。"
  },
  {
    "start": 1771768,
    "end": 1774260,
    "text": "バイアスも27になる。"
  },
  {
    "start": 1774890,
    "end": 1785720,
    "text": "したがって、ニューラルネットの出力であるロジットは、hにwを2倍したものにbを2倍したものになる。"
  },
  {
    "start": 1787050,
    "end": 1787730,
    "text": "ロジット。"
  },
  {
    "start": 1787810,
    "end": 1792874,
    "text": "形は32×27で、ロジットも良好だ。"
  },
  {
    "start": 1792992,
    "end": 1794970,
    "text": "さて、前のビデオで見たとおりだ。"
  },
  {
    "start": 1795040,
    "end": 1799906,
    "text": "このロジットをまず指数化して、偽のカウントを求める。"
  },
  {
    "start": 1800038,
    "end": 1802350,
    "text": "そして、それらを確率に正規化したい。"
  },
  {
    "start": 1802850,
    "end": 1813700,
    "text": "のプロブレムはカウントが分割され、今度はカウントが最初の次元に沿って合計され、前のビデオとまったく同じように真を保つ。"
  },
  {
    "start": 1814390,
    "end": 1819220,
    "text": "というわけで、現在の確率は32×27。"
  },
  {
    "start": 1819990,
    "end": 1825890,
    "text": "をクリックすると、probの各行の合計が1になっていることがわかる。"
  },
  {
    "start": 1826470,
    "end": 1828274,
    "text": "これは確率を与えてくれる。"
  },
  {
    "start": 1828402,
    "end": 1837506,
    "text": "もちろん、次に来る実際の文字は、データセット作成時に作成したyという配列に由来する。"
  },
  {
    "start": 1837618,
    "end": 1843610,
    "text": "yはこの最後のピースで、これから予測したいシークエンスの次のキャラクターの正体である。"
  },
  {
    "start": 1844750,
    "end": 1850202,
    "text": "前のビデオと同じように、probの行にインデックスを作りたい。"
  },
  {
    "start": 1850336,
    "end": 1856480,
    "text": "それぞれの行から、正しい文字に割り当てられた確率を抜き出したい。"
  },
  {
    "start": 1856930,
    "end": 1865538,
    "text": "つまり、まず32のトーチ・アレンジメントがある。これはゼロから31までの数字に対するイテレータのようなものだ。"
  },
  {
    "start": 1865704,
    "end": 1868834,
    "text": "であれば、次のようにprobにインデックスをつけることができる。"
  },
  {
    "start": 1869032,
    "end": 1873566,
    "text": "行を反復する32のトーチ配列のプロブレム。"
  },
  {
    "start": 1873678,
    "end": 1878520,
    "text": "そして各行で、yで与えられるこの列をつかみたい。"
  },
  {
    "start": 1879290,
    "end": 1887602,
    "text": "これは、このニューラルネットワークが、この重みの設定で、シーケンス内の正しい文字に割り当てた現在の確率を与える。"
  },
  {
    "start": 1887746,
    "end": 1899702,
    "text": "例えば、これは基本的にゼロ2であるが、他の多くの文字、例えば、これはゼロポイント71の確率である。"
  },
  {
    "start": 1899846,
    "end": 1902918,
    "text": "というわけで、ネットワークはこれらのいくつかは極めて可能性が低いと考えている。"
  },
  {
    "start": 1903014,
    "end": 1908122,
    "text": "もちろん、まだニューラルネットワークをトレーニングしていないので、これは改善されるだろう。"
  },
  {
    "start": 1908186,
    "end": 1913610,
    "text": "理想を言えば、ここにある数字はすべて1である。"
  },
  {
    "start": 1913770,
    "end": 1926530,
    "text": "さて、前のビデオと同じように、これらの確率を取り出し、ロック確率を調べ、平均ロック確率とそのマイナスを調べて、負の対数尤度損失を作成したい。"
  },
  {
    "start": 1927430,
    "end": 1936690,
    "text": "この損失は17であり、ネットワークがシーケンス内の正しい文字を予測するために最小化したい損失である。"
  },
  {
    "start": 1936770,
    "end": 1940286,
    "text": "さて、ここですべてを書き直し、もう少し立派なものにした。"
  },
  {
    "start": 1940418,
    "end": 1941834,
    "text": "これがデータセットだ。"
  },
  {
    "start": 1941952,
    "end": 1944170,
    "text": "ここに定義したすべてのパラメーターがある。"
  },
  {
    "start": 1944510,
    "end": 1947478,
    "text": "今は再現性を高めるためにジェネレーターを使っている。"
  },
  {
    "start": 1947654,
    "end": 1956826,
    "text": "すべてのパラメーターを1つのパラメーターのリストにまとめ、例えば、パラメーターを簡単に数えて、現在合計で約3400のパラメーターがあることがわかるようにした。"
  },
  {
    "start": 1957018,
    "end": 1959502,
    "text": "これが私たちが開発したフォワードパスだ。"
  },
  {
    "start": 1959636,
    "end": 1968510,
    "text": "つまり、現在のパラメーターの設定で、このニューラルネットワークがどの程度うまく機能するかを表す損失である。"
  },
  {
    "start": 1968670,
    "end": 1970974,
    "text": "今は、さらに立派なものにしたい。"
  },
  {
    "start": 1971102,
    "end": 1976390,
    "text": "特に、ロジットを取って損失を計算するこの行をご覧ください。"
  },
  {
    "start": 1977370,
    "end": 1979990,
    "text": "ここで車輪の再発明をするわけではない。"
  },
  {
    "start": 1980060,
    "end": 1982370,
    "text": "これは単なる分類だ。"
  },
  {
    "start": 1982530,
    "end": 1990642,
    "text": "そのためPytorchには、より効率的に計算するための機能的なクロスエントロピー関数が用意されている。"
  },
  {
    "start": 1990786,
    "end": 1997930,
    "text": "単純にf dot cross entropyを呼び出すことができ、logitsを渡すことができ、ターゲットyの配列を渡すことができる。"
  },
  {
    "start": 1998080,
    "end": 2000410,
    "text": "これはまったく同じ損失を計算する。"
  },
  {
    "start": 2002190,
    "end": 2009710,
    "text": "だから実際、これをここに置いて、この3本の線を消すだけで、まったく同じ結果が得られる。"
  },
  {
    "start": 2009860,
    "end": 2015726,
    "text": "さて、このように独自の実装をするよりもfドットクロスエントロピーの方がいいという理由は、実はたくさんある。"
  },
  {
    "start": 2015828,
    "end": 2019118,
    "text": "これは教育的な理由でやったことだが、実際に使うことはないだろう。"
  },
  {
    "start": 2019214,
    "end": 2020260,
    "text": "なぜですか？"
  },
  {
    "start": 2020630,
    "end": 2028738,
    "text": "第一に、fクロスエントロピーを使うとき、Pytorchは実際にこれらの中間テンソルをすべて作成するわけではない。"
  },
  {
    "start": 2028834,
    "end": 2032070,
    "text": "このように実行するのはかなり非効率的だ。"
  },
  {
    "start": 2032220,
    "end": 2044010,
    "text": "その代わりに、Pytorchはこれらの演算をすべてクラスタ化し、非常に多くの場合、クラスタ化された数学演算のような式を非常に効率的に評価する融合カーネルを作成する。"
  },
  {
    "start": 2044670,
    "end": 2052590,
    "text": "その2、バックワードパスは、単に融合カーネルだからというだけでなく、解析的、数学的にもはるかに効率的にできる。"
  },
  {
    "start": 2053010,
    "end": 2057226,
    "text": "バックワードパスの方がはるかにシンプルなことが多い。"
  },
  {
    "start": 2057418,
    "end": 2060254,
    "text": "実際にマイクログラッドでこれを見た。"
  },
  {
    "start": 2060292,
    "end": 2067730,
    "text": "私たちがten hを実装したとき、ten hを計算するためのこの操作のフォワードパスは、実際にはかなり複雑な数式だった。"
  },
  {
    "start": 2068230,
    "end": 2083042,
    "text": "これはクラスタ化された数式であるため、バックワードパスを行う際には、xや2回、マイナス1、除算などを個別に後戻りすることはせず、ただ1マイナスtの2乗と言っただけで、よりシンプルな数式となる。"
  },
  {
    "start": 2083186,
    "end": 2090198,
    "text": "このようなことができたのは、計算を再利用することができたからであり、数学的、解析的に微分を導き出すことができたからである。"
  },
  {
    "start": 2090294,
    "end": 2095878,
    "text": "多くの場合、この式は数学的に単純化されるため、実装することははるかに少なくなる。"
  },
  {
    "start": 2096054,
    "end": 2104510,
    "text": "つまり、融合カーネルで実行されるため、より効率的になるだけでなく、式が数学的にはるかに単純な形になるのだ。"
  },
  {
    "start": 2105890,
    "end": 2115046,
    "text": "これは第一に、第二に、ドットクロスエントロピーの方が数値的により良い振る舞いをすることができる。"
  },
  {
    "start": 2115098,
    "end": 2117140,
    "text": "どのように機能するかの例をお見せしよう。"
  },
  {
    "start": 2119270,
    "end": 2127522,
    "text": "仮にマイナス2、3、マイナス30、5の対数があり、その指数を取り、合計が1になるように正規化するとする。"
  },
  {
    "start": 2127656,
    "end": 2133030,
    "text": "ロジットがこの値を取るとき、すべてがうまくいき、良い確率分布が得られる。"
  },
  {
    "start": 2133690,
    "end": 2140250,
    "text": "ここで、これらのロジットのいくつかがより極端な値をとったときに何が起こるかを考えてみよう。"
  },
  {
    "start": 2140750,
    "end": 2145034,
    "text": "これらの数字のいくつかが、例えばマイナス100のように、非常にマイナスになったとしよう。"
  },
  {
    "start": 2145232,
    "end": 2147610,
    "text": "そうすれば、実はすべてがうまくいく。"
  },
  {
    "start": 2147680,
    "end": 2154000,
    "text": "それでも確率はお行儀よく、和が1になり、すべてがうまくいく。"
  },
  {
    "start": 2154450,
    "end": 2164218,
    "text": "というのも、xが機能する方法上、非常に正のロジット、例えばここに正の100があると、実際に問題が発生し始めるからだ。"
  },
  {
    "start": 2164404,
    "end": 2170290,
    "text": "その理由は、これらのカウントにはInFがあるからだ。"
  },
  {
    "start": 2170440,
    "end": 2179670,
    "text": "XPに非常にマイナスの数値を渡すと、非常にマイナスの、申し訳ないがマイナスではなく、非常に小さな、ゼロに近い数値が表示されるだけだ。"
  },
  {
    "start": 2179820,
    "end": 2188034,
    "text": "非常に正の数を渡すと、これらのカウントを表す浮動小数点数の範囲を突然超えてしまう。"
  },
  {
    "start": 2188162,
    "end": 2197180,
    "text": "基本的には、eを100の累乗にしている。"
  },
  {
    "start": 2197870,
    "end": 2202990,
    "text": "したがって、非常に大きなロジットをこの式に通すことはできない。"
  },
  {
    "start": 2203810,
    "end": 2206830,
    "text": "では、この数字を妥当なものにリセットしてみよう。"
  },
  {
    "start": 2207330,
    "end": 2213358,
    "text": "ピトーチがこの問題を解決した方法は、お行儀の良い結果が得られたということだ。"
  },
  {
    "start": 2213524,
    "end": 2220706,
    "text": "この正規化のおかげで、ロジットを任意の定数値でオフセットできることがわかった。"
  },
  {
    "start": 2220808,
    "end": 2224610,
    "text": "ここに1つ加えると、まったく同じ結果になる。"
  },
  {
    "start": 2224760,
    "end": 2231990,
    "text": "あるいは、2つ足しても、3つ引いても、どのオフセットでもまったく同じ確率になる。"
  },
  {
    "start": 2232730,
    "end": 2244458,
    "text": "というのも、負の数は問題ないが、正の数は実際にこのxをオーバーフローする可能性があるため、Pytripが行うのは、ロジットで発生する最大値を内部で計算し、それを差し引くことである。"
  },
  {
    "start": 2244544,
    "end": 2246540,
    "text": "この場合、5を引くことになる。"
  },
  {
    "start": 2246910,
    "end": 2252926,
    "text": "従って、対数の最大の数はゼロになり、他の数はすべて負の数になる。"
  },
  {
    "start": 2253108,
    "end": 2256170,
    "text": "その結果、常にお行儀よくなる。"
  },
  {
    "start": 2256250,
    "end": 2260014,
    "text": "以前は100人いたとしても、良くない。"
  },
  {
    "start": 2260132,
    "end": 2263520,
    "text": "というのも、Pytorchは100を引くからだ。"
  },
  {
    "start": 2264290,
    "end": 2268882,
    "text": "だから、クロスエントロピーをナンバーワンと呼ぶべき理由はたくさんある。"
  },
  {
    "start": 2268936,
    "end": 2270702,
    "text": "フォワードパスはもっと効率的だ。"
  },
  {
    "start": 2270766,
    "end": 2276510,
    "text": "バックワードパスははるかに効率的で、また数値的なお行儀もよくなる。"
  },
  {
    "start": 2276590,
    "end": 2279094,
    "text": "では、このニューラルネットのトレーニングを設定しよう。"
  },
  {
    "start": 2279292,
    "end": 2289606,
    "text": "フォワード・パスがあるから、これは必要ない。ロスはfドットのクロスエントロピーに等しい。"
  },
  {
    "start": 2289788,
    "end": 2291622,
    "text": "それならバックパスが必要だ。"
  },
  {
    "start": 2291756,
    "end": 2294362,
    "text": "まず、グラデーションをゼロにする。"
  },
  {
    "start": 2294496,
    "end": 2304266,
    "text": "これは、PytorchでP gradをゼロに設定し、ロスバックして勾配を入力するのと同じである。"
  },
  {
    "start": 2304458,
    "end": 2306986,
    "text": "グラデーションができたら、パラメータを更新する。"
  },
  {
    "start": 2307098,
    "end": 2312286,
    "text": "PMパラメータについては、すべてのデータを取り込んで、それをナッジしたい。"
  },
  {
    "start": 2312468,
    "end": 2320820,
    "text": "学習率をp倍にし、これを数回繰り返す。"
  },
  {
    "start": 2323910,
    "end": 2326900,
    "text": "ここでも負けを印字しよう。"
  },
  {
    "start": 2328790,
    "end": 2338810,
    "text": "これでは不十分で、PNパラメータも必要で、Pytorchのp dot requires gradがtrueに設定されていることを確認しなければならないため、エラーが発生する。"
  },
  {
    "start": 2339790,
    "end": 2342780,
    "text": "これでうまくいくはずだ。"
  },
  {
    "start": 2343710,
    "end": 2347914,
    "text": "さて、17の損失でスタートし、それを減らしている。"
  },
  {
    "start": 2348112,
    "end": 2349340,
    "text": "もっと長く走ろう。"
  },
  {
    "start": 2350030,
    "end": 2353582,
    "text": "ここでいかに損失が減っているかがわかるだろう。"
  },
  {
    "start": 2353636,
    "end": 2361290,
    "text": "1000回走れば、損失はとてもとても少なくなる。"
  },
  {
    "start": 2361370,
    "end": 2363374,
    "text": "ということは、私たちは非常に良い予測をしているということだ。"
  },
  {
    "start": 2363502,
    "end": 2372142,
    "text": "今、これがとても簡単なのは、32例しかオーバーフィットしていないからだ。"
  },
  {
    "start": 2372286,
    "end": 2375940,
    "text": "最初の5単語は32例しかない。"
  },
  {
    "start": 2376310,
    "end": 2386082,
    "text": "したがって、このニューラルネットをこの232の例だけに適合させるのは非常に簡単である。"
  },
  {
    "start": 2386146,
    "end": 2393370,
    "text": "いわゆるオーバーフィッティングを行い、非常に低い損失と良好な予測を得ているのだ。"
  },
  {
    "start": 2394030,
    "end": 2396854,
    "text": "それは、少ない例に対して多くのパラメーターがあるからだ。"
  },
  {
    "start": 2396902,
    "end": 2399946,
    "text": "これを非常に低くするのは簡単だ。"
  },
  {
    "start": 2400128,
    "end": 2402462,
    "text": "今、正確にゼロを達成することはできない。"
  },
  {
    "start": 2402596,
    "end": 2412670,
    "text": "その理由は、例えば、予測されるロジットを見て、最初の次元に沿って最大値を見ることができるからだ。"
  },
  {
    "start": 2413490,
    "end": 2421794,
    "text": "Pytorchでは、maxは最大数を取る実際の値だけでなく、それらのインデックスも報告します。"
  },
  {
    "start": 2421992,
    "end": 2428518,
    "text": "を見ると、インデックスがラベルに非常に近いことがわかるが、場合によっては異なることもある。"
  },
  {
    "start": 2428684,
    "end": 2434520,
    "text": "例えば、この最初の例では、予測指数は19だが、ラベルは5である。"
  },
  {
    "start": 2434970,
    "end": 2437030,
    "text": "損失をゼロにすることはできない。"
  },
  {
    "start": 2437100,
    "end": 2445610,
    "text": "というのも、ここでは一番最初の、つまり0番目のインデックスがドット・ドット・ドットでeを予測する例だからだ。"
  },
  {
    "start": 2445680,
    "end": 2452926,
    "text": "ドット・ドット・ドットもoを予測することになっているし、ドット・ドット・ドットもIやsを予測することになっている。"
  },
  {
    "start": 2453028,
    "end": 2460186,
    "text": "つまり、基本的にEOAまたはsは、まったく同じ入力に対するトレーニングセットで可能なすべての結果である。"
  },
  {
    "start": 2460298,
    "end": 2472430,
    "text": "完全にオーバーフィットして損失を正確にゼロにすることはできないが、一意な出力に対して一意な入力があるケースでは非常に近いところまで来ている。"
  },
  {
    "start": 2472510,
    "end": 2478420,
    "text": "そのような場合、オーバーフィットと呼ばれるものを行い、基本的にまったく同じ、まったく正しい結果を得る。"
  },
  {
    "start": 2478950,
    "end": 2485270,
    "text": "あとは全データを読み込み、ニューラルネットを最適化するだけだ。"
  },
  {
    "start": 2485420,
    "end": 2488918,
    "text": "では、データセットを作成した場所に戻ってみよう。"
  },
  {
    "start": 2489084,
    "end": 2491610,
    "text": "ここでは最初の5語しか使っていないことがわかる。"
  },
  {
    "start": 2491680,
    "end": 2497340,
    "text": "これを消して、printステートメントも消そう。"
  },
  {
    "start": 2497950,
    "end": 2505006,
    "text": "そのため、全単語の全データセットを処理すると、わずか32例ではなく、228,000例になった。"
  },
  {
    "start": 2505188,
    "end": 2507358,
    "text": "では、下にスクロールしてみよう。"
  },
  {
    "start": 2507524,
    "end": 2509194,
    "text": "今日はもっと大きい。"
  },
  {
    "start": 2509322,
    "end": 2514046,
    "text": "重みを再初期化し、パラメーターの数は同じで、すべて勾配を必要とする。"
  },
  {
    "start": 2514238,
    "end": 2518386,
    "text": "では、このプリント・ロス・アイテムをここにプッシュしよう。"
  },
  {
    "start": 2518568,
    "end": 2521540,
    "text": "これを実行した場合、最適化がどうなるかを見てみよう。"
  },
  {
    "start": 2524310,
    "end": 2529160,
    "text": "では、かなり高い損失からスタートして、最適化するにつれて損失が減ってきている。"
  },
  {
    "start": 2531950,
    "end": 2535798,
    "text": "繰り返しのたびにかなりの時間がかかることに気づくだろう。"
  },
  {
    "start": 2535894,
    "end": 2542378,
    "text": "というのも、22万8,000の例を前進させたり後退させたりするのは、あまりに多くの仕事をしすぎているからだ。"
  },
  {
    "start": 2542554,
    "end": 2549406,
    "text": "実際には、多くのデータに対してフォワードパスやバックワードパス、アップデートを行うのが一般的だ。"
  },
  {
    "start": 2549588,
    "end": 2558766,
    "text": "データセットの一部をランダムに選択してミニバッチとし、そのミニバッチに対して前進、後退、更新のみを行うのだ。"
  },
  {
    "start": 2558878,
    "end": 2561902,
    "text": "そして、そのミニバッチを反復する。"
  },
  {
    "start": 2562046,
    "end": 2565002,
    "text": "Pytorchでは、例えばtorch randintを使うことができる。"
  },
  {
    "start": 2565166,
    "end": 2569240,
    "text": "0から5までの数字を生成し、32個の数字を作ることができる。"
  },
  {
    "start": 2572170,
    "end": 2577634,
    "text": "Pytorchでは、サイズはタプルでなければならないと思います。"
  },
  {
    "start": 2577762,
    "end": 2581834,
    "text": "0から5までの32個のタプルを持つことができる。"
  },
  {
    "start": 2581952,
    "end": 2584860,
    "text": "実際には、xの形がゼロであることが望ましい。"
  },
  {
    "start": 2585470,
    "end": 2590942,
    "text": "これはデータセットにインデックスを付ける整数を作成するもので、32個ある。"
  },
  {
    "start": 2591076,
    "end": 2598990,
    "text": "ミニバッチのサイズが32であれば、ここに来て、まずミニバッチのコンストラクトを行うことができる。"
  },
  {
    "start": 2600210,
    "end": 2614046,
    "text": "この1回の反復で最適化したい整数をixに入れ、xにixでインデックスを付け、それらの行だけを取り出したい。"
  },
  {
    "start": 2614158,
    "end": 2622840,
    "text": "の場合、xは32行しか得られないので、埋め込みは20万×3×2ではなく、再び32×3×2になる。"
  },
  {
    "start": 2623210,
    "end": 2629480,
    "text": "であるならば、このIXはxへのインデックスとしてだけでなく、yへのインデックスとしても使われなければならない。"
  },
  {
    "start": 2630570,
    "end": 2634570,
    "text": "これでミニバッシュになり、もっともっと速くなるはずだ。"
  },
  {
    "start": 2635470,
    "end": 2637180,
    "text": "そう、ほとんど即席なんだ。"
  },
  {
    "start": 2637870,
    "end": 2645230,
    "text": "こうすることで、多くの例をほぼ瞬時に実行し、ロスをずっとずっと早く減らすことができる。"
  },
  {
    "start": 2645570,
    "end": 2652154,
    "text": "今、私たちが扱っているのは多くのバッチだけなので、グラデーションの質は低くなり、方向性の信頼性は低くなる。"
  },
  {
    "start": 2652202,
    "end": 2661630,
    "text": "実際の勾配の方向ではないが、勾配の方向は、わずか32の例で推定する場合でも、十分に役に立つ。"
  },
  {
    "start": 2661790,
    "end": 2670578,
    "text": "だから、正確な勾配を評価してステップ数を減らすよりも、近似勾配を設定してステップ数を増やしたほうがずっといい。"
  },
  {
    "start": 2670674,
    "end": 2674278,
    "text": "だから、実際にはうまく機能しているのだ。"
  },
  {
    "start": 2674444,
    "end": 2676390,
    "text": "では、最適化を続けよう。"
  },
  {
    "start": 2678330,
    "end": 2685020,
    "text": "ここからこの落し物を取り出して、最後にここに置いておこう。"
  },
  {
    "start": 2685870,
    "end": 2688860,
    "text": "2.5前後で推移している。"
  },
  {
    "start": 2690110,
    "end": 2692314,
    "text": "しかし、これはそのミニバッチの損失でしかない。"
  },
  {
    "start": 2692362,
    "end": 2704340,
    "text": "このモデルが今どの程度うまくいっているのかを正確に把握するために、xの全損失とyの全損失を実際に評価してみよう。"
  },
  {
    "start": 2705510,
    "end": 2708946,
    "text": "今はトレーニングセット全体で2.7くらいだ。"
  },
  {
    "start": 2709128,
    "end": 2712020,
    "text": "最適化をしばらく実行してみよう。"
  },
  {
    "start": 2712550,
    "end": 2718920,
    "text": "よし、2.62.57 2.53だ。"
  },
  {
    "start": 2721930,
    "end": 2727880,
    "text": "もちろん、1つの問題は、自分たちがステップを踏むのが遅すぎるのか、それとも速すぎるのかがわからないということだ。"
  },
  {
    "start": 2728650,
    "end": 2729226,
    "text": "これだ。"
  },
  {
    "start": 2729248,
    "end": 2730746,
    "text": ".1 推測だけどね。"
  },
  {
    "start": 2730848,
    "end": 2740330,
    "text": "問題は、この学習速度をどのように決定するのか、そして正しい速度でステップを踏んでいるという確信をどのように得るのか、ということだ。"
  },
  {
    "start": 2740410,
    "end": 2743358,
    "text": "妥当な学習率を決定する一つの方法をお見せしよう。"
  },
  {
    "start": 2743524,
    "end": 2744554,
    "text": "仕組みは以下の通りだ。"
  },
  {
    "start": 2744602,
    "end": 2750918,
    "text": "パラメーターを初期設定に戻してみよう。"
  },
  {
    "start": 2751114,
    "end": 2754500,
    "text": "では、ステップごとにプリントしてみよう。"
  },
  {
    "start": 2754870,
    "end": 2760894,
    "text": "10歩かそこら、あるいは100歩くらいしか歩かないようにしよう。"
  },
  {
    "start": 2761022,
    "end": 2765270,
    "text": "我々は、非常に合理的な検索範囲を見つけたい。"
  },
  {
    "start": 2765340,
    "end": 2772034,
    "text": "例えば、この値が非常に低ければ、損失はほとんど減っていないことになる。"
  },
  {
    "start": 2772082,
    "end": 2775062,
    "text": "基本的に低すぎる。"
  },
  {
    "start": 2775196,
    "end": 2777560,
    "text": "これを試してみよう。"
  },
  {
    "start": 2778730,
    "end": 2781210,
    "text": "それで、損失は減っているが、それほど早くはない。"
  },
  {
    "start": 2781280,
    "end": 2783398,
    "text": "これはかなりいい低域だ。"
  },
  {
    "start": 2783574,
    "end": 2785180,
    "text": "もう一度リセットしてみよう。"
  },
  {
    "start": 2785790,
    "end": 2789478,
    "text": "では、損失が爆発する場所を探してみよう。"
  },
  {
    "start": 2789654,
    "end": 2791420,
    "text": "もしかしたら、マイナス1点かもしれない。"
  },
  {
    "start": 2793090,
    "end": 2797402,
    "text": "損失を最小限に抑えているのはわかったが、不安定なのはわかるだろう。"
  },
  {
    "start": 2797466,
    "end": 2799120,
    "text": "かなり上下する。"
  },
  {
    "start": 2799810,
    "end": 2803154,
    "text": "ネガティブなのは、学習速度が速いということだろう。"
  },
  {
    "start": 2803272,
    "end": 2805140,
    "text": "マイナス10を試してみよう。"
  },
  {
    "start": 2805750,
    "end": 2808078,
    "text": "なるほど、これは最適化ではない。"
  },
  {
    "start": 2808174,
    "end": 2809474,
    "text": "これはあまりうまくいっていない。"
  },
  {
    "start": 2809512,
    "end": 2810978,
    "text": "マイナス10は大きすぎる。"
  },
  {
    "start": 2811064,
    "end": 2818194,
    "text": "マイナス1はすでにちょっと大きかったから、マイナス1はある程度合理的だった。"
  },
  {
    "start": 2818242,
    "end": 2819430,
    "text": "リセットすればね。"
  },
  {
    "start": 2820330,
    "end": 2828050,
    "text": "正しい学習率はマイナス0.1からマイナス1の間だと思う。"
  },
  {
    "start": 2828220,
    "end": 2836586,
    "text": "トーチレンズ・スペースを使って、0と1の間をこのようにすることができます。"
  },
  {
    "start": 2836688,
    "end": 2842154,
    "text": "ステップ数はもうひとつ必要なパラメーターだ。"
  },
  {
    "start": 2842202,
    "end": 2843690,
    "text": "1000歩歩こう。"
  },
  {
    "start": 2843850,
    "end": 2849280,
    "text": "これにより、0.1から1までの1000個の数字が作られる。"
  },
  {
    "start": 2849730,
    "end": 2852842,
    "text": "リニアにステップを踏んでも意味がない。"
  },
  {
    "start": 2852906,
    "end": 2856130,
    "text": "代わりに、学習率指数を作ってみよう。"
  },
  {
    "start": 2856470,
    "end": 2861458,
    "text": "0.1ではなく、マイナス3、ゼロとなる。"
  },
  {
    "start": 2861624,
    "end": 2866870,
    "text": "とすれば、実際に検索したいLRはLREの10乗になる。"
  },
  {
    "start": 2868250,
    "end": 2872610,
    "text": "今やっているのは、これらの学習率の指数の間をリニアにステップしていくことだ。"
  },
  {
    "start": 2872690,
    "end": 2878378,
    "text": "これは0.1であり、10の0乗は1であるから、これは1である。"
  },
  {
    "start": 2878544,
    "end": 2881974,
    "text": "したがって、我々はこの区間で指数関数的な間隔を空けている。"
  },
  {
    "start": 2882102,
    "end": 2887450,
    "text": "これらは、大まかに検索したい学習率の候補である。"
  },
  {
    "start": 2887790,
    "end": 2903940,
    "text": "ここで、1000ステップの最適化を実行するのですが、固定数を使用する代わりに、学習率のインデックスを使用し、IのlrsをIにします。"
  },
  {
    "start": 2905590,
    "end": 2919030,
    "text": "基本的に、これをもう一度リセットして、ランダムから始めて、マイナス0.0、0.1から1の間で、指数関数的に段階的に学習レートを作ってみよう。"
  },
  {
    "start": 2919530,
    "end": 2923126,
    "text": "ここで私たちがやっているのは、1000回の反復練習だ。"
  },
  {
    "start": 2923308,
    "end": 2928182,
    "text": "最初は非常に低い学習率を使うつもりだ。"
  },
  {
    "start": 2928236,
    "end": 2932940,
    "text": "最初は0.1だが、最後には1になっている。"
  },
  {
    "start": 2933390,
    "end": 2936140,
    "text": "そして、その学習率でステップを踏んでいく。"
  },
  {
    "start": 2937070,
    "end": 2949310,
    "text": "今、私たちがしたいことは、私たちが使用した学習率を記録し、その結果生じた損失を調べたいのです。"
  },
  {
    "start": 2949730,
    "end": 2953570,
    "text": "では、スタッツを記録してみよう。"
  },
  {
    "start": 2954070,
    "end": 2961810,
    "text": "LRIはlrを、lossiは損失項目を追加する。"
  },
  {
    "start": 2962390,
    "end": 2968440,
    "text": "よし、もう一度、すべてをリセットしてから走れ。"
  },
  {
    "start": 2970330,
    "end": 2975782,
    "text": "だから、基本的には非常に低い学習率から始めて、マイナス1の学習率まで上げていった。"
  },
  {
    "start": 2975916,
    "end": 2980594,
    "text": "ここでできることは、plt plotで2つをプロットすることだ。"
  },
  {
    "start": 2980652,
    "end": 2985690,
    "text": "X軸に学習率、Y軸に損失をプロットすることができる。"
  },
  {
    "start": 2986110,
    "end": 2993530,
    "text": "最初は学習率が非常に低かった。"
  },
  {
    "start": 2993610,
    "end": 2996400,
    "text": "基本的にほとんど何も起こらなかった。"
  },
  {
    "start": 2997010,
    "end": 2999870,
    "text": "それから、ここのいい場所に着いたんだ。"
  },
  {
    "start": 3000020,
    "end": 3005698,
    "text": "その後、学習率を十分に高めると、基本的に不安定になり始めた。"
  },
  {
    "start": 3005864,
    "end": 3009540,
    "text": "良い学習率はこの辺りであることがわかった。"
  },
  {
    "start": 3010550,
    "end": 3022754,
    "text": "ここではLRIがあるので、実際には学習率ではなく、指数を求めることになるだろう。"
  },
  {
    "start": 3022802,
    "end": 3026486,
    "text": "それはLREで、おそらく私たちが記録したいものだろう。"
  },
  {
    "start": 3026588,
    "end": 3029970,
    "text": "リセットして計算をやり直そう。"
  },
  {
    "start": 3030130,
    "end": 3035594,
    "text": "今、X軸に学習率の指数を置いている。"
  },
  {
    "start": 3035712,
    "end": 3043566,
    "text": "というわけで、学習率の指数を使用するのに適した値は、だいたいこの谷間のようなものであることがわかる。"
  },
  {
    "start": 3043668,
    "end": 3047118,
    "text": "となると、ここでは比較的良い学習率が期待できる。"
  },
  {
    "start": 3047204,
    "end": 3049114,
    "text": "そして、ここで事態は爆発し始めた。"
  },
  {
    "start": 3049242,
    "end": 3054074,
    "text": "学習率の指数であるマイナス1xあたりは、かなり良い設定である。"
  },
  {
    "start": 3054202,
    "end": 3057314,
    "text": "10からマイナス1はゼロ1である。"
  },
  {
    "start": 3057432,
    "end": 3062002,
    "text": "ゼロ1は実際、このあたりではかなりいい学習率だった。"
  },
  {
    "start": 3062136,
    "end": 3067798,
    "text": "これは初期設定にあったものだが、大まかにはこのように決めることになる。"
  },
  {
    "start": 3067964,
    "end": 3080630,
    "text": "だからここで、これらのトラッキングを取り除き、単純にalarをマイナス10に設定すればいい。"
  },
  {
    "start": 3080780,
    "end": 3084186,
    "text": "今、私たちは、この学習率が実際にはかなり良いものだという確信を持っている。"
  },
  {
    "start": 3084368,
    "end": 3095920,
    "text": "反復回数を増やし、最適化をリセットし、この学習率を使ってかなり長い時間実行することができる。"
  },
  {
    "start": 3096450,
    "end": 3097082,
    "text": "おっと。"
  },
  {
    "start": 3097146,
    "end": 3099930,
    "text": "印刷はしたくないんだ。"
  },
  {
    "start": 3100090,
    "end": 3103970,
    "text": "もう一度リセットして1万歩走らせてください。"
  },
  {
    "start": 3108470,
    "end": 3111710,
    "text": "よし、大体2.48になった。"
  },
  {
    "start": 3111790,
    "end": 3113810,
    "text": "もう1万歩走ろう。"
  },
  {
    "start": 3118590,
    "end": 3120026,
    "text": "2.46."
  },
  {
    "start": 3120208,
    "end": 3122522,
    "text": "では、学習率の減衰をひとつやってみよう。"
  },
  {
    "start": 3122586,
    "end": 3132094,
    "text": "これはどういうことかというと、学習レートを10倍下げて、トレーニングの後期段階にある可能性があるので、もう少しゆっくりした方がいいかもしれない、ということだ。"
  },
  {
    "start": 3132222,
    "end": 3138674,
    "text": "ゼロ1でもう1回やってみよう。"
  },
  {
    "start": 3138872,
    "end": 3140270,
    "text": "よし、まだへこんでいるところだ。"
  },
  {
    "start": 3140350,
    "end": 3145606,
    "text": "ちなみに、前回のビデオで達成したビグラムの損失は2.45だった。"
  },
  {
    "start": 3145708,
    "end": 3148360,
    "text": "我々はすでにバイグラム・モデルを超えている。"
  },
  {
    "start": 3149130,
    "end": 3155446,
    "text": "学習率の減衰は、先ほど申し上げたように、実際にプラトーになり始めたという感覚を得ると、人はやりたくなるものです。"
  },
  {
    "start": 3155558,
    "end": 3164714,
    "text": "損失、つまり学習率を減衰させてみよう。"
  },
  {
    "start": 3164752,
    "end": 3171806,
    "text": "しかし、これはあなたが経験していることの大まかな流れだ。"
  },
  {
    "start": 3171908,
    "end": 3175550,
    "text": "まず、私がお見せしたアプローチを使って、適切な学習率を見つけます。"
  },
  {
    "start": 3175700,
    "end": 3178622,
    "text": "そしてその学習率でスタートし、しばらくトレーニングする。"
  },
  {
    "start": 3178756,
    "end": 3188498,
    "text": "そこで学習率を例えば10分の1に減衰させ、さらにいくつかのステップを経て、大まかに言って訓練されたネットワークが出来上がる。"
  },
  {
    "start": 3188664,
    "end": 3199922,
    "text": "私たちは2.3を達成し、この3400のパラメーターを用いて、ここで説明したようなシンプルなニューラルネットを使ったBigram言語モデルを劇的に改善した。"
  },
  {
    "start": 3200066,
    "end": 3202040,
    "text": "さて、気をつけなければならないことがある。"
  },
  {
    "start": 3202410,
    "end": 3210550,
    "text": "ビラム・モデルの2.45よりはるかに低い2.3という低損失を達成しているからだ。"
  },
  {
    "start": 3210710,
    "end": 3212266,
    "text": "今、それは正確には正しくない。"
  },
  {
    "start": 3212368,
    "end": 3219034,
    "text": "それが真実でない理由は、これが実際にはかなり小さなモデルだからだ。"
  },
  {
    "start": 3219152,
    "end": 3223242,
    "text": "これらのモデルは、ニューロンやパラメーターを追加し続ければ、どんどん大きくなる。"
  },
  {
    "start": 3223386,
    "end": 3226442,
    "text": "潜在的に1000ものパラメーターを持っているわけではないことは想像がつくだろう。"
  },
  {
    "start": 3226506,
    "end": 3229690,
    "text": "1万個、1万個、あるいは何百万個ものパラメータを持つことができる。"
  },
  {
    "start": 3229850,
    "end": 3236786,
    "text": "ニューラルネットワークの容量が大きくなるにつれて、トレーニングセットをオーバーフィットさせることができるようになる。"
  },
  {
    "start": 3236968,
    "end": 3244626,
    "text": "つまり、トレーニングセットの損失、つまりトレーニング中のデータの損失は、非常に小さくなる。"
  },
  {
    "start": 3244808,
    "end": 3248866,
    "text": "モデルがやっていることは、トレーニングセットをそのまま記憶しているだけだ。"
  },
  {
    "start": 3248978,
    "end": 3258906,
    "text": "もしそのモデルがとてもうまく機能しているように見えても、そこからサンプリングしようとすると、基本的にトレーニングセットとまったく同じ例しか得られず、新しいデータは得られない。"
  },
  {
    "start": 3259088,
    "end": 3268282,
    "text": "それに加えて、保留された名前などの損失を評価しようとすれば、実際にその損失が非常に大きいことがわかるだろう。"
  },
  {
    "start": 3268416,
    "end": 3270606,
    "text": "だから、基本的には良いモデルではない。"
  },
  {
    "start": 3270788,
    "end": 3275998,
    "text": "この分野での標準は、データセットを3つに分割することである。"
  },
  {
    "start": 3276084,
    "end": 3281890,
    "text": "トレーニング分割、開発分割、検証分割、テスト分割がある。"
  },
  {
    "start": 3282310,
    "end": 3291726,
    "text": "トレーニングスプリット、テストスプリット、申し訳ないが開発スプリット、検証スプリット、テストスプリット。"
  },
  {
    "start": 3291918,
    "end": 3294982,
    "text": "通常、これはデータセットの80%に相当する。"
  },
  {
    "start": 3295036,
    "end": 3297910,
    "text": "これが10％で、これが大体10％かもしれない。"
  },
  {
    "start": 3298250,
    "end": 3300760,
    "text": "この3つの分割されたデータがある。"
  },
  {
    "start": 3301290,
    "end": 3310090,
    "text": "この80％のデータセットのトレーニングは、ここで勾配降下を使ってやっているように、モデルのパラメーターを最適化するために使われる。"
  },
  {
    "start": 3310670,
    "end": 3318842,
    "text": "これらの10％の例、つまり開発または検証のための分割は、モデルのすべてのハイパーパラメータに対する開発に使用されます。"
  },
  {
    "start": 3318976,
    "end": 3324138,
    "text": "ハイパーパラメータとは、例えば隠れ層のサイズや埋め込みサイズなどである。"
  },
  {
    "start": 3324234,
    "end": 3327520,
    "text": "これは僕らにとっては100点か2点だけど、いろいろ試してみることはできる。"
  },
  {
    "start": 3327970,
    "end": 3331822,
    "text": "正則化の強さは、今のところまだ使っていない。"
  },
  {
    "start": 3331956,
    "end": 3342690,
    "text": "ニューラルネットの定義には、さまざまなハイパーパラメーターや設定があり、それらのさまざまなバリエーションを試して、どれがあなたの検証スプリットで最もうまくいくかを見ることができる。"
  },
  {
    "start": 3343110,
    "end": 3353990,
    "text": "これはパラメータを訓練するために使用され、これはハイパーパラメータを訓練するために使用され、テスト分割は基本的に最後にモデルの性能を評価するために使用される。"
  },
  {
    "start": 3354140,
    "end": 3367974,
    "text": "なぜなら、テストでの負けを評価し、そこから何かを学ぶたびに、基本的にテストでの負けもトレーニングするようになるからだ。"
  },
  {
    "start": 3368102,
    "end": 3374238,
    "text": "テストセットでロスをテストすることは、ごくわずかな回数しか許されない。"
  },
  {
    "start": 3374324,
    "end": 3379294,
    "text": "そうでなければ、自分のモデルで実験するのと同様に、オーバーフィットの危険性がある。"
  },
  {
    "start": 3379492,
    "end": 3384242,
    "text": "トレーニングデータもtrain、dev、testに分けよう。"
  },
  {
    "start": 3384376,
    "end": 3389214,
    "text": "それなら、トレーニングはトレーニングで行い、テストでの評価はごく控えめにするつもりだ。"
  },
  {
    "start": 3389342,
    "end": 3390820,
    "text": "さて、それでは始めよう。"
  },
  {
    "start": 3391190,
    "end": 3395970,
    "text": "ここで、すべての単語をxとyのテンソルに置き換えた。"
  },
  {
    "start": 3396130,
    "end": 3403874,
    "text": "その代わり、ここに新しいセルを作って、コードをコピー・ペーストしてみよう。"
  },
  {
    "start": 3404002,
    "end": 3407386,
    "text": "少し時間を節約しようと思う。"
  },
  {
    "start": 3407568,
    "end": 3409578,
    "text": "今、これを関数に変換しているところだ。"
  },
  {
    "start": 3409664,
    "end": 3415820,
    "text": "この関数は、単語のリストを受け取り、それらの単語の配列xとyを作成する。"
  },
  {
    "start": 3416510,
    "end": 3419838,
    "text": "そして、私はここですべての言葉をシャッフルしている。"
  },
  {
    "start": 3419924,
    "end": 3422142,
    "text": "これが入力された言葉である。"
  },
  {
    "start": 3422276,
    "end": 3436082,
    "text": "ランダムにシャッフルし、n1を例文の数、つまり単語の80％、n2を単語の90％とする。"
  },
  {
    "start": 3436216,
    "end": 3443540,
    "text": "基本的に、語数が32,000語であれば、1語は......すみません、実行したほうがよさそうです。"
  },
  {
    "start": 3444230,
    "end": 3448006,
    "text": "N1は25,000、N2は28,000。"
  },
  {
    "start": 3448188,
    "end": 3456426,
    "text": "ここでは、最大n個のインデックスを付けてトレーニングセットxとyを構築するために、ビルドデータセットを呼び出していることがわかる。"
  },
  {
    "start": 3456528,
    "end": 3459434,
    "text": "私たちは25,000語しかトレーニングできない。"
  },
  {
    "start": 3459632,
    "end": 3470346,
    "text": "となると、およそn2からn1を引いた3000の検証例か開発例があることになる。"
  },
  {
    "start": 3470458,
    "end": 3483140,
    "text": "テスト・セットには、基本的に単語数lenからnを2引いた3204の例がある。"
  },
  {
    "start": 3483670,
    "end": 3489010,
    "text": "これで、3つのスプリットすべてのXとYが揃った。"
  },
  {
    "start": 3493370,
    "end": 3496120,
    "text": "そうそう、この関数の中では、サイズも表示しているんだ。"
  },
  {
    "start": 3498890,
    "end": 3500342,
    "text": "ここには言葉がない。"
  },
  {
    "start": 3500396,
    "end": 3503800,
    "text": "これらはすでに、これらの言葉から作られた個々の例である。"
  },
  {
    "start": 3505150,
    "end": 3512380,
    "text": "ここで下にスクロールすると、トレーニング用のデータセットはこのようになっている。"
  },
  {
    "start": 3513150,
    "end": 3526880,
    "text": "そうすると、ネットワークをリセットするとき、トレーニングするときは、x train、x train、y trainを使ってトレーニングすることになる。"
  },
  {
    "start": 3527730,
    "end": 3530100,
    "text": "それだけをトレーニングしているんだ。"
  },
  {
    "start": 3537750,
    "end": 3541330,
    "text": "では、シングルバッチの状況を見てみよう。"
  },
  {
    "start": 3542150,
    "end": 3544790,
    "text": "では、もう少しトレーニングしてみよう。"
  },
  {
    "start": 3547930,
    "end": 3549782,
    "text": "ニューラルネットワークのトレーニングには時間がかかる。"
  },
  {
    "start": 3549836,
    "end": 3556554,
    "text": "通常はインラインで行わず、多くのジョブを立ち上げ、それらが終了するのを待つ。"
  },
  {
    "start": 3556672,
    "end": 3558570,
    "text": "幸運なことに、これは非常に小さなネットワークだ。"
  },
  {
    "start": 3560910,
    "end": 3563178,
    "text": "よし、それで負けはかなりいい。"
  },
  {
    "start": 3563344,
    "end": 3567274,
    "text": "ああ、私たちは誤って低すぎる学習率を使用してしまった。"
  },
  {
    "start": 3567472,
    "end": 3572960,
    "text": "では、実際に戻ってみよう......我々は減衰学習率0.1を使用した。"
  },
  {
    "start": 3575090,
    "end": 3576938,
    "text": "この方が、より速くトレーニングできる。"
  },
  {
    "start": 3577114,
    "end": 3586290,
    "text": "では、ここで評価するとき、X devとY devというdevセットを使って損失を評価しよう。"
  },
  {
    "start": 3588630,
    "end": 3593190,
    "text": "学習率を低下させ、10,000例だけ学習することにしよう。"
  },
  {
    "start": 3595370,
    "end": 3598934,
    "text": "ここで一度、デベロッパーの損失を評価しよう。"
  },
  {
    "start": 3599132,
    "end": 3601414,
    "text": "よし、開発チームでは2.3くらいになりそうだ。"
  },
  {
    "start": 3601532,
    "end": 3605586,
    "text": "そのため、ニューラルネットワークのトレーニング時には、このような逸脱した例を見ることはなかった。"
  },
  {
    "start": 3605618,
    "end": 3607050,
    "text": "彼らには最適化されていない。"
  },
  {
    "start": 3607120,
    "end": 3612214,
    "text": "しかし、これらのデブの損失を評価すると、実際にはかなりの損失となる。"
  },
  {
    "start": 3612342,
    "end": 3618160,
    "text": "そのため、すべてのトレーニングセットでどの程度の損失があるかを見ることもできる。"
  },
  {
    "start": 3620530,
    "end": 3624010,
    "text": "つまり、トレーニングとデベロップの損失はほぼ等しいことがわかる。"
  },
  {
    "start": 3624090,
    "end": 3625630,
    "text": "オーバーフィッティングはしていない。"
  },
  {
    "start": 3626290,
    "end": 3630446,
    "text": "このモデルは、純粋にデータを記憶しているだけでは力不足だ。"
  },
  {
    "start": 3630548,
    "end": 3637838,
    "text": "これまでのところ、学習損失と偏差損失またはテスト損失がほぼ等しいため、いわゆるアンダーフィッティングとなっている。"
  },
  {
    "start": 3637934,
    "end": 3647670,
    "text": "これは一般的に、私たちのネットワークが非常に小さく、非常に小さいということを意味し、私たちはこのニューラルネットのサイズを拡大することで、パフォーマンスの向上を期待している。"
  },
  {
    "start": 3647740,
    "end": 3648710,
    "text": "今すぐそうしよう。"
  },
  {
    "start": 3648780,
    "end": 3652534,
    "text": "ここでニューラルネットのサイズを大きくしてみよう。"
  },
  {
    "start": 3652652,
    "end": 3656814,
    "text": "最も簡単な方法は、この隠れ層に来ることだ。隠れ層には現在100個のニューラルがある。"
  },
  {
    "start": 3656882,
    "end": 3658106,
    "text": "これをアップしよう。"
  },
  {
    "start": 3658128,
    "end": 3663094,
    "text": "ニューラルを300本にして、これも300本のバイアスをかけよう。"
  },
  {
    "start": 3663222,
    "end": 3666830,
    "text": "ここでは、最終レイヤーに300の入力がある。"
  },
  {
    "start": 3667250,
    "end": 3670590,
    "text": "ニューラルネットを初期化しよう。"
  },
  {
    "start": 3670660,
    "end": 3674510,
    "text": "パラメータが3000から10000になった。"
  },
  {
    "start": 3675810,
    "end": 3678046,
    "text": "それなら、これは使わない。"
  },
  {
    "start": 3678228,
    "end": 3684020,
    "text": "そこで僕がやりたいのは、実際に歩数を記録することなんだ。"
  },
  {
    "start": 3687590,
    "end": 3689010,
    "text": "よし、こうしよう。"
  },
  {
    "start": 3689080,
    "end": 3690660,
    "text": "もう一度統計をとってみよう。"
  },
  {
    "start": 3690970,
    "end": 3698482,
    "text": "ここで負けを記録するとき、歩数も記録しておこう。"
  },
  {
    "start": 3698546,
    "end": 3716540,
    "text": "というか、30,000を試してみて、0.1になったので、これを実行してニューラルネットを最適化できるはずだ。"
  },
  {
    "start": 3717250,
    "end": 3724030,
    "text": "ここで、基本的には、損失に対するステップをプロットしたい。"
  },
  {
    "start": 3729010,
    "end": 3736366,
    "text": "これがXとYで、これが損失関数とその最適化方法だ。"
  },
  {
    "start": 3736558,
    "end": 3745380,
    "text": "これは、ミニバッチを最適化しているためで、ミニバッチがノイズを少し発生させている。"
  },
  {
    "start": 3745990,
    "end": 3747558,
    "text": "デフセットの位置は？"
  },
  {
    "start": 3747644,
    "end": 3748950,
    "text": "現在2.5だ。"
  },
  {
    "start": 3749020,
    "end": 3753806,
    "text": "我々はまだこのニューラルネットをうまく最適化できていない。"
  },
  {
    "start": 3753858,
    "end": 3756410,
    "text": "このニューラルネットが収束するには、もっと時間がかかるかもしれない。"
  },
  {
    "start": 3757470,
    "end": 3759420,
    "text": "だからトレーニングを続けよう。"
  },
  {
    "start": 3762750,
    "end": 3764460,
    "text": "ああ、トレーニングを続けよう。"
  },
  {
    "start": 3766510,
    "end": 3772574,
    "text": "可能性として考えられるのは、バッチサイズが小さすぎて、トレーニングにノイズが多すぎるということだ。"
  },
  {
    "start": 3772692,
    "end": 3782130,
    "text": "バッチサイズを大きくすることで、グラデーションがもう少し正しくなり、あまりスラスターせずに、より適切に最適化できるようになるかもしれない。"
  },
  {
    "start": 3787430,
    "end": 3788180,
    "text": "それだ。"
  },
  {
    "start": 3788710,
    "end": 3792022,
    "text": "これで、これらを再初期化したので、意味がなくなる。"
  },
  {
    "start": 3792076,
    "end": 3798066,
    "text": "だから、そう、今は嬉しくないように見えるが、おそらくほんの少し改善されるはずだ。"
  },
  {
    "start": 3798098,
    "end": 3799480,
    "text": "それを伝えるのはとても難しい。"
  },
  {
    "start": 3800330,
    "end": 3801800,
    "text": "もう一度行こう。"
  },
  {
    "start": 3802570,
    "end": 3804220,
    "text": "2.52."
  },
  {
    "start": 3805470,
    "end": 3808380,
    "text": "学習率を2分の1にしてみよう。"
  },
  {
    "start": 3829990,
    "end": 3832066,
    "text": "よし、2.32になった。"
  },
  {
    "start": 3832248,
    "end": 3833620,
    "text": "トレーニングを続けよう。"
  },
  {
    "start": 3845610,
    "end": 3851986,
    "text": "我々は基本的に、以前よりも損失が少なくなることを期待している。"
  },
  {
    "start": 3852098,
    "end": 3855500,
    "text": "モデルのサイズを大きくすることがニューラルネットの助けになるはずだ。"
  },
  {
    "start": 3856030,
    "end": 3857082,
    "text": "2.32."
  },
  {
    "start": 3857136,
    "end": 3859226,
    "text": "なるほど、それはあまりうまくいっていない。"
  },
  {
    "start": 3859408,
    "end": 3859754,
    "text": "今すぐだ。"
  },
  {
    "start": 3859792,
    "end": 3870186,
    "text": "もうひとつ懸念されるのは、10hレイヤーや隠れレイヤーをもっともっと大きくしても、今のネットワークのボトルネックは2次元の埋め込みかもしれないということだ。"
  },
  {
    "start": 3870298,
    "end": 3878026,
    "text": "たった2次元の中にあまりにも多くの文字を詰め込みすぎて、ニューラルネットがそのスペースを有効に使えていないだけかもしれない。"
  },
  {
    "start": 3878138,
    "end": 3881330,
    "text": "それは、ネットワークのパフォーマンスのボトルネックのようなものだ。"
  },
  {
    "start": 3882230,
    "end": 3883650,
    "text": "よし、2.23だ。"
  },
  {
    "start": 3883720,
    "end": 3887266,
    "text": "学習率を下げるだけで、かなりの進歩を遂げることができた。"
  },
  {
    "start": 3887378,
    "end": 3894390,
    "text": "これをもう1回実行し、トレーニングとデブ・ロスを評価してみよう。"
  },
  {
    "start": 3896650,
    "end": 3913020,
    "text": "このボトルネックを解消したいからです。"
  },
  {
    "start": 3913470,
    "end": 3917086,
    "text": "これを2つ以上にすると、可視化できなくなる。"
  },
  {
    "start": 3917268,
    "end": 3918142,
    "text": "ここにある。"
  },
  {
    "start": 3918276,
    "end": 3921358,
    "text": "2.23と2.24だ。"
  },
  {
    "start": 3921524,
    "end": 3924062,
    "text": "これ以上、進歩はない。"
  },
  {
    "start": 3924116,
    "end": 3927620,
    "text": "たぶん、今の一番下の足は文字埋め込みサイズで、これは2つだ。"
  },
  {
    "start": 3928470,
    "end": 3930930,
    "text": "ここに図を作成するコードの束がある。"
  },
  {
    "start": 3931080,
    "end": 3939926,
    "text": "そして、ニューラルネットがこれらの文字に対して学習した埋め込みを可視化する。"
  },
  {
    "start": 3940028,
    "end": 3947030,
    "text": "x座標とy座標を、それぞれの文字の2つの埋め込み位置として、すべての文字を視覚化することができる。"
  },
  {
    "start": 3947690,
    "end": 3952650,
    "text": "そこで、Cの列であるx座標とy座標を示す。"
  },
  {
    "start": 3952800,
    "end": 3957450,
    "text": "そして、それぞれの小さな文字のテキストも掲載する。"
  },
  {
    "start": 3958270,
    "end": 3960620,
    "text": "ここで私たちが目にするのは、実に興味深いものだ。"
  },
  {
    "start": 3962430,
    "end": 3967438,
    "text": "ネットワークは基本的に、キャラクターを分離し、少しクラスタ化することを学んだ。"
  },
  {
    "start": 3967604,
    "end": 3972702,
    "text": "例えば、母音のaeiouがここに集まっているのがわかるだろう。"
  },
  {
    "start": 3972836,
    "end": 3976574,
    "text": "つまり、ニューラルネットはこれらを非常によく似たものとして扱うということだ。"
  },
  {
    "start": 3976612,
    "end": 3982146,
    "text": "ニューラルネットに入力されるとき、これらの文字の埋め込みはすべて非常に似ているからだ。"
  },
  {
    "start": 3982248,
    "end": 3985790,
    "text": "だからニューラルネットは、両者は非常に似ていて、ある意味、互換性があると考える。"
  },
  {
    "start": 3985870,
    "end": 3992806,
    "text": "もしそれが理にかなっているなら、本当に遠い点は例えばQのようなものだ。"
  },
  {
    "start": 3992908,
    "end": 3998566,
    "text": "Qは一種の例外として扱われ、Qはいわば非常に特殊な埋め込みベクトルを持っている。"
  },
  {
    "start": 3998748,
    "end": 4002326,
    "text": "同様に、特別なキャラクターであるドットも、ここではずっと外だ。"
  },
  {
    "start": 4002508,
    "end": 4006138,
    "text": "他の文字の多くはここに集まっている。"
  },
  {
    "start": 4006304,
    "end": 4013654,
    "text": "だから、トレーニングの後にちょっとした構造があるのは興味深いし、決してランダムではない。"
  },
  {
    "start": 4013702,
    "end": 4015726,
    "text": "これらの埋め込みは理にかなっている。"
  },
  {
    "start": 4015908,
    "end": 4023498,
    "text": "エンベッディングのサイズを拡大し、直接視覚化することはできないだろう。"
  },
  {
    "start": 4023674,
    "end": 4028126,
    "text": "我々はこのレイヤーをかなり大きくしたが、ロスを十分に改善することはできなかった。"
  },
  {
    "start": 4028238,
    "end": 4035054,
    "text": "今、より良いパフォーマンスを発揮するための制約は、この埋め込みベクトルではないかと考えている。"
  },
  {
    "start": 4035102,
    "end": 4036142,
    "text": "もっと大きくしよう"
  },
  {
    "start": 4036206,
    "end": 4037734,
    "text": "さて、ここで上にスクロールしてみよう。"
  },
  {
    "start": 4037852,
    "end": 4039682,
    "text": "今、我々は2次元の埋め込みを持っていない。"
  },
  {
    "start": 4039746,
    "end": 4044200,
    "text": "各単語について、たとえば10次元の埋め込みを用意する。"
  },
  {
    "start": 4045130,
    "end": 4049606,
    "text": "すると、この層は10の3乗を受けることになる。"
  },
  {
    "start": 4049708,
    "end": 4055050,
    "text": "30個の入力が隠れ層に入る。"
  },
  {
    "start": 4055710,
    "end": 4057478,
    "text": "隠しレイヤーももう少し小さくしよう。"
  },
  {
    "start": 4057494,
    "end": 4061210,
    "text": "隠れ層のニューロンを300ではなく200にしよう。"
  },
  {
    "start": 4061550,
    "end": 4066160,
    "text": "これでエレメントの総数は11,000と少し増える。"
  },
  {
    "start": 4067090,
    "end": 4073038,
    "text": "というのも、学習率を0.1に設定したからだ。"
  },
  {
    "start": 4073204,
    "end": 4075086,
    "text": "ここでは6つをハードコーディングしている。"
  },
  {
    "start": 4075188,
    "end": 4079314,
    "text": "明らかに、もしあなたがプロダクションで仕事をしているなら、マジックナンバーをハードコーディングしたくはないだろう。"
  },
  {
    "start": 4079432,
    "end": 4081780,
    "text": "6から30になるはずだ。"
  },
  {
    "start": 4084070,
    "end": 4086634,
    "text": "を50,000回繰り返してみよう。"
  },
  {
    "start": 4086782,
    "end": 4095350,
    "text": "このセルを複数回実行したときに、損失が帳消しにならないようにするためだ。"
  },
  {
    "start": 4097450,
    "end": 4104620,
    "text": "ここではそれに加えて、損失項目を記録するのではなく、実際に記録してみよう。"
  },
  {
    "start": 4105470,
    "end": 4107900,
    "text": "ログ10をやろう。"
  },
  {
    "start": 4108510,
    "end": 4114090,
    "text": "その理由は後で説明する。"
  },
  {
    "start": 4114160,
    "end": 4115600,
    "text": "これを最適化しよう。"
  },
  {
    "start": 4117090,
    "end": 4126660,
    "text": "基本的には、ロスの代わりに対数ロスをプロットしたい。ロスを何度もプロットすると、ホッケースティック状になり、対数ロスがそれを押し込んでしまうからだ。"
  },
  {
    "start": 4127030,
    "end": 4128990,
    "text": "なんというか、より素敵に見えるんだ。"
  },
  {
    "start": 4129070,
    "end": 4134020,
    "text": "x軸はステップI、y軸は損失Iとなる。"
  },
  {
    "start": 4140690,
    "end": 4142560,
    "text": "では、これが30だ。"
  },
  {
    "start": 4143090,
    "end": 4145520,
    "text": "理想を言えば、ハードコーディングはしないことだ。"
  },
  {
    "start": 4148770,
    "end": 4150430,
    "text": "では、敗因を見てみよう。"
  },
  {
    "start": 4151490,
    "end": 4155054,
    "text": "ミニバッチのサイズがとても小さいので、これまたとても分厚い。"
  },
  {
    "start": 4155172,
    "end": 4158002,
    "text": "の場合、トレーニングセットでの総損失は2.3である。"
  },
  {
    "start": 4158056,
    "end": 4161730,
    "text": "テスト、あるいはデフセットも2.38である。"
  },
  {
    "start": 4161880,
    "end": 4163374,
    "text": "ここまでは順調だ。"
  },
  {
    "start": 4163512,
    "end": 4171510,
    "text": "学習率を10分の1にして、さらに50,000回反復訓練してみよう。"
  },
  {
    "start": 4175210,
    "end": 4185954,
    "text": "2.32を上回ることを望んでいるが、また、行き当たりばったりでやっているようなものだ。"
  },
  {
    "start": 4186002,
    "end": 4195020,
    "text": "学習率がうまく設定されているかどうか、ランダムに行う学習率の減衰がうまく設定されているかどうか、実は自信がないんだ。"
  },
  {
    "start": 4196190,
    "end": 4198778,
    "text": "正直なところ、ここでの最適化は疑わしい。"
  },
  {
    "start": 4198864,
    "end": 4201030,
    "text": "これは一般的なプロダクションでのやり方ではない。"
  },
  {
    "start": 4201110,
    "end": 4205370,
    "text": "本番では、これらすべての設定からパラメータやハイパーパラメータを作成することになる。"
  },
  {
    "start": 4205450,
    "end": 4209920,
    "text": "そうすれば、たくさんの実験を行い、どれが自分にとってうまくいくかを見極めることができるだろう。"
  },
  {
    "start": 4211330,
    "end": 4216030,
    "text": "よし、これで2.17と2.2が揃った。"
  },
  {
    "start": 4216100,
    "end": 4222990,
    "text": "さて、トレーニングの結果と検証の結果が、少しずつずれてきているのがわかるだろう。"
  },
  {
    "start": 4223150,
    "end": 4233430,
    "text": "ニューラルネットが十分に良くなってきている、あるいはパラメーターの数が十分に大きくなってきていて、徐々にオーバーフィッティングが始まっている、という感覚を得ているのかもしれない。"
  },
  {
    "start": 4234170,
    "end": 4239320,
    "text": "もう1回繰り返して、どうなるか見てみよう。"
  },
  {
    "start": 4241530,
    "end": 4248214,
    "text": "ええ、基本的にはたくさんの実験を行い、どれが最高の守備力を発揮するかをゆっくりと精査していくことになります。"
  },
  {
    "start": 4248342,
    "end": 4256766,
    "text": "そして、開発のパフォーマンスを向上させるハイパーパラメータをすべて見つけたら、そのモデルを使ってテストセットのパフォーマンスを1回だけ評価する。"
  },
  {
    "start": 4256868,
    "end": 4262880,
    "text": "この数字は、新聞やその他の場所で自分のモデルを自慢したいときに報告する数字だ。"
  },
  {
    "start": 4265490,
    "end": 4269460,
    "text": "では、プロットを再実行し、列車とデベロッパーを再実行しよう。"
  },
  {
    "start": 4271110,
    "end": 4277890,
    "text": "今はロスが少なくなっているため、これらの埋め込みサイズが足かせになっている可能性が高い。"
  },
  {
    "start": 4280070,
    "end": 4283640,
    "text": "つまり、2.162.19が大まかな数字だ。"
  },
  {
    "start": 4284410,
    "end": 4286166,
    "text": "からの道はいくらでもある。"
  },
  {
    "start": 4286268,
    "end": 4287670,
    "text": "ここから先はいろいろな道がある。"
  },
  {
    "start": 4287740,
    "end": 4289670,
    "text": "最適化のチューニングを続けることができる。"
  },
  {
    "start": 4290250,
    "end": 4299414,
    "text": "例えば、ニューラルネットの大きさを変えたり、入力となる単語や文字の数を増やしたりすることができる。"
  },
  {
    "start": 4299462,
    "end": 4306326,
    "text": "3文字だけでなく、もっと多くの文字を入力とすることで、さらに損失が改善される可能性がある。"
  },
  {
    "start": 4306518,
    "end": 4308102,
    "text": "それで、コードを少し変えてみた。"
  },
  {
    "start": 4308166,
    "end": 4311402,
    "text": "ここに20万ステップの最適化がある。"
  },
  {
    "start": 4311546,
    "end": 4314542,
    "text": "最初の10万回では、学習率を0.1としている。"
  },
  {
    "start": 4314596,
    "end": 4318334,
    "text": "ということは、次の100,000回では、学習率は0.1ということになる。"
  },
  {
    "start": 4318532,
    "end": 4323758,
    "text": "これは私が達成した損失であり、これらはトレーニングと検証の損失に対するパフォーマンスである。"
  },
  {
    "start": 4323934,
    "end": 4330100,
    "text": "特に、ここ30分ほどで得られた最高の検証損失は2.17だ。"
  },
  {
    "start": 4330470,
    "end": 4336774,
    "text": "この数字を上回るために、あなたにはかなりの数のノブが用意されている。"
  },
  {
    "start": 4336972,
    "end": 4341542,
    "text": "まず、このモデルの隠れ層のニューロン数を変えることができる。"
  },
  {
    "start": 4341676,
    "end": 4345382,
    "text": "埋め込みルックアップテーブルの次元を変更することができます。"
  },
  {
    "start": 4345526,
    "end": 4351660,
    "text": "このモデルへの文脈として、入力として送り込まれる文字数を変更することができる。"
  },
  {
    "start": 4352350,
    "end": 4354934,
    "text": "もちろん、最適化の詳細を変更することもできる。"
  },
  {
    "start": 4355062,
    "end": 4356314,
    "text": "どれくらい走っているのか？"
  },
  {
    "start": 4356432,
    "end": 4357422,
    "text": "学習率は？"
  },
  {
    "start": 4357476,
    "end": 4359134,
    "text": "時間の経過とともにどのように変化するのか？"
  },
  {
    "start": 4359332,
    "end": 4360750,
    "text": "どのように腐敗するのか？"
  },
  {
    "start": 4361170,
    "end": 4374850,
    "text": "バッチサイズを変更することで、モデルをトレーニングするのにかかる時間や、本当に良い損失という結果を得るのにかかる時間という点で、収束速度が大幅に改善される可能性があります。"
  },
  {
    "start": 4375670,
    "end": 4378290,
    "text": "それなら、もちろん、この論文を読んでいただきたい。"
  },
  {
    "start": 4378360,
    "end": 4386390,
    "text": "19ページだが、この時点で、この論文のかなりの部分を読み、理解することができるはずだ。"
  },
  {
    "start": 4386540,
    "end": 4390774,
    "text": "この論文には、あなたが遊べる改良のアイデアもかなりある。"
  },
  {
    "start": 4390972,
    "end": 4395098,
    "text": "この数字を上回ることができるはずだ。"
  },
  {
    "start": 4395184,
    "end": 4397398,
    "text": "それは読者の皆さんへの練習として残しておく。"
  },
  {
    "start": 4397494,
    "end": 4400220,
    "text": "それではまた次回。"
  },
  {
    "start": 4404230,
    "end": 4407480,
    "text": "最後に、このモデルからどのようにサンプリングするかを紹介したい。"
  },
  {
    "start": 4408170,
    "end": 4410898,
    "text": "サンプルを20個作る。"
  },
  {
    "start": 4411074,
    "end": 4415170,
    "text": "最初はすべての点から始める。"
  },
  {
    "start": 4415330,
    "end": 4425580,
    "text": "その後、再びゼロ文字が生成されるまで、埋め込みテーブルcを使って現在のコンテキストを埋め込む。"
  },
  {
    "start": 4426190,
    "end": 4433226,
    "text": "通常、最初の次元はトレーニング・セットのサイズだが、ここでは生成する1つの例だけを扱う。"
  },
  {
    "start": 4433338,
    "end": 4436830,
    "text": "わかりやすくするために、これは1次元だけだ。"
  },
  {
    "start": 4438530,
    "end": 4443402,
    "text": "このエンベッディングをインスタントに投影すると、ロジットが得られる。"
  },
  {
    "start": 4443546,
    "end": 4445402,
    "text": "次に確率を計算する。"
  },
  {
    "start": 4445546,
    "end": 4453554,
    "text": "この場合、f dot soft max of logitsを使えば、基本的にlogitsを指数化して合計を1にすることができる。"
  },
  {
    "start": 4453752,
    "end": 4458130,
    "text": "クロスエントロピーと同様、オーバーフローがないように注意する。"
  },
  {
    "start": 4458630,
    "end": 4463586,
    "text": "確率が得られたら、そこからトルショット多項式を使ってサンプリングし、次のインデックスを得る。"
  },
  {
    "start": 4463698,
    "end": 4467958,
    "text": "次に、コンテキスト・ウィンドウを移動してインデックスを追加し、それを記録する。"
  },
  {
    "start": 4468124,
    "end": 4473242,
    "text": "であれば、すべての整数を文字列にデコードして出力すればいい。"
  },
  {
    "start": 4473376,
    "end": 4475094,
    "text": "これはサンプルです。"
  },
  {
    "start": 4475142,
    "end": 4477706,
    "text": "このモデルの動作が格段に良くなったのがわかるだろう。"
  },
  {
    "start": 4477808,
    "end": 4481258,
    "text": "ここでの言葉は、もっと言葉のような、あるいは名前のようなものだ。"
  },
  {
    "start": 4481344,
    "end": 4487770,
    "text": "ハム、ホセ、リラなどだ。"
  },
  {
    "start": 4488370,
    "end": 4490750,
    "text": "ちょっと名前っぽくなってきたね。"
  },
  {
    "start": 4490820,
    "end": 4495040,
    "text": "我々は間違いなく前進しているが、このモデルはまだかなり改善できる。"
  },
  {
    "start": 4495410,
    "end": 4496094,
    "text": "オーケー、申し訳ない。"
  },
  {
    "start": 4496132,
    "end": 4497282,
    "text": "ボーナスコンテンツもある。"
  },
  {
    "start": 4497416,
    "end": 4505538,
    "text": "このノートブックをもっと身近なものにしたいので、Jupyter notebooksやtorch、その他もろもろをインストールする必要はない、ということをお伝えしたかったのです。"
  },
  {
    "start": 4505624,
    "end": 4519062,
    "text": "Google Collabへのリンクを共有します。Google Collabはブラウザ上でノートブックのように表示され、URLにアクセスするだけで、Google Collabで見たコードをすべて実行することができます。"
  },
  {
    "start": 4519206,
    "end": 4523850,
    "text": "これは私がこの講義でコードを実行しているところですが、少し短くしました。"
  },
  {
    "start": 4524000,
    "end": 4529098,
    "text": "基本的には、まったく同じネットワークを訓練し、プロットしてモデルからサンプリングすることができる。"
  },
  {
    "start": 4529184,
    "end": 4533014,
    "text": "ブラウザ上で数字をいじる準備はすべて整っている。"
  },
  {
    "start": 4533062,
    "end": 4534570,
    "text": "インストールは不要。"
  },
  {
    "start": 4535470,
    "end": 4538630,
    "text": "そのことを指摘しておきたかっただけで、このリンクはビデオの説明文にあります。"
  }
]