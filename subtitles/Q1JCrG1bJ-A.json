[
  {
    "start": 250,
    "end": 558,
    "text": "そうだ。"
  },
  {
    "start": 644,
    "end": 6974,
    "text": "ディープ・ニューラル・ネットワークは信じられないほど強力だが、残念ながら訓練が非常に難しい。"
  },
  {
    "start": 7172,
    "end": 12990,
    "text": "パラメータが増えれば増えるほど、より多くのデータが必要になるからだ。"
  },
  {
    "start": 13140,
    "end": 16158,
    "text": "トレーニングのエポックごとに時間がかかる。"
  },
  {
    "start": 16324,
    "end": 23120,
    "text": "データセットに関係なく、ネットワークの深さがトレーニングの障害になることがある。"
  },
  {
    "start": 23490,
    "end": 32166,
    "text": "浅いニューラルネットワークのトレーニングに成功すると、トレーニングの最初の数エポックで損失が大きく減少する傾向がある。"
  },
  {
    "start": 32348,
    "end": 49526,
    "text": "密な層や畳み込み層からディープ・ニューラル・ネットワークを構築すると、損失グラフはこのようになることが多い。"
  },
  {
    "start": 49718,
    "end": 55022,
    "text": "莫大なデータセットがある場合、トレーニングの各エポックに時間がかかることを覚えておいてほしい。"
  },
  {
    "start": 55156,
    "end": 60586,
    "text": "そのため、ディープネットワークの実用的なトレーニングには大きな障害となる。"
  },
  {
    "start": 60778,
    "end": 71170,
    "text": "ネットワークが深くなるにつれてトレーニングが非常に遅くなる理由を解明するために、非常に深いニューラルネットワークにデータを通すとどうなるか考えてみよう。"
  },
  {
    "start": 71670,
    "end": 79538,
    "text": "ニューラルネットワークを初期化するとき、すべての層の重みはランダムに選ばれる。"
  },
  {
    "start": 79714,
    "end": 89850,
    "text": "つまり、ネットワークの各レイヤーに入力を通すと、活性化にはランダムな重み行列が掛けられる。"
  },
  {
    "start": 90270,
    "end": 103914,
    "text": "これを何度も何度も繰り返すと、データが出力レイヤーに到達する頃には、入力に関連した実際のシグナルはほとんど生成されなくなる。"
  },
  {
    "start": 104042,
    "end": 108510,
    "text": "我々は本質的に、入力をランダムなノイズにスクランブルしている。"
  },
  {
    "start": 108930,
    "end": 117650,
    "text": "そして、更新を実行するときに、その出力の損失を計算し、それをネットワークに伝搬させる。"
  },
  {
    "start": 117990,
    "end": 126370,
    "text": "その入力はほとんどランダムなノイズであるため、ネットワークの後期層では、損失はあまり参考にならないかもしれない。"
  },
  {
    "start": 126530,
    "end": 135538,
    "text": "勾配をネットワークに戻すとき、各レイヤーはその重み行列にデルタを掛け合わせる。"
  },
  {
    "start": 135714,
    "end": 151470,
    "text": "そのため、データとの関連に意味のある入力を持つ初期の層に戻る頃には、ランダムなウェイト行列の乗算によって勾配も乱れてしまっている。"
  },
  {
    "start": 151810,
    "end": 161120,
    "text": "そのため、後のレイヤーに行うアップデートは、入力がスクランブルされていてあまり意味がない。"
  },
  {
    "start": 161570,
    "end": 170274,
    "text": "初期のレイヤーに行った更新は、グラデーションが乱れてあまり意味がない。"
  },
  {
    "start": 170472,
    "end": 180950,
    "text": "そのため、勾配降下法は基本的にランダムに彷徨うため、非常に多くのトレーニング時間に対して、あまり改善が見られない傾向がある。"
  },
  {
    "start": 181450,
    "end": 183800,
    "text": "どうすればいいのだろう？"
  },
  {
    "start": 184250,
    "end": 207470,
    "text": "そしてまた、損失勾配がネットワークの初期層に到達し、その更新がより意味のあるものになるようにしたい。"
  },
  {
    "start": 208130,
    "end": 220100,
    "text": "スキップコネクションのアイデアは、ネットワークのレイヤーをブロックにまとめ、各ブロックごとにデータを通過と迂回の両方をさせるというものだ。"
  },
  {
    "start": 227010,
    "end": 235730,
    "text": "各ブロック内では、レイヤーは普通にデータを転送するが、ブロック間では新しいタイプのコネクションを持つことになる。"
  },
  {
    "start": 237750,
    "end": 252920,
    "text": "この接続は、ブロックへの入力とブロックからの出力を組み合わせることで機能し、データにはブロックを通過する経路と迂回する経路の2種類がある。"
  },
  {
    "start": 263100,
    "end": 271300,
    "text": "そこで、ブロックの出力とブロックへの入力を組み合わせる方法が必要になる。"
  },
  {
    "start": 271460,
    "end": 277560,
    "text": "この関数は、勾配を乱すことなく通過させる単純な関数にしたい。"
  },
  {
    "start": 277720,
    "end": 280024,
    "text": "そのためには、いくつかの選択肢がある。"
  },
  {
    "start": 280152,
    "end": 292080,
    "text": "入力のテンソルと出力のテンソルを要素ごとに加算することもできるし、それらのテンソルを連結することもできる。"
  },
  {
    "start": 299240,
    "end": 312650,
    "text": "その結果、残差ブロックから構成されるネットワークができあがった。このスキップ接続によってトレーニングが加速され、損失グラフがこのようになることを期待している。"
  },
  {
    "start": 314060,
    "end": 319770,
    "text": "残差ブロックから構築されたネットワークは、なぜ訓練しやすいと期待するのが妥当なのだろうか？"
  },
  {
    "start": 320300,
    "end": 323240,
    "text": "まあ、彼らにはいくつかの大きな利点がある。"
  },
  {
    "start": 336980,
    "end": 354948,
    "text": "残差ブロックの第一の大きな利点は、計算中の関数が既存のデータを補強していることである。"
  },
  {
    "start": 355124,
    "end": 372540,
    "text": "このレイヤーの仕事は、もはや、受け渡すべき入力について重要なことをすべて把握することではなく、むしろ、その後の処理を容易にするために、入力の上にどのような情報を加えることができるかを把握することである。"
  },
  {
    "start": 372880,
    "end": 383676,
    "text": "この種の関数は、ブロックが入力にどんな情報が含まれているかを把握するところから始める必要がないため、習得がはるかに簡単な傾向がある。"
  },
  {
    "start": 383868,
    "end": 390496,
    "text": "その代わりに、ブロックはすべての入力を渡すことから始め、ほとんど修正しない。"
  },
  {
    "start": 390598,
    "end": 397540,
    "text": "連結を使う場合は、ブロックが出力するものに加えて、入力もそのまま渡される。"
  },
  {
    "start": 397880,
    "end": 412632,
    "text": "足し算を使うとしても、レイヤーをランダムな重みで初期化すると、その重みはゼロを中心に小さくなる傾向がある。"
  },
  {
    "start": 412766,
    "end": 419340,
    "text": "だから、最初は比較的そのままの情報を伝えることになる。"
  },
  {
    "start": 419680,
    "end": 428990,
    "text": "そのため、各ブロックはより単純なタスクを学習し、各ブロックはより良い情報にアクセスできる。"
  },
  {
    "start": 433380,
    "end": 443000,
    "text": "2つ目の大きな利点は、ネットワークの各レイヤーに到達するために勾配がたどるパスが大幅に短くなったことだ。"
  },
  {
    "start": 443180,
    "end": 454420,
    "text": "各ブロックには、レイヤーを通るパスと、レイヤーを回り込むパスがあるので、グラデーションはその両方のパスに沿って流れることになる。"
  },
  {
    "start": 460020,
    "end": 474992,
    "text": "これは、ネットワーク内のどのレイヤーも、損失勾配が到着して、そのレイヤーが計算しているものを更新するのに役立つ、比較的短い経路を持つことを意味する。"
  },
  {
    "start": 475136,
    "end": 494776,
    "text": "したがって、学習の初期エポックでは、すべてのレイヤーを通過する経路に沿った情報が比較的有益でない場合でも、これらの短い経路から有用な更新を得ることができ、すぐに損失を減少させることができる。"
  },
  {
    "start": 494958,
    "end": 514400,
    "text": "さらに、時間が経つにつれて、ネットワーク内の後のブロックがより多くの有用な機能を計算し始めると、このパスに沿って入ってくる情報はますます有益になり、勾配は減少し続け、浅いネットワークを上回ることができる。"
  },
  {
    "start": 517390,
    "end": 530330,
    "text": "学習すべき関数を単純化し、勾配情報の伝搬を高速化するという最初の2つの利点が、残差ネットワークを構築する主な動機である。"
  },
  {
    "start": 530490,
    "end": 536302,
    "text": "残留ブロックには、モジュール性というさらなる利点があることがわかった。"
  },
  {
    "start": 536446,
    "end": 553910,
    "text": "これらの各ブロックは基本的に同じ構造を持っており、最初のトレーニングでこれらのブロックを短絡的に回避することができるため、ブロックを増やしてより深く強力なネットワークを構築することは比較的容易である。"
  },
  {
    "start": 556570,
    "end": 579440,
    "text": "そのため、論文で残差ネットワークについて読んだり、モデルズーで実装を見たりすると、ネットワークの訓練に必要なリソースと結果として得られるモデルのパワーの間で異なるトレードオフを達成するために、ブロックの数が異なる同じネットワークにいくつかの異なるバリエーションを見つけることがよくある。"
  },
  {
    "start": 579890,
    "end": 592818,
    "text": "このようなことから、残留ネットワーク・アーキテクチャは非常に強力であり、同じアイデアの多くが、これから研究する他のネットワーク・アーキテクチャにも取り入れられている。"
  },
  {
    "start": 592984,
    "end": 599190,
    "text": "残留ネットワークをどのように構築するかを考えるとき、いくつかの懸念事項を念頭に置く必要がある。"
  },
  {
    "start": 613310,
    "end": 629006,
    "text": "残差ブロックの入力と出力を加算や連結で組み合わせたい場合、それらのレイヤーの形状をどのように一致させるかを考える必要がある。"
  },
  {
    "start": 629188,
    "end": 649270,
    "text": "密な層のように活性化ベクトルを持つだけなら、層によってニューロンの数が異なるかもしれないし、ネットワークの各層の形状が入力表現の形状によって決定されることを望まない限り、多少のミスマッチは避けられない。"
  },
  {
    "start": 649610,
    "end": 668010,
    "text": "もし最初のブロックの出力に入力を追加するのであれば、入力を再形成する操作が必要であるか、そのブロックの出力の一部にのみ入力を追加する必要がある。"
  },
  {
    "start": 668450,
    "end": 683086,
    "text": "ブロックに畳み込み層が含まれている場合、これはさらに複雑になる。入力のカラー・チャンネルに基づいて、後のすべての層の深度を制約することは絶対に避けたい。"
  },
  {
    "start": 683278,
    "end": 694466,
    "text": "また、畳み込み層の高さと幅が入力の高さと幅と組み合わせられるようにする必要がある。"
  },
  {
    "start": 694658,
    "end": 710090,
    "text": "そのため、残差ブロックを使用する場合は、通常、入力の形状を最初のブロックの形状と一致させるための特別なケースを用意したい。"
  },
  {
    "start": 710750,
    "end": 720190,
    "text": "となると、この種の問題を最小限に抑えるために、後続のブロックが共通の形状を持つようにしたい。"
  },
  {
    "start": 721250,
    "end": 735890,
    "text": "もうひとつの懸念は、連結を繰り返し使って多くのブロックの出力を組み合わせると、非常に大きな活性化テンソルになってしまい、パラメーターの数が爆発的に増えてしまうことだ。"
  },
  {
    "start": 743690,
    "end": 754440,
    "text": "連結できるほど形が合っていても、連結の回数が増えれば増えるほど、この活性化テンソルは大きくなる。"
  },
  {
    "start": 754830,
    "end": 771210,
    "text": "というのも、後続のレイヤーはそれぞれの活性化から得られる重みを持つので、連結を使いすぎるとパラメーターの数が爆発的に増えてしまうからだ。"
  },
  {
    "start": 771370,
    "end": 778010,
    "text": "したがって、残差ブロックをたくさん使うのであれば、連結よりも加算を選ぶべきである。"
  },
  {
    "start": 778170,
    "end": 785410,
    "text": "そのため、1つのネットワークで連結を使用するスキップ接続が1つか2つ以上になることはほとんどない。"
  },
  {
    "start": 793590,
    "end": 809930,
    "text": "大規模な残余ネットワークを構築する場合、一般的には、ほとんどのスキップ接続に加算を使用し、入力と出力を一致させることができるように、ほとんどのブロックに一貫した形状を使用するようにします。"
  },
  {
    "start": 810270,
    "end": 813194,
    "text": "少し時間をかけて考える価値がある。"
  },
  {
    "start": 813312,
    "end": 818410,
    "text": "畳み込みレイヤーを使う場合、どのようにすればこの目標を達成できるのだろうか？"
  },
  {
    "start": 841350,
    "end": 854358,
    "text": "畳み込みブロックは、1つ1つのストライドと適切なパディングを使用し、画像の高さと幅が保たれるようにする。"
  },
  {
    "start": 854534,
    "end": 863120,
    "text": "そうすれば、あるブロックの入力テンソルと出力テンソルを足したときに、少なくとも高さと幅の寸法が一致することがわかる。"
  },
  {
    "start": 874860,
    "end": 883668,
    "text": "チャンネルの次元も一致させたい場合は、1つずつの畳み込みを使って実現できる。"
  },
  {
    "start": 883844,
    "end": 892008,
    "text": "1つ1つの畳み込みは、画像の1ピクセルからしか入力を得ないので、ちょっと馬鹿げているように見えるかもしれない。"
  },
  {
    "start": 892184,
    "end": 897820,
    "text": "このようなニューロンがどのような計算をするのか、イラストを描いてみよう。"
  },
  {
    "start": 908940,
    "end": 918380,
    "text": "もし1対1のカーネルで畳み込みを行えば、入力ブロックと同じ縦横の出力ブロックが得られる。"
  },
  {
    "start": 918800,
    "end": 929040,
    "text": "出力の各ニューロンは、入力のたった1ピクセルから入力を得る。"
  },
  {
    "start": 929380,
    "end": 948560,
    "text": "それが入力画像の3つのカラー・チャンネルであろうと、前のレイヤーの同じウィンドウで単純な関数を計算した複数の異なるニューロンであろうと、そのピクセルのチャンネルの深さ全体から入力を得る。"
  },
  {
    "start": 948720,
    "end": 967272,
    "text": "1対1の畳み込みのフィルタやチャネルの数を設定することで、出力層の深さを指定することができ、これらの各ニューロンは、前の層の深さを越えてすべてのニューロンから入力を得る。"
  },
  {
    "start": 967416,
    "end": 971848,
    "text": "だから、これを使えば、深さを増やすことも減らすこともできる。"
  },
  {
    "start": 972024,
    "end": 987010,
    "text": "したがって、密な層になぞらえて考えるなら、この1つ1つの畳み込みステップを加えることは、入力と出力の形状を一致させるために、このパスに沿って1つの密な層を挿入するようなものだ。"
  },
  {
    "start": 987700,
    "end": 1002900,
    "text": "残差ネットワークや残差ブロックの作り方には、他にもたくさんのバリエーションがあり、その多くは、正規化を行ったり、ブロック内で複数のパスを介してデータ・データを渡すような、他のタイプのレイヤーを含んでいる。"
  },
  {
    "start": 1003060,
    "end": 1018340,
    "text": "そのため、さまざまなタイプの残差アーキテクチャについて読むことを勧めるが、それらはすべて、ディープ・ニューラル・ネットワークを訓練する能力という点で、はるかに優れたパフォーマンスを達成するという共通のテーマに沿っている。"
  }
]